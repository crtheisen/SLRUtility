TY  - CONF
JO  - Globecom Workshops (GC Wkshps), 2013 IEEE
TI  - Massive distributed and parallel log analysis for organizational security
T2  - Globecom Workshops (GC Wkshps), 2013 IEEE
IS  - 
SN  - 
VO  - 
SP  - 194
EP  - 199
AU  - Xiaokui Shu
AU  - Smiy, J.
AU  - Danfeng Yao
AU  - Heshan Lin
Y1  - 9-13 Dec. 2013
PY  - 2013
KW  - cloud computing
KW  - distributed processing
KW  - security of data
KW  - system monitoring
KW  - Amazon cloud environments
KW  - EC2
KW  - MapReduce
KW  - S3
KW  - cloud-based distributed framework
KW  - dynamic task scheduling
KW  - log data demands
KW  - massive distributed frameworks
KW  - organizational security
KW  - parallel security log analysis framework
KW  - streaming logs
KW  - transaction logs
KW  - Cloud computing
KW  - Conferences
KW  - Data privacy
KW  - Organizations
KW  - Security
VL  - 
JA  - Globecom Workshops (GC Wkshps), 2013 IEEE
DO  - 10.1109/GLOCOMW.2013.6824985
AB  - Security log analysis is extremely useful for uncovering intrusions and anomalies. However, the sheer volume of log data demands new frameworks and techniques of computing and security. We present a lightweight distributed and parallel security log analysis framework that allows organizations to analyze a massive number of system, network, and transaction logs efficiently and scalably. Different from the general distributed frameworks, e.g., MapReduce, our framework is specifically designed for security log analysis. It features a minimum set of necessary properties, such as dynamic task scheduling for streaming logs. For prototyping, we implement our framework in Amazon cloud environments (EC2 and S3) with a basic analysis application. Our evaluation demonstrates the effectiveness of our design and shows the potential of our cloud-based distributed framework in large-scale log analysis scenarios.
ER  - 

TY  - CONF
JO  - Software Engineering (ICSE), 2013 35th International Conference on
TI  - Measuring the forensic-ability of audit logs for nonrepudiation
T2  - Software Engineering (ICSE), 2013 35th International Conference on
IS  - 
SN  - 
VO  - 
SP  - 1419
EP  - 1422
AU  - King, J.
Y1  - 18-26 May 2013
PY  - 2013
KW  - auditing
KW  - digital forensics
KW  - educational computing
KW  - financial data processing
KW  - fraud
KW  - health care
KW  - software metrics
KW  - system monitoring
KW  - activity logging mechanism
KW  - application performance monitoring
KW  - audit log
KW  - compliance checking
KW  - data field
KW  - debugging
KW  - education
KW  - finance
KW  - forensic ability measurement
KW  - forensic analysis
KW  - fraud detection
KW  - grounded theory method
KW  - healthcare
KW  - log file attribute
KW  - software log files
KW  - software security events
KW  - software security metrics
KW  - software system
KW  - system resources
KW  - unique user identifier
KW  - user access tracking
KW  - user behavior profile extraction
KW  - user nonrepudiation
KW  - user privilege revocation
KW  - Forensics
KW  - Measurement
KW  - Medical services
KW  - Security
KW  - Software systems
KW  - Standards
KW  - forensics
KW  - grounded theory
KW  - logging
KW  - metric
KW  - nonrepudiation
KW  - security
KW  - software logs
VL  - 
JA  - Software Engineering (ICSE), 2013 35th International Conference on
DO  - 10.1109/ICSE.2013.6606732
AB  - Forensic analysis of software log files is used to extract user behavior profiles, detect fraud, and check compliance with policies and regulations. Software systems maintain several types of log files for different purposes. For example, a system may maintain logs for debugging, monitoring application performance, and/or tracking user access to system resources. The objective of my research is to develop and validate a minimum set of log file attributes and software security metrics for user nonrepudiation by measuring the degree to which a given audit log file captures the data necessary to allow for meaningful forensic analysis of user behavior within the software system. For a log to enable user nonrepudiation, the log file must record certain data fields, such as a unique user identifier. The log must also record relevant user activity, such as creating, viewing, updating, and deleting system resources, as well as software security events, such as the addition or revocation of user privileges. Using a grounded theory method, I propose a methodology for observing the current state of activity logging mechanisms in healthcare, education, and finance, then I quantify differences between activity logs and logs not specifically intended to capture user activity. I will then propose software security metrics for quantifying the forensic-ability of log files. I will evaluate my work with empirical analysis by comparing the performance of my metrics on several types of log files, including both activity logs and logs not directly intended to record user activity. My research will help software developers strengthen user activity logs for facilitating forensic analysis for user nonrepudiation.
ER  - 

TY  - CONF
JO  - Military Communications Conference (MILCOM), 2014 IEEE
TI  - Using Security Logs for Collecting and Reporting Technical Security Metrics
T2  - Military Communications Conference (MILCOM), 2014 IEEE
IS  - 
SN  - 
VO  - 
SP  - 294
EP  - 299
AU  - Vaarandi, R.
AU  - Pihelgas, M.
Y1  - 6-8 Oct. 2014
PY  - 2014
KW  - Big Data
KW  - computer network security
KW  - big data
KW  - log analysis methods
KW  - log analysis techniques
KW  - open source technology
KW  - security logs
KW  - technical security metric collection
KW  - technical security metric reporting
KW  - Correlation
KW  - Internet
KW  - Measurement
KW  - Monitoring
KW  - Peer-to-peer computing
KW  - Security
KW  - Workstations
KW  - security log analysis
KW  - security metrics
VL  - 
JA  - Military Communications Conference (MILCOM), 2014 IEEE
DO  - 10.1109/MILCOM.2014.53
AB  - During recent years, establishing proper metrics for measuring system security has received increasing attention. Security logs contain vast amounts of information which are essential for creating many security metrics. Unfortunately, security logs are known to be very large, making their analysis a difficult task. Furthermore, recent security metrics research has focused on generic concepts, and the issue of collecting security metrics with log analysis methods has not been well studied. In this paper, we will first focus on using log analysis techniques for collecting technical security metrics from security logs of common types (e.g., Network IDS alarm logs, workstation logs, and Net flow data sets). We will also describe a production framework for collecting and reporting technical security metrics which is based on novel open-source technologies for big data.
ER  - 

TY  - CONF
JO  - Systematic Approaches to Digital Forensic Engineering (SADFE), 2010 Fifth IEEE International Workshop on
TI  - Explorative Visualization of Log Data to Support Forensic Analysis and Signature Development
T2  - Systematic Approaches to Digital Forensic Engineering (SADFE), 2010 Fifth IEEE International Workshop on
IS  - 
SN  - 
VO  - 
SP  - 109
EP  - 118
AU  - Schmerl, S.
AU  - Vogel, M.
AU  - Rietz, R.
AU  - Ko&#x0308;nig, H.
Y1  - 20-20 May 2010
PY  - 2010
KW  - computer forensics
KW  - data visualisation
KW  - digital signatures
KW  - computers security threats
KW  - explorative log data visualization
KW  - forensic analysis
KW  - intrusion detection systems
KW  - signature development
KW  - Character generation
KW  - Communication networks
KW  - Communication system security
KW  - Data security
KW  - Data visualization
KW  - Digital forensics
KW  - Event detection
KW  - Information analysis
KW  - Information security
KW  - Proposals
KW  - Attack Signatures
KW  - Audit Data Analysis
KW  - Computer Forensic
KW  - Computer Security
KW  - Data Visualization
KW  - Intrusion Detection
KW  - Misuse Detection
VL  - 
JA  - Systematic Approaches to Digital Forensic Engineering (SADFE), 2010 Fifth IEEE International Workshop on
DO  - 10.1109/SADFE.2010.10
AB  - Today's growing number of security threats to computers and networks also increase the importance of log inspections to support the detection of possible breaches. The investigation and assessment of security incidents becomes more and more a daily business. Further, the manual log analysis is essentially in the context of developing signatures for intrusion detection systems (IDS), which allow for an automated defense against security attacks or incidents. But the analysis of log data in the context of fo-rensic investigations and IDS signature development is a tedious and time-consuming task, due to the large amount of textual data. Moreover, this task requires a skilled knowledge to differentiate between the important and the non-relevant information. In this paper, we propose an approach for log resp. audit data representation, which aims at simplifying the analysis process for the security officer. For this purpose audit data and existing relations between audit events are represented graphically in a three-dimensional space. We describe a general approach for analyzing and exploring audit or log data in the context of this presentation paradigm. Further, we introduce our tool, which implements this approach and demonstrate the strengths and benefits of this presentation and exploration form.
ER  - 

TY  - CONF
JO  - Cloud Computing (CLOUD), 2014 IEEE 7th International Conference on
TI  - Progger: An Efficient, Tamper-Evident Kernel-Space Logger for Cloud Data Provenance Tracking
T2  - Cloud Computing (CLOUD), 2014 IEEE 7th International Conference on
IS  - 
SN  - 
VO  - 
SP  - 881
EP  - 889
AU  - Ko, R.K.L.
AU  - Will, M.A.
Y1  - June 27 2014-July 2 2014
PY  - 2014
KW  - cloud computing
KW  - security of data
KW  - system monitoring
KW  - Progger
KW  - cloud computing systems
KW  - cloud data provenance tracking
KW  - data accountability
KW  - data activity audit
KW  - data activity tracking
KW  - data security component
KW  - kernel-space logger
KW  - Cloud computing
KW  - Data security
KW  - Kernel
KW  - Sockets
KW  - Synchronization
KW  - Virtual machining
KW  - Accountability
KW  - Cloud Computing
KW  - Data Provenance
KW  - Data Security
KW  - Tamper-evident logging
KW  - Time Synchronisation
VL  - 
JA  - Cloud Computing (CLOUD), 2014 IEEE 7th International Conference on
DO  - 10.1109/CLOUD.2014.121
AB  - Cloud data provenance, or "what has happened to my data in the cloud", is a critical data security component which addresses pressing data accountability and data governance issues in cloud computing systems. In this paper, we present Progger (Provenance Logger), a kernel-space logger which potentially empowers all cloud stakeholders to trace their data. Logging from the kernel space empowers security analysts to collect provenance from the lowest possible atomic data actions, and enables several higher-level tools to be built for effective end-to-end tracking of data provenance. Within the last few years, there has been an increasing number of proposed kernel space provenance tools but they faced several critical data security and integrity problems. Some of these prior tools' limitations include (1) the inability to provide log tamper-evidence and prevention of fake/manual entries, (2) accurate and granular timestamp synchronisation across several machines, (3) log space requirements and growth, and (4) the efficient logging of root usage of the system. Progger has resolved all these critical issues, and as such, provides high assurance of data security and data activity audit. With this in mind, the paper will discuss these elements of high-assurance cloud data provenance, describe the design of Progger and its efficiency, and present compelling results which paves the way for Progger being a foundation tool used for data activity tracking across all cloud systems.
ER  - 

TY  - CONF
JO  - MILITARY COMMUNICATIONS CONFERENCE, 2010 - MILCOM 2010
TI  - Implementation and evaluation of accountability using flow-net in wireless networks
T2  - MILITARY COMMUNICATIONS CONFERENCE, 2010 - MILCOM 2010
IS  - 
SN  - 2155-7578
VO  - 
SP  - 7
EP  - 12
AU  - Yang Xiao
AU  - Ke Meng
AU  - Takahashi, D.
Y1  - Oct. 31 2010-Nov. 3 2010
PY  - 2010
KW  - authorisation
KW  - performance evaluation
KW  - radio networks
KW  - telecommunication security
KW  - accountability
KW  - audit log files
KW  - flow-net methodology
KW  - logging mechanism
KW  - logging system
KW  - performance evaluation
KW  - wireless networks
KW  - Complexity theory
KW  - Computers
KW  - IEEE 802.11 Standards
KW  - Security
KW  - Wireless LAN
KW  - Wireless networks
KW  - Accountability
KW  - Logging
KW  - Media Access Control (MAC)
KW  - Trace
KW  - routing
KW  - wireless networks
VL  - 
JA  - MILITARY COMMUNICATIONS CONFERENCE, 2010 - MILCOM 2010
DO  - 10.1109/MILCOM.2010.5680278
AB  - In order to provide accountability, a better logging system is needed so that not only the activities but also their relationships are captured. To this end, our previous work proposed a novel logging mechanism, flow-net methodology, for accountability. In this paper, we extend the flow-net methodology and present its design and implementation in wireless networks. We also evaluate the performance of flow-net and compare it to that of audit log files.
ER  - 

TY  - CONF
JO  - Network and Parallel Computing Workshops, 2007. NPC Workshops. IFIP International Conference on
TI  - A Log Analysis Audit Model Based on Optimized Clustering Algorithm
T2  - Network and Parallel Computing Workshops, 2007. NPC Workshops. IFIP International Conference on
IS  - 
SN  - 
VO  - 
SP  - 841
EP  - 848
AU  - Hui Yu
AU  - Xingjian Shi
Y1  - 18-21 Sept. 2007
PY  - 2007
KW  - auditing
KW  - pattern clustering
KW  - security of data
KW  - cluster number
KW  - network attack type
KW  - optimized clustering algorithm
KW  - security log analysis audit model
KW  - unknown intrusion detection
KW  - Algorithm design and analysis
KW  - Application software
KW  - Automatic control
KW  - Clustering algorithms
KW  - Computer science
KW  - Computer security
KW  - Data mining
KW  - Intrusion detection
KW  - Parallel processing
KW  - Protection
VL  - 
JA  - Network and Parallel Computing Workshops, 2007. NPC Workshops. IFIP International Conference on
DO  - 10.1109/NPC.2007.116
AB  - In view of the problem how to detect the network unknown attacks, a security log analysis audit model based on optimized clustering algorithm is proposed in this paper. Since the main question which influence the clustering algorithm application in the log analysis is uneasy to determine the network attack type and the cluster number, so we bring forward an optimized cluster algorithm to solve this problem. By means of simulated experiments, this algorithm is proved feasible, efficient and extensible for unknown intrusion detection.
ER  - 

TY  - CONF
JO  - Cloud Computing and Intelligence Systems (CCIS), 2014 IEEE 3rd International Conference on
TI  - A model for website anomaly detection based on log analysis
T2  - Cloud Computing and Intelligence Systems (CCIS), 2014 IEEE 3rd International Conference on
IS  - 
SN  - 
VO  - 
SP  - 604
EP  - 608
AU  - Xu Han
AU  - Tao Lv
AU  - Lin Wei
AU  - Yanyan Wu
AU  - Jianyi Liu
AU  - Cong Wang
Y1  - 27-29 Nov. 2014
PY  - 2014
KW  - Algorithm design and analysis
KW  - Analytical models
KW  - Classification algorithms
KW  - Data models
KW  - Databases
KW  - Feature extraction
KW  - Security
KW  - Anomaly detection
KW  - C4.5 algorithm
KW  - Feature sets
KW  - Log analysis
VL  - 
JA  - Cloud Computing and Intelligence Systems (CCIS), 2014 IEEE 3rd International Conference on
DO  - 10.1109/CCIS.2014.7175806
AB  - To found security events from web logs has become an important aspect of network security. This paper proposes a website anomaly detection model based on security-log-analysis. After creating a anomaly feature sets of the model, C4.5 algorithm was used to improve feature sets, making the abnormal records in feature sets store hierarchically. Compared logs in website with the treated feature stes, the model ultimately achieves the purpose of checking website's security event fast and accurately.
ER  - 

TY  - CONF
JO  - Trust, Security and Privacy in Computing and Communications (TrustCom), 2013 12th IEEE International Conference on
TI  - S2Logger: End-to-End Data Tracking Mechanism for Cloud Data Provenance
T2  - Trust, Security and Privacy in Computing and Communications (TrustCom), 2013 12th IEEE International Conference on
IS  - 
SN  - 
VO  - 
SP  - 594
EP  - 602
AU  - Chun Hui Suen
AU  - Ko, R.K.L.
AU  - Yu Shyang Tan
AU  - Jagadpramana, P.
AU  - Bu Sung Lee
Y1  - 16-18 July 2013
PY  - 2013
KW  - cloud computing
KW  - data analysis
KW  - data loggers
KW  - data protection
KW  - security of data
KW  - system monitoring
KW  - S2Logger
KW  - atomic data events
KW  - cloud computing environments
KW  - cloud data provenance
KW  - cloud data provenance records
KW  - cloud servers
KW  - cloud stakeholders
KW  - critical data-related cloud security problems
KW  - data activities
KW  - data event analysis
KW  - data event capturing
KW  - data event logging mechanism
KW  - data event visualization
KW  - data leakages
KW  - data lifecycle
KW  - data movement accountability
KW  - data movement transparency
KW  - data policy violations
KW  - data tracking tools
KW  - data-centric logging techniques
KW  - end-to-end data tracking mechanism
KW  - file creation
KW  - file deletions
KW  - file duplication
KW  - file edition
KW  - file transfers
KW  - logging mechanisms
KW  - malicious actions
KW  - system-centric security tools
KW  - Cloud computing
KW  - Distributed databases
KW  - Kernel
KW  - Linux
KW  - Monitoring
KW  - Security
KW  - Virtual machine monitors
KW  - Cloud Computing
KW  - Cloud data provenance
KW  - S2Logger
KW  - accountability in cloud computing
KW  - cloud computing security
KW  - cloud computing transparency
KW  - data tracing
KW  - data tracking
KW  - file tracking
VL  - 
JA  - Trust, Security and Privacy in Computing and Communications (TrustCom), 2013 12th IEEE International Conference on
DO  - 10.1109/TrustCom.2013.73
AB  - The inability to effectively track data in cloud computing environments is becoming one of the top concerns for cloud stakeholders. This inability is due to two main reasons. Firstly, the lack of data tracking tools built for clouds. Secondly, current logging mechanisms are only designed from a system-centric perspective. There is a need for data-centric logging techniques which can trace data activities (e.g. file creation, edition, duplication, transfers, deletions, etc.) within and across all cloud servers. This will effectively enable full transparency and accountability for data movements in the cloud. In this paper, we introduce S2Logger, a data event logging mechanism which captures, analyses and visualizes data events in the cloud from the data point of view. By linking together atomic data events captured at both file and block level, the resulting sequence of data events depicts the cloud data provenance records throughout the data lifecycle. With this information, we can then detect critical data-related cloud security problems such as malicious actions, data leakages and data policy violations by analysing the data provenance. S2Logger also enables us to address the gaps and inadequacies of existing system-centric security tools.
ER  - 

TY  - CONF
JO  - Engineering in Medicine and Biology Society, 2005. IEEE-EMBS 2005. 27th Annual International Conference of the
TI  - HIPPA's compliant Auditing System for Medical Imaging System
T2  - Engineering in Medicine and Biology Society, 2005. IEEE-EMBS 2005. 27th Annual International Conference of the
IS  - 
SN  - 
VO  - 
SP  - 562
EP  - 563
AU  - Xiaomeng Chen
AU  - Jianguo Zhang
AU  - Dongjing Wu
AU  - RuoLing Han
Y1  - 17-18 Jan. 2006
PY  - 2005
KW  - PACS
KW  - authorisation
KW  - data privacy
KW  - Audit Trail Messages
KW  - DICOM
KW  - HIPPA compliant auditing system
KW  - HIPPA compliant auditing trails
KW  - Health Insurance Portability and Accountability Act
KW  - PACS
KW  - Protected Health Information
KW  - RIS
KW  - access control
KW  - audit controls
KW  - authorization control
KW  - data authentication
KW  - entity authentication
KW  - healthcare privacy
KW  - healthcare security
KW  - medical imaging system
KW  - security monitoring system
KW  - Access control
KW  - Authentication
KW  - Authorization
KW  - Biomedical imaging
KW  - Data privacy
KW  - Data security
KW  - Information security
KW  - Insurance
KW  - Medical control systems
KW  - Medical services
VL  - 
JA  - Engineering in Medicine and Biology Society, 2005. IEEE-EMBS 2005. 27th Annual International Conference of the
DO  - 10.1109/IEMBS.2005.1616473
AB  - As an official rule for healthcare privacy and security, Health Insurance Portability and Accountability Act (HIPAA) requires security services supporting implementation features: access control; audit controls; authorization control; data authentication; and entity authentication. Audit controls proposed by HIPPA Security Standards are audit trails, which audit activities, to assess compliance with a secure domain's policies, to detect instances of non-compliant behavior, and to facilitate detection of improper creation, access, modification and deletion of Protected Health Information (PHI). Although current medical imaging systems generate activity logs, there is a lack of regular description to integrate these large volumes of log data into generating HIPPA compliant auditing trails. The paper outlines the design of a HIPAA's compliant auditing system for medical imaging system such as PACS and RIS and discusses the development of this security monitoring system based on the Supplement 95 of the DICOM standard: Audit Trail Messages
ER  - 

TY  - CONF
JO  - Data and Software Engineering (ICODSE), 2014 International Conference on
TI  - Information system log visualization to monitor anomalous user activity based on time
T2  - Data and Software Engineering (ICODSE), 2014 International Conference on
IS  - 
SN  - 
VO  - 
SP  - 1
EP  - 6
AU  - Hanniel, J.J.
AU  - Widagdo, T.E.
AU  - Asnar, Y.D.W.
Y1  - 26-27 Nov. 2014
PY  - 2014
KW  - Internet
KW  - cognition
KW  - data analysis
KW  - data visualisation
KW  - information systems
KW  - security of data
KW  - Web-based data visualization
KW  - anomalous user activity detection
KW  - anomalous user activity monitoring
KW  - anomaly detection
KW  - cognition
KW  - data variables
KW  - design concept
KW  - dot plot
KW  - focused exploration
KW  - heatmap
KW  - information system log visualization
KW  - log data analysis
KW  - log files analysis
KW  - security
KW  - time-based data visualization method
KW  - Data visualization
KW  - Geology
KW  - Heating
KW  - IP networks
KW  - Information systems
KW  - Java
KW  - Monitoring
KW  - anomalous user activity
KW  - data visualization
KW  - log file
VL  - 
JA  - Data and Software Engineering (ICODSE), 2014 International Conference on
DO  - 10.1109/ICODSE.2014.7062673
AB  - As information systems start to manage the more crucial parts of human lives, their security cannot be neglected. One way to ensure the security is by analyzing their generated log files of anomalous user activity. Data visualization has become a common solution to help get around the problems in log analysis. In this paper, we tried to determine key characteristics of effective data visualization on detecting those anomalous user activity recorded in log files. First we analyzed the log data we have and derived 4 anomalies whose indicators are made into visualization topics. Hence we built 4 data visualizations to detect the 4 anomalies. Next, we transformed our data so that they can be visualized. After that, we analyzed the suitable time-based data visualization method to represent our data and decided on heatmap for its wide application on existing solutions and dot plot for it is able to accommodate all data variables needed on every visualization topic and has the suitable nuance for monitoring purposes. Next we decided on design concept of our data visualizations and implemented them as web-based data visualization. We conducted 2 tests in this paper to determine the key characteristics of effective data visualization. Even though the results are inconclusive, but they hinted that an effective data visualization on this matter should support large amount of perceived information through cognition and support focused exploration.
ER  - 

TY  - JOUR
JO  - Network, IEEE
TI  - Flow-net methodology for accountability in wireless networks
T2  - Network, IEEE
IS  - 5
SN  - 0890-8044
VO  - 23
SP  - 30
EP  - 37
AU  - Yang Xiao
Y1  - September 2009
PY  - 2009
KW  - access protocols
KW  - computer network management
KW  - mobile computing
KW  - security of data
KW  - telecommunication network routing
KW  - computing infrastructure
KW  - flow net methodology
KW  - forensic purposes
KW  - intrusion detection purposes
KW  - media access control
KW  - network infrastructure
KW  - routing layers
KW  - traffic data collection
KW  - wireless network accountability
KW  - Communication system traffic control
KW  - Computer crime
KW  - Computer networks
KW  - Forensics
KW  - Information security
KW  - Intrusion detection
KW  - Media Access Protocol
KW  - Reconnaissance
KW  - Routing
KW  - Wireless networks
VL  - 23
JA  - Network, IEEE
DO  - 10.1109/MNET.2009.5274919
AB  - Accountability implies that any entity should be held responsible for its own specific action or behavior so that the entity is part of larger chains of accountability. One of the goals of accountability is that once an event has transpired, the events that took place are traceable so that the causes can be determined afterward. The poor accountability provided by today's computers and networks wastes a great deal of money and effort; examples include activities to identify whether a system is under reconnaissance or attack, and the difficulties of distinguishing legitimate emails from phishing attacks. This is due to the simple fact that today's computing and network infrastructure was not built with accountability in mind. In this article we propose a novel methodology called flow-net for accountability. We apply this methodology to media access control and routing layers in wireless networks. We then compare the performance of flow-net with audit log files. This article presents a novel approach for traffic data collection that can also be used for forensics and intrusion detection purposes.
ER  - 

TY  - CONF
JO  - Trust, Security and Privacy in Computing and Communications (TrustCom), 2011 IEEE 10th International Conference on
TI  - Flogger: A File-Centric Logger for Monitoring File Access and Transfers within Cloud Computing Environments
T2  - Trust, Security and Privacy in Computing and Communications (TrustCom), 2011 IEEE 10th International Conference on
IS  - 
SN  - 
VO  - 
SP  - 765
EP  - 771
AU  - Ko, R.K.L.
AU  - Jagadpramana, P.
AU  - Bu Sung Lee
Y1  - 16-18 Nov. 2011
PY  - 2011
KW  - authorisation
KW  - cloud computing
KW  - system monitoring
KW  - virtual machines
KW  - Flogger
KW  - cloud computing environments
KW  - cloud providers
KW  - data accountability
KW  - file access monitoring
KW  - file centric logger
KW  - file-centric access
KW  - kernel spaces
KW  - log file access
KW  - private cloud environments
KW  - public cloud environments
KW  - system tools
KW  - virtual machines
KW  - Cloud computing
KW  - Communication channels
KW  - Databases
KW  - Kernel
KW  - Linux
KW  - Monitoring
KW  - Servers
KW  - Cloud computing
KW  - Cloud computing security
KW  - accountability
KW  - auditability
KW  - detective mechanisms
KW  - file-centric logging mechanisms
KW  - file-centric logs
KW  - logging
KW  - trust in Cloud computing
KW  - trusted Cloud
VL  - 
JA  - Trust, Security and Privacy in Computing and Communications (TrustCom), 2011 IEEE 10th International Conference on
DO  - 10.1109/TrustCom.2011.100
AB  - Trust is one of the main obstacles to widespread Cloud adoption. In order to increase trust in Cloud computing, we need to increase transparency and accountability of data in the Cloud for both enterprises and end-users. However, current system tools are unable to log file accesses and transfers effectively within a Cloud environment. In this paper, we present Flogger, a novel file-centric logger suitable for both private and public Cloud environments. Flogger records file- centric access and transfer information from within the kernel spaces of both virtual machines (VMs) and physical machines (PMs) in the Cloud, thus giving full transparency of the entire data landscape in the Cloud. With Flogger, services can be built above it to provide Cloud providers, end-users and regulators with the relevant provenance, e.g. a tool for an end- user to track whether his/ her file was 'touched' by an unauthorized user. We present the initial developments of Flogger, and interesting results from our experiments. We also present compelling future work that will shape the beginnings of a new logging paradigm: distributed VM/ PM file-centric logging.
ER  - 

TY  - JOUR
JO  - Dependable and Secure Computing, IEEE Transactions on
TI  - Ensuring Distributed Accountability for Data Sharing in the Cloud
T2  - Dependable and Secure Computing, IEEE Transactions on
IS  - 4
SN  - 1545-5971
VO  - 9
SP  - 556
EP  - 568
AU  - Sundareswaran, S.
AU  - Squicciarini, A.C.
AU  - Lin, D.
Y1  - July-Aug. 2012
PY  - 2012
KW  - authorisation
KW  - cloud computing
KW  - security of data
KW  - system monitoring
KW  - Internet
KW  - JAR programmable capabilities
KW  - cloud computing
KW  - cloud services
KW  - data sharing
KW  - decentralized information accountability framework
KW  - distributed accountability ensurance
KW  - distributed auditing mechanisms
KW  - logging mechanism
KW  - object-centered approach
KW  - Access control
KW  - Authentication
KW  - Cryptography
KW  - Distributed databases
KW  - Monitoring
KW  - Privacy
KW  - Cloud computing
KW  - accountability
KW  - data sharing.
VL  - 9
JA  - Dependable and Secure Computing, IEEE Transactions on
DO  - 10.1109/TDSC.2012.26
AB  - Cloud computing enables highly scalable services to be easily consumed over the Internet on an as-needed basis. A major feature of the cloud services is that users' data are usually processed remotely in unknown machines that users do not own or operate. While enjoying the convenience brought by this new emerging technology, users' fears of losing control of their own data (particularly, financial and health data) can become a significant barrier to the wide adoption of cloud services. To address this problem, in this paper, we propose a novel highly decentralized information accountability framework to keep track of the actual usage of the users' data in the cloud. In particular, we propose an object-centered approach that enables enclosing our logging mechanism together with users' data and policies. We leverage the JAR programmable capabilities to both create a dynamic and traveling object, and to ensure that any access to users' data will trigger authentication and automated logging local to the JARs. To strengthen user's control, we also provide distributed auditing mechanisms. We provide extensive experimental studies that demonstrate the efficiency and effectiveness of the proposed approaches.
ER  - 

TY  - CONF
JO  - Communication Systems and Network Technologies (CSNT), 2014 Fourth International Conference on
TI  - User and Device Tracking in Private Networks by Correlating Logs: A System for Responsive Forensic Analysis
T2  - Communication Systems and Network Technologies (CSNT), 2014 Fourth International Conference on
IS  - 
SN  - 
VO  - 
SP  - 1142
EP  - 1147
AU  - Chaudhari, S.
AU  - Chauhan, H.
AU  - Tomar, S.S.
AU  - Rawat, A.
Y1  - 7-9 April 2014
PY  - 2014
KW  - computer network security
KW  - local area networks
KW  - relational databases
KW  - DHCP
KW  - Flat file based sequential search system
KW  - IP address
KW  - Internet protocol address
KW  - RDBMS based tracking systems
KW  - browsing habits
KW  - device connection time
KW  - device location
KW  - device physical address
KW  - device tracking
KW  - electronic mail
KW  - email server logs
KW  - information context
KW  - mail access transactions
KW  - network access control
KW  - private networks
KW  - relational database management system
KW  - responsive forensic analysis
KW  - user tracking
KW  - Correlation
KW  - Databases
KW  - Electronic mail
KW  - Forensics
KW  - IP networks
KW  - Postal services
KW  - Servers
KW  - Logs; DHCP; NAC; squid; email; security logs; data monitoring and analysis tool
VL  - 
JA  - Communication Systems and Network Technologies (CSNT), 2014 Fourth International Conference on
DO  - 10.1109/CSNT.2014.253
AB  - IP address of a device, from where an offending activity was performed, is of limited value, because it does not specify a physical device/user, but an endpoint in network. It is useful to have information about where a device/user was at the time the offending activity was performed. It would be desirable to correlate different pieces of evidence to discover information, such as IP addresses used by the same device, physical address and location of the device, connection time of the device, browsing habits and mail access transactions carried out by the user using the device. Log data from various sources are required to be correlated together to create contexts of information, which is not visible from one source alone. In large networks, users/devices accessing a private network repeatedly can be tracked by analyzing and correlating DHCP, Network Access Control, WWW, Email server logs. With huge amount of logs, the common approach of manual browsing, correlating of log events, based on timelines is tedious, unresponsive approach. Flat file based sequential search system is not responsive, hence RDBMS based tracking systems are desirable. To build a responsive system requires identifying, consolidating log files, conversion, transmission and storage into relational databases. An automated system has been developed at our organization for forensic analysis of network accesses, with device and user tracking as its goal. We present, our approach to perform log management, correlation, which assists in performing responsive forensic analysis of real network with more than 2500 nodes, aimed at tracking users/devices.
ER  - 

TY  - CONF
JO  - Services (SERVICES), 2014 IEEE World Congress on
TI  - Audit Log Management in MongoDB
T2  - Services (SERVICES), 2014 IEEE World Congress on
IS  - 
SN  - 
VO  - 
SP  - 53
EP  - 57
AU  - Murugesan, P.
AU  - Ray, I.
Y1  - June 27 2014-July 2 2014
PY  - 2014
KW  - Internet
KW  - database management systems
KW  - MongoDB
KW  - NIST standard
KW  - NoSQL databases
KW  - Web-based applications
KW  - agile database
KW  - audit log management
KW  - complex multisite architectures
KW  - data management
KW  - log management techniques
KW  - mongosniff
KW  - single server environment
KW  - Indexes
KW  - Monitoring
KW  - NIST
KW  - Security
KW  - Servers
KW  - Audit Trail
KW  - Log Management
KW  - MongoDB
KW  - NoSQL
VL  - 
JA  - Services (SERVICES), 2014 IEEE World Congress on
DO  - 10.1109/SERVICES.2014.19
AB  - In the past few years, web-based applications and their data management needs have changed dramatically. Relational databases are often being replaced by other viable alternatives, such as NoSQL databases, for reasons of scalability and heterogeneity. MongoDB, a NoSQL database, is an agile database built for scalability, performance and high availability. It can be deployed in single server environment and also on complex multi-site architectures. MongoDB provides high performance for read and write operations by leveraging in-memory computing. Although researchers have motivated the need for MongoDB, not much appears in the area of log management. Efficient log management techniques are needed for various reasons including security, accountability, and improving the performance of the system. Towards this end, we analyze the different logging methods offered by MongoDB and compare them to the NIST standard. Our analysis indicates that profiling and mongosniff are useful for log management and we present a simple model that combines the two techniques.
ER  - 

TY  - CONF
JO  - Network Operations and Management Symposium, 2008. NOMS 2008. IEEE
TI  - Mining event logs with SLCT and LogHound
T2  - Network Operations and Management Symposium, 2008. NOMS 2008. IEEE
IS  - 
SN  - 1542-1201
VO  - 
SP  - 1071
EP  - 1074
AU  - Vaarandi, R.
Y1  - 7-11 April 2008
PY  - 2008
KW  - data mining
KW  - security of data
KW  - telecommunication computing
KW  - LogHound
KW  - communication networks
KW  - event log analysis
KW  - event logs mining
KW  - log data
KW  - security events
KW  - system management personnel
KW  - Algorithm design and analysis
KW  - Clustering algorithms
KW  - Communication networks
KW  - Communication system security
KW  - Data analysis
KW  - Data mining
KW  - Data security
KW  - Event detection
KW  - Monitoring
KW  - Personnel
KW  - data mining
KW  - data security
KW  - event log analysis
VL  - 
JA  - Network Operations and Management Symposium, 2008. NOMS 2008. IEEE
DO  - 10.1109/NOMS.2008.4575281
AB  - With the growth of communication networks, event logs are increasing in size at a fast rate. Today, it is not uncommon to have systems that generate tens of gigabytes of log data per day. Log data are likely to contain information that deserves closer attention - such as security events - but the task of reviewing logs manually is beyond the capabilities of a human. This paper discusses data mining tools SLCT and log hound that were designed for assisting system management personnel in extracting knowledge from event logs.
ER  - 

TY  - CONF
JO  - Information and Telecommunication Technologies (APSITT), 2010 8th Asia-Pacific Symposium on
TI  - A study on the requirements of accountable cloud services and log management
T2  - Information and Telecommunication Technologies (APSITT), 2010 8th Asia-Pacific Symposium on
IS  - 
SN  - 
VO  - 
SP  - 1
EP  - 6
AU  - Nakahara, S.
AU  - Ishimoto, H.
Y1  - 15-18 June 2010
PY  - 2010
KW  - Web services
KW  - cryptography
KW  - network servers
KW  - Web-based cloud services
KW  - accountable cloud services
KW  - cloud-based services tracing
KW  - computer resources
KW  - cryptography
KW  - link log data accountability
KW  - log management
KW  - nonlocal servers
KW  - Application software
KW  - Cloud computing
KW  - Cryptography
KW  - Laboratories
KW  - Network servers
KW  - Outsourcing
KW  - Resource virtualization
KW  - Robustness
KW  - Service oriented architecture
KW  - Web server
KW  - Accountability
KW  - Accountability Framework
KW  - Cloud Service
KW  - Data Security
KW  - Log Management
VL  - 
JA  - Information and Telecommunication Technologies (APSITT), 2010 8th Asia-Pacific Symposium on
DO  - 
AB  - As services themselves and computer resources of cloud services become increasingly non-local, accountability to users for the results and implementation status of the service have emerged as important issues. This paper gives a detailed overview of what would be required to (1) link log data for multiple services supported by non-local servers and (2) trace cloud-based services, focusing on prevailing web-based cloud services. The paper presents a robust accountability framework and discusses how cryptography could be applied to log data accountability.
ER  - 

TY  - CONF
JO  - Communication Systems and Network Technologies (CSNT), 2014 Fourth International Conference on
TI  - Data Generation and Analysis for Digital Forensic Application Using Data Mining
T2  - Communication Systems and Network Technologies (CSNT), 2014 Fourth International Conference on
IS  - 
SN  - 
VO  - 
SP  - 458
EP  - 462
AU  - Khobragade, P.K.
AU  - Malik, L.G.
Y1  - 7-9 April 2014
PY  - 2014
KW  - computer crime
KW  - data analysis
KW  - data mining
KW  - digital forensics
KW  - firewalls
KW  - storage management
KW  - FTK 4.0
KW  - Web browser
KW  - cyber crime huge log data
KW  - cyber system
KW  - data analysis
KW  - data collection
KW  - data generation
KW  - data mining
KW  - data storage
KW  - digital forensic application
KW  - firewall logs
KW  - intrusion detection system
KW  - memory forensic analysis
KW  - network attack detection
KW  - network forensic analysis
KW  - network traces
KW  - network traffic
KW  - packet captures
KW  - remote system forensic
KW  - transactional data
KW  - Computers
KW  - Data mining
KW  - Data visualization
KW  - Databases
KW  - Digital forensics
KW  - Security
KW  - Clustering
KW  - Data Collection
KW  - Digital forensic tool
KW  - Log Data collection
VL  - 
JA  - Communication Systems and Network Technologies (CSNT), 2014 Fourth International Conference on
DO  - 10.1109/CSNT.2014.97
AB  - In the cyber crime huge log data, transactional data occurs which tends to plenty of data for storage and analyze them. It is difficult for forensic investigators to play plenty of time to find out clue and analyze those data. In network forensic analysis involves network traces and detection of attacks. The trace involves an Intrusion Detection System and firewall logs, logs generated by network services and applications, packet captures by sniffers. In network lots of data is generated in every event of action, so it is difficult for forensic investigators to find out clue and analyzing those data. In network forensics is deals with analysis, monitoring, capturing, recording, and analysis of network traffic for detecting intrusions and investigating them. This paper focuses on data collection from the cyber system and web browser. The FTK 4.0 is discussing for memory forensic analysis and remote system forensic which is to be used as evidence for aiding investigation.
ER  - 

TY  - CONF
JO  - Computer Software and Applications Conference Workshops (COMPSACW), 2010 IEEE 34th Annual
TI  - ULMS: An Accelerator for the Applications by Shifting Writing Log from Local Disk to Clouds
T2  - Computer Software and Applications Conference Workshops (COMPSACW), 2010 IEEE 34th Annual
IS  - 
SN  - 
VO  - 
SP  - 104
EP  - 108
AU  - Li Zhou
AU  - Yong Zhang
AU  - Chunxiao Xing
Y1  - 19-23 July 2010
PY  - 2010
KW  - Web services
KW  - data analysis
KW  - data mining
KW  - database management systems
KW  - disc storage
KW  - software architecture
KW  - system monitoring
KW  - SaaS method
KW  - ULMS
KW  - accelerator
KW  - database system
KW  - local disk
KW  - local storage efficiency
KW  - log analysis
KW  - log data analysis
KW  - log management
KW  - shift-log-by-ActiveMQ
KW  - shift-log-by-Webservice
KW  - user log mining system
KW  - writing log data shift
KW  - Log
KW  - Log Analysis
KW  - Performance
KW  - SaaS
VL  - 
JA  - Computer Software and Applications Conference Workshops (COMPSACW), 2010 IEEE 34th Annual
DO  - 10.1109/COMPSACW.2010.28
AB  - Log data is critical to applications and the management and analysis of log data is a hot research topic. Existing log managements are normally tightly integrated with applications themselves, which may lead to problems including performance, local storage efficiency, security and non realtime functionality. To solve these problems, we present a SaaS method which shifts writing log data from local disk to clouds, at the same time the log management and analysis functionalities are also done by a cloud. We analyze two architectures to implement this method which are Shift-Log-by-WebService and Shift-Log-by-ActiveMQ. Initial experiments show the efficiency of later one. In the future, we can apply this tool to application systems which are based on web and database systems to improve their performances.
ER  - 

TY  - JOUR
JO  - Information Forensics and Security, IEEE Transactions on
TI  - Trail of Bytes: New Techniques for Supporting Data Provenance and Limiting Privacy Breaches
T2  - Information Forensics and Security, IEEE Transactions on
IS  - 6
SN  - 1556-6013
VO  - 7
SP  - 1876
EP  - 1889
AU  - Krishnan, S.
AU  - Snow, K.Z.
AU  - Monrose, F.
Y1  - Dec. 2012
PY  - 2012
KW  - computer forensics
KW  - data privacy
KW  - computer systems
KW  - data access
KW  - data exfiltration attempts
KW  - data provenance
KW  - forensic analysis
KW  - forensic layer records
KW  - forensic platform
KW  - hypervisor
KW  - multiple disks
KW  - privacy breaches
KW  - tracking mechanism
KW  - version-based audit log
KW  - virtualized environment
KW  - Couplings
KW  - Forensics
KW  - Monitoring
KW  - Semantics
KW  - Virtual machine monitors
KW  - Virtual machining
KW  - Computer security
KW  - checkpointing
KW  - information security
KW  - intrusion detection
KW  - operating systems
KW  - system recovery
KW  - virtual machine monitors
VL  - 7
JA  - Information Forensics and Security, IEEE Transactions on
DO  - 10.1109/TIFS.2012.2210217
AB  - Forensic analysis of computer systems requires that one first identify suspicious objects or events, and then examine them in enough detail to form a hypothesis as to their cause and effect. Sadly, while our ability to gather vast amounts of data has improved significantly over the past two decades, it is all too often the case that we lack detailed information just when we need it the most. In this paper, we attempt to improve on the state of the art by providing a forensic platform that transparently monitors and records data access events within a virtualized environment using only the abstractions exposed by the hypervisor. Our approach monitors accesses to objects on disk and follows the causal chain of these accesses across processes, even after the objects are copied into memory. Our forensic layer records these transactions in a tamper evident version-based audit log that allows for faithful, and efficient, reconstruction of the recorded events and the changes they induced. To demonstrate the utility of our approach, we provide an extensive empirical evaluation, including a real-world case study demonstrating how our platform can be used to reconstruct valuable information about the what, when, and how, after a compromise has been detected. We also extend our earlier work by providing a tracking mechanism that can monitor data exfiltration attempts across multiple disks and also block attempts to copy data over the network.
ER  - 

TY  - CONF
JO  - Information Security (AsiaJCIS), 2015 10th Asia Joint Conference on
TI  - iF2: An Interpretable Fuzzy Rule Filter for Web Log Post-Compromised Malicious Activity Monitoring
T2  - Information Security (AsiaJCIS), 2015 10th Asia Joint Conference on
IS  - 
SN  - 
VO  - 
SP  - 130
EP  - 137
AU  - Chih-Hung Hsieh
AU  - Yu-Siang Shen
AU  - Chao-Wen Li
AU  - Jain-Shing Wu
Y1  - 24-26 May 2015
PY  - 2015
KW  - Internet
KW  - data mining
KW  - fuzzy set theory
KW  - learning (artificial intelligence)
KW  - neural nets
KW  - pattern classification
KW  - statistical analysis
KW  - support vector machines
KW  - Internet address
KW  - SVM
KW  - Web log file tracking
KW  - Web log post-compromised malicious activity monitoring
KW  - Web-access log dataset
KW  - decision tree
KW  - expectation maximization based kernel algorithm
KW  - fuzzy rule filter
KW  - iF<sup>2</sup>
KW  - interpretable fuzzy rule filter
KW  - kernel based techniques
KW  - log data analysis
KW  - logic based classifiers
KW  - logic based techniques
KW  - machine learning methods
KW  - malicious activities
KW  - neural network
KW  - parameter optimization problem
KW  - recall rate
KW  - rule-based algorithm
KW  - support vector machine
KW  - Accuracy
KW  - Internet
KW  - Kernel
KW  - Monitoring
KW  - Optimization
KW  - Prediction algorithms
KW  - Support vector machines
KW  - Fuzzy Rule Based Filter
KW  - Machine Learning
KW  - Parameter Optimization
KW  - Pattern Recognition
KW  - Post-Compromised Threat Identification
KW  - Web Log Analysis
VL  - 
JA  - Information Security (AsiaJCIS), 2015 10th Asia Joint Conference on
DO  - 10.1109/AsiaJCIS.2015.19
AB  - To alleviate the loads of tracking web log file by human effort, machine learning methods are now commonly used to analyze log data and to identify the pattern of malicious activities. Traditional kernel based techniques, like the neural network and the support vector machine (SVM), typically can deliver higher prediction accuracy. However, the user of a kernel based techniques normally cannot get an overall picture about the distribution of the data set. On the other hand, logic based techniques, such as the decision tree and the rule-based algorithm, feature the advantage of presenting a good summary about the distinctive characteristics of different classes of data such that they are more suitable to generate interpretable feedbacks to domain experts. In this study, a real web-access log dataset from a certain organization was collected. An efficient interpretable fuzzy rule filter (iF<sup>2</sup>) was proposed as a filter to analyze the data and to detect suspicious internet addresses from the normal ones. The historical information of each internet address recorded in web log file is summarized as multiple statistics. And the design process of iF<sup>2</sup> is elaborately modeled as a parameter optimization problem which simultaneously considers 1) maximizing prediction accuracy, 2) minimizing number of used rules, and 3) minimizing number of selected statistics. Experimental results show that the fuzzy rule filter constructed with the proposed approach is capable of delivering superior prediction accuracy in comparison with the conventional logic based classifiers and the expectation maximization based kernel algorithm. On the other hand, though it cannot match the prediction accuracy delivered by the SVM, however, when facing real web log file where the ratio of positive and negative cases is extremely unbalanced, the proposed iF<sup>2</sup> of having optimization flexibility results in a better recall rate and enjoys one major advantage due to providing th- user with an overall picture of the underlying distributions.
ER  - 

TY  - CONF
JO  - Security Technology, 2003. Proceedings. IEEE 37th Annual 2003 International Carnahan Conference on
TI  - Firearm dataloggers
T2  - Security Technology, 2003. Proceedings. IEEE 37th Annual 2003 International Carnahan Conference on
IS  - 
SN  - 
VO  - 
SP  - 354
EP  - 357
AU  - Sutcliff, J.
Y1  - 14-16 Oct. 2003
PY  - 2003
KW  - data acquisition
KW  - data loggers
KW  - military computing
KW  - weapons
KW  - armoury inventory
KW  - digital video
KW  - electronic data capture system
KW  - firearm datalogger
KW  - firing event detection
KW  - invasive mounting system
KW  - spatial sensing
KW  - synchronization
KW  - tracking facility
KW  - Base stations
KW  - Control systems
KW  - Data security
KW  - Digital cameras
KW  - Gunshot detection systems
KW  - Law
KW  - Legal factors
KW  - Sensor arrays
KW  - Sensor systems
KW  - Weapons
VL  - 
JA  - Security Technology, 2003. Proceedings. IEEE 37th Annual 2003 International Carnahan Conference on
DO  - 10.1109/CCST.2003.1297586
AB  - We discuss the development of a versatile electronic data capture system. This system is unobtrusively fitted to the lethal or less-than-lethal firearms carried by armed police, security forces etc in the course of their duties. The system has the potential to increase accountability and the effectiveness of post incident evaluation by capturing secure, legally submitable digital evidence. This evidence can then be used to determine the users actions and those of persons challenged or fired upon. We specifically focus on firing event detection, synchronizing data capture to the firing event, inherent armoury inventory and tracking facilities and minimally invasive mounting systems. The system currently incorporates digital video and limited spatial sensing capabilities. We evaluate these and explore possible future developments such as GPS, telemetry and full 3D spatial tracking. The research project is being conducted with Partnership Venture grants from Durham University and with Durham University School of Engineering. Facilities and consultancy are being provided by Durham Constabulary Special Operations Department.
ER  - 

TY  - CONF
JO  - Business Management and Electronic Information (BMEI), 2011 International Conference on
TI  - An improved algorithm with key attributes constraints for mining interesting association rules in network log
T2  - Business Management and Electronic Information (BMEI), 2011 International Conference on
IS  - 
SN  - 
VO  - 3
SP  - 104
EP  - 107
AU  - Jin Kezhong
AU  - Wu Chengwen
Y1  - 13-15 May 2011
PY  - 2011
KW  - computer forensics
KW  - data mining
KW  - pattern classification
KW  - security of data
KW  - association rule mining
KW  - computer forensic analysis
KW  - computer log data source
KW  - intrusion detection analysis
KW  - key attribute constraint
KW  - network access
KW  - network log data
KW  - outlier detection
KW  - user pattern mining
KW  - Algorithm design and analysis
KW  - Association rules
KW  - Computers
KW  - Databases
KW  - Performance evaluation
KW  - Protocols
KW  - association rule
KW  - data mining
KW  - key attribute
KW  - network log
VL  - 3
JA  - Business Management and Electronic Information (BMEI), 2011 International Conference on
DO  - 10.1109/ICBMEI.2011.5920405
AB  - Computer logs are generated by application activities, network accesses and system audit, which are important data sources for user pattern mining, computer forensic analysis, intrusion detection analysis and outlier detection. Algorithms for mining association rule are useful methods to find interesting rules implied in large computer log data. But existing algorithms which based on confidence and support are unfit for mining computer log data, many uninteresting rules will be generated and useful rules will be shadowed. To solve this problem, the concept of key attributes of network log data is introduced, and an algorithm with key attributes constraints for mining interesting association rules in network log data is designed. Experimental result shows that the number of uninteresting rules can be reduced effectively and the validity of rules which mined are improved.
ER  - 

TY  - CONF
JO  - Network Operations and Management Symposium (NOMS), 2012 IEEE
TI  - A distance-based method to detect anomalous attributes in log files
T2  - Network Operations and Management Symposium (NOMS), 2012 IEEE
IS  - 
SN  - 1542-1201
VO  - 
SP  - 498
EP  - 501
AU  - Hommes, S.
AU  - State, R.
AU  - Engel, T.
Y1  - 16-20 April 2012
PY  - 2012
KW  - computer network security
KW  - data analysis
KW  - information theory
KW  - statistical process control
KW  - system monitoring
KW  - ISP-provided firewall logs
KW  - anomalous attribute detection
KW  - automated log analysis
KW  - data volume
KW  - distance-based method
KW  - domain-specific knowledge
KW  - haystack problem
KW  - human expertise
KW  - information theory
KW  - log data analysis
KW  - operational logs
KW  - proverbial needle
KW  - real time analysis
KW  - statistical process control
KW  - suspicious network activity detection
KW  - Control charts
KW  - Correlation
KW  - Humans
KW  - IP networks
KW  - Process control
KW  - Protocols
KW  - Real time systems
VL  - 
JA  - Network Operations and Management Symposium (NOMS), 2012 IEEE
DO  - 10.1109/NOMS.2012.6211940
AB  - Dealing with large volumes of logs is like the proverbial needle in the haystack problem. Finding relevant events that might be associated with an incident, or real time analysis of operational logs is extremely difficult when the underlying data volume is huge and when no explicit misuse model exists. While domain-specific knowledge and human expertise may be useful in analysing log data, automated approaches for detecting anomalies and track incidents are the only viable solutions when confronted with large volumes of data. In this paper we address the issue of automated log analysis and consider more specifically the case of ISP-provided firewall logs. We leverage approaches derived from statistical process control and information theory in order to track potential incidents and detect suspicious network activity.
ER  - 

TY  - CONF
JO  - Computer Software and Applications Conference (COMPSAC), 2012 IEEE 36th Annual
TI  - Domain Independent Event Analysis for Log Data Reduction
T2  - Computer Software and Applications Conference (COMPSAC), 2012 IEEE 36th Annual
IS  - 
SN  - 0730-3157
VO  - 
SP  - 225
EP  - 232
AU  - Kalamatianos, T.
AU  - Kontogiannis, K.
AU  - Matthews, P.
Y1  - 16-20 July 2012
PY  - 2012
KW  - data analysis
KW  - data reduction
KW  - program diagnostics
KW  - security of data
KW  - DARPA Intrusion Detection Evaluation 1999 data sets
KW  - KDD 1999 data sets
KW  - domain independent event analysis
KW  - large software systems
KW  - log analysis technique
KW  - log data reduction
KW  - run time behavior analysis
KW  - similarity score
KW  - Algorithm design and analysis
KW  - Analytical models
KW  - Intrusion detection
KW  - Software
KW  - Standards
KW  - Weight measurement
KW  - Software engineering
KW  - dynamic analysis
KW  - log analysis
KW  - log reduction
KW  - software maintenance
KW  - system understanding
VL  - 
JA  - Computer Software and Applications Conference (COMPSAC), 2012 IEEE 36th Annual
DO  - 10.1109/COMPSAC.2012.33
AB  - Analyzing the run time behavior of large software systems is a difficult and challenging task. Log analysis has been proposed as a possible solution. However, such an analysis poses unique challenges, mostly due to the volume and diversity of the logged data that is collected, thus making this analysis often intractable for practical purposes. In this paper, we present a log analysis technique that aims to compute a smaller, compared to the original, collection of events that relate to a given analysis objective. The technique is based on computing a similarity score between the logged events and a collection of significant events that we refer to as beacons. The major novelties of the proposed technique are that it is domain independent and that it does not require the use of a pre-existing training data set. The technique has been evaluated against the DARPA Intrusion Detection Evaluation 1999 and the KDD 1999 data sets with promising results.
ER  - 

TY  - CONF
JO  - Systematic Approaches to Digital Forensic Engineering, 2005. First International Workshop on
TI  - SecSyslog: an approach to secure logging based on covert channels
T2  - Systematic Approaches to Digital Forensic Engineering, 2005. First International Workshop on
IS  - 
SN  - 
VO  - 
SP  - 248
EP  - 263
AU  - Forte, D.V.
AU  - Maruti, C.
AU  - Vetturi, M.R.
AU  - Zambelli, M.
Y1  - 7-9 Nov. 2005
PY  - 2005
KW  - Linux
KW  - management information systems
KW  - protocols
KW  - security of data
KW  - telecommunication channels
KW  - LINUX
KW  - SecSyslog
KW  - corporate information systems
KW  - covert channels
KW  - digital forensic tool
KW  - level 3 ISO/OSI traffic
KW  - log traces
KW  - pcap-compatible output
KW  - secure logging
KW  - Communication channels
KW  - Computer hacking
KW  - Conferences
KW  - Digital forensics
KW  - ISO standards
KW  - Information security
KW  - Linux
KW  - Management information systems
KW  - Open systems
KW  - Protocols
KW  - Covert Channel
KW  - Forensic
KW  - Log Correlation
KW  - Log Integrity
KW  - Log analysis
KW  - Spyware.
VL  - 
JA  - Systematic Approaches to Digital Forensic Engineering, 2005. First International Workshop on
DO  - 10.1109/SADFE.2005.21
AB  - Today log traces are widely used to identify and prevent violations of corporate information systems. The most recent logging trend is to manage most level 3 ISO/OSI traffic via pcap-compatible output. But use of syslog is still very widespread, as are the security issues it entails, especially in its 'pure' version. This paper outlines the basic syslog problems as foreseen in the RFCs, examines the 'secure' alternatives to the protocol (and relative implementations) and proposes a transmission approach based on covert channels which, applied on the LINUX platform, might answer some of the intrinsic reliability problems which undermine its effectiveness as a digital forensic tool.
ER  - 

TY  - CONF
JO  - Computer Software and Applications, 2008. COMPSAC '08. 32nd Annual IEEE International
TI  - Pattern and Policy Driven Log Analysis for Software Monitoring
T2  - Computer Software and Applications, 2008. COMPSAC '08. 32nd Annual IEEE International
IS  - 
SN  - 0730-3157
VO  - 
SP  - 108
EP  - 111
AU  - Razavi, A.
AU  - Kontogiannis, Kostas
Y1  - July 28 2008-Aug. 1 2008
PY  - 2008
KW  - object-oriented programming
KW  - program diagnostics
KW  - risk analysis
KW  - security of data
KW  - Viterbi algorithm
KW  - component-based software
KW  - industrial software systems
KW  - pattern driven log analysis
KW  - pattern recognition
KW  - policy driven log analysis
KW  - software monitoring
KW  - system auditing
KW  - system diagnosis
KW  - system maintenance
KW  - system monitoring
KW  - system risk
KW  - system threat profile
KW  - Application software
KW  - Collaborative software
KW  - Computer industry
KW  - Context modeling
KW  - Monitoring
KW  - Pattern analysis
KW  - Pattern matching
KW  - Pattern recognition
KW  - Risk analysis
KW  - Software systems
KW  - Software Auditing
KW  - Software Monitoring
KW  - Trace Analysis
VL  - 
JA  - Computer Software and Applications, 2008. COMPSAC '08. 32nd Annual IEEE International
DO  - 10.1109/COMPSAC.2008.81
AB  - The component-based nature of large industrial software systems that consist of a number of diverse collaborating applications, pose significant challenges with respect to system maintenance, monitoring, auditing, and diagnosing. In this context, a monitoring and diagnostic system interprets log data to recognize patterns of significant events that conform to specific threat models. Threat models have been used by the software industry for analyzing and documenting a systempsilas risks in order to understand a systempsilas threat profile. In this paper, we propose a framework whereby patterns of significant events are represented as expressions of a specialized monitoring language that are used to annotate specific threat models. An approximate matching technique that is based on the Viterbi algorithm is then used to identify whether system generated events, fit the given patterns. The technique has been applied and evaluated considering threat models and monitoring policies in logs that have been obtained from multi-user MS-Windows based systems.
ER  - 

TY  - CONF
JO  - Cloud Computing Technology and Science (CloudCom), 2013 IEEE 5th International Conference on
TI  - Supporting Cloud Accountability by Collecting Evidence Using Audit Agents
T2  - Cloud Computing Technology and Science (CloudCom), 2013 IEEE 5th International Conference on
IS  - 
SN  - 
VO  - 1
SP  - 185
EP  - 190
AU  - Ruebsamen, T.
AU  - Reich, C.
Y1  - 2-5 Dec. 2013
PY  - 2013
KW  - auditing
KW  - cloud computing
KW  - data privacy
KW  - agent based architecture
KW  - audit processing
KW  - audit tasks
KW  - cloud computing
KW  - cloud services process data
KW  - cloud-adopted evidence collection process
KW  - complexity reduction
KW  - log data analysis
KW  - multi-provider scenario
KW  - privacy issues
KW  - Cloud computing
KW  - Customer relationship management
KW  - Data privacy
KW  - Monitoring
KW  - Security
KW  - Virtual machine monitors
KW  - Virtual machining
KW  - Accountability
KW  - Audit
KW  - Cloud Computing
KW  - Evidence
VL  - 1
JA  - Cloud Computing Technology and Science (CloudCom), 2013 IEEE 5th International Conference on
DO  - 10.1109/CloudCom.2013.32
AB  - Today's cloud services process data and let it often unclear to customers, how and by whom data is collected, stored and processed. This hinders the adoption of cloud computing by businesses. One way to address this problem is to make clouds more accountable, which has to be provable by third parties through audits. In this paper we present a cloud-adopted evidence collection process, possible evidence sources and discuss privacy issues in the context of audits. We introduce an agent based architecture, which is able to perform audit processing and reporting continuously. Agents can be specialized to perform specific audit tasks (e.g., log data analysis) whenever necessary, to reduce complexity and the amount of collected evidence information. Finally, a multi-provider scenario is discussed, which shows the usefulness of this approach.
ER  - 

TY  - CONF
JO  - Securecomm and Workshops, 2006
TI  - System Anomaly Detection: Mining Firewall Logs
T2  - Securecomm and Workshops, 2006
IS  - 
SN  - 
VO  - 
SP  - 1
EP  - 5
AU  - Winding, R.
AU  - Wright, T.
AU  - Chapple, M.
Y1  - Aug. 28 2006-Sept. 1 2006
PY  - 2006
KW  - authorisation
KW  - computer networks
KW  - data mining
KW  - learning (artificial intelligence)
KW  - statistical analysis
KW  - telecommunication traffic
KW  - data mining
KW  - firewall audit log mining
KW  - machine learning
KW  - network traffic anomalies
KW  - statistical analysis
KW  - system anomaly detection
KW  - Data mining
KW  - Data security
KW  - Forensics
KW  - Intrusion detection
KW  - Machine learning
KW  - Protection
KW  - Reconnaissance
KW  - Statistical analysis
KW  - Telecommunication traffic
KW  - Traffic control
KW  - Data mining
KW  - Firewall log analysis
KW  - Intrusion Detection
VL  - 
JA  - Securecomm and Workshops, 2006
DO  - 10.1109/SECCOMW.2006.359572
AB  - This paper describes an application of data mining and machine learning to discovering network traffic anomalies in firewall logs. There is a variety of issues and problems that can occur with systems that are protected by firewalls. These systems can be improperly configured, operate unexpected services, or fall victim to intrusion attempts. Firewall logs often generate hundreds of thousands of audit entries per day. It is often easy to use these records for forensics if one knows that something happened and when. However, it can be burdensome to attempt to manually review logs for anomalies. This paper uses data mining techniques to analyze network traffic, based on firewall audit logs, to determine if statistical analysis of the logs can be used to identify anomalies
ER  - 

TY  - CONF
JO  - Complex, Intelligent, and Software Intensive Systems (CISIS), 2013 Seventh International Conference on
TI  - An Integrated Distributed Log Management System with Metadata for Network Operation
T2  - Complex, Intelligent, and Software Intensive Systems (CISIS), 2013 Seventh International Conference on
IS  - 
SN  - 
VO  - 
SP  - 747
EP  - 750
AU  - Ikebe, M.
AU  - Yoshida, K.
Y1  - 3-5 July 2013
PY  - 2013
KW  - Internet
KW  - computer network management
KW  - computer network security
KW  - distributed sensors
KW  - meta data
KW  - system monitoring
KW  - campus networks
KW  - cross-processing system
KW  - integrated distributed log management system
KW  - log analysis
KW  - log collection
KW  - metadata
KW  - network administrators
KW  - network management tasks
KW  - network operation
KW  - sensor network
KW  - server administrators
KW  - server management tasks
KW  - Artificial intelligence
KW  - Distributed databases
KW  - Educational institutions
KW  - IP networks
KW  - Protocols
KW  - Prototypes
KW  - Servers
KW  - Distributed System
KW  - Metadata
KW  - Network Operation
KW  - Server Log
VL  - 
JA  - Complex, Intelligent, and Software Intensive Systems (CISIS), 2013 Seventh International Conference on
DO  - 10.1109/CISIS.2013.134
AB  - An enormous amount of log data is generated by servers and other devices on the network, and server/network administrators analyze the logs to investigate anomalous communications or troubleshoot. However, server/network management tasks increase in volume and complexity, resulting in greater burden on the administrator. In this paper, we propose a integrated management system for a sensor network where log data is output from many different kinds of sensors. We consider a server or network device as one of the sensors. We also propose a cross-processing system for several kinds of log data. In particular, we describe the management and collection of logs in our campus networks.
ER  - 


