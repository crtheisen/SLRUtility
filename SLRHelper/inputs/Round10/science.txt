TY  - JOUR
T1  - Cloud forensics: Technical challenges, solutions and comparative analysis
JO  - Digital Investigation
VL  - 13
IS  - 
SP  - 38
EP  - 57
PY  - 2015/6//
T2  - 
AU  - Pichan, Ameer
AU  - Lazarescu, Mihai
AU  - Soh, Sie Teng
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2015.03.002
UR  - http://www.sciencedirect.com/science/article/pii/S1742287615000407
KW  - Cloud computing
KW  - Cloud forensics
KW  - Cloud service provider
KW  - Cloud customer
KW  - Digital forensics
KW  - Digital evidence
KW  - Service level agreement
KW  - Amazon EC2
AB  - Abstract
Cloud computing is arguably one of the most significant advances in information technology (IT) services today. Several cloud service providers (CSPs) have offered services that have produced various transformative changes in computing activities and presented numerous promising technological and economic opportunities. However, many cloud customers remain reluctant to move their IT needs to the cloud, mainly due to their concerns on cloud security and the threat of the unknown. The CSPs indirectly escalate their concerns by not letting customers see what is behind virtual wall of their clouds that, among others, hinders digital investigations. In addition, jurisdiction, data duplication and multi-tenancy in cloud platform add to the challenge of locating, identifying and separating the suspected or compromised targets for digital forensics. Unfortunately, the existing approaches to evidence collection and recovery in a non-cloud (traditional) system are not practical as they rely on unrestricted access to the relevant system and user data; something that is not available in the cloud due its decentralized data processing. In this paper we systematically survey the forensic challenges in cloud computing and analyze their most recent solutions and developments. In particular, unlike the existing surveys on the topic, we describe the issues in cloud computing using the phases of traditional digital forensics as the base. For each phase of the digital forensic process, we have included a list of challenges and analysis of their possible solutions. Our description helps identifying the differences between the problems and solutions for non-cloud and cloud digital forensics. Further, the presentation is expected to help the investigators better understand the problems in cloud environment. More importantly, the paper also includes most recent development in cloud forensics produced by researchers, National Institute of Standards and Technology and Amazon.
ER  - 

TY  - JOUR
T1  - Insider Threat Detection Using Log Analysis and Event Correlation
JO  - Procedia Computer Science
VL  - 45
IS  - 
SP  - 436
EP  - 445
PY  - 2015///
T2  - International Conference on Advanced Computing Technologies and Applications (ICACTA)
AU  - Ambre, Amruta
AU  - Shekokar, Narendra
SN  - 1877-0509
DO  - http://dx.doi.org/10.1016/j.procs.2015.03.175
UR  - http://www.sciencedirect.com/science/article/pii/S1877050915004184
KW  - Security
KW  - log analysis
KW  - event correlation
KW  - Bayesian detection rate
KW  - false alarm rate
AB  - Abstract
Insider threat is one of the most dangerous security threat, and a much more complex issue. These insiders can be a former or a disgruntled employee or any business associate that has or had an authorised access to information for any particular organization. They have control and security measures. Hence continuous monitoring is essential to track each and every activity within the network. Log management is a strong technique which includes both Log analysis with event correlation which provides the root cause of any attack and network can be protected from security violations. Though intrusion detection is complex process, while checking the ability to detect intrusive behaviour within the internal environment, it has to take care of suppressing the false alarm rate. Some strong approach is required on the basis of which decisions can be taken fast. This paper proposes a probabilistic approach which illustrates the frequency of occurrence of event in percentage while still considering the false alarm rate at an acceptable level.
ER  - 

TY  - JOUR
T1  - Security and privacy issues in implantable medical devices: A comprehensive survey
JO  - Journal of Biomedical Informatics
VL  - 55
IS  - 
SP  - 272
EP  - 289
PY  - 2015/6//
T2  - 
AU  - Camara, Carmen
AU  - Peris-Lopez, Pedro
AU  - Tapiador, Juan E.
SN  - 1532-0464
DO  - http://dx.doi.org/10.1016/j.jbi.2015.04.007
UR  - http://www.sciencedirect.com/science/article/pii/S153204641500074X
KW  - Implantable medical devices
KW  - Security
KW  - Privacy
KW  - m-Health
KW  - Survey
AB  - Abstract
Bioengineering is a field in expansion. New technologies are appearing to provide a more efficient treatment of diseases or human deficiencies. Implantable Medical Devices (IMDs) constitute one example, these being devices with more computing, decision making and communication capabilities. Several research works in the computer security field have identified serious security and privacy risks in IMDs that could compromise the implant and even the health of the patient who carries it. This article surveys the main security goals for the next generation of IMDs and analyzes the most relevant protection mechanisms proposed so far. On the one hand, the security proposals must have into consideration the inherent constraints of these small and implanted devices: energy, storage and computing power. On the other hand, proposed solutions must achieve an adequate balance between the safety of the patient and the security level offered, with the battery lifetime being another critical parameter in the design phase.
ER  - 

TY  - JOUR
T1  - Information security incident management: Current practice as reported in the literature
JO  - Computers & Security
VL  - 45
IS  - 
SP  - 42
EP  - 57
PY  - 2014/9//
T2  - 
AU  - Tøndel, Inger Anne
AU  - Line, Maria B.
AU  - Jaatun, Martin Gilje
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/j.cose.2014.05.003
UR  - http://www.sciencedirect.com/science/article/pii/S0167404814000819
KW  - Information security
KW  - Incident management
KW  - Incident response
KW  - ISO/IEC 27035
KW  - Systematic review
AB  - Abstract
This paper reports results of a systematic literature review on current practice and experiences with incident management, covering a wide variety of organisations. Identified practices are summarised according to the incident management phases of ISO/IEC 27035. The study shows that current practice and experience seem to be in line with the standard. We identify some inspirational examples that will be useful for organisations looking to improve their practices, and highlight which recommended practices generally are challenging to follow. We provide suggestions for addressing the challenges, and present identified research needs within information security incident management.
ER  - 

TY  - JOUR
T1  - Towards a forensic-aware database solution: Using a secured database replication protocol and transaction management for digital investigations
JO  - Digital Investigation
VL  - 11
IS  - 4
SP  - 336
EP  - 348
PY  - 2014/12//
T2  - 
AU  - Frühwirt, Peter
AU  - Kieseberg, Peter
AU  - Krombholz, Katharina
AU  - Weippl, Edgar
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2014.09.003
UR  - http://www.sciencedirect.com/science/article/pii/S1742287614001078
KW  - MySQL
KW  - InnoDB
KW  - Digital forensics
KW  - Databases
KW  - Data tempering
KW  - Replication
KW  - Transaction management
AB  - Abstract
Databases contain an enormous amount of structured data. While the use of forensic analysis on the file system level for creating (partial) timelines, recovering deleted data and revealing concealed activities is very popular and multiple forensic toolsets exist, the systematic analysis of database management systems has only recently begun. Databases contain a large amount of temporary data files and metadata which are used by internal mechanisms. These data structures are maintained in order to ensure transaction authenticity, to perform rollbacks, or to set back the database to a predefined earlier state in case of e.g. an inconsistent state or a hardware failure. However, these data structures are intended to be used by the internal system methods only and are in general not human-readable.

In this work we present a novel approach for a forensic-aware database management system using transaction- and replication sources. We use these internal data structures as a vital baseline to reconstruct evidence during a forensic investigation. The overall benefit of our method is that no additional logs (such as administrator logs) are needed. Furthermore, our approach is invariant to retroactive malicious modifications by an attacker. This assures the authenticity of the evidence and strengthens the chain of custody. To evaluate our approach, we present a formal description, a prototype implementation in MySQL alongside and a comprehensive security evaluation with respect to the most relevant attack scenarios.
ER  - 

TY  - JOUR
T1  - A survey of information security incident handling in the cloud
JO  - Computers & Security
VL  - 49
IS  - 
SP  - 45
EP  - 69
PY  - 2015/3//
T2  - 
AU  - Ab Rahman, Nurul Hidayah
AU  - Choo, Kim-Kwang Raymond
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/j.cose.2014.11.006
UR  - http://www.sciencedirect.com/science/article/pii/S0167404814001680
KW  - Capability Maturity Model For Services (CMMI-SVC)
KW  - Cloud computing
KW  - Cloud response
KW  - Incident handling
KW  - Incident management
KW  - Incident response
AB  - Abstract
Incident handling strategy is one key strategy to mitigate risks to the confidentiality, integrity and availability (CIA) of organisation assets, as well as minimising loss (e.g. financial, reputational and legal) particularly as organisations move to the cloud. In this paper, we surveyed existing incident handling and digital forensic literature with the aims of contributing to the knowledge gap(s) in handling incidents in the cloud environment. 139 English language publications between January 2009 and May 2014 were located by searching various sources including the websites of standard bodies (e.g. National Institute of Standards and Technology) and academic databases (e.g. Google Scholar, IEEEXplore, ACM Digital Library, Springer and ScienceDirect). We then propose a conceptual cloud incident handling model that brings together incident handling, digital forensic and the Capability Maturity Model for Services to more effectively handle incidents for organisations using the cloud. A discussion of open research issues concludes this survey.
ER  - 

TY  - JOUR
T1  - Standardising business application security assessments with pattern-driven audit automations
JO  - Computer Standards & Interfaces
VL  - 30
IS  - 4
SP  - 262
EP  - 270
PY  - 2008/5//
T2  - Special Issue on Frameworks for Secure, Forensically Safe and Auditable Applications
AU  - Tryfonas, Theodore
AU  - Kearney, Bob
SN  - 0920-5489
DO  - http://dx.doi.org/10.1016/j.csi.2007.10.002
UR  - http://www.sciencedirect.com/science/article/pii/S0920548907001006
KW  - Audit automation standardisation
KW  - Security design patterns
KW  - Integrated business applications
KW  - ISO 27002
AB  - In the light of recent corporate corruption scandals the requirement for Corporate Governance and Responsibility has emerged as a top management priority, as reflected on the recent regulatory environment and compliance requirements e.g. Sarbanes–Oxley Act. The need for explicitly demonstrated assurance of the financial and accounting information in an IT-fuelled business environment has shifted interest to the information and the IT systems themselves. Assurance of information is based on the art and science of IT audit, a set of recurring tasks by nature both in time and in space. In environments of integrated business applications and enterprise resource planning systems, auditing is particularly laborious and the requirement for automation of auditing tasks was never more demanding. The belief that audit automation is part of the means to achieve governance is developing amongst scholars and practitioners alike. However there is no common understanding yet developed as of how such automation could be achieved across different systems and applications. We argue that through appropriate standardisation of the automation requirements such cross-system implementation may be possible and we propose as a means of standardisation the use of security design patterns. In this paper we explore the use of security patterns for audit automation and we implement them as a means of supporting its standardisation within integrated business application systems.
ER  - 

TY  - JOUR
T1  - The importance of log files in security incident prevention
JO  - Network Security
VL  - 2009
IS  - 7
SP  - 18
EP  - 20
PY  - 2009/7//
T2  - 
AU  - Forte, Dario
SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/S1353-4858(09)70090-X
UR  - http://www.sciencedirect.com/science/article/pii/S135348580970090X
AB  - There are currently at least five standards and five regulations that require log file management as part of security systems for every single continent. Well-established security standards and more recent ones regarding auditing all imply log file management, a practice that is also written into a number of national and international laws and directives.
ER  - 

TY  - JOUR
T1  - A survey of cyber security management in industrial control systems
JO  - International Journal of Critical Infrastructure Protection
VL  - 9
IS  - 
SP  - 52
EP  - 80
PY  - 2015/6//
T2  - 
AU  - Knowles, William
AU  - Prince, Daniel
AU  - Hutchison, David
AU  - Disso, Jules Ferdinand Pagna
AU  - Jones, Kevin
SN  - 1874-5482
DO  - http://dx.doi.org/10.1016/j.ijcip.2015.02.002
UR  - http://www.sciencedirect.com/science/article/pii/S1874548215000207
KW  - Industrial control systems
KW  - SCADA systems
KW  - Risk assessment
KW  - Risk management
KW  - Security metrics
KW  - Risk metrics
AB  - Abstract
Contemporary industrial control systems no longer operate in isolation, but use other networks (e.g., corporate networks and the Internet) to facilitate and improve business processes. The consequence of this development is the increased exposure to cyber threats. This paper surveys the latest methodologies and research for measuring and managing this risk. A dearth of industrial-control-system-specific security metrics has been identified as a barrier to implementing these methodologies. Consequently, an agenda for future research on industrial control system security metrics is outlined. The “functional assurance” concept is also introduced to deal with fail-safe and fail-secure industrial control system operations.
ER  - 

TY  - JOUR
T1  - PCI DSS audit and compliance
JO  - Information Security Technical Report
VL  - 15
IS  - 4
SP  - 138
EP  - 144
PY  - 2010/11//
T2  - Matchmaking between PCI-DSS and Security
AU  - Ataya, Georges
SN  - 1363-4127
DO  - http://dx.doi.org/10.1016/j.istr.2011.02.004
UR  - http://www.sciencedirect.com/science/article/pii/S136341271100015X
KW  - PCI_DSS audit compliance
KW  - Information security management
KW  - Data protection
AB  - PCI DSS compliance involves responding to a series of requirements imposed by the credit card industry. To succeed, organisation must implement strict information security management processes and should master the risks related to the protection of credit card sensitive data. There are many actions that could be accomplished before hand to ease the audit process, to reduce the effort and time consumed by the audit engagement and to ensure audit conclusions reflect the exact risk posture of the organisation.
ER  - 

TY  - JOUR
T1  - The “Art” of log correlation: Tools and Techniques for Correlating Events and Log Files
JO  - Computer Fraud & Security
VL  - 2004
IS  - 8
SP  - 15
EP  - 17
PY  - 2004/8//
T2  - 
AU  - Forte, Dario Valentino
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(04)00101-0
UR  - http://www.sciencedirect.com/science/article/pii/S1361372304001010
AB  - This is the last article in a three part series on log correlation. Log file correlation is related to two distinct activities: Intrusion Detection and Network Forensics. It is more important than ever that these two disciplines work together in a mutualistic relationship in order to avoid Points of Failure. This paper, intended as a tutorial for those dealing with such issues, presents an overview of log analysis and correlation, with special emphasis on the tools and techniques for managing them within a network forensics context.
ER  - 

TY  - JOUR
T1  - Forensic discovery auditing of digital evidence containers
JO  - Digital Investigation
VL  - 4
IS  - 2
SP  - 88
EP  - 97
PY  - 2007/6//
T2  - 
AU  - Richard III, Golden G.
AU  - Roussev, Vassil
AU  - Marziale, Lodovico
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2007.04.002
UR  - http://www.sciencedirect.com/science/article/pii/S1742287607000291
KW  - Digital forensics
KW  - Operating systems internals
KW  - Filesystems
KW  - Digital evidence containers Auditing
AB  - Current digital forensics methods capture, preserve, and analyze digital evidence in general-purpose electronic containers (typically, plain files) with no dedicated support to help establish that the evidence has been properly handled. Auditing of a digital investigation, from identification and seizure of evidence through duplication and investigation is, essentially, ad hoc, recorded in separate log files or in an investigator's case notebook. Auditing performed in this fashion is bound to be incomplete, because different tools provide widely disparate amounts of auditing information – including none at all – and there is ample room for human error. The latter is a particularly pressing concern given the fast growth of the size of forensic targets.

Recently, there has been a serious community effort to develop an open standard for specialized digital evidence containers (DECs). A DEC differs from a general purpose container in that, in addition to the actual evidence, it bundles arbitrary metadata associated with it, such as logs and notes, and provides the basic means to detect evidence-tampering through digital signatures. Current approaches consist of defining a container format and providing a specialized library that can be used to manipulate it. While a big step in the right direction, this approach has some non-trivial shortcomings – it requires the retooling of existing forensic software and, thereby, limits the number of tools available to the investigator. More importantly, however, it does not provide a complete solution since it only records snapshots of the state of the DEC without being able to provide a trusted log of all data operations actually performed on the evidence. Without a trusted log the question of whether a tool worked exactly as advertised cannot be answered with certainty, which opens the door to challenges (both legitimate and frivolous) of the results.

In this paper, we propose a complementary mechanism, called the Forensic Discovery Auditing Module (FDAM), aimed at closing this loophole in the discovery process. FDAM can be thought of as a ‘clean-room’ environment for the manipulation of digital evidence, where evidence from containers is placed for controlled manipulation. It functions as an operating system component, which monitors and logs all access to the evidence and enforces policy restrictions. This allows the immediate, safe, and verifiable use of any tool deemed necessary by the examiner. In addition, the module can provide transparent support for multiple DEC formats, thereby greatly simplifying the adoption of open standards.
ER  - 

TY  - JOUR
T1  - Secured Temporal Log Management Techniques for Cloud
JO  - Procedia Computer Science
VL  - 46
IS  - 
SP  - 589
EP  - 595
PY  - 2015///
T2  - Proceedings of the International Conference on Information and Communication Technologies, ICICT 2014, 3-5 December 2014 at Bolgatty Palace &amp; Island Resort, Kochi, India
AU  - Muthurajkumar, S.
AU  - Ganapathy, S.
AU  - Vijayalakshmi, M.
AU  - Kannan, A.
SN  - 1877-0509
DO  - http://dx.doi.org/10.1016/j.procs.2015.02.098
UR  - http://www.sciencedirect.com/science/article/pii/S1877050915001623
KW  - Cloud computing
KW  - Log management
KW  - Cloud Storage
KW  - Cloud Security;
AB  - Abstract
Log Management has been an important service in Cloud Computing. In any business, maintaining the log records securely over a particular period of time is absolutely necessary for various reasons such as auditing, forensic analysis, evidence etc. In this work, Integrity and confidentiality of the log records are maintained at every stage of Log Management namely the Log Generation phase, Transmission phase and Storage phase. In addition to this, Log records may often contain sensitive information about the organization which should not be leaked to the outside world. In this paper, Temporal Secured Cloud Log Management Algorithm techniques are implemented to provide security to maintain transaction history in cloud within time period. In this work, security to temporal log management is provided by encrypting the log data before they are stored in the cloud storage. They are also stored in batches for easy retrieval. This work was implemented in Java programming language in the Google drive environment.
ER  - 

TY  - JOUR
T1  - LINCS: Towards building a trustworthy litigation hold enabled cloud storage system
JO  - Digital Investigation
VL  - 14, Supplement 1
IS  - 
SP  - S55
EP  - S67
PY  - 2015/8//
T2  - The Proceedings of the Fifteenth Annual DFRWS Conference
AU  - Zawoad, Shams
AU  - Hasan, Ragib
AU  - Grimes, John
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2015.05.014
UR  - http://www.sciencedirect.com/science/article/pii/S1742287615000596
KW  - Litigation hold
KW  - Cloud security
KW  - Cloud forensics
KW  - Storage security
KW  - Regulatory compliance
AB  - Abstract
Litigation holds are inevitable parts of modern civil lawsuits that mandate an organization to preserve all forms of documents related to a lawsuit. In current data storage models, this includes documents stored in clouds. However, due to the fundamental natures of today's clouds, incorporating a trustworthy litigation hold management system is very challenging. To make the situation more complicated, defendants or plaintiffs may collude with the cloud service provider (CSP) to manipulate the documents under the hold. Serious consequences can follow if a litigant party fails to comply with the litigation hold for evidence stored in the cloud, resulting in legal sanctions for spoliation. This will not only harm the reputation of an organization but also levy of sanctions, such as fines, penalties, etc.

In this paper, we define a model of trustworthy litigation hold management for cloud-based storage systems and identify the key security properties. Based on the model, we propose a trustworthy LIitigation hold eNabled Cloud Storage (LINCS) system. We show that LINCS can provide the required security properties in a strong adversarial scenario, where a plaintiff or defendant colludes with a malicious CSP. Our prototype implementation reveals that the performance overhead of using LINCS is very low (average 1.4% for the user), which suggests that such litigation hold enabled storage system can be integrated with real clouds.
ER  - 

TY  - JOUR
T1  - Analyzing the security of Windows 7 and Linux for cloud computing
JO  - Computers & Security
VL  - 34
IS  - 
SP  - 113
EP  - 122
PY  - 2013/5//
T2  - 
AU  - Salah, Khaled
AU  - Alcaraz Calero, Jose M.
AU  - Bernabé, Jorge Bernal
AU  - Marín Perez, Juan M.
AU  - Zeadally, Sherali
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/j.cose.2012.12.001
UR  - http://www.sciencedirect.com/science/article/pii/S0167404812001800
KW  - Cloud computing
KW  - Operating system security
KW  - Privacy
KW  - Security
KW  - Authorization
KW  - Authentication
AB  - We review and analyze the major security features and concerns in deploying modern commodity operating systems such as Windows 7 and Linux 2.6.38 in a cloud computing environment. We identify the security weaknesses and open challenges of these two operating systems when deployed in the cloud environment. In particular, we examine and compare various operating system security features which are critical in providing a secure cloud. These security features include authentication, authorization and access control, physical memory protection, privacy and encryption of stored data, network access and firewalling capabilities, and virtual memory.
ER  - 

TY  - JOUR
T1  - Comparing image search behaviour in the ARRS GoldMiner search engine and a clinical PACS/RIS
JO  - Journal of Biomedical Informatics
VL  - 56
IS  - 
SP  - 57
EP  - 64
PY  - 2015/8//
T2  - 
AU  - De-Arteaga, Maria
AU  - Eggel, Ivan
AU  - Do, Bao
AU  - Rubin, Daniel
AU  - Kahn Jr., Charles E.
AU  - Müller, Henning
SN  - 1532-0464
DO  - http://dx.doi.org/10.1016/j.jbi.2015.04.013
UR  - http://www.sciencedirect.com/science/article/pii/S1532046415000878
KW  - Medical image search
KW  - Log file analysis
KW  - User behaviour
KW  - Information retrieval
AB  - Abstract
Information search has changed the way we manage knowledge and the ubiquity of information access has made search a frequent activity, whether via Internet search engines or increasingly via mobile devices. Medical information search is in this respect no different and much research has been devoted to analyzing the way in which physicians aim to access information. Medical image search is a much smaller domain but has gained much attention as it has different characteristics than search for text documents. While web search log files have been analysed many times to better understand user behaviour, the log files of hospital internal systems for search in a PACS/RIS (Picture Archival and Communication System, Radiology Information System) have rarely been analysed. Such a comparison between a hospital PACS/RIS search and a web system for searching images of the biomedical literature is the goal of this paper. Objectives are to identify similarities and differences in search behaviour of the two systems, which could then be used to optimize existing systems and build new search engines.

Log files of the ARRS GoldMiner medical image search engine (freely accessible on the Internet) containing 222,005 queries, and log files of Stanford’s internal PACS/RIS search called radTF containing 18,068 queries were analysed. Each query was preprocessed and all query terms were mapped to the RadLex (Radiology Lexicon) terminology, a comprehensive lexicon of radiology terms created and maintained by the Radiological Society of North America, so the semantic content in the queries and the links between terms could be analysed, and synonyms for the same concept could be detected. RadLex was mainly created for the use in radiology reports, to aid structured reporting and the preparation of educational material (Lanlotz, 2006) [1]. In standard medical vocabularies such as MeSH (Medical Subject Headings) and UMLS (Unified Medical Language System) specific terms of radiology are often underrepresented, therefore RadLex was considered to be the best option for this task.

The results show a surprising similarity between the usage behaviour in the two systems, but several subtle differences can also be noted. The average number of terms per query is 2.21 for GoldMiner and 2.07 for radTF, the used axes of RadLex (anatomy, pathology, findings, …) have almost the same distribution with clinical findings being the most frequent and the anatomical entity the second; also, combinations of RadLex axes are extremely similar between the two systems. Differences include a longer length of the sessions in radTF than in GoldMiner (3.4 and 1.9 queries per session on average). Several frequent search terms overlap but some strong differences exist in the details. In radTF the term “normal” is frequent, whereas in GoldMiner it is not. This makes intuitive sense, as in the literature normal cases are rarely described whereas in clinical work the comparison with normal cases is often a first step.

The general similarity in many points is likely due to the fact that users of the two systems are influenced by their daily behaviour in using standard web search engines and follow this behaviour in their professional search. This means that many results and insights gained from standard web search can likely be transferred to more specialized search systems. Still, specialized log files can be used to find out more on reformulations and detailed strategies of users to find the right content.
ER  - 

TY  - JOUR
T1  - Challenges ahead on the digital forensics and audit trails
JO  - Network Security
VL  - 2014
IS  - 6
SP  - 12
EP  - 17
PY  - 2014/6//
T2  - 
AU  - Gold, Steve
SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/S1353-4858(14)70060-1
UR  - http://www.sciencedirect.com/science/article/pii/S1353485814700601
AB  - Digital forensics is a relatively new science in the increasing complex field of IT security, data governance and auditing. This was why the Forensic Science Society recently held its inaugural conference in York on the topic – at which a range of speakers drawn from the rapidly-emerging industry gave a series of presentations – along with a discussion of some of the key issues that affecting IT security professionals at the sharp end.

Digital forensics is a relatively new science in the increasingly complex field of IT security, data governance and auditing. This was why the Forensic Science Society recently held its inaugural conference in York.

A range of speakers drawn from this rapidly-emerging industry revealed a lot about the challenges that face even senior and well-experienced IT security professionals, largely as a result of the rapid pace of evolution in the security ecosphere. Steve Gold on the issues raised at the event
ER  - 

TY  - JOUR
T1  - The 2012 CLSR-LSPI seminar on privacy, data protection &amp; cyber-security – Presented at the 7th international conference on Legal, Security and Privacy Issues in IT law (LSPI) October 2–4, 2012, Athens
JO  - Computer Law & Security Review
VL  - 29
IS  - 1
SP  - 4
EP  - 12
PY  - 2013/2//
T2  - 
AU  - Saxby, Steve
SN  - 0267-3649
DO  - http://dx.doi.org/10.1016/j.clsr.2012.11.007
UR  - http://www.sciencedirect.com/science/article/pii/S0267364912002038
KW  - Cyber-privacy
KW  - Data security
KW  - Data abuse
KW  - Function-creep
KW  - Privacy regulation
KW  - Surveillance society
KW  - Privacy by design
KW  - Social media privacy
KW  - Cyber-crime
AB  - This has been a big year for privacy with so much going on within the EU regarding reform of data protection. What are the implications of reform here and what are the issues that concern us about the proposed new data protection regime contained in the proposed Regulation? We hear a lot about the ‘right to be forgotten’. How is that possible in the digital age within the online world? And what can be done about the big players who stand charged with the erosion of privacy viz Facebook, Google, Skype &amp; YouTube etc? How can the law keep up with technological change when the latter is moving so fast e.g. with RFID, Cloud and social networking? To what extent can data breach notification, net neutrality and privacy impact assessment help and how should the law approach issues of liability and criminality in relation to privacy? What is the state of play too in the relationship between privacy policy and state surveillance and, given its implications for privacy, what obligations should governments adopt in response to cybersecurity regulation and data management? Is there a place for privacy self-regulation and if so in what respects and how effective are the Information Commissioners who often complain of being under resourced? In reviewing the way privacy law has emerged do we now need a completely new approach to the whole issue? Has the law crept into its present form simply by default? Do we need some new thinking now that reflects the fact that law is only one dimension in the battle for privacy? If so what are the other factors we need to recognise?
ER  - 

TY  - JOUR
T1  - SCADA security in the light of Cyber-Warfare
JO  - Computers & Security
VL  - 31
IS  - 4
SP  - 418
EP  - 436
PY  - 2012/6//
T2  - 
AU  - Nicholson, A.
AU  - Webber, S.
AU  - Dyer, S.
AU  - Patel, T.
AU  - Janicke, H.
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/j.cose.2012.02.009
UR  - http://www.sciencedirect.com/science/article/pii/S0167404812000429
KW  - SCADA
KW  - Security
KW  - Cyber-Warfare
KW  - Adversary classification
KW  - SCADA incidents
AB  - Supervisory Control and Data Acquisition (SCADA) systems are deployed worldwide in many critical infrastructures ranging from power generation, over public transport to industrial manufacturing systems. Whilst contemporary research has identified the need for protecting SCADA systems, these information are disparate and do not provide a coherent view of the threats and the risks resulting from the tendency to integrate these once isolated systems into corporate networks that are prone to cyber attacks. This paper surveys ongoing research and provides a coherent overview of the threats, risks and mitigation strategies in the area of SCADA security.
ER  - 

TY  - JOUR
T1  - Continuous auditing technologies and models: A discussion
JO  - Computers & Security
VL  - 25
IS  - 5
SP  - 325
EP  - 331
PY  - 2006/7//
T2  - 
AU  - Flowerday, S.
AU  - Blundell, A.W.
AU  - Von Solms, R.
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/j.cose.2006.06.004
UR  - http://www.sciencedirect.com/science/article/pii/S0167404806000964
KW  - Continuous auditing
KW  - Real-time assurances
KW  - Information integrity
KW  - Internal controls
KW  - Technology-based prevention
AB  - In the age of real-time accounting and real-time communication current audit practices, while effective, often provide audit results long after fraud and/or errors have occurred. Real-time assurances can assist in preventing intentional or unintentional errors. This can best be achieved through continuous auditing which relies heavily on technology. These technologies are embedded within and are crucial to continuous auditing models.
ER  - 

TY  - JOUR
T1  - Three cyber-security strategies to mitigate the impact of a data breach
JO  - Network Security
VL  - 2015
IS  - 1
SP  - 5
EP  - 8
PY  - 2015/1//
T2  - 
AU  - Densham, Ben
SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/S1353-4858(15)70007-3
UR  - http://www.sciencedirect.com/science/article/pii/S1353485815700073
AB  - We know that our mindsets need to shift these days and we must start by expecting to be hacked. But what then? How do we really manage an effective, secure environment? What steps do we need to consider?

The most common approach to keeping organisations safe is through layers of security. Yet frequent high-profile attacks show that this approach is no longer enough to ward off sophisticated threats.

Organisations must now alter their mindsets to expect a breach. It's better to assume your systems will be compromised. And with this in mind, they must have a ‘response in depth’ plan in order to mitigate the breach when it comes. Ben Densham of Nettitude provides a step-by-step guide to implementing an effective cyber-security strategy to ensure a potential disaster can be turned into a manageable incident.
ER  - 

TY  - JOUR
T1  - Information fusion for computer security: State of the art and open issues
JO  - Information Fusion
VL  - 10
IS  - 4
SP  - 274
EP  - 284
PY  - 2009/10//
T2  - Special Issue on Information Fusion in Computer Security
AU  - Corona, Igino
AU  - Giacinto, Giorgio
AU  - Mazzariello, Claudio
AU  - Roli, Fabio
AU  - Sansone, Carlo
SN  - 1566-2535
DO  - http://dx.doi.org/10.1016/j.inffus.2009.03.001
UR  - http://www.sciencedirect.com/science/article/pii/S156625350900030X
KW  - Computer security
KW  - Information fusion
AB  - In this paper, we critically review the issue of information fusion for computer security, both in terms of problem formulation and in terms of state-of-the-art solutions. We also analyze main strengths and weaknesses of currently used approaches and propose some research issues that should be investigated in the future.
ER  - 

TY  - JOUR
T1  - Use patterns of health information exchange through a multidimensional lens: Conceptual framework and empirical validation
JO  - Journal of Biomedical Informatics
VL  - 52
IS  - 
SP  - 212
EP  - 221
PY  - 2014/12//
T2  - Special Section: Methods in Clinical Research Informatics
AU  - Politi, Liran
AU  - Codish, Shlomi
AU  - Sagy, Iftach
AU  - Fink, Lior
SN  - 1532-0464
DO  - http://dx.doi.org/10.1016/j.jbi.2014.07.003
UR  - http://www.sciencedirect.com/science/article/pii/S1532046414001506
KW  - Use pattern
KW  - Health information exchange
KW  - Log file
KW  - Multidimensional analysis
AB  - Abstract
Insights about patterns of system use are often gained through the analysis of system log files, which record the actual behavior of users. In a clinical context, however, few attempts have been made to typify system use through log file analysis. The present study offers a framework for identifying, describing, and discerning among patterns of use of a clinical information retrieval system. We use the session attributes of volume, diversity, granularity, duration, and content to define a multidimensional space in which each specific session can be positioned. We also describe an analytical method for identifying the common archetypes of system use in this multidimensional space. We demonstrate the value of the proposed framework with a log file of the use of a health information exchange (HIE) system by physicians in an emergency department (ED) of a large Israeli hospital. The analysis reveals five distinct patterns of system use, which have yet to be described in the relevant literature. The results of this study have the potential to inform the design of HIE systems for efficient and effective use, thus increasing their contribution to the clinical decision-making process.
ER  - 

TY  - JOUR
T1  - Combating advanced persistent threats: From network event correlation to incident detection
JO  - Computers & Security
VL  - 48
IS  - 
SP  - 35
EP  - 57
PY  - 2015/2//
T2  - 
AU  - Friedberg, Ivo
AU  - Skopik, Florian
AU  - Settanni, Giuseppe
AU  - Fiedler, Roman
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/j.cose.2014.09.006
UR  - http://www.sciencedirect.com/science/article/pii/S0167404814001461
KW  - Advanced persistent threat
KW  - Anomaly detection
KW  - Log file analysis
KW  - Intrusion detection
KW  - Event correlation
KW  - Self-learning system model
AB  - Abstract
An advanced persistent threat (also known as APT) is a deliberately slow-moving cyberattack that is applied to quietly compromise interconnected information systems without revealing itself. APTs often use a variety of attack methods to get unauthorized system access initially and then gradually spread throughout the network. In contrast to traditional attacks, they are not used to interrupt services but primarily to steal intellectual property, sensitive internal business and legal documents and other data. If an attack on a system is successful, timely detection is of paramount importance to mitigate its impact and prohibit APTs from further spreading. However, recent security incidents, such as Operation Shady Rat, Operation Red October or the discovery of MiniDuke – just to name a few – have impressively demonstrated that current security mechanisms are mostly insufficient to prohibit targeted and customized attacks. This paper therefore proposes a novel anomaly detection approach which is a promising basis for modern intrusion detection systems. In contrast to other common approaches, which apply a kind of black-list approach and consider only actions and behaviour that match to well-known attack patterns and signatures of malware traces, our system works with a white-list approach. Our anomaly detection technique keeps track of system events, their dependencies and occurrences, and thus learns the normal system behaviour over time and reports all actions that differ from the created system model. In this work, we describe this system in theory and show evaluation results from a pilot study under real-world conditions.
ER  - 

TY  - JOUR
T1  - Teaching information systems security courses: A hands-on approach
JO  - Computers & Security
VL  - 26
IS  - 4
SP  - 290
EP  - 299
PY  - 2007/6//
T2  - 
AU  - Sharma, Sushil K.
AU  - Sefchek, Joshua
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/j.cose.2006.11.005
UR  - http://www.sciencedirect.com/science/article/pii/S0167404806002045
KW  - Computer security
KW  - Education
KW  - Hands-on
KW  - Information systems security
KW  - Information assurance
AB  - It has become imperative for companies, governments, and organizations to understand how to guard against hackers, outsiders, and even disgruntled employees who threaten their information security, integrity and daily business operations. To address national needs for computer security education, many universities have incorporated computer and security courses into their undergraduate and graduate curricula. At the Miller College of Business, Department of Information Systems and Operations Management, Ball State University, we have introduced an information systems security option for students majoring in information systems. This paper describes our approach, our experiences and lessons learned for teaching security courses using a hands-on approach.
ER  - 

TY  - JOUR
T1  - Web software fault prediction under fuzzy environment using MODULO-M multivariate overlapping fuzzy clustering algorithm and newly proposed revised prediction algorithm
JO  - Applied Soft Computing
VL  - 22
IS  - 
SP  - 372
EP  - 396
PY  - 2014/9//
T2  - 
AU  - Chatterjee, Subhashish
AU  - Roy, Arunava
SN  - 1568-4946
DO  - http://dx.doi.org/10.1016/j.asoc.2014.03.030
UR  - http://www.sciencedirect.com/science/article/pii/S1568494614001379
KW  - Web software reliability
KW  - Fuzzy clustering
KW  - Fuzzy time series
KW  - Server logs
KW  - Web errors
KW  - Algorithm
AB  - Abstract
In recent years some research works have been carried out on web software error analysis and reliability predictions. In all these works the web environment has been considered as crisp one, which is not a very realistic assumption. Moreover, web error forecasting remains unworthy for the researchers for quite a long time. Furthermore, among various well known forecasting techniques, fuzzy time series based methods are extensively used, though they are suffering from some serious drawbacks, viz., fixed sized intervals, using some fixed membership values (0, 0.5, and 1) and moreover, the defuzzification process only deals with the factor that is to be predicted. Prompted by these facts, the present authors have proposed a novel multivariate fuzzy forecasting algorithm that is able to remove all the aforementioned drawbacks as also can predict the future occurrences of different web failures (considering the web environment as fuzzy) with better predictive accuracy. Also, the complexity analysis of the proposed algorithm is done to unveil its better run time complexity. Moreover, the comparisons with the other existing frequently used forecasting algorithms were performed to demonstrate its better efficiency and predictive accuracy. Additionally, at the very end, the developed algorithm was applied on the real web failure data of http://www.ismdhanbad.ac.in/, the official website of ISM Dhanbad, India, collected from the corresponding HTTP log files.
ER  - 

TY  - JOUR
T1  - Privacy-preserving network flow recording
JO  - Digital Investigation
VL  - 8, Supplement
IS  - 
SP  - S90
EP  - S100
PY  - 2011/8//
T2  - 11th Annual Digital Forensics Research Conference
AU  - Shebaro, Bilal
AU  - Crandall, Jedidiah R.
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2011.05.011
UR  - http://www.sciencedirect.com/science/article/pii/S1742287611000351
KW  - NetFlow
KW  - Network forensics
KW  - Identity based encryption
KW  - Privacy preserving semantics
KW  - Statistical database
AB  - Network flow recording is an important tool with applications that range from legal compliance and security auditing to network forensics, troubleshooting, and marketing. Unfortunately, current network flow recording technologies do not allow network operators to enforce a privacy policy on the data that is recorded, in particular how this data is stored and used within the organization. Challenges to building such a technology include the public key infrastructure, scalability, and gathering statistics about the data while still preserving privacy.

We present a network flow recording technology that addresses these challenges by using Identity Based Encryption in combination with privacy-preserving semantics for on-the-fly statistics. We argue that our implementation supports a wide range of policies that cover many current applications of network flow recording. We also characterize the performance and scalability of our implementation and find that the encryption and statistics scale well and can easily keep up with the rate at which commodity systems can capture traffic, with a couple of interesting caveats about the size of the subnet that data is being recorded for and how statistics generation is affected by implementation details. We conclude that privacy-preserving network flow recording is possible at 10 gigabit rates for subnets as large as a /20 (4096 hosts).

Because network flow recording is one of the most serious threats to web privacy today, we believe that developing technology to enforce a privacy policy on the recorded data is an important first step before policy makers can make decisions about how network operators can and should store and use network flow data. Our goal in this paper is to explore the tradeoffs of performance and scalability vs. privacy, and the usefulness of the recorded data in forensics vs. privacy.
ER  - 

TY  - JOUR
T1  - Distributed component architectures security issues
JO  - Computer Standards & Interfaces
VL  - 27
IS  - 3
SP  - 269
EP  - 284
PY  - 2005/3//
T2  - 
AU  - Gousios, Giorgos
AU  - Aivaloglou, Efthimia
AU  - Gritzalis, Stefanos
SN  - 0920-5489
DO  - http://dx.doi.org/10.1016/j.csi.2004.08.003
UR  - http://www.sciencedirect.com/science/article/pii/S0920548904000984
KW  - Components
KW  - Component architectures security
KW  - CORBA
KW  - J2EE
KW  - .NET
AB  - Enterprise information systems and e-commerce applications are tightly integrated in today's modern enterprises. Component architectures are the base for building such multitier distributed applications. This paper examines the security threats those systems must confront and the solutions proposed by major existing component architectures. A comparative evaluation of both security features and implementation issues is carried out to determine each architecture's strong points and drawbacks.
ER  - 

TY  - JOUR
T1  - NetHost-sensor: Monitoring a target host's application via system calls
JO  - Information Security Technical Report
VL  - 11
IS  - 4
SP  - 166
EP  - 175
PY  - 2006///
T2  - 
AU  - Abimbola, A.A.
AU  - Munoz, J.M.
AU  - Buchanan, W.J.
SN  - 1363-4127
DO  - http://dx.doi.org/10.1016/j.istr.2006.10.003
UR  - http://www.sciencedirect.com/science/article/pii/S1363412706000550
KW  - Intrusion detection
KW  - Network
KW  - Host
KW  - Application security
KW  - Dynamic link libraries
KW  - System calls
AB  - Intrusion detection has emerged as an important approach to network, host and application security. Network security includes analysing network packet payload and other inert network packet profiles for intrusive trends; whereas, host security may employ system logs for intrusion detection. In this paper, we contribute to the research community by tackling application security and attempt to detect intrusion via differentiating normal and abnormal application behaviour. A method for anomaly intrusion detection for applications is proposed based on deterministic system call traces derived from a monitored target application's dynamic link libraries (DLLs). We isolate associated DLLs of a monitored target application; log system call traces of the application in real time and use heuristic method to detect intrusion before the application is fully compromised. Our investigative research experiment methodology and set-up are reported, alongside our experimental procedure and results that show our research effort is effective and efficient, and can be used in practice to monitor a target application in real time.
ER  - 

TY  - JOUR
T1  - A pilot study of distributed knowledge management and clinical decision support in the cloud
JO  - Artificial Intelligence in Medicine
VL  - 59
IS  - 1
SP  - 45
EP  - 53
PY  - 2013/9//
T2  - Special Issue: Evaluation of Clinical Decision Support Systems of Health Care
AU  - Dixon, Brian E.
AU  - Simonaitis, Linas
AU  - Goldberg, Howard S.
AU  - Paterno, Marilyn D.
AU  - Schaeffer, Molly
AU  - Hongsermeier, Tonya
AU  - Wright, Adam
AU  - Middleton, Blackford
SN  - 0933-3657
DO  - http://dx.doi.org/10.1016/j.artmed.2013.03.004
UR  - http://www.sciencedirect.com/science/article/pii/S0933365713000389
KW  - Log file analysis
KW  - Qualitative analysis
KW  - Computer-Assisted Decision Making
KW  - Clinical decision support systems
KW  - Medical informatics
KW  - Preventive health services
KW  - Knowledge management
KW  - Information dissemination
AB  - AbstractObjective
Implement and perform pilot testing of web-based clinical decision support services using a novel framework for creating and managing clinical knowledge in a distributed fashion using the cloud. The pilot sought to (1) develop and test connectivity to an external clinical decision support (CDS) service, (2) assess the exchange of data to and knowledge from the external CDS service, and (3) capture lessons to guide expansion to more practice sites and users.
Materials and methods
The Clinical Decision Support Consortium created a repository of shared CDS knowledge for managing hypertension, diabetes, and coronary artery disease in a community cloud hosted by Partners HealthCare. A limited data set for primary care patients at a separate health system was securely transmitted to a CDS rules engine hosted in the cloud. Preventive care reminders triggered by the limited data set were returned for display to clinician end users for review and display. During a pilot study, we (1) monitored connectivity and system performance, (2) studied the exchange of data and decision support reminders between the two health systems, and (3) captured lessons.
Results
During the six month pilot study, there were 1339 patient encounters in which information was successfully exchanged. Preventive care reminders were displayed during 57% of patient visits, most often reminding physicians to monitor blood pressure for hypertensive patients (29%) and order eye exams for patients with diabetes (28%). Lessons learned were grouped into five themes: performance, governance, semantic interoperability, ongoing adjustments, and usability.
Discussion
Remote, asynchronous cloud-based decision support performed reasonably well, although issues concerning governance, semantic interoperability, and usability remain key challenges for successful adoption and use of cloud-based CDS that will require collaboration between biomedical informatics and computer science disciplines.
Conclusion
Decision support in the cloud is feasible and may be a reasonable path toward achieving better support of clinical decision-making across the widest range of health care providers.
ER  - 

TY  - JOUR
T1  - Assessing insider threats to information security using technical, behavioural and organisational measures
JO  - Information Security Technical Report
VL  - 15
IS  - 3
SP  - 112
EP  - 133
PY  - 2010/8//
T2  - Computer Crime - A 2011 Update
AU  - Roy Sarkar, Kuheli
SN  - 1363-4127
DO  - http://dx.doi.org/10.1016/j.istr.2010.11.002
UR  - http://www.sciencedirect.com/science/article/pii/S1363412710000488
AB  - The UK government took a bruising in the headlines (Sep 2008) after a Home Office contractor lost a USB stick containing unencrypted data on all 84,000 prisoners in England and Wales. As a result, the Home Office terminated the £1.5 million contract with the management consultancy firm.

The world woke up to the largest attempted bank fraud ever when the UK’s National Hi-Tech Crime Unit foiled the world’s largest potential bank robbery in March 2005. With the help of the security supervisor, thieves masquerading as cleaning staff installed hardware keystroke loggers on computers within the London branch of a Japanese bank, to steal £220m.

It is indeed sobering to imagine that any organisation could fall victim to such events and the damage an insider can do. The consulting firm lost the contract worth £1.5 million due to a small mistake by an employee. The London branch of the Japanese Bank would have lost £220 million had not the crime been foiled.

Insider threat is a reality. Insiders commit fraud or steal sensitive information when motivated by money or revenge. Well-meaning employees can compromise the security of an organisation with their overzealousness in getting their job done. Every organisation has a varied mix of employees, consultants, management, partners and complex infrastructure and that makes handling insider threats a daunting challenge. With insider attacks, organisations face potential damage through loss of revenue, loss of reputation, loss of intellectual property or even loss of human life.

The insider threat problem is more elusive and perplexing than any other threat. Assessing the insider threat is the first step to determine the likelihood of any insider attack. Technical solutions do not suffice since insider threats are fundamentally a people issue. Therefore, a three-pronged approach - technological, behavioural and organisational assessment is essential in facilitating the prediction of insider threats and pre-empt any insider attack thus improving the organization’s security, survivability, and resiliency in light of insider threats.
ER  - 

TY  - JOUR
T1  - Automating security events management
JO  - Network Security
VL  - 2008
IS  - 12
SP  - 6
EP  - 9
PY  - 2008/12//
T2  - 
AU  - Libeau, Fabian
SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/S1353-4858(08)70139-9
UR  - http://www.sciencedirect.com/science/article/pii/S1353485808701399
AB  - Millions of security events are generated daily from network devices, operating systems, applications, and even security solutions themselves. These events are typically siloed within files and databases or sent via syslog.

Much attention is paid within organisations to preventing security events, but some incidents will inevitably still occur. When this happens, organisations must be prepared with an events management strategy. The more that this can be automated, the better.
ER  - 

TY  - JOUR
T1  - Improving evidence acquisition from live network sources
JO  - Digital Investigation
VL  - 3
IS  - 2
SP  - 89
EP  - 96
PY  - 2006/6//
T2  - 
AU  - Nikkel, Bruce J.
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2006.05.002
UR  - http://www.sciencedirect.com/science/article/pii/S1742287606000363
KW  - Network forensics
KW  - Live network evidence
KW  - Live network acquisition
KW  - Live network forensics
KW  - NFAT
AB  - The pervasiveness of network technology is causing a shift in the location of digital evidence. What was once largely found on individual disks tied to single individuals is now becoming distributed across remote networked machines, under the control of multiple organizations, and scattered over multiple jurisdictions. The network interactions between these machines are also becoming recognized as a source of network evidence. These live network sources of evidence bring additional challenges which need to be addressed. This paper discusses these issues and suggests some improvements in the methods used for the collection of evidence from live network sources.
ER  - 

TY  - JOUR
T1  - Making sense of log management for security purposes – an approach to best practice log collection, analysis and management
JO  - Computer Fraud & Security
VL  - 2007
IS  - 5
SP  - 5
EP  - 10
PY  - 2007/5//
T2  - 
AU  - Gorge, Mathieu
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(07)70047-7
UR  - http://www.sciencedirect.com/science/article/pii/S1361372307700477
AB  - Every computer action registers a log somewhere – giving a rich source of data that can help businesses identify any trace of corruption within their networks. Log collection is also a strong component of keeping in line with legislation such as Sarbanes-Oxley, HIPAA, GLBA in the US, and the European Data Protection Directive in the EU. Mathieu Gorge looks at what logs organizations need to keep and what standards require their storage.

He recommends proactive monitoring of firewalls, anti-virus, VPNs and IDS logs among other security systems. The main goal is to link a transaction back to an individual user in order to perform a forensic investigation. But it is important to be wary, as some countries do not allow companies to monitor staff usage of IT systems. See page 3 on how the European court ruled that a British college's monitoring of one employee was a breach of human rights. Therefore, linking a log with a person's actions may not stand up in court.

Gorge says logs can give as good an insight into external attacks as well as internally driven ones.

Logs should be analyzed for the following:
				•
User account activity: creation, elevation of privilege, changes, inactivity.
•
Client requests and server response.
•
Operational status: shutdown (planned or unplanned), system failure and automatic restart.
•
Usage information and trends – basic user behaviour analysis.


It is best practice to collect, store and analyze logs with a view to being able to get complete, accurate and verifiable information. This will improve the organization's ability to comply with key standards and legislation as regards e-evidence. It could save an organization from potential liability and repair costs and will give visibility over mission critical and security systems, performance and usage. The main advice is to remain proactive so as to be able to respond to a security incident and comply with legal requests should anything happen.

Mathieu Gorge looks at what logs can do for your business and how governance demands them.
ER  - 

TY  - JOUR
T1  - DSS for computer security incident response applying CBR and collaborative response
JO  - Expert Systems with Applications
VL  - 37
IS  - 1
SP  - 852
EP  - 870
PY  - 2010/1//
T2  - 
AU  - Kim, Huy Kang
AU  - Im, Kwang Hyuk
AU  - Park, Sang Chan
SN  - 0957-4174
DO  - http://dx.doi.org/10.1016/j.eswa.2009.05.100
UR  - http://www.sciencedirect.com/science/article/pii/S0957417409005223
KW  - Log analysis
KW  - System security
KW  - Misuse detection
KW  - Anomaly detection
KW  - Decision support system
KW  - Expert system
KW  - RFM analysis methodology
KW  - CBR (case based reasoning)
AB  - Recently, as hacking attempts increase dramatically; most enterprises are forced to employ some safeguards for hacking proof. For example, firewall or IPS (Intrusion Prevention System) selectively accepts the incoming packets, and IDS (Intrusion Detection System) detects the attack attempts from network. The latest version of firewall works in cooperation with IDS to immediately response to hacking attempts. However, it may make false alarms that misjudge normal traffic as hacking traffic and cause network problems to block the normal IP address by false alarms. By these false alarms made by IDS, system administrators or CSOs make wrong decisions and important data may be exposed or the availability of network or server system may be exhausted. Therefore, it is important to minimize the false alarms.

As a way of minimizing false alarms and supporting adequate decisions, we suggest the RFM (Recency, Frequency, Monetary) analysis methodology, which analyzes log files with incorporating three criteria of recency, frequency and monetary with statistical process control chart, and thus leads to an intuitive detection of anomaly and misuse events. Moreover, to cope with hacking attempts proactively, we apply CBR (case based reasoning) to find out similarities between already known hacking patterns and new hacking patterns. With the RFM analysis methodology and CBR, we develop DSS which can minimize false alarms and decrease the time to respond to hacking events. In case that RFM analysis module finds out unknown viruses or worms occurred, this CBR system matches the most similar incident case from case-based database. System administrators can easily get information about how to fix and how we fixed in similar cases. And CSOs can build a blacklist of frequently detected IP addresses and users. This blacklist can be used for incident handling.

Finally, we propose collaborative incident response system with DSS, this distributed agent systems interactively exchange the suspicious users and source IP addresses data and decide who is true-anomalous users and which IP addresses is the most riskiest and then deny all connections from that users and IP addresses automatically with less false-positives.
ER  - 

TY  - JOUR
T1  - The times they are a'changing: trends that impact security strategy
JO  - Network Security
VL  - 2006
IS  - 5
SP  - 18
EP  - 19
PY  - 2006/5//
T2  - 
AU  - Potter, Bruce
SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/S1353-4858(06)70389-0
UR  - http://www.sciencedirect.com/science/article/pii/S1353485806703890
AB  - As network and security professionals, it is of course important to keep up with the day-to-day events in the security industry. Knowing the latest vulnerabilities, the newest vendor products, and critical standards updates are key piece of information for keeping your enterprise secure. However, equally important is the need to keep a strategic view of the security industry and know what trends are important to you and your systems.
ER  - 

TY  - JOUR
T1  - Differences between web sessions according to the origin of their visits
JO  - Journal of Informetrics
VL  - 4
IS  - 3
SP  - 331
EP  - 337
PY  - 2010/7//
T2  - 
AU  - Ortega, José Luis
AU  - Aguillo, Isidro
SN  - 1751-1577
DO  - http://dx.doi.org/10.1016/j.joi.2010.02.001
UR  - http://www.sciencedirect.com/science/article/pii/S1751157710000076
KW  - Webometrics
KW  - Web usage mining
KW  - Web session
KW  - Log file analysis
KW  - Search engines
KW  - Navigational patterns
AB  - The aim of this paper is to characterize the distribution of number of hits and spent time by web session. It also expects to find if there are significant differences between the length and the duration of a session with regard to the point of access–search engine, link or root. Web usage mining was used to analyse 17,174 web sessions that were identified from the webometrics.info web site. Results show that both distribution of length and duration follow an exponential decay. Significant differences between the different origins of the visits were also found, being the search engines’ users those who spent most time and did more clicks in their sessions. We conclude that a good SEO policy would be justified, because search engines are the principal intermediaries to this web site.
ER  - 

TY  - JOUR
T1  - Security views - Malware update
JO  - Computers & Security
VL  - 25
IS  - 1
SP  - 3
EP  - 12
PY  - 2006/2//
T2  - 

SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/j.cose.2005.12.007
UR  - http://www.sciencedirect.com/science/article/pii/S0167404805002075
ER  - 

TY  - JOUR
T1  - Windows device interface security
JO  - Information Security Technical Report
VL  - 11
IS  - 4
SP  - 160
EP  - 165
PY  - 2006///
T2  - 
AU  - Wolthusen, Stephen D.
SN  - 1363-4127
DO  - http://dx.doi.org/10.1016/j.istr.2006.10.001
UR  - http://www.sciencedirect.com/science/article/pii/S1363412706000537
AB  - This paper discusses both risks and mitigation strategies for risks and threats associated with physical device interfaces. To this end, a brief discussion of the I/O architecture found in the Microsoft Windows operating system is followed by a review of several classes of attacks possible using only external devices attached to standard device interfaces of host computers. Based on this analysis, a selection of possible countermeasures including the modification of the host operating system by wrapping the I/O mechanisms into a hardened protective layer is discussed.
ER  - 

TY  - JOUR
T1  - Balancing the Security Budget
JO  - Computer Fraud & Security
VL  - 2003
IS  - 10
SP  - 8
EP  - 11
PY  - 2003/10//
T2  - 
AU  - Pemble, Matthew
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(03)10007-3
UR  - http://www.sciencedirect.com/science/article/pii/S1361372303100073
AB  - Regardless of the prevailing economic climate and, especially when, as now, the outlook is reasonably poor, the budget and time allocated to information security are never going to be enough to allow you to do everything you want to (or think that you want to) do. It is therefore incumbent upon all of us to maximise the effectiveness of the way we spend the pittance our masters decree unto us.
ER  - 

TY  - JOUR
T1  - A framework for incident response management in the petroleum industry
JO  - International Journal of Critical Infrastructure Protection
VL  - 2
IS  - 1–2
SP  - 26
EP  - 37
PY  - 2009/5//
T2  - 
AU  - Jaatun, Martin Gilje
AU  - Albrechtsen, Eirik
AU  - Line, Maria B.
AU  - Tøndel, Inger Anne
AU  - Longva, Odd Helge
SN  - 1874-5482
DO  - http://dx.doi.org/10.1016/j.ijcip.2009.02.004
UR  - http://www.sciencedirect.com/science/article/pii/S1874548209000043
KW  - Incident response
KW  - Process control systems
KW  - Learning
KW  - Security culture
AB  - Incident response is the process of responding to and handling security-related incidents involving information and communications technology (ICT) infrastructure and data. Incident response has traditionally been reactive in nature, focusing mainly on technical issues. This paper presents the Incident Response Management (IRMA) method, which combines traditional incident response with proactive learning and socio-technical perspectives. The IRMA method is targeted at integrated operations within the petroleum industry, but it is also applicable to other industries that rely on process control systems.
ER  - 

TY  - JOUR
T1  - Towards an empirical method of efficiency testing of system parts: A methodological study
JO  - Interacting with Computers
VL  - 19
IS  - 3
SP  - 342
EP  - 356
PY  - 2007/5//
T2  - 
AU  - Brinkman, Willem-Paul
AU  - Haakma, Reinder
AU  - Bouwhuis, Don G.
SN  - 0953-5438
DO  - http://dx.doi.org/10.1016/j.intcom.2007.01.002
UR  - http://www.sciencedirect.com/science/article/pii/S0953543807000094
KW  - Efficiency
KW  - Usability testing
KW  - HCI methodology
KW  - Usability evaluation method
KW  - Log file analysis
KW  - Empirical method
AB  - Current usability evaluation methods are essentially holistic in nature. However, engineers that apply a component-based software engineering approach might also be interested in understanding the usability of individual parts of an interactive system. This paper examines the efficiency dimension of usability by describing a method, which engineers can use to test, empirically and objectively, the physical interaction effort to operate components in a single device. The method looks at low-level events, such as button clicks, and attributes the physical effort associated with these interaction events to individual components in the system. This forms the basis for engineers to prioritise their improvement effort. The paper discusses face validity, content validity, criterion validity, and construct validity of the method. The discussion is set within the context of four usability tests, in which 40 users participated to evaluate the efficiency of four different versions of a mobile phone. The results of the study show that the method can provide a valid estimation of the physical interaction event effort users made when interacting with a specific part of a device.
ER  - 

TY  - JOUR
T1  - Advances in Onion Routing: Description and backtracing/investigation problems
JO  - Digital Investigation
VL  - 3
IS  - 2
SP  - 85
EP  - 88
PY  - 2006/6//
T2  - 
AU  - Forte, Dario
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2006.04.007
UR  - http://www.sciencedirect.com/science/article/pii/S1742287606000272
KW  - Onion routing
KW  - Antiforensic
KW  - Anonymizers
KW  - Privacy
KW  - Security
KW  - Network investigation
KW  - Log files analysis
KW  - Traffic analysis
KW  - Network intelligence
KW  - Digital forensics
AB  - Onion routers were born about 10 years ago as a sort of blended military/research project. The main goal of it was the avoidance of traffic analysis (TA). TA is used in part to identify the remote IP addresses that a given host seeks to contact. This technique may have various purposes, from simple statistical analysis to illegal interception. In response, to protect personnel whose communications are being monitored by hostile forces, researchers from the US Naval Research Laboratory conceived a system, dubbed “Onion Routing”, that eludes the above two operations.

In this author's opinion, the original “owners” of the project lost control of it. Software is freely available that enables anyone to utilize a network of onion routers. The result is a strong evolution (in terms of “privacy”) that is very difficult to manage, especially when an international investigation must be performed.
ER  - 

TY  - JOUR
T1  - Unix Filesystem Security
JO  - Information Security Technical Report
VL  - 7
IS  - 1
SP  - 11
EP  - 25
PY  - 2002/3/31/
T2  - 
AU  - Mellander, Jim
SN  - 1363-4127
DO  - http://dx.doi.org/10.1016/S1363-4127(02)00103-6
UR  - http://www.sciencedirect.com/science/article/pii/S1363412702001036
ER  - 

TY  - JOUR
T1  - FILE: a tool for the study of inquiry learning
JO  - Computers in Human Behavior
VL  - 21
IS  - 6
SP  - 945
EP  - 956
PY  - 2005/11//
T2  - 
AU  - Hulshof, C.D.
AU  - Wilhelm, P.
AU  - Beishuizen, J.J.
AU  - van Rijn, H.
SN  - 0747-5632
DO  - http://dx.doi.org/10.1016/j.chb.2004.03.014
UR  - http://www.sciencedirect.com/science/article/pii/S0747563204000433
KW  - Inquiry learning
KW  - Learning environment
KW  - Authoring environment
KW  - Discovery learning
KW  - Log-file analysis
AB  - A computerized learning environment (Flexible Inquiry Learning Environment; FILE) is discussed. FILE allows researchers in inquiry learning to design, administer, and analyze learning tasks in which content domain and task complexity can be configured independently, while other factors (e.g., the interface) are held constant. This allows for more valid across-task generalizations. FILE is based on a descriptive model of inquiry learning and its monitoring facilities allow for the extraction of learning indicators derived from the model. Furthermore, FILE is suitable from the age of eight years, which allows developmental issues in inquiry learning to be addressed. It is concluded that FILE can be used to set up studies in inquiry learning in an efficient way, saving expensive programming time.
ER  - 

TY  - JOUR
T1  - Managing network security: Returning fire
JO  - Network Security
VL  - 1999
IS  - 2
SP  - 11
EP  - 15
PY  - 1999/2//
T2  - 
AU  - Cohen, Fred
SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/S1353-4858(00)80025-2
UR  - http://www.sciencedirect.com/science/article/pii/S1353485800800252
AB  - Computing operates in an almost universally networked environment, but the technical aspects of information protection have not kept up. As a result, the success of information security programmes has increasingly become a function of our ability to make prudent management decisions about organizational activities. Managing Network Security takes a management view of protection and seeks to reconcile the need for security with the limitations of technology.
ER  - 

TY  - JOUR
T1  - Wired and wireless intrusion detection system: Classifications, good characteristics and state-of-the-art
JO  - Computer Standards & Interfaces
VL  - 28
IS  - 6
SP  - 670
EP  - 694
PY  - 2006/9//
T2  - 
AU  - Sobh, Tarek S.
SN  - 0920-5489
DO  - http://dx.doi.org/10.1016/j.csi.2005.07.002
UR  - http://www.sciencedirect.com/science/article/pii/S092054890500098X
KW  - Wireless network
KW  - Network-based security
KW  - Host-based security
KW  - Intrusion detection system
KW  - Intrusion prevention systems
KW  - Sniffering
AB  - In computer and network security, standard approaches to intrusion detection and response attempt to detect and prevent individual attacks. Intrusion Detection System (IDS) and intrusion prevention systems (IPS) are real-time software for risk assessment by monitoring for suspicious activity at the network and system layer. Software scanner allows network administrator to audit the network for vulnerabilities and thus securing potential holes before attackers take advantage of them.

In this paper we try to define the intruder, types of intruders, detection behaviors, detection approaches and detection techniques. This paper presents a structural approach to the IDS by introducing a classification of IDS. It presents important features, advantages and disadvantages of each detection approach and the corresponding detection techniques. Furthermore, this paper introduces the wireless intrusion protection systems.

The goal of this paper is to place some characteristics of good IDS and examine the positioning of intrusion prevention as part of an overall layered security strategy and a review of evaluation criteria for identifying and selecting IDS and IPS. With this, we hope to introduce a good characteristic in order to improve the capabilities for early detection of distributed attacks in the preliminary phases against infrastructure and take a full spectrum of manual and automatic response actions against the source of attacks.
ER  - 

TY  - JOUR
T1  - Windows NT security
JO  - Information Security Technical Report
VL  - 2
IS  - 3
SP  - 53
EP  - 65
PY  - 1997///
T2  - 
AU  - White, Ian
SN  - 1363-4127
DO  - http://dx.doi.org/10.1016/S1363-4127(97)89711-7
UR  - http://www.sciencedirect.com/science/article/pii/S1363412797897117
AB  - This article describes the underlying Windows NT security model and provides best practice advice on how Windows NT may be implemented within an organization securely. In addition this article also discusses how the system has evolved to meet recent security, related attacks and what potential weaknesses still remain.
ER  - 

TY  - JOUR
T1  - Windows NT security architecture
JO  - Information Security Technical Report
VL  - 4, Supplement 1
IS  - 
SP  - 17
EP  - 18
PY  - 1999///
T2  - 
AU  - Hayday, John
SN  - 1363-4127
DO  - http://dx.doi.org/10.1016/S1363-4127(99)80038-7
UR  - http://www.sciencedirect.com/science/article/pii/S1363412799800387
ER  - 

TY  - JOUR
T1  - Characterizing browsing strategies in the World-Wide web
JO  - Computer Networks and ISDN Systems
VL  - 27
IS  - 6
SP  - 1065
EP  - 1073
PY  - 1995/4//
T2  - Proceedings of the Third International World-Wide Web Conference
AU  - Catledge, Lara D.
AU  - Pitkow, James E.
SN  - 0169-7552
DO  - http://dx.doi.org/10.1016/0169-7552(95)00043-7
UR  - http://www.sciencedirect.com/science/article/pii/0169755295000437
KW  - Hypertext navigation
KW  - Log files
KW  - User modeling
AB  - This paper presents the results of a study conducted at Georgia Institute of Technology that captured client-side user events of NCSA's XMosaic. Actual user behavior, as determined from client-side log file analysis, supplemented our understanding of user navigation strategies as well as provided real interface usage data. Log file analysis also yielded design and usability suggestions for WWW pages, sites and browsers. The methodology of the study and findings are discussed along with future research directions.
ER  - 

TY  - JOUR
T1  - Vulnerabilities and mitigation techniques toning in the cloud: A cost and vulnerabilities coverage optimization approach using Cuckoo search algorithm with Lévy flights
JO  - Computers & Security
VL  - 48
IS  - 
SP  - 1
EP  - 18
PY  - 2015/2//
T2  - 
AU  - Zineddine, Mhamed
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/j.cose.2014.09.002
UR  - http://www.sciencedirect.com/science/article/pii/S0167404814001333
KW  - Cloud computing
KW  - ICT security
KW  - Vulnerabilities mapping
KW  - Cuckoo search algorithm
KW  - Lévy flights algorithm
KW  - Optimization
AB  - Abstract
Information and Communication Technology (ICT) security issues have been a major concern for decades. Today's ICT infrastructure faces sophisticated attacks using combinations of multiple vulnerabilities to penetrate networks with devastating impact. With the recent rise of cloud computing as a new utility computing paradigm, organizations have been considering it as a viable option to outsource major IT services in order to cut costs. Some organizations have opted for a private or hybrid cloud to take advantage of the emerging technologies and services. However, ICT security issues have to be appropriately mitigated. This research proposes a cloud security framework and an approach for vulnerabilities coverage and cost optimization using Cuckoo search algorithm with Lévy flights as random walks. The objective is to mitigate an identified set of vulnerabilities using a selected set of techniques when minimizing cost and maximizing coverage. The results show that Cloud Computing providers and organizations implementing cloud technology within their premises can effectively balance IT security coverage and cost using the proposed approach.
ER  - 

TY  - JOUR
T1  - Critical infrastructure protection: Requirements and challenges for the 21st century
JO  - International Journal of Critical Infrastructure Protection
VL  - 8
IS  - 
SP  - 53
EP  - 66
PY  - 2015/1//
T2  - 
AU  - Alcaraz, Cristina
AU  - Zeadally, Sherali
SN  - 1874-5482
DO  - http://dx.doi.org/10.1016/j.ijcip.2014.12.002
UR  - http://www.sciencedirect.com/science/article/pii/S1874548214000791
KW  - Critical infrastructure protection
KW  - SCADA systems
KW  - Risk
KW  - Security
KW  - Requirements
KW  - Challenges
AB  - Abstract
Critical infrastructures play a vital role in supporting modern society. The reliability, performance, continuous operation, safety, maintenance and protection of critical infrastructures are national priorities for countries around the world. This paper explores the vulnerabilities and threats facing modern critical infrastructures with special emphasis on industrial control systems, and describes a number of protection measures. The paper also discusses some of the challenging areas related to critical infrastructure protection such as governance and security management, secure network architectures, self-healing, modeling and simulation, wide-area situational awareness, forensics and learning, and trust management and privacy.
ER  - 

TY  - JOUR
T1  - DAG-based attack and defense modeling: Don’t miss the forest for the attack trees
JO  - Computer Science Review
VL  - 13–14
IS  - 
SP  - 1
EP  - 38
PY  - 2014/11//
T2  - 
AU  - Kordy, Barbara
AU  - Piètre-Cambacédès, Ludovic
AU  - Schweitzer, Patrick
SN  - 1574-0137
DO  - http://dx.doi.org/10.1016/j.cosrev.2014.07.001
UR  - http://www.sciencedirect.com/science/article/pii/S1574013714000100
KW  - Graphical models for security
KW  - Attack trees
KW  - Bayesian networks
KW  - Attack and defense modeling
KW  - Quantitative and qualitative security assessment
KW  - Security measures
AB  - Abstract
This paper presents the current state of the art on attack and defense modeling approaches that are based on directed acyclic graphs (DAGs). DAGs allow for a hierarchical decomposition of complex scenarios into simple, easily understandable and quantifiable actions. Methods based on threat trees and Bayesian networks are two well-known approaches to security modeling. However there exist more than 30 DAG-based methodologies, each having different features and goals.

The objective of this survey is to summarize the existing methodologies, compare their features, and propose a taxonomy of the described formalisms. This article also supports the selection of an adequate modeling technique depending on user requirements.
ER  - 

TY  - JOUR
T1  - One-time encryption-key technique for the traditional DL-based encryption scheme with anonymity
JO  - Information Sciences
VL  - 180
IS  - 22
SP  - 4420
EP  - 4429
PY  - 2010/11/15/
T2  - 
AU  - Xu, Peng
AU  - Cui, Guo-Hua
AU  - Lei, Feng-Yu
AU  - Xu, Jing-Fang
SN  - 0020-0255
DO  - http://dx.doi.org/10.1016/j.ins.2010.07.032
UR  - http://www.sciencedirect.com/science/article/pii/S0020025510003609
KW  - Anonymity
KW  - Indistinguishability
KW  - Public-key encryption
KW  - Discrete-logarithm-based encryption
KW  - Chosen ciphertext attack
AB  - In modern cryptosystem, Anonymity means that in some sense any adversary cannot tell which one of public keys has been used for encrypting a plaintext, and was first formally defined as the indistinguishability of keys by Bellare et al. in 2001. Recently, several well-known techniques have been proposed in order to achieve the anonymity of public-key encryption schemes. In this paper, anonymity is considered first from a new perspective. And then basing on this new perspective, a one-time encryption-key technique is proposed to achieve the anonymity of traditional discrete-logarithm-based (DL-based) encryption scheme. In this new technique, for each encryption, a random one-time encryption-key will be generated to encrypt the plaintext, instead of the original public-key. Consequently, in roughly speaking, by the randomness of the generated one-time encryption-key, this new technique should achieve the anonymity. Furthermore, in the formal proof of anonymity, only based on several weaker conditions, the one-time encryption-key technique efficiently achieves the provable indistinguishability of keys under chosen ciphertext attack (IK-CCA anonymity). As a result, compared with the work of Hayashi and Tanaka in 2006, the one-time encryption-key technique presented here has fewer requirements for achieving the provable anonymity.
ER  - 

TY  - JOUR
T1  - Investigating around mainframes and other high-end systems: the revenge of big iron
JO  - Digital Investigation
VL  - 1
IS  - 2
SP  - 90
EP  - 93
PY  - 2004/6//
T2  - 
AU  - Pemble, Matthew
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2004.04.006
UR  - http://www.sciencedirect.com/science/article/pii/S1742287604000349
KW  - Mainframe
KW  - Audit
KW  - Investigation
KW  - Forensic
KW  - Log
AB  - Normal computer forensic tools and techniques are ineffective or inefficient when applied to mainframes or other very large systems. Incidents involving mainframes are likely to have a significant business impact, so preparation is essential, both of any specialist tools and of staff with mainframe experience. Proper design of system audit logs can also provide admissible material of evidential weight, without the requirement for forensic treatment.
ER  - 

TY  - JOUR
T1  - Automatic high-performance reconstruction and recovery
JO  - Computer Networks
VL  - 51
IS  - 5
SP  - 1361
EP  - 1377
PY  - 2007/4/11/
T2  - From Intrusion Detection to Self-Protection
AU  - Goel, Ashvin
AU  - Feng, Wu-chang
AU  - Feng, Wu-chi
AU  - Maier, David
SN  - 1389-1286
DO  - http://dx.doi.org/10.1016/j.comnet.2006.09.013
UR  - http://www.sciencedirect.com/science/article/pii/S1389128606002465
KW  - Self-healing computers
KW  - Computer forensics
KW  - Operating systems
KW  - Auditing
AB  - Self-protecting systems require the ability to instantaneously detect malicious activity at run-time and prevent execution. We argue that it is impossible to perfectly self-protect systems without false positives due to the limited amount of information one might have at run-time and that eventually some undesirable activity will occur that will need to be rolled back. As a consequence of this, it is important that self-protecting systems have the ability to completely and automatically roll back malicious activity which has occurred.

As the cost of human resources currently dominates the cost of CPU, network, and storage resources, we contend that computing systems should be built with automated analysis and recovery as a primary goal. Towards this end, we describe the design, implementation, and evaluation of Forensix: a robust, high-precision analysis and recovery system for supporting self-healing. The Forensix system records all activity of a target computer and allows for efficient, automated reconstruction of activity when needed. Such a system can be used to automatically detect patterns of malicious activity and selectively undo their operations.

Forensix uses three key mechanisms to improve the accuracy and reduce the human overhead of performing analysis and recovery. First, it performs comprehensive monitoring of the execution of a target system at the kernel event level, giving a high-resolution, application-independent view of all activity. Second, it streams the kernel event information, in real-time, to append-only storage on a separate, hardened, logging machine, making the system resilient to a wide variety of attacks. Third, it uses database technology to support high-level querying of the archived log, greatly reducing the human cost of performing analysis and recovery.
ER  - 

TY  - JOUR
T1  - In-execution dynamic malware analysis and detection by mining information in process control blocks of Linux OS
JO  - Information Sciences
VL  - 231
IS  - 
SP  - 45
EP  - 63
PY  - 2013/5/10/
T2  - Data Mining for Information Security
AU  - Shahzad, Farrukh
AU  - Shahzad, M.
AU  - Farooq, Muddassar
SN  - 0020-0255
DO  - http://dx.doi.org/10.1016/j.ins.2011.09.016
UR  - http://www.sciencedirect.com/science/article/pii/S0020025511004737
KW  - Intrusion detection system
KW  - Kernel task structure
KW  - Malware forensic
KW  - Operating system security
KW  - Malicious process detection
AB  - Run-time behavior of processes – running on an end-host – is being actively used to dynamically detect malware. Most of these detection schemes build model of run-time behavior of a process on the basis of its data flow and/or sequence of system calls. These novel techniques have shown promising results but an efficient and effective technique must meet the following performance metrics: (1) high detection accuracy, (2) low false alarm rate, (3) small detection time, and (4) the technique should be resilient to run-time evasion attempts.

To meet these challenges, a novel concept of genetic footprint is proposed, by mining the information in the kernel process control blocks (PCB) of a process, that can be used to detect malicious processes at run time. The genetic footprint consists of selected parameters – maintained inside the PCB of a kernel for each running process – that define the semantics and behavior of an executing process. A systematic forensic study of the execution traces of benign and malware processes is performed to identify discriminatory parameters of a PCB (task_struct is PCB in case of Linux OS). As a result, 16 out of 118 task structure parameters are short listed using the time series analysis. A statistical analysis is done to corroborate the features of the genetic footprint and to select suitable machine learning classifiers to detect malware.

The scheme has been evaluated on a dataset that consists of 105 benign processes and 114 recently collected malware processes for Linux. The results of experiments show that the presented scheme achieves a detection accuracy of 96% with 0% false alarm rate in less than 100 ms of the start of a malicious activity. Last but not least, the presented technique utilizes partial knowledge that is available at a given time while the process is still executing; as a result, the kernel of OS can devise mitigation strategies. It is also shown that the presented technique is robust to well known run-time evasion attempts.
ER  - 

TY  - JOUR
T1  - The 1999 DARPA off-line intrusion detection evaluation
JO  - Computer Networks
VL  - 34
IS  - 4
SP  - 579
EP  - 595
PY  - 2000/10//
T2  - Recent Advances in Intrusion Detection Systems
AU  - Lippmann, Richard
AU  - Haines, Joshua W
AU  - Fried, David J
AU  - Korba, Jonathan
AU  - Das, Kumar
SN  - 1389-1286
DO  - http://dx.doi.org/10.1016/S1389-1286(00)00139-0
UR  - http://www.sciencedirect.com/science/article/pii/S1389128600001390
KW  - Intrusion detection
KW  - Evaluate
KW  - Attack
KW  - Audit
KW  - Test bed
AB  - Eight sites participated in the second Defense Advanced Research Projects Agency (DARPA) off-line intrusion detection evaluation in 1999. A test bed generated live background traffic similar to that on a government site containing hundreds of users on thousands of hosts. More than 200 instances of 58 attack types were launched against victim UNIX and Windows NT hosts in three weeks of training data and two weeks of test data. False-alarm rates were low (less than 10 per day). The best detection was provided by network-based systems for old probe and old denial-of-service (DoS) attacks and by host-based systems for Solaris user-to-root (U2R) attacks. The best overall performance would have been provided by a combined system that used both host- and network-based intrusion detection. Detection accuracy was poor for previously unseen, new, stealthy and Windows NT attacks. Ten of the 58 attack types were completely missed by all systems. Systems missed attacks because signatures for old attacks did not generalize to new attacks, auditing was not available on all hosts, and protocols and TCP services were not analyzed at all or to the depth required. Promising capabilities were demonstrated by host-based systems, anomaly detection systems and a system that performs forensic analysis on file system data.
ER  - 

TY  - JOUR
T1  - An analysis of digital forensic examinations: Mobile devices versus hard disk drives utilising ACPO &amp; NIST guidelines
JO  - Digital Investigation
VL  - 8
IS  - 2
SP  - 135
EP  - 140
PY  - 2011/11//
T2  - Standards, professionalization and quality in digital forensics
AU  - Owen, Paul
AU  - Thomas, Paula
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2011.03.002
UR  - http://www.sciencedirect.com/science/article/pii/S1742287611000211
KW  - Mobile forensics
KW  - Computer forensics
KW  - Information security
KW  - ACPO
KW  - NIST
AB  - The aims of this paper are to compare and contrast the current guidelines involved in the forensic examinations of mobile devices and hard disk drives. The paper then identifies areas of mobile device examinations where current guidelines are different and could be lacking strength and solidity. Guidelines and research into the forensic examination of hard disk drives is much more established when compared to that of mobile devices.

Both the United Kingdom and the United States of America have published guidelines for the forensic analysis of mobile devices; these guidelines are examined throughout this paper. In the United Kingdom they are issued by ACPO (Association of Chief Police Officers) Good Practice Guide for Computer-Based Electronic Evidence. In the United States of America these are issued by NIST (National Institute of Standards and Technology). Special Publication 800-101, Guidelines on Cell Phone Forensics.
ER  - 

TY  - JOUR
T1  - Distributed forensics and incident response in the enterprise
JO  - Digital Investigation
VL  - 8, Supplement
IS  - 
SP  - S101
EP  - S110
PY  - 2011/8//
T2  - 11th Annual Digital Forensics Research Conference
AU  - Cohen, M.I.
AU  - Bilby, D.
AU  - Caronni, G.
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2011.05.012
UR  - http://www.sciencedirect.com/science/article/pii/S1742287611000363
KW  - Remote forensics
KW  - Live forensics
KW  - Digital forensics
KW  - Incident response
KW  - Information security
KW  - Malware
KW  - Memory forensics
KW  - Distributed computing
AB  - Remote live forensics has recently been increasingly used in order to facilitate rapid remote access to enterprise machines. We present the GRR Rapid Response Framework (GRR), a new multi-platform, open source tool for enterprise forensic investigations enabling remote raw disk and memory access. GRR is designed to be scalable, opening the door for continuous enterprise wide forensic analysis. This paper describes the architecture used by GRR and illustrates how it is used routinely to expedite enterprise forensic investigations.
ER  - 

TY  - JOUR
T1  - Encryption safe harbours and data breach notification laws
JO  - Computer Law & Security Review
VL  - 26
IS  - 5
SP  - 520
EP  - 534
PY  - 2010/9//
T2  - 
AU  - Burdon, Mark
AU  - Reid, Jason
AU  - Low, Rouhshi
SN  - 0267-3649
DO  - http://dx.doi.org/10.1016/j.clsr.2010.07.002
UR  - http://www.sciencedirect.com/science/article/pii/S0267364910001056
KW  - Data breach notification
KW  - Encryption
KW  - Information security management
KW  - Data protection
AB  - Data breach notification laws require organisations to notify affected persons or regulatory authorities when an unauthorised acquisition of personal data occurs. Most laws provide a safe harbour to this obligation if acquired data has been encrypted. There are three types of safe harbour: an exemption; a rebuttable presumption and factor-based analysis. We demonstrate, using three condition-based scenarios, that the broad formulation of most encryption safe harbours is based on the flawed assumption that encryption is the silver bullet for personal information protection. We then contend that reliance upon an encryption safe harbour should be dependent upon a rigorous and competent risk-based review that is required on a case-by-case basis. Finally, we recommend the use of both an encryption safe harbour and a notification trigger as our preferred choice for a data breach notification regulatory framework.
ER  - 

TY  - JOUR
T1  - HIPAA compliant auditing system for medical images
JO  - Computerized Medical Imaging and Graphics
VL  - 29
IS  - 2–3
SP  - 235
EP  - 241
PY  - 2005/3//
Y2  - 2005/4//
T2  - Imaging Informatics
AU  - Zhou, Zheng
AU  - Liu, Brent J.
SN  - 0895-6111
DO  - http://dx.doi.org/10.1016/j.compmedimag.2004.09.009
UR  - http://www.sciencedirect.com/science/article/pii/S0895611104001223
KW  - HIPAA
KW  - Security
KW  - HIPAA compliant auditing system
KW  - Auditing
KW  - monitoring
AB  - As an official regulation for healthcare privacy and security, Health Insurance Portability and Accountability Act (HIPAA) mandates health institutions to protect health information against unauthorized use or disclosure. One such method proposed by HIPAA Security Standards is audit trail, which records and examines health information access activities. HIPAA mandates healthcare providers to have the ability to generate audit trails on data access activities for any specific patient. Although current medical imaging systems generate activity logs, there is a lack of formal methodology to interpret these large volumes of log data and generate HIPAA compliant auditing trails.

This paper outlines the design of a HIPAA compliant auditing system (HCAS) for medical images in imaging systems such as PACS and discusses the development of a security monitoring (SM) toolkit based on some of the partial components in HCAS.
ER  - 

TY  - JOUR
T1  - Intrusion detection using an ensemble of intelligent paradigms
JO  - Journal of Network and Computer Applications
VL  - 28
IS  - 2
SP  - 167
EP  - 182
PY  - 2005/4//
T2  - Computational Intelligence on the Internet
AU  - Mukkamala, Srinivas
AU  - Sung, Andrew H.
AU  - Abraham, Ajith
SN  - 1084-8045
DO  - http://dx.doi.org/10.1016/j.jnca.2004.01.003
UR  - http://www.sciencedirect.com/science/article/pii/S1084804504000049
KW  - Computer security
KW  - Support vector machines
KW  - Network security
AB  - Soft computing techniques are increasingly being used for problem solving. This paper addresses using an ensemble approach of different soft computing and hard computing techniques for intrusion detection. Due to increasing incidents of cyber attacks, building effective intrusion detection systems are essential for protecting information systems security, and yet it remains an elusive goal and a great challenge. We studied the performance of Artificial Neural Networks (ANNs), Support Vector Machines (SVMs) and Multivariate Adaptive Regression Splines (MARS). We show that an ensemble of ANNs, SVMs and MARS is superior to individual approaches for intrusion detection in terms of classification accuracy.
ER  - 

TY  - JOUR
T1  - DDoS attacks and defense mechanisms: classification and state-of-the-art
JO  - Computer Networks
VL  - 44
IS  - 5
SP  - 643
EP  - 666
PY  - 2004/4/5/
T2  - 
AU  - Douligeris, Christos
AU  - Mitrokotsa, Aikaterini
SN  - 1389-1286
DO  - http://dx.doi.org/10.1016/j.comnet.2003.10.003
UR  - http://www.sciencedirect.com/science/article/pii/S1389128603004250
KW  - DoS attacks
KW  - DDoS attacks
KW  - Defenses
KW  - Network security
KW  - Intrusion detection
AB  - Denial of Service (DoS) attacks constitute one of the major threats and among the hardest security problems in today’s Internet. Of particular concern are Distributed Denial of Service (DDoS) attacks, whose impact can be proportionally severe. With little or no advance warning, a DDoS attack can easily exhaust the computing and communication resources of its victim within a short period of time. Because of the seriousness of the problem many defense mechanisms have been proposed to combat these attacks. This paper presents a structural approach to the DDoS problem by developing a classification of DDoS attacks and DDoS defense mechanisms. Furthermore, important features of each attack and defense system category are described and advantages and disadvantages of each proposed scheme are outlined. The goal of the paper is to place some order into the existing attack and defense mechanisms, so that a better understanding of DDoS attacks can be achieved and subsequently more efficient and effective algorithms, techniques and procedures to combat these attacks may be developed.
ER  - 

TY  - JOUR
T1  - Intrusion detection using autonomous agents
JO  - Computer Networks
VL  - 34
IS  - 4
SP  - 547
EP  - 570
PY  - 2000/10//
T2  - Recent Advances in Intrusion Detection Systems
AU  - Spafford, Eugene H
AU  - Zamboni, Diego
SN  - 1389-1286
DO  - http://dx.doi.org/10.1016/S1389-1286(00)00136-5
UR  - http://www.sciencedirect.com/science/article/pii/S1389128600001365
KW  - Intrusion detection
KW  - Software agents
KW  - Distributed systems
KW  - Security
KW  - Perl
AB  - AAFID is a distributed intrusion detection architecture and system, developed in CERIAS at Purdue University. AAFID was the first architecture that proposed the use of autonomous agents for doing intrusion detection. With its prototype implementation, it constitutes a useful framework for the research and testing of intrusion detection algorithms and mechanisms. We describe the AAFID architecture and the existing prototype, as well as some design and implementation experiences and future research issues.
ER  - 

TY  - JOUR
T1  - Log management for effective incident response
JO  - Network Security
VL  - 2005
IS  - 9
SP  - 4
EP  - 7
PY  - 2005/9//
T2  - 
AU  - Forte, Dario
SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/S1353-4858(05)70279-8
UR  - http://www.sciencedirect.com/science/article/pii/S1353485805702798
AB  - Log file correlation is related to two distinct activities: Intrusion Detection and Network Forensics. It is more important than ever that these two disciplines work together, and in cooperation, to avoid points of failure. This article presents an overview of log analysis and correlation, with special emphasis on the tools and techniques for managing them within a network forensics context.
ER  - 

TY  - JOUR
T1  - Get ready for PCI DSS 3.0 with real-time monitoring
JO  - Computer Fraud & Security
VL  - 2015
IS  - 2
SP  - 17
EP  - 18
PY  - 2015/2//
T2  - 
AU  - Fernandes, Joel John
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(15)30009-9
UR  - http://www.sciencedirect.com/science/article/pii/S1361372315300099
AB  - PCI DSS 3.0 compliance has gained worldwide acceptance by card service providers – card issuers, banks, and merchants – that plan to protect their customers' cardholder data from being misused. PCI DSS 3.0 has 12 security requirements concerning the protection of cardholder data. All businesses that accept, store, process or transmit customers card data either online or offline have to adhere to those requirements.
ER  - 

TY  - JOUR
T1  - On Incident Handling and Response: A state-of-the-art approach
JO  - Computers & Security
VL  - 25
IS  - 5
SP  - 351
EP  - 370
PY  - 2006/7//
T2  - 
AU  - Mitropoulos, Sarandis
AU  - Patsos, Dimitrios
AU  - Douligeris, Christos
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/j.cose.2005.09.006
UR  - http://www.sciencedirect.com/science/article/pii/S0167404805001574
KW  - Incident Handling
KW  - Incident Response
KW  - Computer forensics
KW  - Internet forensics
KW  - Software forensics
KW  - Trace-back mechanisms
AB  - Incident Response has always been an important aspect of Information Security but it is often overlooked by security administrators. Responding to an incident is not solely a technical issue but has many management, legal, technical and social aspects that are presented in this paper. We propose a detailed management framework along with a complete structured methodology that contains best practices and recommendations for appropriately handling a security incident. We also present the state-of-the art technology in computer, network and software forensics as well as automated trace-back artifacts, schemas and protocols. Finally, we propose a generic Incident Response process within a corporate environment.
ER  - 

TY  - JOUR
T1  - Protecting critical control systems
JO  - Network Security
VL  - 2012
IS  - 3
SP  - 7
EP  - 10
PY  - 2012/3//
T2  - 
AU  - Brewer, Ross
SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/S1353-4858(12)70044-2
UR  - http://www.sciencedirect.com/science/article/pii/S1353485812700442
AB  - With cyber-attacks continuing to grow in sophistication and frequency, both public and private organisations have been forced to change their outlook on cyber-security and re-examine their strategies when it comes to protecting their networks. System breaches are no longer considered unlikely, and the mindset has shifted to cyber-attacks being a matter of ‘when’ rather than ‘if’.

With cyber-attacks continuing to grow in sophistication and frequency, both public and private organisations have been forced to change their outlook on cyber-security and re-examine their strategies when it comes to protecting their networks. System breaches are no longer considered unlikely, and the mindset has shifted to cyber-attacks being a matter of ‘when’ rather than ‘if’.

Many critical infrastructure installations are controlled by Supervisory Control And Data Acquisition (SCADA) solutions that were never designed to be secure. However, with the correct strategic focus and resources applied, SCADA systems can be secured. Ross Brewer of LogRhythm suggests that a ‘protective monitoring’ approach can be tailored around networks that support high-value cyber-assets.
ER  - 

TY  - JOUR
T1  - Use patterns of health information exchange systems and admission decisions: Reductionistic and configurational approaches
JO  - International Journal of Medical Informatics
VL  - 
IS  - 
SP  - 
EP  - 
PY  - 
T2  - 
AU  - Politi, Liran
AU  - Codish, Shlomi
AU  - Sagy, Iftach
AU  - Fink, Lior
SN  - 1386-5056
DO  - http://dx.doi.org/10.1016/j.ijmedinf.2015.06.012
UR  - http://www.sciencedirect.com/science/article/pii/S1386505615300101
KW  - Health information exchange
KW  - Use pattern
KW  - Clinical decision making
KW  - Emergency department
KW  - Intensive care unit
AB  - AbstractBackground
Research that endeavors to identify the value of electronic health information exchange (HIE) systems to the healthcare industry and, specifically, to clinical decision making is often inconclusive or theory-based. Studies seeking to identify how clinical decisions relate to patterns of actual HIE use, often by analyzing system log files, generally rely on dichotomous distinctions between system use and no-use, disregard the availability of information in the system, and control for few user characteristics.
Objective
We aim at empirically exploring the associations between use patterns of HIE systems and subsequent clinical decisions on the basis of broad definitions of use patterns, available information, and control variables.
Methods
We examine the decision to admit critically-ill patients either to the intensive care unit (ICU) or to another ward at a busy emergency department in the period 2010–2012. Using HIE log files, use patterns are characterized by the variables of number of users, volume, diversity, granularity, duration, and content. We test the association between HIE use patterns and the admission decision, after controlling for multiple demographic, clinical, physician, and situational variables and for available HIE information. This association is examined by taking a reductionistic approach that focuses on independent use variables and a configurational approach that focuses on use profiles.
Results
Five use profiles were identified, the largest of which (46.95% of encounters) described basic HIE access. ICU admission is more probable when the HIE system is perused by multiple users (odds increase by 31%) and when use profiles include prolonged screen viewing (odds increase by 159%) or access to diverse and multiple types of information, specifically on test results, procedures, and previous encounters.
Discussion
Reductionistic and configurational approaches yield complementary insights, which advance the understanding of how actual HIE use is associated with clinical decision making. The study shows that congruent profiles of HIE use enhance the predictability of the admission decision beyond what can be explained by independent variables of HIE use.
ER  - 

TY  - JOUR
T1  - An extensible analysable system model
JO  - Information Security Technical Report
VL  - 13
IS  - 4
SP  - 235
EP  - 246
PY  - 2008/11//
T2  - 
AU  - Probst, Christian W.
AU  - Hansen, René Rydhof
SN  - 1363-4127
DO  - http://dx.doi.org/10.1016/j.istr.2008.10.012
UR  - http://www.sciencedirect.com/science/article/pii/S1363412708000502
AB  - Analysing real-world systems for vulnerabilities with respect to security and safety threats is a difficult undertaking, not least due to a lack of availability of formalisations for those systems. While both formalisations and analyses can be found for artificial systems such as software, this does not hold for real physical systems. Approaches such as threat modelling try to target the formalisation of the real-world domain, but still are far from the rigid techniques available in security research. Many currently available approaches to assurance of critical infrastructure security are based on (quite successful) ad-hoc techniques. We believe they can be significantly improved beyond the state-of-the-art by pairing them with static analyses techniques.

In this paper we present an approach to both formalising those real-world systems, as well as providing an underlying semantics, which allows for easy development of analyses for the abstracted systems. We briefly present one application of our approach, namely the analysis of systems for potential insider threats.
ER  - 

TY  - JOUR
T1  - Digital forensics as a service: Game on
JO  - Digital Investigation
VL  - 
IS  - 
SP  - 
EP  - 
PY  - 
T2  - 
AU  - van Beek, H.M.A.
AU  - van Eijk, E.J.
AU  - van Baar, R.B.
AU  - Ugen, M.
AU  - Bodde, J.N.C.
AU  - Siemelink, A.J.
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2015.07.004
UR  - http://www.sciencedirect.com/science/article/pii/S1742287615000857
KW  - Distributed systems
KW  - Digital forensics
KW  - Big data
KW  - xiraf
KW  - Hansken
AB  - Abstract
The big data era has a high impact on forensic data analysis. Work is done in speeding up the processing of large amounts of data and enriching this processing with new techniques. Doing forensics calls for specific design considerations, since the processed data is incredibly sensitive. In this paper we explore the impact of forensic drivers and major design principles like security, privacy and transparency on the design and implementation of a centralized digital forensics service.
ER  - 

TY  - JOUR
T1  - Short- and long-term effects of students’ self-directed metacognitive prompts on navigation behavior and learning performance
JO  - Computers in Human Behavior
VL  - 52
IS  - 
SP  - 293
EP  - 306
PY  - 2015/11//
T2  - 
AU  - Bannert, Maria
AU  - Sonnenberg, Christoph
AU  - Mengelkamp, Christoph
AU  - Pieger, Elisabeth
SN  - 0747-5632
DO  - http://dx.doi.org/10.1016/j.chb.2015.05.038
UR  - http://www.sciencedirect.com/science/article/pii/S0747563215004197
KW  - Metacognition
KW  - Metacognitive skills
KW  - Metacognitive prompts
KW  - Self-directed prompts
KW  - Knowledge acquisition
KW  - Long-term effects
AB  - Abstract
This study seeks to promote learning in computer-based learning environments utilizing students’ self-directed metacognitive prompts. Such prompts are based on the idea of instructing students to design their own metacognitive scaffolds and learn with them afterward. In a pre-post experimental design, students in the experimental group (n = 35) were instructed to configure their own metacognitive prompts before learning whereas students in the control group (n = 35) learned without prompts. Log file analysis of navigation behavior indicates that students who learned with their individually designed, self-directed prompts visited relevant webpages significantly more often and spent a longer time on them compared with students in the control group. Moreover, participants in the experimental group attained better transfer performance immediately after learning. The long-term effect in transfer performance was even greater in a follow-up learning session conducted after three weeks without any instructional support in either group. These results are consistent with theories of metacognition and self-regulated learning and indicate that self-directed prompts can lead to sustainable effects.
ER  - 

TY  - JOUR
T1  - Automated identification of installed malicious Android applications
JO  - Digital Investigation
VL  - 10, Supplement
IS  - 
SP  - S96
EP  - S104
PY  - 2013/8//
T2  - 13th Annual Digital Forensics Research Conference
AU  - Guido, Mark
AU  - Ondricek, Jared
AU  - Grover, Justin
AU  - Wilburn, David
AU  - Nguyen, Thanh
AU  - Hunt, Andrew
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2013.06.011
UR  - http://www.sciencedirect.com/science/article/pii/S1742287613000571
KW  - Android
KW  - Mobile forensics
KW  - Periodic
KW  - Enterprise
KW  - Monitoring
AB  - Abstract
Increasingly, Android smartphones are becoming more pervasive within the government and industry, despite the limited ways to detect malicious applications installed to these phones' operating systems. Although enterprise security mechanisms are being developed for use on Android devices, these methods cannot detect previously unknown malicious applications. As more sensitive enterprise information becomes available and accessible on these smartphones, the risk of data loss inherently increases. A malicious application's actions could potentially leave sensitive data exposed with little recourse. Without an effective corporate monitoring solution in place for these mobile devices, organizations will continue to lack the ability to determine when a compromise has occurred. This paper presents research that applies traditional digital forensic techniques to remotely monitor and audit Android smartphones. The smartphone sends changed file system data to a remote server, allowing for expensive forensic processing and the offline application of traditional tools and techniques rarely applied to the mobile environment. The research aims at ascertaining new ways of identifying malicious Android applications and ultimately attempts to improve the state of enterprise smartphone monitoring. An on-phone client, server, database, and analysis framework was developed and tested using real mobile malware. The results are promising that the developed detection techniques identify changes to important system partitions; recognize file system changes, including file deletions; and find persistence and triggering mechanisms in newly installed applications. It is believed that these detection techniques should be performed by enterprises to identify malicious applications affecting their phone infrastructure.
ER  - 

TY  - JOUR
T1  - Analysis of Internet Download Manager for collection of digital forensic artefacts
JO  - Digital Investigation
VL  - 7
IS  - 1–2
SP  - 90
EP  - 94
PY  - 2010/10//
T2  - 
AU  - Yasin, Muhammad
AU  - Cheema, Ahmad R.
AU  - Kausar, Firdous
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2010.08.005
UR  - http://www.sciencedirect.com/science/article/pii/S1742287610000575
KW  - Digital forensics
KW  - Download manager
KW  - Forensic artefacts
KW  - Internet Download Manager
KW  - Password Cracking
KW  - Windows registry analysis
KW  - IDM
AB  - Internet Download Manager (IDM) provides accelerated download speed and flexibility in features. Its attractiveness lies behind video content processing and automatic handling of downloads. This paper analyzes IDM activities recorded across multiple files that includes Windows Registry, history and log files from artefacts collection view point. The tools and techniques used for extracting evidence are also elaborated. In case of download managers, the foremost concerns are installation location, download path, downloaded file, URL address, login credentials for password protected websites, date and time the activity was performed. This enables digital forensic investigators to envisage and deduce suspicious activities.
ER  - 

TY  - JOUR
T1  - Approximate TF–IDF based on topic extraction from massive message stream using the GPU
JO  - Information Sciences
VL  - 292
IS  - 
SP  - 143
EP  - 161
PY  - 2015/1/20/
T2  - 
AU  - Erra, Ugo
AU  - Senatore, Sabrina
AU  - Minnella, Fernando
AU  - Caggianese, Giuseppe
SN  - 0020-0255
DO  - http://dx.doi.org/10.1016/j.ins.2014.08.062
UR  - http://www.sciencedirect.com/science/article/pii/S0020025514008676
KW  - Twitter
KW  - TF–IDF
KW  - GPU
KW  - Topic extraction
KW  - Frequent items
KW  - Massive data stream
AB  - Abstract
The Web is a constantly expanding global information space that includes disparate types of data and resources. Recent trends demonstrate the urgent need to manage the large amounts of data stream, especially in specific domains of application such as critical infrastructure systems, sensor networks, log file analysis, search engines and more recently, social networks. All of these applications involve large-scale data-intensive tasks, often subject to time constraints and space complexity. Algorithms, data management and data retrieval techniques must be able to process data stream, i.e., process data as it becomes available and provide an accurate response, based solely on the data stream that has already been provided. Data retrieval techniques often require traditional data storage and processing approach, i.e., all data must be available in the storage space in order to be processed. For instance, a widely used relevance measure is Term Frequency–Inverse Document Frequency (TF–IDF), which can evaluate how important a word is in a collection of documents and requires to a priori know the whole dataset.

To address this problem, we propose an approximate version of the TF–IDF measure suitable to work on continuous data stream (such as the exchange of messages, tweets and sensor-based log files). The algorithm for the calculation of this measure makes two assumptions: a fast response is required, and memory is both limited and infinitely smaller than the size of the data stream. In addition, to face the great computational power required to process massive data stream, we present also a parallel implementation of the approximate TF–IDF calculation using Graphical Processing Units (GPUs).

This implementation of the algorithm was tested on generated and real data stream and was able to capture the most frequent terms. Our results demonstrate that the approximate version of the TF–IDF measure performs at a level that is comparable to the solution of the precise TF–IDF measure.
ER  - 

TY  - JOUR
T1  - A triage framework for digital forensics
JO  - Computer Fraud & Security
VL  - 2015
IS  - 3
SP  - 8
EP  - 18
PY  - 2015/3//
T2  - 
AU  - Bashir, Muhammad Shamraiz
AU  - Khan, Muhammad Naeem Ahmed
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(15)30018-X
UR  - http://www.sciencedirect.com/science/article/pii/S136137231530018X
AB  - A sharp increase in malware and cyber-attacks has been observed in recent years. Analysing cyber-attacks on the affected digital devices falls under the purview of digital forensics. The Internet is the main source of cyber and malware attacks, which sometimes result in serious damage to the digital assets. The motive behind digital crimes varies – such as online banking fraud, information stealing, denial of services, security breaches, deceptive output of running programs and data distortion.

Digital forensics analysts use a variety of tools for data acquisition, evidence analysis and presentation of malicious activities. This leads to device diversity posing serious challenges for investigators.

For this reason, some attack scenarios have to be examined repeatedly, which entails tremendous effort on the part of the examiners when analysing the evidence. To counter this problem, Muhammad Shamraiz Bashir and Muhammad Naeem Ahmed Khan at the Shaheed Zulfikar Ali Bhutto Institute of Science and Technology, Islamabad, Pakistan propose a novel triage framework for digital forensics.
ER  - 

TY  - JOUR
T1  - If you can't stop the breach, at least spot the breach
JO  - Network Security
VL  - 2015
IS  - 4
SP  - 11
EP  - 12
PY  - 2015/4//
T2  - 
AU  - Kedgley, Mark
SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/S1353-4858(15)30027-1
UR  - http://www.sciencedirect.com/science/article/pii/S1353485815300271
AB  - Over a year on from the debacle of Target's security breach and has anything really changed? Despite the weeks of forensic analysis and the astronomical cost incurred by the company, retailer after retailer is still falling foul of the same form of malware attack. So just what is going wrong?

Organisations are blithely continuing day-to-day operations while an attack is in progress because they are simply not spotting the breaches as they occur.

With non-stop, continuous visibility of what is going on in the IT estate, an organisation can at least spot the unusual changes being made to files that may represent an ongoing cyber-attack in real time and take action to stop it before it is too late, says Mark Kedgley of New Net Technologies.
ER  - 

TY  - JOUR
T1  - Android forensics: Automated data collection and reporting from a mobile device
JO  - Digital Investigation
VL  - 10, Supplement
IS  - 
SP  - S12
EP  - S20
PY  - 2013/8//
T2  - 13th Annual Digital Forensics Research Conference
AU  - Grover, Justin
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2013.06.002
UR  - http://www.sciencedirect.com/science/article/pii/S1742287613000480
KW  - Android
KW  - Mobile device
KW  - Enterprise user monitoring
KW  - Insider threat
KW  - Internal investigation
AB  - Abstract
In this research, a prototype enterprise monitoring system for Android smartphones was developed to continuously collect many data sets of interest to incident responders, security auditors, proactive security monitors, and forensic investigators. Many of the data sets covered were not found in other available enterprise monitoring tools. The prototype system neither requires root privileges nor the exploiting of the Android architecture for proper operation, thereby increasing interoperability among Android devices and avoiding a spyware classification for the system. An anti-forensics analysis on the system was performed to identify and further strengthen areas vulnerable to tampering. The contributions of this research include the release of the first open-source Android enterprise monitoring solution of its kind, a comprehensive guide of data sets available for collection without elevated privileges, and the introduction of a novel design strategy implementing various Android application components useful for monitoring on the Android platform.
ER  - 

TY  - JOUR
T1  - Case study: Network intrusion investigation – lessons in forensic preparation
JO  - Digital Investigation
VL  - 2
IS  - 4
SP  - 254
EP  - 260
PY  - 2005/12//
T2  - 
AU  - Casey, Eoghan
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2005.11.007
UR  - http://www.sciencedirect.com/science/article/pii/S1742287605000940
KW  - Forensic readiness
KW  - Forensic preparation
KW  - Incident response
KW  - Incident handling
KW  - Network forensics
KW  - Computer intrusion
KW  - Rootkit
KW  - Forensic computing
KW  - Tracking intruders
KW  - Attribution
AB  - Investigations of network security breaches are both complex and costly. Even a moderate amount of forensic preparation in an organization can mitigate the impact of a major incident and can enable the organization to obtain restitution. A case study of an intrusion is outlined in which the victim organization worked with law enforcement agencies to apprehend the perpetrator. This case study contains examples of challenges that can arise during this type of investigation, and discusses practical steps that an organization can take to prepare for a major incident. The overlapping roles of System Administrators, Incident Handlers, and Forensic Examiners in a network intrusion are explored, with an emphasis on the need for collaboration and proper evidence handling. This case study also shows how effective case management and methodical reconstruction of events can help create a more complete picture of the crime and help establish links between computer intruders and their illegal activities.
ER  - 

TY  - JOUR
T1  - Identifying threats in real time
JO  - Network Security
VL  - 2013
IS  - 11
SP  - 5
EP  - 8
PY  - 2013/11//
T2  - 
AU  - Macrae, Alistair
SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/S1353-4858(13)70119-3
UR  - http://www.sciencedirect.com/science/article/pii/S1353485813701193
AB  - The technology landscape is constantly evolving, and as it does cyber-criminals have no problem keeping pace by altering their increasingly sophisticated tactics to exploit vulnerabilities in the networks of organisations, individuals and nation-states. As such, the fast and accurate detection of potential cyber-threats has become a critical capability in order to avoid significant damage to reputations, sensitive information and, in some extreme cases, lives.

As cyber-attacks become an increasing risk for every organisation, the detection and remediation of any issues must occur immediately to avoid irreparable damage. And IT security professionals should gather as much intelligence about a breach as possible.

Only through advanced correlative, statistical, behavioural and pattern recognition techniques can threats be identified in real time. Alistair Macrae of LogRhythm explores the techniques that can be used for such real-time identification and analysis of breaches, as well as how to navigate the complicated minefield of cyber-security forensic requirements and investigative procedures.
ER  - 

TY  - JOUR
T1  - Technological alternatives in incident response
JO  - Network Security
VL  - 2008
IS  - 11
SP  - 16
EP  - 18
PY  - 2008/11//
T2  - 
AU  - Forte, Dario
SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/S1353-4858(08)70130-2
UR  - http://www.sciencedirect.com/science/article/pii/S1353485808701302
AB  - We have discussed incident response many times in Network Security. The technical tasks (which include many pertinent to computer forensics) are handled mainly with the use of technology based on scientifically established methodologies.
ER  - 

TY  - JOUR
T1  - A Hierarchical Visibility theory for formal digital investigation of anti-forensic attacks
JO  - Computers & Security
VL  - 31
IS  - 8
SP  - 967
EP  - 982
PY  - 2012/11//
T2  - 
AU  - Rekhis, Slim
AU  - Boudriga, Noureddine
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/j.cose.2012.06.009
UR  - http://www.sciencedirect.com/science/article/pii/S0167404812001022
KW  - Formal methods
KW  - Digital forensic investigation
KW  - Provable anti-forensics attacks
KW  - Hierarchical Visibility
KW  - Digital evidence modelling
AB  - Among the leading topics of research in digital forensic investigation is the development of theoretical and scientifically proven techniques of incident analysis. However, two main problems, which remain unsolved in the literature, could lead the use of formal approaches of attack scenarios reconstruction and incident analysis to be inconclusive. The former is related to the absence of techniques to model and characterize anti-forensic attacks, and cope with the reconstruction of attack scenarios based on evidences compromised by these attacks. The latter is related to the lack of theoretical techniques usable during the preparation of systems to forensic analysis (i.e., the first phase of a forensic process that precedes the occurrence of an incident and the collection of evidences). These techniques are expected to determine the optimal set of security solutions to deploy so that the evidences to be generated further to a security incident would be sufficient to prove a wide range of anti-forensic attacks.

In this paper we propose a formal approach, based on a novel theory of Hierarchical Visibility, allowing to forensically investigate security incidents that are conducted over complex systems and integrate anti-forensic attacks. We develop a formal logic-based model useful for the representation of complex systems and scenarios of attacks under different levels of abstractions, and the description of the deployed security solutions together with the evidences they generated. The theory of Hierarchical Visibility that we provide in this paper allows reasoning on anti-forensic attacks over complex systems, characterize situations under which they are provable, and prove their occurrence starting from incomplete evidences. An extension of the forensic process showing the use of Hierarchical Visibility theory to increase the number of provable anti-forensic attacks, is described. We illustrate the proposal using a case study related to the investigation of a denial of service attack over an SSH service.
ER  - 

TY  - JOUR
T1  - Design and development of Distributed Virtual Geographic Environment system based on web services
JO  - Information Sciences
VL  - 177
IS  - 19
SP  - 3968
EP  - 3980
PY  - 2007/10/1/
T2  - 
AU  - Zhang, Jianqin
AU  - Gong, Jianhua
AU  - Lin, Hui
AU  - Wang, Gang
AU  - Huang, JianLing
AU  - Zhu, Jun
AU  - Xu, Bingli
AU  - Teng, Jack
SN  - 0020-0255
DO  - http://dx.doi.org/10.1016/j.ins.2007.02.049
UR  - http://www.sciencedirect.com/science/article/pii/S0020025507001338
KW  - Virtual Environment
KW  - Virtual Geographic Environment
KW  - Distributed computing
KW  - Web services
KW  - J2EE
KW  - Grid services
AB  - This paper aims to design and develop a Distributed Virtual Geographic Environment (DVGE) system. A DVGE system is an Internet-based virtual 2D and 3D environment that provides users with a shared space and a collaborative platform for publishing multidimensional geo-data, and for simulating and analyzing complex geo-phenomena. Users logging into the system from different clients can share distributed geo-information resources, including geo-data and geo-models, and can complete collaborative tasks. Web service technology provides effective solutions for constructing DVGE systems because of its ability to support multi-platform, multi-architecture, and multi-program-language interoperability on the Internet, but also because of its ability to share programs, data, and software. This paper analyzes the characteristics, relevant technologies, and specifications of web services, such as grid services, Open Geo-data Interoperability Specifications (OpenGIS), and Geography Markup Languages (GML). The architecture and working mechanisms of the DVGE system based on web services are then elaborated. To demonstrate DVGE systems based on web services, we examine a case study of water pollution in Yangzhou City, Jiangsu Province, China, using a prototype DVGE system that is developed with Jbuilder9.0 and Java3D 1.0 packages, and the Weblogic platform 8.1.
ER  - 

TY  - JOUR
T1  - Multi-faceted quality and defect measurement for web software and source contents
JO  - Journal of Systems and Software
VL  - 83
IS  - 1
SP  - 18
EP  - 28
PY  - 2010/1//
T2  - SI: Top Scholars
AU  - Li, Zhao
AU  - Alaeddine, Nasser
AU  - Tian, Jeff
SN  - 0164-1212
DO  - http://dx.doi.org/10.1016/j.jss.2009.04.055
UR  - http://www.sciencedirect.com/science/article/pii/S0164121209001149
KW  - Quality and reliability
KW  - Defect measurement and analysis
KW  - Web software and contents
KW  - Dynamic web applications
KW  - Web application development and maintenance
AB  - In this paper, we examine external failures and internal faults traceable to web software and source contents. We develop related defect and quality measurements based on different perspectives of customers, users, information or service hosts, maintainers, developers, integrators, and managers. These measurements can help web information and service providers with their quality assessment and improvement activities to meet the quality expectations of their customers and users. The different usages of our measurement framework by different stakeholders of web sites and web applications are also outlined and discussed. The data sources include existing web server logs and statistics reports, defect repositories from web application development and maintenance activities, and source files. We applied our approach to four diverse websites: one educational website, one open source software project website, one online catalog showroom for a small company, and one e-Commerce website for a large company. The results demonstrated the viability and effectiveness of our approach.
ER  - 

TY  - JOUR
T1  - Network forensic frameworks: Survey and research challenges
JO  - Digital Investigation
VL  - 7
IS  - 1–2
SP  - 14
EP  - 27
PY  - 2010/10//
T2  - 
AU  - Pilli, Emmanuel S.
AU  - Joshi, R.C.
AU  - Niyogi, Rajdeep
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2010.02.003
UR  - http://www.sciencedirect.com/science/article/pii/S1742287610000113
KW  - Network forensics
KW  - NFATs
KW  - Distributed systems
KW  - Soft computing
KW  - Honeypots
KW  - Data fusion
KW  - Attribution
KW  - Traceback
KW  - Incident response
AB  - Network forensics is the science that deals with capture, recording, and analysis of network traffic for detecting intrusions and investigating them. This paper makes an exhaustive survey of various network forensic frameworks proposed till date. A generic process model for network forensics is proposed which is built on various existing models of digital forensics. Definition, categorization and motivation for network forensics are clearly stated. The functionality of various Network Forensic Analysis Tools (NFATs) and network security monitoring tools, available for forensics examiners is discussed. The specific research gaps existing in implementation frameworks, process models and analysis tools are identified and major challenges are highlighted. The significance of this work is that it presents an overview on network forensics covering tools, process models and framework implementations, which will be very much useful for security practitioners and researchers in exploring this upcoming and young discipline.
ER  - 

TY  - JOUR
T1  - Preparing for the end
JO  - Computer Fraud & Security
VL  - 2009
IS  - 10
SP  - 5
EP  - 7
PY  - 2009/10//
T2  - 
AU  - Pemble, Matthew
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(09)70126-5
UR  - http://www.sciencedirect.com/science/article/pii/S1361372309701265
AB  - Failing to ensure correct disposal (and retention) of information assets has caused significant data leaks for numerous organisations. Increased legal and regulatory powers and attention, media interest and formal compliance regime requirements all mean that this often neglected area deserves the same consideration applied to more in-vogue aspects of security. It should be incorporated into organisational policies and processes as well as specifically considered as part of infrastructure and location projects.

Destroying stuff, while occasionally satisfying in a mindlessly violent way, is an important but often neglected aspect of information security. Whether you are considering the physical destruction of defective equipment, the sanitisation of corporate data before publication or how to ensure that your suppliers do not accidentally release your sensitive data to the world and its dog, proper disposal of information assets is both critical and easy to get horribly wrong.
ER  - 

TY  - JOUR
T1  - Intrusion detection system: A comprehensive review
JO  - Journal of Network and Computer Applications
VL  - 36
IS  - 1
SP  - 16
EP  - 24
PY  - 2013/1//
T2  - 
AU  - Liao, Hung-Jen
AU  - Richard Lin, Chun-Hung
AU  - Lin, Ying-Chih
AU  - Tung, Kuang-Yuan
SN  - 1084-8045
DO  - http://dx.doi.org/10.1016/j.jnca.2012.09.004
UR  - http://www.sciencedirect.com/science/article/pii/S1084804512001944
KW  - Intrusion detection
KW  - Anomaly
KW  - Misuse
AB  - With the increasing amount of network throughput and security threat, the study of intrusion detection systems (IDSs) has received a lot of attention throughout the computer science field. Current IDSs pose challenges on not only capricious intrusion categories, but also huge computational power. Though there is a number of existing literatures to IDS issues, we attempt to give a more elaborate image for a comprehensive review. Through the extensive survey and sophisticated organization, we propose the taxonomy to outline modern IDSs. In addition, tables and figures we summarized in the content contribute to easily grasp the overall picture of IDSs.
ER  - 

TY  - JOUR
T1  - An integrated architecture for tightly coupled design and evaluation of educational multimedia
JO  - Information Sciences
VL  - 140
IS  - 1–2
SP  - 127
EP  - 152
PY  - 2002/1//
T2  - Interactive virtual environments and distance education
AU  - Holmquist, Selma
AU  - Narayanan, N.Hari
SN  - 0020-0255
DO  - http://dx.doi.org/10.1016/S0020-0255(01)00184-0
UR  - http://www.sciencedirect.com/science/article/pii/S0020025501001840
KW  - Educational multimedia
KW  - Iterative design
KW  - Interaction data visualization
KW  - Authoring
KW  - Evaluation
AB  - Multimedia technology is increasingly being used to create instructional environments for distance education. However, the design of educational multimedia is currently driven more by intuition than by empirically or theoretically derived design guidelines. In the absence of prescriptive design principles, iterative design – a cyclical process of design, test and redesign – becomes critically important for creating effective educational multimedia. This paper proposes a framework for the iterative design of a class of educational multimedia called Hypermedia Educational Manuals. The architecture of an authoring and evaluation platform designed to support this framework and ease the designer's task is presented. Two unique features of this tool are the automatic creation of a system structure definition of the multimedia system being authored, and the automatic incorporation of interaction-logging elements in the system. This is done by an authoring component that assists the designer with the design and implementation of educational multimedia. An evaluation component then parses both the system structure definition and interaction logs created while students work with the multimedia system, in order to generate statistical analyses and graphical visualizations of how students interacted with the multimedia. The integrated architecture of the tool embodies a tight coupling between the design of educational multimedia and the evaluation of its effectiveness. This coupling reduces the time and effort required in repeating evaluation–redesign cycles in order to iteratively refine the initial design. We also briefly describe an experimental demonstration of the utility of this tool.
ER  - 

TY  - JOUR
T1  - Presentation of dermatological images on the Internet
JO  - Computer Methods and Programs in Biomedicine
VL  - 65
IS  - 2
SP  - 111
EP  - 121
PY  - 2001/5//
T2  - 
AU  - Ribari?, Samo
AU  - Todorovski, Ljup?o
AU  - Dimec, Jure
AU  - Lunder, Tomaž
SN  - 0169-2607
DO  - http://dx.doi.org/10.1016/S0169-2607(00)00118-8
UR  - http://www.sciencedirect.com/science/article/pii/S0169260700001188
KW  - Medical images
KW  - Dermatology database
KW  - Self-learning
KW  - Internet
KW  - User interfaces
AB  - In this paper, we focused on selected problems of integrating and presenting medical images organised in a World Wide Web (WWW) database. To solve these problems we developed a prototype of a bilingual (Slovenian and English) WWW database of medical images for the field of dermatology. This dermatology database includes a graphic interface with four modes of access: (1) browsing, (2) searching, (3) comparison of images, and (4) self-testing. The quantity and quality of requests to this WWW database was estimated with log file analysis. There was a steady increase in the number of users and volume of data transferred from the dermatology WWW database.
ER  - 

TY  - JOUR
T1  - Network attacks: Taxonomy, tools and systems
JO  - Journal of Network and Computer Applications
VL  - 40
IS  - 
SP  - 307
EP  - 324
PY  - 2014/4//
T2  - 
AU  - Hoque, N.
AU  - Bhuyan, Monowar H.
AU  - Baishya, R.C.
AU  - Bhattacharyya, D.K.
AU  - Kalita, J.K.
SN  - 1084-8045
DO  - http://dx.doi.org/10.1016/j.jnca.2013.08.001
UR  - http://www.sciencedirect.com/science/article/pii/S1084804513001756
KW  - Network attacks
KW  - Tools
KW  - Systems
KW  - Protocol
KW  - DoS
AB  - Abstract
To prevent and defend networks from the occurrence of attacks, it is highly essential that we have a broad knowledge of existing tools and systems available in the public domain. Based on the behavior and possible impact or severity of damages, attacks are categorized into a number of distinct classes. In this survey, we provide a taxonomy of attack tools in a consistent way for the benefit of network security researchers. This paper also presents a comprehensive and structured survey of existing tools and systems that can support both attackers and network defenders. We discuss pros and cons of such tools and systems for better understanding of their capabilities. Finally, we include a list of observations and some research challenges that may help new researchers in this field based on our hands-on experience.
ER  - 

TY  - JOUR
T1  - InnoDB database forensics: Enhanced reconstruction of data manipulation queries from redo logs
JO  - Information Security Technical Report
VL  - 17
IS  - 4
SP  - 227
EP  - 238
PY  - 2013/5//
T2  - Special Issue: ARES 2012 7th International Conference on Availability, Reliability and Security
AU  - Frühwirt, Peter
AU  - Kieseberg, Peter
AU  - Schrittwieser, Sebastian
AU  - Huber, Markus
AU  - Weippl, Edgar
SN  - 1363-4127
DO  - http://dx.doi.org/10.1016/j.istr.2013.02.003
UR  - http://www.sciencedirect.com/science/article/pii/S1363412713000137
KW  - MySQL
KW  - InnoDB
KW  - Digital forensics
KW  - Databases
KW  - Log files
AB  - Abstract
The InnoDB storage engine is one of the most widely used storage engines for MySQL. This paper discusses possibilities of utilizing the redo logs of InnoDB databases for forensic analysis, as well as the extraction of the information needed from the MySQL definition files, in order to carry out this kind of analysis. Since the redo logs are internal log files of the storage engine and thus cannot easily be changed undetected, this forensic method can be very useful against adversaries with administrator privileges, which could otherwise cover their tracks by manipulating traditional log files intended for audit and control purposes. Based on a prototype implementation, we show methods for recovering Insert, Delete and Update statements issued against a database.
ER  - 

TY  - JOUR
T1  - A game-based intrusion detection mechanism to confront internal attackers
JO  - Computers & Security
VL  - 29
IS  - 8
SP  - 859
EP  - 874
PY  - 2010/11//
T2  - 
AU  - Kantzavelou, Ioanna
AU  - Katsikas, Sokratis
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/j.cose.2010.06.002
UR  - http://www.sciencedirect.com/science/article/pii/S0167404810000611
KW  - Intrusion detection
KW  - Internal attacker
KW  - Non-cooperative game theory
KW  - Repeated game
KW  - Quantal response equilibrium
KW  - Detection mechanism
AB  - Insiders might threaten organizations’ systems any time. By interacting with a system, an insider plays games with the security mechanisms employed to protect it. We apply game theory to model these interactions in an extensive form game that is being played repeatedly with an Intrusion Detection System (IDS). The outcomes of the game are quantified by first specifying players’ preferences, and then, by using the von Neumann–Morgenstern utility function, to assign numbers that reflect these preferences. Examining players’ best responses, the solution of the game follows by locating all the Nash Equilibria (NE). We extend the NE notion to the logit Quantal Response Equilibrium (QRE), to capture players’ bounded rationality and model insider’s behavior. The QRE results are more realistic, and show that the solution of the game might be significantly different than the corresponding NE solution. Thus, we determine how an insider will interact in the future, and how an IDS will react to protect the system. To easily exploit QRE results in ID, we propose the use of a detection mechanism. To present a possible implementation scheme of the detection mechanism, we give the application model and a detailed game-based detection algorithm.

Categories and Subject Descriptors: C.2.0 [Computer-Communication Networks]: General -- Security and protection. D.4.6 [Operating Systems]: Security and Protection.
ER  - 

TY  - JOUR
T1  - Foundations of computer forensics: A technology for the fight against computer crime
JO  - Computer Law & Security Review
VL  - 21
IS  - 2
SP  - 119
EP  - 127
PY  - 2005///
T2  - 
AU  - Wang, Yun
AU  - Cannady, James
AU  - Rosenbluth, James
SN  - 0267-3649
DO  - http://dx.doi.org/10.1016/j.clsr.2005.02.007
UR  - http://www.sciencedirect.com/science/article/pii/S0267364905000725
AB  - With the rapid advance in computer and network technology, computer-based electronic evidence has increasingly played an important role in the courtroom over the last decade. Computer forensics, a growing discipline rooted in forensic science and computer security technology, focuses on acquiring electronic evidence from computer systems to prosecute computer crimes, national security threats, and computer abuse. It has lost its mystique as a technique used solely by law enforcement and intelligence agents, and has become a popular and powerful application employed by corporations for civil disputes, employee terminations, and intellectual property proceedings. This article provides an introduction to computer forensics and outlines the associated inspection steps.
ER  - 

TY  - JOUR
T1  - Two case studies in grammar-based test generation
JO  - Journal of Systems and Software
VL  - 83
IS  - 12
SP  - 2369
EP  - 2378
PY  - 2010/12//
T2  - TAIC PART 2009 - Testing: Academic &amp; Industrial Conference - Practice And Research Techniques
AU  - Hoffman, Daniel
AU  - Wang, Hong-Yi
AU  - Chang, Mitch
AU  - Ly-Gagnon, David
AU  - Sobotkiewicz, Lewis
AU  - Strooper, Paul
SN  - 0164-1212
DO  - http://dx.doi.org/10.1016/j.jss.2010.07.048
UR  - http://www.sciencedirect.com/science/article/pii/S0164121210002050
KW  - Automated testing
KW  - Grammar-based test generation
KW  - Covering array
KW  - Really Simple Syndication (RSS)
KW  - eXtended Markup Language (XML)
KW  - TCP
AB  - Grammar-based test generation (GBTG) has seen extensive study and practical use since the 1970s. GBTG was introduced to generate source code for testing compilers from context-free grammars specifying language syntax. More recently, GBTG has been applied to many other testing problems, including the generation of eXtensible Markup Language (XML) documents and the generation of packets for testing communications protocols. Recent research has shown how to integrate covering-array techniques such as pairwise testing into GBTG tools. While the integration offers considerable power to the tester, there are few practical demonstrations in the literature. We present two case studies showing how to use grammars and covering arrays for automated software testing. The first case study exposes HTML injection vulnerabilities in an RSS feed parser. The second case study determines the effectiveness of network firewalls when faced with TCP flag attacks. The case studies illustrate the use of covering arrays in a GBTG context, the use of visualization to understand large test logs, and the issues and tradeoffs in the design of fully automated GBTG test suites.
ER  - 

TY  - JOUR
T1  - Digital investigations for IPv6-based Wireless Sensor Networks
JO  - Digital Investigation
VL  - 11, Supplement 2
IS  - 
SP  - S66
EP  - S75
PY  - 2014/8//
T2  - Fourteenth Annual DFRWS Conference
AU  - Kumar, Vijay
AU  - Oikonomou, George
AU  - Tryfonas, Theo
AU  - Page, Dan
AU  - Phillips, Iain
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2014.05.005
UR  - http://www.sciencedirect.com/science/article/pii/S1742287614000486
KW  - Internet of Things
KW  - Wireless Sensor Networks
KW  - RAM and flash memory extraction
KW  - RAM content analysis
KW  - Contiki Operating System
KW  - Wireless sensor forensics
AB  - Abstract
Developments in the field of Wireless Sensor Networks (WSNs) and the Internet of Things (IoT) mean that sensor devices can now be uniquely identified using an IPv6 address and, if suitably connected, can be directly reached from the Internet. This has a series of advantages but also introduces new security vulnerabilities and exposes sensor deployments to attack. A compromised Internet host can send malicious information to the system and trigger incorrect actions. Should an attack take place, post-incident analysis can reveal information about the state of the network at the time of the attack and ultimately provide clues about the tools used to implement it, or about the attackr's identity. In this paper we critically assess and analyse information retrieved from a device used for IoT networking, in order to identify the factors which may have contributed to a security breach. To achieve this, we present an approach for the extraction of RAM and flash contents from a sensor node. Subsequently, we analyse extracted network connectivity information and we investigate the possibility of correlating information gathered from multiple devices in order to reconstruct the network topology. Further, we discuss experiments and analyse how much information can be retrieved in different scenarios. Our major contribution is a mechanism for the extraction, analysis and correlation of forensic data for IPv6-based WSN deployments, accompanied by a tool which can analyse RAM dumps from devices running the Contiki Operating System (OS) and powered by 8051-based, 8-bit micro-controllers.
ER  - 

TY  - JOUR
T1  - Making the link—providing mobile media for novice communities in the developing world
JO  - International Journal of Human-Computer Studies
VL  - 69
IS  - 10
SP  - 647
EP  - 657
PY  - 2011/9//
T2  - Locative media and communities
AU  - Maunder, Andrew
AU  - Marsden, Gary
AU  - Harper, Richard
SN  - 1071-5819
DO  - http://dx.doi.org/10.1016/j.ijhcs.2010.12.009
UR  - http://www.sciencedirect.com/science/article/pii/S1071581910001667
KW  - ICT4D
KW  - M4D
KW  - Situated media
AB  - In this paper we investigate the media needs of low-income mobile users in a South African township. We develop and deploy a system that allows users to download media at no costs to themselves, in order to probe future media requirements for similar user groups. We discover that not only are the community interested in developmental information, but are also just as interested in sharing local music or videos. Furthermore, the community consume the media in ways that we did not expect, which had direct impacts on their lives. Finally, we conclude with some reflections on the value of media and the most appropriate ways to deliver it in developing world communities.
ER  - 

TY  - JOUR
T1  - A study on the forensic mechanisms of VoIP attacks: Analysis and digital evidence
JO  - Digital Investigation
VL  - 8
IS  - 1
SP  - 56
EP  - 67
PY  - 2011/7//
T2  - 
AU  - Yen, Yun-Sheng
AU  - Lin, I.-Long
AU  - Wu, Bo-Lin
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2011.03.003
UR  - http://www.sciencedirect.com/science/article/pii/S1742287611000223
KW  - Computer forensics
KW  - Digital evidence
KW  - VoIP
KW  - Forensic computing
KW  - Cyber crime
AB  - This paper discusses the use of communication technology to commit crimes, including crime facts and crime techniques. The analysis focuses on the security of voice over Internet protocol (VoIP), a prevention method against VoIP call attack and the attention points for setting up an Internet phone. The importance of digital evidence and digital forensics are emphasised. This paper provides the VoIP digital evidence forensics standard operating procedures (DEFSOP) to help police organisations and establishes an experimental platform to simulate phone calls, hacker attacks and forensic data. Finally, this paper provides a general discussion of a digital evidence strategy that includes VoIP for crime investigators who are interested in digital evidence forensics.
ER  - 

TY  - JOUR
T1  - A distributed requirements management framework for legal compliance and accountability
JO  - Computers & Security
VL  - 28
IS  - 1–2
SP  - 8
EP  - 17
PY  - 2009/2//
Y2  - 2009/3//
T2  - 
AU  - Breaux, Travis D.
AU  - Antón, Annie I.
AU  - Spafford, Eugene H.
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/j.cose.2008.08.001
UR  - http://www.sciencedirect.com/science/article/pii/S0167404808000679
KW  - Requirements engineering
KW  - Compliance
KW  - Accountability
KW  - Policy
KW  - regulation
AB  - Increasingly, new regulations are governing organizations and their information systems. Individuals responsible for ensuring legal compliance and accountability currently lack sufficient guidance and support to manage their legal obligations within relevant information systems. While software controls provide assurances that business processes adhere to specific requirements, such as those derived from government regulations, there is little support to manage these requirements and their relationships to various policies and regulations. We propose a requirements management framework that enables executives, business managers, software developers and auditors to distribute legal obligations across business units and/or personnel with different roles and technical capabilities. This framework improves accountability by integrating traceability throughout the policy and requirements lifecycle. We illustrate the framework within the context of a concrete healthcare scenario in which obligations incurred from the Health Insurance Portability and Accountability Act (HIPAA) are delegated and refined into software requirements. Additionally, we show how auditing mechanisms can be integrated into the framework and how auditors can certify that specific chains of delegation and refinement decisions comply with government regulations.
ER  - 

TY  - JOUR
T1  - Vehicular telematics over heterogeneous wireless networks: A survey
JO  - Computer Communications
VL  - 33
IS  - 7
SP  - 775
EP  - 793
PY  - 2010/5/3/
T2  - 
AU  - Hossain, Ekram
AU  - Chow, Garland
AU  - Leung, Victor C.M.
AU  - McLeod, Robert D.
AU  - Miši?, Jelena
AU  - Wong, Vincent W.S.
AU  - Yang, Oliver
SN  - 0140-3664
DO  - http://dx.doi.org/10.1016/j.comcom.2009.12.010
UR  - http://www.sciencedirect.com/science/article/pii/S0140366410000022
KW  - Intelligent transportation systems
KW  - Vehicular telematics
KW  - Heterogeneous wireless networks
AB  - This article presents a survey on vehicular telematics over heterogeneous wireless networks. An advanced heterogeneous vehicular network (AHVN) architecture is outlined which uses multiple access technologies and multiple radios in a collaborative manner. The challenges in designing the essential functional components of AHVN and the corresponding protocols (for radio link control, routing, congestion control, security and privacy, and application development) are discussed and the related work in the literature are reviewed. The open research challenges and several avenues for future research on vehicular telematics over heterogeneous wireless access networks are outlined.
ER  - 

TY  - JOUR
T1  - High precision framework for chaos many-body engine
JO  - Computer Physics Communications
VL  - 185
IS  - 4
SP  - 1339
EP  - 1342
PY  - 2014/4//
T2  - 
AU  - Grossu, I.V.
AU  - Besliu, C.
AU  - Felea, D.
AU  - Jipa, Al.
SN  - 0010-4655
DO  - http://dx.doi.org/10.1016/j.cpc.2013.12.024
UR  - http://www.sciencedirect.com/science/article/pii/S0010465513004360
KW  - Gravitation
KW  - Chaos
KW  - Many-body
KW  - C#
KW  - Butterfly effect
KW  - Intermittency
AB  - Abstract
In this paper we present a C# 4.0 high precision framework for simulation of relativistic many-body systems. In order to benefit from the, previously developed, chaos analysis instruments, all new modules were integrated with Chaos Many-Body Engine (Grossu et al. 2010, 2013). As a direct application, we used 46 digits precision for analyzing the “Butterfly Effect” of the gravitational force in a specific relativistic nuclear collision toy-model.
Program summary
Program title: Chaos Many-Body Engine v04.1

Catalogue identifier: AEGH_v4_0

Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AEGH_v4_0.html

Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland

Licensing provisions: Microsoft Public License (Ms-PL)

No. of lines in distributed program, including test data, etc.: 307938

No. of bytes in distributed program, including test data, etc.: 11953299

Distribution format: tar.gz

Programming language: Visual C# Express 2010.

Computer: PC.

Operating system:  .Net Framework 4.0 running on MS Windows.

RAM: 100 Megabytes

Classification: 6.2, 6.5.

External routines: BigRational structure provided by Microsoft

Does the new version supersede the previous version?: yes

Nature of problem:

high precision simulation of relativistic many-body systems.

Solution method:

high precision calculations based on BigInteger .Net Framework 4.0 new feature.

Reasons for new version:

development of a high precision framework

Summary of revisions:

 • high precision framework based on the new BigInteger .Net Framework 4.0 structure

 • high precision relativistic many-body engine

 • concrete application: using 46 digit precision for analyzing the gravitational Butterfly Effect in a specific relativistic nuclear collision toy-model

 • CMBE Reactions Module Bug Correction: in the particular case of two identical particles head-on collision, reactions were not treated in earlier versions of CMBE.

 • Chaos Analysis: implementation of a new measure “Average Y” for computing the average of any function loaded in this module.

 • Chaos Analysis: implementation of the phase space distance between two many-body systems, as a function of time.

 • Chaos Analysis: Implementation of a decimal version of the Chaos Analysis module.

 • Chaos Analysis: Implementation of some usual relativistic formulas for facilitating processing of Monte Carlo log files (Analysis ? Relativistic Formulas XLS).

Additional comments:

XCopy deployment strategy.

Running time:

Quadratic complexity, around 2 h for one C+C event, 50 Fm/c, on a dual core @ 2.0 GHz CPU
ER  - 

TY  - JOUR
T1  - Assessing learning outcomes in two information retrieval learning environments
JO  - Information Processing & Management
VL  - 41
IS  - 4
SP  - 949
EP  - 972
PY  - 2005/7//
T2  - 
AU  - Halttunen, Kai
AU  - Järvelin, Kalervo
SN  - 0306-4573
DO  - http://dx.doi.org/10.1016/j.ipm.2004.02.004
UR  - http://www.sciencedirect.com/science/article/pii/S0306457304000123
KW  - Learning outcomes
KW  - Information retrieval instruction
KW  - Learning environments
KW  - Conceptual change
KW  - Performance assessment
AB  - In order to design information retrieval (IR) learning environments and instruction, it is important to explore learning outcomes of different pedagogical solutions. Learning outcomes have seldom been evaluated in IR instruction. The particular focus of this study is the assessment of learning outcomes in an experimental, but naturalistic, learning environment compared to more traditional instruction. The 57 participants of an introductory course on IR were selected for this study, and the analysis illustrates their learning outcomes regarding both conceptual change and development of IR skill. Concept mapping of student essays was used to analyze conceptual change and log-files of search exercises provided data for performance assessment. Students in the experimental learning environment changed their conceptions more regarding linguistic aspects of IR and paid more emphasis on planning and management of search process. Performance assessment indicates that anchored instruction and scaffolding with an instructional tool, the IR Game, with performance feedback enables students to construct queries with fewer semantic knowledge errors also in operational IR systems.
ER  - 

TY  - JOUR
T1  - Is the mouse click mighty enough to bring society to its knees?
JO  - Computers & Security
VL  - 22
IS  - 4
SP  - 322
EP  - 336
PY  - 2003/5//
T2  - 
AU  - Bhalla, Neelam
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/S0167-4048(03)00412-7
UR  - http://www.sciencedirect.com/science/article/pii/S0167404803004127
KW  - Information warfare
KW  - Defensive warfare
KW  - Offensive warfare
KW  - Battlefield
KW  - Commanders
KW  - Army
KW  - Weapons
AB  - Over the years, we have created an information infrastructure, with most of us connected but nobody totally responsible, which is easily targeted for attacks by adversaries. A series of deadly viruses and denial-of-service attacks are warnings of the fragile state of information security. Information warfare (IW) is a serious concern as it has no border and operates in a different realm. This paper articulates how IW can devastatingly attack the critical information infrastructure. Two aspects of IW are considered, viz., Defensive IW and Offensive IW. Currently, security solutions lag far behind the potential threats. Loosely coupled defenders cannot avert the damage caused by orchestrated coordinated attacks. Society is locked up in a vicious circle of wits and resources. This situation is likely to continue and we need to be proactive and make progress on a rocky path. It is time we got battle space ready. Without the coordinated efforts of government, disparate groups and organisations, society is one click away from grave danger of exploitation and dominance.
ER  - 

TY  - JOUR
T1  - Forensic analysis of logs: Modeling and verification
JO  - Knowledge-Based Systems
VL  - 20
IS  - 7
SP  - 671
EP  - 682
PY  - 2007/10//
T2  - Special Issue on Techniques to Produce Intelligent Secure Software
AU  - Saleh, Mohamed
AU  - Arasteh, Ali Reza
AU  - Sakha, Assaad
AU  - Debbabi, Mourad
SN  - 0950-7051
DO  - http://dx.doi.org/10.1016/j.knosys.2007.05.002
UR  - http://www.sciencedirect.com/science/article/pii/S0950705107000561
KW  - Forensic analysis
KW  - Log analysis
KW  - Formal methods
KW  - Model checking
KW  - Logging systems
AB  - Information stored in logs of a computer system is of crucial importance to gather forensic evidence of investigated actions or attacks against the system. Analysis of this information should be rigorous and credible, hence it lends itself to formal methods. We propose a model checking approach to the formalization of the forensic analysis of logs. The set of logs of a certain system is modeled as a tree whose labels are events extracted from the logs. In order to provide a structure to these events, we express each event as a term of a term algebra. The signature of the algebra is carefully chosen to include all relevant information necessary to conduct the analysis. Properties of the model are expressed as formulas of a logic having dynamic, linear, temporal, and modal characteristics. Moreover, we provide a tableau-based proof system for this logic upon which a model checking algorithm can be developed. In order to illustrate the proposed approach, the Windows auditing system is studied. The properties that we capture in our logic include invariant properties of a system, forensic hypotheses, and generic or specific attack signatures. Moreover, we discuss the admissibility of forensics hypotheses and the underlying verification issues.
ER  - 

TY  - JOUR
T1  - Intrusion detection techniques and approaches
JO  - Computer Communications
VL  - 25
IS  - 15
SP  - 1356
EP  - 1365
PY  - 2002/9/15/
T2  - 
AU  - Verwoerd, Theuns
AU  - Hunt, Ray
SN  - 0140-3664
DO  - http://dx.doi.org/10.1016/S0140-3664(02)00037-3
UR  - http://www.sciencedirect.com/science/article/pii/S0140366402000373
KW  - Intrusion detection
KW  - Probe technique
KW  - scanning
KW  - honeynet
KW  - Worm/virus attack
AB  - Recent security incidents and analysis have demonstrated that manual response to such attacks is no longer feasible. Intrusion detection systems (IDS) offer techniques for modelling and recognising normal and abusive system behaviour. Such methodologies include statistical models, immune system approaches, protocol verification, file and taint checking, neural networks, whitelisting, expression matching, state transition analysis, dedicated languages, genetic algorithms and burglar alarms. This paper describes these techniques including an IDS architectural outline and an analysis of IDS probe techniques finishing with a summary of associated technologies.
ER  - 

TY  - JOUR
T1  - File Marshal: Automatic extraction of peer-to-peer data
JO  - Digital Investigation
VL  - 4, Supplement
IS  - 
SP  - 43
EP  - 48
PY  - 2007/9//
T2  - 
AU  - Adelstein, Frank
AU  - Joyce, Robert A.
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2007.06.016
UR  - http://www.sciencedirect.com/science/article/pii/S1742287607000400
KW  - Peer-to-peer
KW  - P2P
KW  - Forensics
KW  - LimeWire
KW  - File sharing
AB  - Digital forensic investigators often find peer-to-peer, or file sharing, software present on the computers, or the images of the disks, that they examine. Investigators must first determine what P2P software is present and where the associated information is stored, retrieve the information from the appropriate directories, and then analyze the results. File Marshal is a tool that will automatically detect and analyze peer-to-peer client use on a disk. The tool automates what is currently a manual and labor intensive process. It will determine what clients currently are or have been installed on a machine, and then extracts per-user usage information, specifically a list of peer servers contacted, and files that were shared and downloaded. The tool was designed to perform its actions in a forensically sound way, including maintaining a detailed audit trail of all actions performed. File Marshal is extensible, using a configuration file to specify details about specific peer-to-peer clients (e.g., location of log files and registry keys indicating installation). This paper describes the general design and features of File Marshal, its current status, and the plans for continued development and release. When complete, File Marshal, a National Institute of Justice funded effort, will be disseminated to law enforcement at no cost.
ER  - 

TY  - JOUR
T1  - A portable network forensic evidence collector
JO  - Digital Investigation
VL  - 3
IS  - 3
SP  - 127
EP  - 135
PY  - 2006/9//
T2  - 
AU  - Nikkel, Bruce J.
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2006.08.012
UR  - http://www.sciencedirect.com/science/article/pii/S1742287606001022
KW  - Network forensics
KW  - Live network evidence
KW  - Live network acquisition
KW  - Live network forensics
KW  - NFAT
KW  - PNFEC
AB  - A small portable network forensic evidence collection device is presented which is built using inexpensive embedded hardware and open source software. The device offers several modes of operation for different live network evidence collection scenarios involving single network nodes. This includes the use of promiscuous packet capturing to enhance evidence collection from remote network sources, such as websites or other remote services. It operates at the link layer allowing the device to be transparently inserted inline between a network node and the rest of a network. It is simple to deploy, requiring no reconfiguration of the node or surrounding network infrastructure. The device can be preconfigured in the forensics lab, and deployment delegated to staff not specifically trained in forensics. Details of the architecture, construction and operation are described. Special attention is given to information security aspects of live network evidence collection.
ER  - 

TY  - JOUR
T1  - A scalable network forensics mechanism for stealthy self-propagating attacks
JO  - Computer Communications
VL  - 36
IS  - 13
SP  - 1471
EP  - 1484
PY  - 2013/7/15/
T2  - 
AU  - Chen, Li Ming
AU  - Chen, Meng Chang
AU  - Liao, Wanjiun
AU  - Sun, Yeali S.
SN  - 0140-3664
DO  - http://dx.doi.org/10.1016/j.comcom.2013.05.005
UR  - http://www.sciencedirect.com/science/article/pii/S0140366413001254
KW  - Network forensics
KW  - Data reduction
KW  - Stealthy self-propagating attack
KW  - Contact activity
AB  - Abstract
Network forensics supports capabilities such as attacker identification and attack reconstruction, which complement the traditional intrusion detection and perimeter defense techniques in building a robust security mechanism. Attacker identification pinpoints attack origin to deter future attackers, while attack reconstruction reveals attack causality and network vulnerabilities. In this paper, we discuss the problem and feasibility of back tracking the origin of a self-propagating stealth attack when given a network traffic trace for a sufficiently long period of time. We propose a network forensics mechanism that is scalable in computation time and space while maintaining high accuracy in the identification of the attack origin. We further develop a data reduction method to filter out attack-irrelevant data and only retain evidence relevant to potential attacks for a post-mortem investigation. Using real-world trace driven experiments, we evaluate the performance of the proposed mechanism and show that we can trim down up to 97% of attack-irrelevant network traffic and successfully identify attack origin.
ER  - 

TY  - JOUR
T1  - Case study: Forensic analysis of a Samsung digital video recorder
JO  - Digital Investigation
VL  - 5
IS  - 1–2
SP  - 19
EP  - 28
PY  - 2008/9//
T2  - 
AU  - van Dongen, Wouter S.
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2008.04.001
UR  - http://www.sciencedirect.com/science/article/pii/S1742287608000261
KW  - Digital video recorder
KW  - DVR
KW  - Digital video
KW  - MPEG-4
KW  - Hard disk recorder
KW  - Video
KW  - Carving
KW  - Recovery
KW  - Berkeley database
KW  - BTree
AB  - In May 2007, a case of potential child abuse was reported to the hospital where the victim was under observation. The child had been in the hospital for several months and there was hope that a digital video recorder (DVR) may have recorded the maltreatment of a hospitalized child. Unfortunately the recordings could not be found on the device by hospital security employees. The DVR was given to digital forensic examiners in an effort to recover footage. This article details how the system was examined, describing the steps that were taken to obtain information and how the information was interpreted. The methods described in this article can be applied to other similar devices.
ER  - 

TY  - JOUR
T1  - Design and implementation of a mediation system enabling secure communication among Critical Infrastructures
JO  - International Journal of Critical Infrastructure Protection
VL  - 5
IS  - 2
SP  - 86
EP  - 97
PY  - 2012/7//
T2  - 
AU  - Castrucci, Marco
AU  - Neri, Alessandro
AU  - Caldeira, Filipe
AU  - Aubert, Jocelyn
AU  - Khadraoui, Djamel
AU  - Aubigny, Matthieu
AU  - Harpes, Carlo
AU  - Simões, Paulo
AU  - Suraci, Vincenzo
AU  - Capodieci, Paolo
SN  - 1874-5482
DO  - http://dx.doi.org/10.1016/j.ijcip.2012.04.001
UR  - http://www.sciencedirect.com/science/article/pii/S1874548212000194
KW  - Critical Infrastructure
KW  - Information sharing
KW  - Web services
KW  - MICIE
KW  - Secure mediation gateway
AB  - Nowadays, the increase of interdependencies among different Critical Infrastructures (CI) makes it more and more difficult to protect without using a systemic approach that considers a single infrastructure as part of a complex system of infrastructures. A strong collaboration among CI owners is required to avoid, or at least to limit the propagation of failures from one infrastructure to another and to put CI in safety mode. The key element enabling this required cooperation is the possibility for them to exchange relevant information related to the status of their infrastructures and to the services provided. In this paper, we present a middleware solution that allows CIs sharing real-time information, enabling the design and implementation of fault mitigation strategies and mechanisms to prevent the cascading phenomena generated by the failure propagation from one infrastructure to another.
ER  - 

TY  - JOUR
T1  - Logs may be found boring, but they are good: NIST
JO  - Computer Fraud & Security
VL  - 2006
IS  - 5
SP  - 2
EP  - 3
PY  - 2006/5//
T2  - 

SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(06)70351-7
UR  - http://www.sciencedirect.com/science/article/pii/S1361372306703517
AB  - A US standards body has said that the benefits of logs are being thrown away because system administrators find it “boring.” It said that the analysis of logs is often “treated as a low-priority task” by administrators because more urgent tasks like fixing vulnerabilities come first.
ER  - 

TY  - JOUR
T1  - An efficient technique for enhancing forensic capabilities of Ext2 file system
JO  - Digital Investigation
VL  - 4, Supplement
IS  - 
SP  - 55
EP  - 61
PY  - 2007/9//
T2  - 
AU  - Barik, Mridul Sankar
AU  - Gupta, Gaurav
AU  - Sinha, Shubhro
AU  - Mishra, Alok
AU  - Mazumdar, Chandan
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2007.06.007
UR  - http://www.sciencedirect.com/science/article/pii/S1742287607000515
KW  - Electronic documents
KW  - Modification access and creation
KW  - date and time stamps (MAC DTS)
KW  - Authentic date and time stamps (ADTS)
KW  - Computer Frauds and Cyber Crimes (CFCC)
KW  - Ext2 file system
KW  - Loadable Kernel Module (LKM)
AB  - As electronic documents become more important and valuable in the modern era, attempts are invariably made to take undue-advantage by tampering with them. Tampering with the modification, access and creation date and time stamps (MAC DTS) of digital documents pose a great threat and proves to be a major handicap in digital forensic investigation. Authentic date and time stamps (ADTS) can provide crucial evidence in linking crime to criminal in cases of Computer Fraud and Cyber Crimes (CFCC) through reliable time lining of digital evidence. But the ease with which the MAC DTS of stored digital documents can be changed raises some serious questions about the integrity and admissibility of digital evidence, potentially leading to rejection of acquired digital evidence in the court of Law. MAC DTS procedures of popular operating systems are inherently flawed and were created only for the sake of convenience and not necessarily keeping in mind the security and digital forensic aspects. This paper explores these issues in the context of the Ext2 file system and also proposes one solution to tackle such issues for the scenario where systems have preinstalled plug-ins in the form of Loadable Kernel Modules, which provide the capability to preserve ADTS.
ER  - 

TY  - JOUR
T1  - Internet forensics on the basis of evidence gathering with Peep attacks
JO  - Computer Standards & Interfaces
VL  - 29
IS  - 4
SP  - 423
EP  - 429
PY  - 2007/5//
T2  - 
AU  - Wang, Shiuh-Jeng
AU  - Kao, Da-Yu
SN  - 0920-5489
DO  - http://dx.doi.org/10.1016/j.csi.2006.06.005
UR  - http://www.sciencedirect.com/science/article/pii/S0920548906000791
KW  - Digital forensic analysis
KW  - Forensic procedure
KW  - Peep attack
KW  - Botnets
KW  - Zombie computer
KW  - Cyber intrusion
KW  - Cybercrime investigation
KW  - Digital evidence
AB  - The Peep attack is a variant application of a Botnet. This paper proposes a forensic procedure to analyze the attack behavior and explains how to carry out a computer crime investigation. We also discuss the well-known Botnets engaged in the execution of a Peep attack. In our paper, we describe a Peep attack on the Internet as the paradigm of how a cyber-investigator needs to act in the case of a Cybercrime. When cyber detectives prepare to investigate a break in cyber security, there are some issues they must keep in mind and face up to. It is crucial to swiftly preserve digital evidence and conduct forensic analysis that any useful evidence is seized soon after the cybercrime has been committed. Furthermore, there are two phases of digital forensic analysis to retrieve useful evidence when facing a cybercrime attack in our scheme. One phase observes the Internet attack action, and the other one introduces how to investigate each case in on-line analysis of sniffing packets and off-line examination of abnormal files. We believe that this analysis model is workable for any other form of Botnets presently being used.
ER  - 

TY  - JOUR
T1  - Leaving a trace
JO  - Infosecurity
VL  - 5
IS  - 6
SP  - 30
EP  - 35
PY  - 2008/9//
T2  - 
AU  - Gold, Steve
SN  - 1754-4548
DO  - http://dx.doi.org/10.1016/S1754-4548(08)70102-5
UR  - http://www.sciencedirect.com/science/article/pii/S1754454808701025
AB  - IT forensics is seen by many in the industry as something of a black art. But it's actually a highly professional discipline, with professional software to assist, Steve Gold discovers
ER  - 

TY  - JOUR
T1  - Software forensics: Can we track code to its authors?
JO  - Computers & Security
VL  - 12
IS  - 6
SP  - 585
EP  - 595
PY  - 1993/10//
T2  - 
AU  - Spafford, Eugene H.
AU  - Weeber, Stephen A.
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/0167-4048(93)90055-A
UR  - http://www.sciencedirect.com/science/article/pii/016740489390055A
KW  - Authorship analysis
KW  - Reverse engineering
KW  - Malicious code
KW  - Software forensics
KW  - Investigation
AB  - Viruses, worms, trojan horses and crackers all exist and threaten the security of our computer systems. Often, we are aware of an intrusion only after it has occured. On some occasions, we may have a fragment of code left behind—used by an adversary to gain access or to damage the system. A natural question to ask is “Can we use this remnant of code to identify the culprit or gain clues as to his identity?rd

In this paper, we define the study of features of code remnants that might be analyzed to identify their authors. We further outline some of the difficulties involved in tracing an intruder by analyzing code. We conclude by discussing some future work that needs to be done before this approach can be more formally applied. We refer to our process as software forensics, similar to medical forensics: we are examining the remains to obtain evidence about the actors involved.
ER  - 

TY  - JOUR
T1  - Call for Papers
JO  - Computer Standards & Interfaces
VL  - 29
IS  - 2
SP  - I
EP  - 
PY  - 2007/2//
T2  - 

SN  - 0920-5489
DO  - http://dx.doi.org/10.1016/S0920-5489(06)00099-7
UR  - http://www.sciencedirect.com/science/article/pii/S0920548906000997
ER  - 

TY  - JOUR
T1  - Call for Papers
JO  - Computer Standards & Interfaces
VL  - 29
IS  - 1
SP  - IV
EP  - 
PY  - 2007/1//
T2  - ADC Modelling and Testing

SN  - 0920-5489
DO  - http://dx.doi.org/10.1016/S0920-5489(06)00097-3
UR  - http://www.sciencedirect.com/science/article/pii/S0920548906000973
ER  - 

TY  - JOUR
T1  - Report highlights
JO  - Information Security Technical Report
VL  - 3
IS  - 4
SP  - 3
EP  - 14
PY  - 1998///
T2  - 

SN  - 1363-4127
DO  - http://dx.doi.org/10.1016/S1363-4127(98)80034-4
UR  - http://www.sciencedirect.com/science/article/pii/S1363412798800344
ER  - 

TY  - JOUR
T1  - Computer forensics challenges in responding to incidents in real-life settings
JO  - Computer Fraud & Security
VL  - 2007
IS  - 12
SP  - 12
EP  - 16
PY  - 2007/12//
T2  - 
AU  - Schultz, E. Eugene
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(07)70169-0
UR  - http://www.sciencedirect.com/science/article/pii/S1361372307701690
AB  - Dr Eugene Shultz looks at the practice of computer forensics during a real-life incident.
ER  - 

TY  - JOUR
T1  - FORZA – Digital forensics investigation framework that incorporate legal issues
JO  - Digital Investigation
VL  - 3, Supplement
IS  - 
SP  - 29
EP  - 36
PY  - 2006/9//
T2  - The Proceedings of the 6th Annual Digital Forensic Research Workshop (DFRWS '06)
AU  - Ieong, Ricci S.C.
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2006.06.004
UR  - http://www.sciencedirect.com/science/article/pii/S1742287606000661
KW  - Digital forensics investigation framework
KW  - Digital forensics
KW  - FORZA framework
KW  - Forensics principles
KW  - Zachman framework
KW  - Legal aspects
AB  - What is Digital Forensics? Mark Pollitt highlighted in DFRWS 2004 [Politt MM. Six blind men from Indostan. Digital forensics research workshop (DFRWS); 2004] that digital forensics is not an elephant, it is a process and not just one process, but a group of tasks and processes in investigation. In fact, many digital forensics investigation processes and tasks were defined on technical implementation details Investigation procedures developed by traditional forensics scientist focused on the procedures in handling the evidence, while those developed by the technologist focused on the technical details in capturing evidence. As a result, many digital forensics practitioners simply followed technical procedures and forget about the actual purpose and core concept of digital forensics investigation.

With all these technical details and complicated procedures, legal practitioners may have difficulties in applying or even understanding their processes and tasks in digital forensics investigations.

In order to break the technical barrier between information technologists, legal practitioners and investigators, and their corresponding tasks together, a technical-independent framework would be required.

In this paper, we first highlighted the fundamental principle of digital forensics investigations (Reconnaissance, Reliability and Relevancy). Based on this principle, we re-visit the investigation tasks and outlined eight different roles and their responsibilities in a digital forensics investigation.

For each role, we defined the sets of six key questions. They are the What (the data attributes), Why (the motivation), How (the procedures), Who (the people), Where (the location) and When (the time) questions. In fact, among all the investigation processes, there are six main questions that each practitioner would always ask.

By incorporating these sets of six questions into the Zachman's framework, a digital forensics investigation framework – FORZA is composed. We will further explain how this new framework can incorporate legal advisors and prosecutors into a bigger picture of digital forensics investigation framework.

Usability of this framework will be illustrated in a web hacking example.

Finally, the road map that interconnects the framework to automatically zero-knowledge data acquisition tools will be briefly described.
ER  - 

TY  - JOUR
T1  - In brief
JO  - Network Security
VL  - 2010
IS  - 6
SP  - 3
EP  - 
PY  - 2010/6//
T2  - 

SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/S1353-4858(10)70079-9
UR  - http://www.sciencedirect.com/science/article/pii/S1353485810700799
ER  - 

TY  - JOUR
T1  - Leveraging CybOX™ to standardize representation and exchange of digital forensic information
JO  - Digital Investigation
VL  - 12, Supplement 1
IS  - 
SP  - S102
EP  - S110
PY  - 2015/3//
T2  - Proceedings of the Second Annual DFRWS Europe
AU  - Casey, Eoghan
AU  - Back, Greg
AU  - Barnum, Sean
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2015.01.014
UR  - http://www.sciencedirect.com/science/article/pii/S1742287615000158
KW  - Digital forensics
KW  - Standard representation
KW  - Digital forensic ontology
KW  - Digital forensic XML
KW  - CybOX
KW  - DFXML
KW  - DFAX
AB  - Abstract
With the growing number of digital forensic tools and the increasing use of digital forensics in various contexts, including incident response and cyber threat intelligence, there is a pressing need for a widely accepted standard for representing and exchanging digital forensic information. Such a standard representation can support correlation between different data sources, enabling more effective and efficient querying and analysis of digital evidence. This work summarizes the strengths and weaknesses of existing schemas, and proposes the open-source CybOX schema as a foundation for storing and sharing digital forensic information. The suitability of CybOX for representing objects and relationships that are common in forensic investigations is demonstrated with examples involving digital evidence. The capability to represent provenance by leveraging CybOX is also demonstrated, including specifics of the tool used to process digital evidence and the resulting output. An example is provided of an ongoing project that uses CybOX to record the state of a system before and after an event in order to capture cause and effect information that can be useful for digital forensics. An additional open-source schema and associated ontology called Digital Forensic Analysis eXpression (DFAX) is proposed that provides a layer of domain specific information overlaid on CybOX. DFAX extends the capability of CybOX to represent more abstract forensic-relevant actions, including actions performed by subjects and by forensic examiners, which can be useful for sharing knowledge and supporting more advanced forensic analysis. DFAX can be used in combination with other existing schemas for representing identity information (CIQ), and location information (KML). This work also introduces and leverages initial steps of a Unified Cyber Ontology (UCO) effort to abstract and express concepts/constructs that are common across the cyber domain.
ER  - 

TY  - JOUR
T1  - An empirical study of automatic event reconstruction systems
JO  - Digital Investigation
VL  - 3, Supplement
IS  - 
SP  - 108
EP  - 115
PY  - 2006/9//
T2  - The Proceedings of the 6th Annual Digital Forensic Research Workshop (DFRWS '06)
AU  - Jeyaraman, Sundararaman
AU  - Atallah, Mikhail J.
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2006.06.013
UR  - http://www.sciencedirect.com/science/article/pii/S1742287606000752
KW  - Intrusion analysis
KW  - Digital forensics
KW  - Event reconstruction
KW  - Incident response
AB  - Reconstructing the sequence of computer events that led to a particular event is an essential part of the digital investigation process. The ability to quantify the accuracy of automatic event reconstruction systems is an essential step in standardizing the digital investigation process thereby making it resilient to tactics such as the Trojan horse defense. In this paper, we present findings from an empirical study to measure and compare the accuracy and effectiveness of a suite of such event reconstruction techniques. We quantify (as applicable) the rates of false positives and false negatives, and scalability in terms of both computational burden and memory-usage. Some of our findings are quite surprising in the sense of not matching a priori expectations, and whereas other findings qualitatively match the a priori expectations they were never before quantitatively put to the test to determine the boundaries of their applicability. For example, our results show that automatic event reconstruction systems proposed in literature have very high false-positive rates (up to 96%).
ER  - 

TY  - JOUR
T1  - Smartphone incident response
JO  - Digital Investigation
VL  - 10
IS  - 1
SP  - 1
EP  - 2
PY  - 2013/6//
T2  - 
AU  - Casey, Eoghan
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2013.04.004
UR  - http://www.sciencedirect.com/science/article/pii/S1742287613000339
ER  - 

TY  - JOUR
T1  - Profiling software applications for forensic analysis
JO  - Computer Fraud & Security
VL  - 2015
IS  - 6
SP  - 13
EP  - 18
PY  - 2015/6//
T2  - 
AU  - Rafique, Mamoona
AU  - Khan, Muhammad Naeem Ahmed
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(15)30058-0
UR  - http://www.sciencedirect.com/science/article/pii/S1361372315300580
AB  - Computers are now a fundamental part of our professional lives. Although advanced technologies are being used to contain digital crimes, alongside these are other technologies that have expanded a criminal community that is constantly searching for new means to commit crimes in more sophisticated ways. Due to the availability of corporate data on the web, coupled with the open access nature of the web, digital miscreants can commit cybercrimes either as legitimate or illegitimate users.

Traditional digital forensics involves static analysis of the data available on permanent storage media, while live analysis allows running systems to be examined to analyse volatile data.

However, live analysis is not without its challenges, not least because each application has different effects on the system. Mamoona Rafique and Muhammad Naeem Ahmed Khan present a model for profiling the behaviour of application programs. This allows investigators to build a behavioural profile of each application in order to understand its effects on the system.
ER  - 

TY  - JOUR
T1  - Stop PC-aids from spreading through your PC—An early warning system
JO  - Computers & Geosciences
VL  - 19
IS  - 2
SP  - 263
EP  - 294
PY  - 1993/2//
T2  - Analysis and Interpretation of Digital Seismograms
AU  - Uniyal, Parashu R
AU  - Manglik, A
SN  - 0098-3004
DO  - http://dx.doi.org/10.1016/0098-3004(93)90126-P
UR  - http://www.sciencedirect.com/science/article/pii/009830049390126P
KW  - PC/DOS
KW  - Virus syndrome
KW  - Early warning system
KW  - Trapping unknown viruses
AB  - A new software system is presented that runs in an IBM PC/DOS environment immediately after the booting up stage, and provides an effective early warning of an infection by a virus. The system identifies all the users of a PC by their names, passwords, and booking codes. An authentication program performs characteristic diagnostic tests for the presence of boot and file viruses in addition to ensuring that access is provided only to authorized users. A record of login-logout times and results of viral diagnostics is appended to a log file. If there was a viral infection during a user session, access is denied at the time of next login with a display of the report on the last session. Thus corrective measures are prompted not only on the infected fixed disk, but also on that unwary user's floppies. The system includes programs that facilitate recovery from infection by viruses that are unknown to existing commercial scanners and therapy programs at an installation. The system has been effective in keeping the size of the virus epidemic under control at an installation with a minimum sacrifice of the comforts that a PC/DOS combination offers.
ER  - 

TY  - JOUR
T1  - AlmaNebula: A Computer Forensics Framework for the Cloud
JO  - Procedia Computer Science
VL  - 19
IS  - 
SP  - 139
EP  - 146
PY  - 2013///
T2  - The 4th International Conference on Ambient Systems, Networks and Technologies (ANT 2013), the 3rd International Conference on Sustainable Energy Information Technology (SEIT-2013)
AU  - Federici, Corrado
SN  - 1877-0509
DO  - http://dx.doi.org/10.1016/j.procs.2013.06.023
UR  - http://www.sciencedirect.com/science/article/pii/S1877050913006315
KW  - Forensics as a service
KW  - Computer forensics framework
KW  - Commodity computing
KW  - Big data
KW  - Web scale
KW  - Distributed processing
AB  - Abstract
Scalability, fault tolerance and collaborative processing across possibly dispersed sites are key enablers of modern computer forensics applications, that must be able to elastically accommodate all kinds of digital investigations, without wasting resources or fail to deliver timely outcomes. Traditional tools running in a standalone or client- server setups may fall short when handling the multi terabyte scale of a case just above average or, conversely, lie mainly underutilized when dealing with few digital evidences. A new category of applications that leverage the opportunities offered by modern Cloud Computing (CC) platforms, where scalable computational power and storage capacity can be engaged and decommissioned on demand, allow one to conveniently master huge amounts of information that otherwise could be impossible to wield. This paper discusses the design goals, technical requirements and architecture of AlmaNebula, a conceptual framework for the analysis of digital evidences built on top of a Cloud infrastructure, which aims to embody the concept of “Forensics as a service”.
ER  - 

TY  - JOUR
T1  - Mastering Windows network forensics and investigations, Steve Anson, Steve Bunting. Sybex (an imprint of Wiley Publishing Inc.), US and Canada (2007), ISBN: 978-0-4700-9762-5
JO  - Digital Investigation
VL  - 5
IS  - 3–4
SP  - 153
EP  - 154
PY  - 2009/3//
T2  - 
AU  - Forster, Peter F.R.
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2008.09.001
UR  - http://www.sciencedirect.com/science/article/pii/S1742287608000947
ER  - 

TY  - JOUR
T1  - Performance analysis of Bayesian networks and neural networks in classification of file system activities
JO  - Computers & Security
VL  - 31
IS  - 4
SP  - 391
EP  - 401
PY  - 2012/6//
T2  - 
AU  - Khan, Muhammad Naeem Ahmed
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/j.cose.2012.03.003
UR  - http://www.sciencedirect.com/science/article/pii/S0167404812000533
KW  - Digital forensics
KW  - Computer forensic analysis
KW  - Digital evidence
KW  - Neural networks
KW  - Bayesian learning
KW  - Bayesian decision theory
AB  - Precise comprehension of a file system state at any given time is vital for performing digital forensic analyses. To uncover evidence of the digital crime, the logical representation of file system activities helps reconstruct post-event timeline of the unauthorized or malicious accesses made on a system. This paper describes a comparative performance analysis of the Bayesian networks and neural networks techniques to classify the state of file system activities in terms of execution of applications based on the pattern of manipulation of specific files during certain period of time. In particular, this paper discusses the construction of a Bayesian networks and neural networks from the predetermined knowledge of the manipulation of file system artifacts and their corresponding metadata information by a set of software applications. The variability amongst the execution patterns of various applications indicate that the Bayesian network-based model is a more appropriate tool as compared to neural networks because of its ability to learn and detect patterns even from an incomplete dataset. The focus of this paper is to highlight intrinsic significance of the learning approach of Bayesian network methodology in comparison to the techniques used for supervised learning in ordinary neural networks. The paper also highlights the efficacy of Bayesian network technique to proficiently handle large volumes of datasets.
ER  - 

TY  - JOUR
T1  - An integrated conceptual digital forensic framework for cloud computing
JO  - Digital Investigation
VL  - 9
IS  - 2
SP  - 71
EP  - 80
PY  - 2012/11//
T2  - 
AU  - Martini, Ben
AU  - Choo, Kim-Kwang Raymond
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2012.07.001
UR  - http://www.sciencedirect.com/science/article/pii/S174228761200059X
KW  - Cloud computing
KW  - Cloud forensics
KW  - Digital forensics
KW  - Forensic computing
KW  - Digital evidence
KW  - Computer forensics
AB  - Increasing interest in and use of cloud computing services presents both opportunities for criminal exploitation and challenges for law enforcement agencies (LEAs). For example, it is becoming easier for criminals to store incriminating files in the cloud computing environment but it may be extremely difficult for LEAs to seize these files as the latter could potentially be stored overseas. Two of the most widely used and accepted forensic frameworks – McKemmish (1999) and NIST (Kent et al., 2006) – are then reviewed to identify the required changes to current forensic practices needed to successfully conduct cloud computing investigations. We propose an integrated (iterative) conceptual digital forensic framework (based on McKemmish and NIST), which emphasises the differences in the preservation of forensic data and the collection of cloud computing data for forensic purposes. Cloud computing digital forensic issues are discussed within the context of this framework. Finally suggestions for future research are made to further examine this field and provide a library of digital forensic methodologies for the various cloud platforms and deployment models.
ER  - 

TY  - JOUR
T1  - Information assurance in a distributed forensic cluster
JO  - Digital Investigation
VL  - 11, Supplement 1
IS  - 
SP  - S36
EP  - S44
PY  - 2014/5//
T2  - Proceedings of the First Annual DFRWS Europe
AU  - Pringle, Nick
AU  - Burgess, Mikhaila
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2014.03.005
UR  - http://www.sciencedirect.com/science/article/pii/S1742287614000103
KW  - Digital forensics
KW  - Distributed processing
KW  - Media analysis
KW  - FUSE file-systems
KW  - Information assurance
AB  - Abstract
When digital forensics started in the mid-1980s most of the software used for analysis came from writing and debugging software. Amongst these tools was the UNIX utility ‘dd’ which was used to create an image of an entire storage device. In the next decade the practice of creating and using ‘an image’ became established as a fundamental base of what we call ‘sound forensic practice’. By virtue of its structure, every file within the media was an integrated part of the image and so we were assured that it was wholesome representation of the digital crime scene. In an age of terabyte media ‘the image’ is becoming increasingly cumbersome to process, simply because of its size. One solution to this lies in the use of distributed systems. However, the data assurance inherent in a single media image file is lost when data is stored in separate files distributed across a system. In this paper we assess current assurance practices and provide some solutions to the need to have assurance within a distributed system.
ER  - 

TY  - JOUR
T1  - Ideal log setting for database forensics reconstruction
JO  - Digital Investigation
VL  - 12
IS  - 
SP  - 27
EP  - 40
PY  - 2015/3//
T2  - 
AU  - Adedayo, Oluwasola Mary
AU  - Olivier, Martin S.
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2014.12.002
UR  - http://www.sciencedirect.com/science/article/pii/S1742287614001200
KW  - Database management system
KW  - Database forensics
KW  - Digital forensics
KW  - Reconstruction
KW  - Ideal log setting
AB  - Abstract
The ability to reconstruct the data stored in a database at an earlier time is an important aspect of database forensics. Past research shows that the log file in a database can be useful for reconstruction. However, in many database systems there are various options that control which information is included in the logs. This paper introduces the notion of the ideal log setting necessary for an effective reconstruction process in database forensics. The paper provides a survey of the default logging preferences in some of the popular database management systems and identifies the information that a database log should contain in order to be useful for reconstruction. The challenges that may be encountered in storing the information as well as ways of overcoming the challenges are discussed. Possible logging preferences that may be considered as the ideal log setting for the popular database systems are also proposed. In addition, the paper relates the identified requirements to the three dimensions of reconstruction in database forensics and points out the additional requirements and/or techniques that may be required in the different dimensions.
ER  - 

TY  - JOUR
T1  - Network intrusion investigation – Preparation and challenges
JO  - Digital Investigation
VL  - 3
IS  - 3
SP  - 118
EP  - 126
PY  - 2006/9//
T2  - 
AU  - Johnston, Andy
AU  - Reust, Jessica
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2006.08.001
UR  - http://www.sciencedirect.com/science/article/pii/S1742287606000922
KW  - Intrusion investigation
KW  - Incident response
KW  - Network forensics
KW  - Digital forensic examination
KW  - Compromise of sensitive information
KW  - Forensic preparedness
AB  - As new legislation is written mandating notification of affected parties following the compromise of confidential data, reliable investigative procedures into unauthorized access of such data assume increasing importance. The increasing costs and penalties associated with exposure of sensitive data can be mitigated through forensic preparation and the ability to employ digital forensics. A case study of the compromise of several systems containing sensitive data is outlined, with particular attention given to the procedures followed during the initial response and their impact on the subsequent digital forensic examination. Practical problems and challenges that arise in intrusion investigations are discussed, along with solutions and methodologies to address these issues. This case study illustrates both the importance of evaluating the evidence analyzed and of corroborating findings and conclusions with multiple independent sources of evidence. An initial response that incorporates forensic procedures provides a solid foundation for a successful and thorough forensic examination.
ER  - 

TY  - JOUR
T1  - Prevent, protect, pursue – a paradigm for preventing fraud
JO  - Computer Fraud & Security
VL  - 2010
IS  - 7
SP  - 5
EP  - 11
PY  - 2010/7//
T2  - 
AU  - Adams, Robin
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(10)70080-4
UR  - http://www.sciencedirect.com/science/article/pii/S1361372310700804
AB  - According to the January 2010 report from the National Fraud Authority, fraud now costs the UK £30bn a year. Some 58 per cent of fraud is committed in the private sector, with tax fraud hitting £15.2bn. And, also in the private sector, financial services companies and organisations are said to suffer yearly losses of £3.8bn through crimes such as mortgage and insurance fraud, online banking fraud, and cheque and card fraud.
ER  - 

TY  - JOUR
T1  - Visual forensics in the field
JO  - Computer Fraud & Security
VL  - 2009
IS  - 5
SP  - 18
EP  - 20
PY  - 2009/5//
T2  - 
AU  - Forte, Dario
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(09)70062-4
UR  - http://www.sciencedirect.com/science/article/pii/S1361372309700624
AB  - In the April 2009 issue of Computer Fraud &amp; Security, we explored the concept of visual forensics. In the second and final part of this article, we examine how this concept can be applied in everyday forensic scenarios to help analysts quickly and efficiently find information that they may have missed.
ER  - 

TY  - JOUR
T1  - A framework for post-event timeline reconstruction using neural networks
JO  - Digital Investigation
VL  - 4
IS  - 3–4
SP  - 146
EP  - 157
PY  - 2007/9//
Y2  - 2007/12//
T2  - 
AU  - Khan, M.N.A.
AU  - Chatwin, C.R.
AU  - Young, R.C.D.
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2007.11.001
UR  - http://www.sciencedirect.com/science/article/pii/S1742287607000837
KW  - Computer forensics
KW  - Digital investigation
KW  - Event reconstruction
KW  - Digital evidence
KW  - Digital forensic analysis
KW  - Neural networks
AB  - Post-event timeline reconstruction plays a critical role in forensic investigation and serves as a means of identifying evidence of the digital crime. We present an artificial neural networks based approach for post-event timeline reconstruction using the file system activities. A variety of digital forensic tools have been developed during the past two decades to assist computer forensic investigators undertaking digital timeline analysis, but most of the tools cannot handle large volumes of data efficiently. This paper looks at the effectiveness of employing neural network methodology for computer forensic analysis by preparing a timeline of relevant events occurring on a computing machine by tracing the previous file system activities. Our approach consists of monitoring the file system manipulations, capturing file system snapshots at discrete intervals of time to characterise the use of different software applications, and then using this captured data to train a neural network to recognise execution patterns of the application programs. The trained version of the network may then be used to generate a post-event timeline of a seized hard disk to verify the execution of different applications at different time intervals to assist in the identification of available evidence.
ER  - 

TY  - JOUR
T1  - Network forensics based on fuzzy logic and expert system
JO  - Computer Communications
VL  - 32
IS  - 17
SP  - 1881
EP  - 1892
PY  - 2009/11/15/
T2  - 
AU  - Liao, Niandong
AU  - Tian, Shengfeng
AU  - Wang, Tinghua
SN  - 0140-3664
DO  - http://dx.doi.org/10.1016/j.comcom.2009.07.013
UR  - http://www.sciencedirect.com/science/article/pii/S0140366409002060
KW  - Network forensics
KW  - Expert system
KW  - Fuzzy logic
KW  - Intrusion detection system
KW  - Vulnerability scanning
AB  - Network forensics is a research area that finds the malicious users by collecting and analyzing the intrusion or infringement evidence of computer crimes such as hacking. In the past, network forensics was only used by means of investigation. However, nowadays, due to the sharp increase of network traffic, not all the information captured or recorded will be useful for analysis or evidence. The existing methods and tools for network forensics show only simple results. The administrators have difficulty in analyzing the state of the damaged system without expert knowledge. Therefore, we need an effective and automated analyzing system for network forensics. In this paper, we firstly guarantee the evidence reliability as far as possible by collecting different forensic information of detection sensors. Secondly, we propose an approach based on fuzzy logic and expert system for network forensics that can analyze computer crimes in network environment and make digital evidences automatically. At the end of the paper, the experimental comparison results between our proposed method and other popular methods are presented. Experimental results show that the system can classify most kinds of attack types (91.5% correct classification rate on average) and provide analyzable and comprehensible information for forensic experts.
ER  - 

TY  - JOUR
T1  - The growing need for on-scene triage of mobile devices
JO  - Digital Investigation
VL  - 6
IS  - 3–4
SP  - 112
EP  - 124
PY  - 2010/5//
T2  - Embedded Systems Forensics: Smart Phones, GPS Devices, and Gaming Consoles
AU  - Mislan, Richard P.
AU  - Casey, Eoghan
AU  - Kessler, Gary C.
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2010.03.001
UR  - http://www.sciencedirect.com/science/article/pii/S1742287610000149
KW  - Mobile device forensics
KW  - Cell phone forensics
KW  - On-scene triage inspection
KW  - Mobile device technician
AB  - The increasing number of mobile devices being submitted to Digital Forensic Laboratories (DFLs) is creating a backlog that can hinder investigations and negatively impact public safety and the criminal justice system. In a military context, delays in extracting intelligence from mobile devices can negatively impact troop and civilian safety as well as the overall mission. To address this problem, there is a need for more effective on-scene triage methods and tools to provide investigators with information in a timely manner, and to reduce the number of devices that are submitted to DFLs for analysis. Existing tools that are promoted for on-scene triage actually attempt to fulfill the needs of both on-scene triage and in-lab forensic examination in a single solution. On-scene triage has unique requirements because it is a precursor to and distinct from the forensic examination process, and may be performed by mobile device technicians rather than forensic analysts. This paper formalizes the on-scene triage process, placing it firmly in the overall forensic handling process and providing guidelines for standardization of on-scene triage. In addition, this paper outlines basic requirements for automated triage tools.
ER  - 

TY  - JOUR
T1  - The black art of digital forensics
JO  - Infosecurity
VL  - 6
IS  - 5
SP  - 12
EP  - 15
PY  - 2009/7//
Y2  - 2009/8//
T2  - 
AU  - Gold, Steve
SN  - 1754-4548
DO  - http://dx.doi.org/10.1016/S1754-4548(09)70102-0
UR  - http://www.sciencedirect.com/science/article/pii/S1754454809701020
AB  - What makes a good digital forensics specialist? Steve Gold looks at some of the latest applications and investigates how the forensic investigator's role has evolved in order to comply with changing customer priorities
ER  - 

TY  - JOUR
T1  - The first 10 years of the Trojan Horse defence
JO  - Computer Fraud & Security
VL  - 2015
IS  - 1
SP  - 5
EP  - 13
PY  - 2015/1//
T2  - 
AU  - Bowles, Stephen
AU  - Hernandez-Castro, Julio
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(15)70005-9
UR  - http://www.sciencedirect.com/science/article/pii/S1361372315700059
AB  - Apprehended criminals throughout history have always attempted to put the blame on someone else, a strategy popularly known as a SODDI defence (Some Other Dude Did It). When this defence is used, the act of the crime (actus reus) and the guilty mind (mens rea) is blamed on another party. A Trojan Horse Defence (THD) is a type of modern SODDI defence, where the mens rea and actus reus are blamed on a piece of software, known as a trojan.1

It has now become common for people accused of some computer-related crime to claim that the responsibility lies with malware placed on their machine without their knowledge.

This so-called Trojan Horse Defence (THD) was first used a decade ago. In this article, Stephen Bowles and Julio Hernandez-Castro of the University of Kent undertake a timely retrospective with an in-depth and critical literature review plus a detailed look at the peculiarities of many court cases from around the world.
ER  - 

TY  - JOUR
T1  - SoTE: Strategy of Triple-E on solving Trojan defense in Cyber-crime cases
JO  - Computer Law & Security Review
VL  - 26
IS  - 1
SP  - 52
EP  - 60
PY  - 2010/1//
T2  - 
AU  - Kao, Da-Yu
AU  - Wang, Shiuh-Jeng
AU  - Fu-Yuan Huang, Frank
SN  - 0267-3649
DO  - http://dx.doi.org/10.1016/j.clsr.2009.09.008
UR  - http://www.sciencedirect.com/science/article/pii/S0267364909001575
KW  - Cyber-crime
KW  - Cyber criminology
KW  - Digital evidence
KW  - Trojan defense
KW  - Triple-E strategy
AB  - Cyber activity has become an essential part of the general public's everyday life. The hacking threats of Cyber-crime are becoming more sophisticated as internet communication services are more popular. To further confirm the final finding of Cyber-crime, this study proposes three analytical tools to clarify the Cyber-crime issues by means of Ideal Log, M-N model and MDFA (Multi-faceted Digital Forensics Analysis) strategy, where Ideal Log is identified as a traceable element of digital evidence including four elements of IP Address, Timestamp, Digital Action, and Response Message. M-N model applies a formal method for collating and analyzing data sets of investigation-relevant logs in view of connected time with ISP logs. MDFA strategy attempts to outline the basic elements of Cyber-crime using new procedural investigative steps, and combining universal types of evidential information in terms of Evidence, Scene, Victim, and Suspect. After researchers figure out what has happened in Cyber-crime events, it will be easier to communicate with offenders, victims or related people. SoTE (Strategy of Triple-E) is discussed to observe Cyber-crime from the viewpoints of Education, Enforcement and Engineering. That approach is further analyzed from the fields of criminology, investigation and forensics. Each field has its different focus in dealing with diverse topics, such as: the policy of 6W1H (What, Which, When, Where, Who, Why, and How) questions, the procedure of MDFA strategy, the process of ideal Logs and M-N model. In addition, the case study and proposed suggestion of this paper are presented to counter Cyber-crime.
ER  - 

TY  - JOUR
T1  - Feature deduction and ensemble design of intrusion detection systems
JO  - Computers & Security
VL  - 24
IS  - 4
SP  - 295
EP  - 307
PY  - 2005/6//
T2  - 
AU  - Chebrolu, Srilatha
AU  - Abraham, Ajith
AU  - Thomas, Johnson P.
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/j.cose.2004.09.008
UR  - http://www.sciencedirect.com/science/article/pii/S016740480400238X
KW  - Hybrid intelligent system
KW  - Feature reduction
KW  - Intrusion detection
KW  - Ensemble design
KW  - Bayesian network
KW  - Markov blanket
KW  - Decision trees
AB  - Current intrusion detection systems (IDS) examine all data features to detect intrusion or misuse patterns. Some of the features may be redundant or contribute little (if anything) to the detection process. The purpose of this study is to identify important input features in building an IDS that is computationally efficient and effective. We investigated the performance of two feature selection algorithms involving Bayesian networks (BN) and Classification and Regression Trees (CART) and an ensemble of BN and CART. Empirical results indicate that significant input feature selection is important to design an IDS that is lightweight, efficient and effective for real world detection systems. Finally, we propose an hybrid architecture for combining different feature selection algorithms for real world intrusion detection.
ER  - 

TY  - JOUR
T1  - MEGA: A tool for Mac OS X operating system and application forensics
JO  - Digital Investigation
VL  - 5, Supplement
IS  - 
SP  - S83
EP  - S90
PY  - 2008/9//
T2  - The Proceedings of the Eighth Annual DFRWS Conference
AU  - Joyce, Robert A.
AU  - Powers, Judson
AU  - Adelstein, Frank
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2008.05.011
UR  - http://www.sciencedirect.com/science/article/pii/S1742287608000376
KW  - Mac OS X
KW  - Computer forensics
KW  - Spotlight
KW  - Disk image analysis
KW  - Application analysis
AB  - Computer forensic tools for Apple Mac hardware have traditionally focused on low-level file system details. Mac OS X and common applications on the Mac platform provide an abundance of information about the user's activities in configuration files, caches, and logs. We are developing MEGA, an extensible tool suite for the analysis of files on Mac OS X disk images. MEGA provides simple access to Spotlight metadata maintained by the operating system, yielding efficient file content search and exposing metadata such as digital camera make and model. It can also help investigators to assess FileVault encrypted home directories. MEGA support tools are under development to interpret files written by common Mac OS applications such as Safari, Mail, and iTunes.
ER  - 

TY  - JOUR
T1  - Forensic investigation of cloud computing systems
JO  - Network Security
VL  - 2011
IS  - 3
SP  - 4
EP  - 10
PY  - 2011/3//
T2  - 
AU  - Taylor, Mark
AU  - Haggerty, John
AU  - Gresty, David
AU  - Lamb, David
SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/S1353-4858(11)70024-1
UR  - http://www.sciencedirect.com/science/article/pii/S1353485811700241
AB  - Cloud computing describes a computing concept where software services, and the resources they use, operate as (and on) a virtualised platform across many different host machines, connected by the Internet or an organisation's internal network. From a business or system user's point of view, the cloud provides, via virtualisation, a single platform or service collection in which it can operate.

Cloud computing is a new concept in the distributed processing of data and is likely to make computer forensic evidence acquisition and evidence analysis increasingly complex.

Currently there do not appear to be any published guidelines that specifically address the conduct of computer forensic investigations of cloud computing systems. In order to understand and analyse evidence within this environment, computer forensics examiners will require a broader range of technical knowledge across multiple hardware platforms and operating systems. Dr Mark Taylor et al examine the issues concerning the forensic investigation of cloud systems.
ER  - 

TY  - JOUR
T1  - Insuring against data breaches
JO  - Computer Fraud & Security
VL  - 2013
IS  - 2
SP  - 11
EP  - 15
PY  - 2013/2//
T2  - 
AU  - Bradbury, Danny
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(13)70020-4
UR  - http://www.sciencedirect.com/science/article/pii/S1361372313700204
AB  - We live in an increasingly uncertain world. Organisations are having to cope with unpredictable events such as extreme weather incidents, and geopolitical strife. Insurance companies have been quick to point out the challenges of managing climate change-related risk. But not all of the new risks are physical.
ER  - 

TY  - JOUR
T1  - News
JO  - Digital Investigation
VL  - 5
IS  - 1–2
SP  - 3
EP  - 5
PY  - 2008/9//
T2  - 
AU  - Hilley, S.
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2008.08.003
UR  - http://www.sciencedirect.com/science/article/pii/S1742287608000686
ER  - 

TY  - JOUR
T1  - Macro attacks: What next after Melissa?
JO  - Computers & Security
VL  - 18
IS  - 5
SP  - 391
EP  - 395
PY  - 1999///
T2  - 
AU  - Docherty, Paul
AU  - Simpson, Peter
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/S0167-4048(99)80084-4
UR  - http://www.sciencedirect.com/science/article/pii/S0167404899800844
ER  - 

TY  - JOUR
T1  - Getting lost on the Internet: the problem with anonymity
JO  - Network Security
VL  - 2013
IS  - 6
SP  - 10
EP  - 13
PY  - 2013/6//
T2  - 
AU  - Gold, Steve
SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/S1353-4858(13)70069-2
UR  - http://www.sciencedirect.com/science/article/pii/S1353485813700692
AB  - The world wide web officially celebrated its 20th anniversary earlier this year, but the reality is that the web – as a concept – dates all the way back to 1980 when Sir Tim Berners-Lee, then employed at the particle physics research establishment CERN, developed a multi-protocol database called Esquire.

The Internet was never designed with anonymity in mind. In fact, the very design of the system allows for easy tracking, at least as far as an IP address.

Early on in the net's development, questions were being raised about privacy and anonymity. Steve Gold outlines the historical foundations of the issue, in the very development of the Internet, and goes on to explain why achieving anonymity is so difficult.
ER  - 

TY  - JOUR
T1  - Bloom filter applications in network security: A state-of-the-art survey
JO  - Computer Networks
VL  - 57
IS  - 18
SP  - 4047
EP  - 4064
PY  - 2013/12/24/
T2  - 
AU  - Geravand, Shahabeddin
AU  - Ahmadi, Mahmood
SN  - 1389-1286
DO  - http://dx.doi.org/10.1016/j.comnet.2013.09.003
UR  - http://www.sciencedirect.com/science/article/pii/S1389128613003083
KW  - Bloom filters
KW  - Security
KW  - Network processing
AB  - Abstract
Undoubtedly, dealing with security issues is one of the most important and complex tasks various networks face today. A large number of security algorithms have been proposed to enhance security in various types of networks. Many of these solutions are either directly or indirectly based on Bloom filter (BF), a space- and time-efficient probabilistic data structure introduced by Burton Bloom in 1970. Obviously, Bloom filters and their variants are getting more and more consideration in network security area. This paper provides an up-to-date survey of the application of BFs and their variants to improve performance of the approaches proposed to address security problems with different types of networks.
ER  - 

TY  - JOUR
T1  - Algorithms for anomaly detection of traces in logs of process aware information systems
JO  - Information Systems
VL  - 38
IS  - 1
SP  - 33
EP  - 44
PY  - 2013/3//
T2  - 
AU  - Bezerra, Fábio
AU  - Wainer, Jacques
SN  - 0306-4379
DO  - http://dx.doi.org/10.1016/j.is.2012.04.004
UR  - http://www.sciencedirect.com/science/article/pii/S0306437912000567
KW  - Anomaly detection
KW  - Process mining
KW  - Process-aware systems
AB  - This paper discusses four algorithms for detecting anomalies in logs of process aware systems. One of the algorithms only marks as potential anomalies traces that are infrequent in the log. The other three algorithms: threshold, iterative and sampling are based on mining a process model from the log, or a subset of it. The algorithms were evaluated on a set of 1500 artificial logs, with different profiles on the number of anomalous traces and the number of times each anomalous traces was present in the log. The sampling algorithm proved to be the most effective solution. We also applied the algorithm to a real log, and compared the resulting detected anomalous traces with the ones detected by a different procedure that relies on manual choices.
ER  - 

TY  - JOUR
T1  - NetWare 4 receives high marks for security
JO  - Computer Fraud & Security Bulletin
VL  - 1995
IS  - 10
SP  - 9
EP  - 10
PY  - 1995/10//
T2  - 
AU  - English, Erin
SN  - 0142-0496
DO  - http://dx.doi.org/10.1016/0142-0496(95)80043-3
UR  - http://www.sciencedirect.com/science/article/pii/0142049695800433
ER  - 

TY  - JOUR
T1  - Secure log management for privacy assurance in electronic communications
JO  - Computers & Security
VL  - 27
IS  - 7–8
SP  - 298
EP  - 308
PY  - 2008/12//
T2  - 
AU  - Stathopoulos, Vassilios
AU  - Kotzanikolaou, Panayiotis
AU  - Magkos, Emmanouil
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/j.cose.2008.07.010
UR  - http://www.sciencedirect.com/science/article/pii/S0167404808000400
KW  - System logging
KW  - Network providers
KW  - Internal attacks
KW  - Integrity
KW  - Digital signatures
AB  - In this paper we examine logging security in the environment of electronic communication providers. We review existing security threat models for system logging and we extend these to a new security model especially suited for communication network providers, which also considers internal modification attacks. We also propose a framework for secure log management in public communication networks as well as an implementation design, in order to provide traceability under the extended security model. A key role to the proposed framework is given to an independent Regulatory Authority, which is responsible to maintain log integrity proofs in a remote environment and verify the integrity of the provider's log files during security audits.
ER  - 

TY  - JOUR
T1  - Outsmarting the smartphone fraudsters
JO  - Network Security
VL  - 2010
IS  - 12
SP  - 7
EP  - 9
PY  - 2010/12//
T2  - 
AU  - Ridley, Philip
SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/S1353-4858(10)70145-8
UR  - http://www.sciencedirect.com/science/article/pii/S1353485810701458
AB  - The use of mobile phone forensics to investigate fraudulent activity is nothing new. But mobile phones have evolved into smartphones, and fraudsters have evolved with them.

Smartphones are packed with functionality that can easily be used for data theft or inappropriate contact with other parties – none of which passes through company-owned systems, and is to all intents and purposes ‘off the grid’. Employers need to be aware of these risks when devices are issued, along with ensuring that processes are in place when suspicions are raised, explains Philip Ridley of CCL-Forensics.

Detecting ‘tech-savvy’ corporate fraudsters is a constant game of catch-up. It's not only about playing catch-up with the intellect, motives and awareness of the e-fraudster, but also the technologies that can be misused. What's more, the methods through which the technology can be manipulated to secrete, disguise and protect fraudulent activities – all while staying away from corporate networks where they can readily be monitored and detected – are constantly evolving. This means a company's intellectual property and sensitive data is at risk of sabotage or just good old-fashioned theft.
ER  - 

TY  - JOUR
T1  - Managed file transfer: the next stage for data in motion?
JO  - Network Security
VL  - 2013
IS  - 9
SP  - 12
EP  - 15
PY  - 2013/9//
T2  - 
AU  - Dunford, Dan
SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/S1353-4858(13)70103-X
UR  - http://www.sciencedirect.com/science/article/pii/S135348581370103X
AB  - Organisations of all types and sizes have come to rely on file transfer technology to help their businesses run smoothly. But basic file transfer technology is inherently limiting and inadequate. These days, regulatory demands and changing corporate needs are putting pressure on traditional methods and solutions.

Organisations of all types and sizes have come to rely on file transfer technology to help their businesses run smoothly. But basic file transfer technology is inherently limiting and inadequate.

Many file transfer applications were designed as simple utilities, not as enterprise solutions, and lack the management, control and integration capabilities needed to support today's challenging business environment. The answer is the next generation of managed file transfer (MFT), explains Dan Dunford of Attachmate.
ER  - 

TY  - JOUR
T1  - Automated event and social network extraction from digital evidence sources with ontological mapping
JO  - Digital Investigation
VL  - 13
IS  - 
SP  - 94
EP  - 106
PY  - 2015/6//
T2  - 
AU  - Turnbull, Benjamin
AU  - Randhawa, Suneel
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2015.04.004
UR  - http://www.sciencedirect.com/science/article/pii/S1742287615000444
KW  - Artificial intelligence
KW  - Big data
KW  - Digital forensics
KW  - Digital evidence
KW  - Event representation
KW  - Forensic tool development
KW  - Knowledge representation
KW  - Ontology
KW  - Software engineering
KW  - Triage
AB  - Abstract
The sharp rise in consumer computing, electronic and mobile devices and data volumes has resulted in increased workloads for digital forensic investigators and analysts. The number of crimes involving electronic devices is increasing, as is the amount of data for each job. This is becoming unscaleable and alternate methods to reduce the time trained analysts spend on each job are necessary.

This work leverages standardised knowledge representations techniques and automated rule-based systems to encapsulate expert knowledge for forensic data. The implementation of this research can provide high-level analysis based on low-level digital artefacts in a way that allows an understanding of what decisions support the facts. Analysts can quickly make determinations as to which artefacts warrant further investigation and create high level case data without manually creating it from the low-level artefacts. Extraction and understanding of users and social networks and translating the state of file systems to sequences of events are the first uses for this work.

A major goal of this work is to automatically derive ‘events’ from the base forensic artefacts. Events may be system events, representing logins, start-ups, shutdowns, or user events, such as web browsing, sending email. The same information fusion and homogenisation techniques are used to reconstruct social networks. There can be numerous social network data sources on a single computer; internet cache can locate Facebook, LinkedIn, Google Plus caches; email has address books and copies of emails sent and received; instant messenger has friend lists and call histories. Fusing these into a single graph allows a more complete, less fractured view for an investigator.

Both event creation and social network creation are expected to assist investigator-led triage and other fast forensic analysis situations.
ER  - 

TY  - JOUR
T1  - Applying a forensic approach to incident response, network investigation and system administration using Digital Evidence Bags
JO  - Digital Investigation
VL  - 4
IS  - 1
SP  - 30
EP  - 35
PY  - 2007/3//
T2  - 
AU  - Turner, Philip
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2007.01.002
UR  - http://www.sciencedirect.com/science/article/pii/S1742287607000035
KW  - Digital forensics
KW  - Digital Evidence Bags
KW  - Digital investigation
KW  - Network investigation
KW  - System administration
KW  - Incident response
AB  - This paper questions the current approach to forensic incident response and network investigations. Although claiming to be ‘forensic’ in nature it shows that the basic processes and mechanisms used in traditional computer forensics are rarely applied in the live incident investigation arena. This paper demonstrates how the newly proposed Digital Evidence Bag (DEB) storage format can be applied to a dynamic environment. A DEB is a universal container for digital evidence from any source. It allows the provenance to be recorded and continuity to be maintained throughout the life of the investigation. With a small amount of forethought a forensically rigorous approach can be applied to incident response, network investigations and system administration with minimal overhead.
ER  - 

TY  - JOUR
T1  - Report Highlights
JO  - Information Security Technical Report
VL  - 5
IS  - 2
SP  - 3
EP  - 14
PY  - 2000/6/1/
T2  - 

SN  - 1363-4127
DO  - http://dx.doi.org/10.1016/S1363-4127(00)02002-1
UR  - http://www.sciencedirect.com/science/article/pii/S1363412700020021
AB  - Highlights from each of the articles in the report have been extracted to give the reader a quick overview of the contents and to summarize the key messages.
ER  - 

TY  - JOUR
T1  - MAREA – From an Agent Simulation Application to the Social Network Analysis
JO  - Procedia Computer Science
VL  - 35
IS  - 
SP  - 1416
EP  - 1425
PY  - 2014///
T2  - Knowledge-Based and Intelligent Information &amp; Engineering Systems 18th Annual Conference, KES-2014 Gdynia, Poland, September 2014 Proceedings
AU  - Vym?tal, Dominik
AU  - Šperka, Roman
SN  - 1877-0509
DO  - http://dx.doi.org/10.1016/j.procs.2014.08.198
UR  - http://www.sciencedirect.com/science/article/pii/S1877050914011636
KW  - decision support
KW  - MAREA
KW  - agents
KW  - simulation
KW  - application
KW  - social network
AB  - Abstract
The aim of the paper is to present an enhanced software simulation application based on a trading company control loop and to validate agents’ behavior from simulation experiments using simple Petri net, and social network analysis. The main purpose of the application is to improve existing decision support systems with the use of a simulation. The ERP system using the REA ontology approach is used as a measuring element in the application. The system has been developed in cooperation between Silesian University in Opava, School of Business Administration in Karvina, Czech Republic and REA technology Copenhagen, Denmark. After the prototype tests at the end of the year 2011, we presented it at the beginning of 2012 for the very first time. Firstly, the enhanced framework with several types of agents and negotiation possibilities is described. This is followed by the decision function explanation, which is the core of the price negotiation. Secondly, a brief look on the graphical user interface and main parts of MAREA simulation monitor is provided. Brief results of the model validation performed by means of ProM software are presented. To conclude, MAREA is a software application with simulation possibilities, which can be used to present trading behavior of a company for decision support.
ER  - 

TY  - JOUR
T1  - A study on decision consolidation methods using analytic models for security systems
JO  - Computers & Security
VL  - 26
IS  - 2
SP  - 145
EP  - 153
PY  - 2007/3//
T2  - 
AU  - Kim, Sangkyun
AU  - Lee, Hong Joo
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/j.cose.2006.08.007
UR  - http://www.sciencedirect.com/science/article/pii/S0167404806001404
KW  - Security controls
KW  - Evaluation criteria
KW  - Package introduction
AB  - The successful management of information security within an organization is vital to its survival and success. The necessary security controls need to be implemented and managed effectively. In this paper, using the characteristics of the AHP, a study on information security management systems is selected from the perspective of Process Model and Criteria. A case study has proven potential value of this methodology in helping decision-makers in supporting their selection of security controls.
ER  - 

TY  - JOUR
T1  - Email and Web Abuse — Monitoring &amp; Investigations: Part II: Web monitoring
JO  - Computer Fraud & Security
VL  - 2003
IS  - 8
SP  - 5
EP  - 9
PY  - 2003/8//
T2  - 
AU  - Pemble, Matthew
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(03)08005-9
UR  - http://www.sciencedirect.com/science/article/pii/S1361372303080059
AB  - With the Web, there is a little more preventative work that can be done to prevent abuse than with email. URL filters, although I fundamentally oppose them from an engineering design point-of-view1, are a necessary and unpleasant evil in the corporate world. They can prevent the vast majority of the innocent and casual browsing of offensive and similar material, although the dedicated seeker after pornography will always be able to find sites that have not yet been picked up by the filter database team.
ER  - 

TY  - JOUR
T1  - An agent based and biological inspired real-time intrusion detection and security model for computer network operations
JO  - Computer Communications
VL  - 30
IS  - 13
SP  - 2649
EP  - 2660
PY  - 2007/9/26/
T2  - Sensor-Actuated NetworksSANETs
AU  - Boukerche, Azzedine
AU  - Machado, Renato B.
AU  - Jucá, Kathia R.L.
AU  - Sobral, João Bosco M.
AU  - Notare, Mirela S.M.A.
SN  - 0140-3664
DO  - http://dx.doi.org/10.1016/j.comcom.2007.03.008
UR  - http://www.sciencedirect.com/science/article/pii/S0140366407001181
KW  - Artificial immune system
KW  - Mobile agent
KW  - Computer networks
KW  - Intrusion detection model
AB  - There is a strong correlation between the human immune system and a computer network security system. The human immune system protects the human body from pathogenic elements in the same way that a computer security system protects the computer from malicious users. This paper presents a novel intrusion detection model based on artificial immune and mobile agent paradigms for network intrusion detection. The construction of the model is based on registries’ signature analysis using both Syslog-ng and Logcheck unix tools. The tasks of monitoring, distributing intrusion detection workload, storing relevant information, and ensuring data persistence and reactivity have been carried out by the mobile agents, which represent the leukocytes of an artificial immune system. Our real-time based intrusion detection and communication model is host-based and adopts the anomaly detection paradigm. We present our intrusion detection model, discuss its implementation, and report on its performance evaluation using real data provided by an Internet Service Provider and a data processing corporation.
ER  - 

TY  - JOUR
T1  - News and comment on recent developments from around the world
JO  - Computer Law & Security Review
VL  - 24
IS  - 4
SP  - 283
EP  - 298
PY  - 2008///
T2  - 
AU  - Saxby, Stephen
SN  - 0267-3649
DO  - http://dx.doi.org/10.1016/j.clsr.2008.06.002
UR  - http://www.sciencedirect.com/science/article/pii/S0267364908000770
ER  - 

TY  - JOUR
T1  - Google Drive: Forensic analysis of data remnants
JO  - Journal of Network and Computer Applications
VL  - 40
IS  - 
SP  - 179
EP  - 193
PY  - 2014/4//
T2  - 
AU  - Quick, Darren
AU  - Choo, Kim-Kwang Raymond
SN  - 1084-8045
DO  - http://dx.doi.org/10.1016/j.jnca.2013.09.016
UR  - http://www.sciencedirect.com/science/article/pii/S1084804513002051
KW  - Cloud storage
KW  - Cloud storage forensics
KW  - Cloud forensics
KW  - Digital forensic analysis
KW  - Google Drive
AB  - Abstract
Cloud storage is an emerging challenge to digital forensic examiners. The services are increasingly used by consumers, business, and government, and can potentially store large amounts of data. The retrieval of digital evidence from cloud storage services (particularly from offshore providers) can be a challenge in a digital forensic investigation, due to virtualisation, lack of knowledge on location of digital evidence, privacy issues, and legal or jurisdictional boundaries. Google Drive is a popular service, providing users a cost-effective, and in some cases free, ability to access, store, collaborate, and disseminate data. Using Google Drive as a case study, artefacts were identified that are likely to remain after the use of cloud storage, in the context of the experiments, on a computer hard drive and Apple iPhone3G, and the potential access point(s) for digital forensics examiners to secure evidence.
ER  - 

TY  - JOUR
T1  - Algorithms for clustering clickstream data
JO  - Information Processing Letters
VL  - 109
IS  - 8
SP  - 381
EP  - 385
PY  - 2009/3/31/
T2  - 
AU  - Antonellis, Panagiotis
AU  - Makris, Christos
AU  - Tsirakis, Nikos
SN  - 0020-0190
DO  - http://dx.doi.org/10.1016/j.ipl.2008.12.011
UR  - http://www.sciencedirect.com/science/article/pii/S0020019008003670
KW  - Data mining
KW  - Clustering algorithms
KW  - Databases
AB  - Clustering is a classic problem in the machine learning and pattern recognition area, however a few complications arise when we try to transfer proposed solutions in the data stream model. Recently there have been proposed new algorithms for the basic clustering problem for massive data sets that produce an approximate solution using efficiently the memory, which is the most critical resource for streaming computation. In this paper, based on these solutions, we present a new model for clustering clickstream data which applies three different phases in the data processing, and is validated through a set of experiments.
ER  - 

TY  - JOUR
T1  - Securing and controlling data in the cloud
JO  - Computer Fraud & Security
VL  - 2012
IS  - 11
SP  - 16
EP  - 20
PY  - 2012/11//
T2  - 
AU  - Ayers, Paul
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(12)70115-X
UR  - http://www.sciencedirect.com/science/article/pii/S136137231270115X
AB  - It is well known that businesses are embracing the compelling economic and operational benefits of cloud computing. By virtualising and pooling computing resources, businesses not only reduce operational costs but can also accelerate the deployment of new applications and services. While cloud computing does not change the fundamental principles of information security, taking advantage of cloud computing's benefits requires consideration of how the security of data can be maintained in the cloud model.
ER  - 

TY  - JOUR
T1  - Amanda Spink, Bernhard J. Jansen, Web Search: Public Searching of the Web, Information and Knowledge Management Series, Kluwer Academic Publishers, Dordrecht, The Netherlands, ISBN: 1402022689, $129.00
JO  - Information Processing & Management
VL  - 42
IS  - 2
SP  - 593
EP  - 594
PY  - 2006/3//
T2  - 
AU  - Thelwall, Mike
SN  - 0306-4573
DO  - http://dx.doi.org/10.1016/j.ipm.2005.07.001
UR  - http://www.sciencedirect.com/science/article/pii/S0306457305000786
ER  - 

TY  - JOUR
T1  - Study on Web Analytics Utilizing Segmentation Knowledge in Business to Business Manufacturer Site
JO  - Procedia Computer Science
VL  - 35
IS  - 
SP  - 902
EP  - 909
PY  - 2014///
T2  - Knowledge-Based and Intelligent Information &amp; Engineering Systems 18th Annual Conference, KES-2014 Gdynia, Poland, September 2014 Proceedings
AU  - Sekiguchi, Akiyuki
AU  - Tsuda, Kazuhiko
SN  - 1877-0509
DO  - http://dx.doi.org/10.1016/j.procs.2014.08.164
UR  - http://www.sciencedirect.com/science/article/pii/S1877050914011296
KW  - Web analytics
KW  - B to B;
AB  - Abstract
Web analytics of B to B sites is mandatory for improving usability and leveraging data for marketing. In this study we tried web analytics by some segmentation and confirmed it is effective. We defined some of the segment models (7 segmentation type) and examined web access using some segments. One of the most important segmentations is registered versus unregistered users and we confirmed user behavior is different with each segment. We confirmed key metrics like bounce rate, referrer, and exit page analysis are especially beneficial for B to B manufacturer site enhancement.
ER  - 

TY  - JOUR
T1  - Technological support for the enactment of collaborative scripted learning activities across multiple spatial locations
JO  - Future Generation Computer Systems
VL  - 31
IS  - 
SP  - 223
EP  - 237
PY  - 2014/2//
T2  - Special Section: Advances in Computer Supported Collaboration: Systems and Technologies
AU  - de-la-Fuente-Valentín, Luis
AU  - Pérez-Sanagustín, Mar
AU  - Hernández-Leo, Davinia
AU  - Pardo, Abelardo
AU  - Blat, Josep
AU  - Delgado Kloos, Carlos
SN  - 0167-739X
DO  - http://dx.doi.org/10.1016/j.future.2013.05.007
UR  - http://www.sciencedirect.com/science/article/pii/S0167739X13001064
KW  - CSCL scripts
KW  - Orchestration
KW  - IMS Learning Design
KW  - Service integration
KW  - Case study
AB  - Abstract
Computer Supported Collaborative Blended Learning (CSCBL) scripts are innovative practices that benefit from interactive devices to combine and coordinate activities occurring in different spatial locations. However, the adoption of CSCBL scripts is hindered by the difficulties in orchestration that they entail for practitioners. As observed in a concrete experiment, these orchestration problems include: adapting group formation according to students’ actions in previous activities, supporting transitions between activities and artifacts across locations using diverse technologies, or displaying the appropriate tools to students depending on their group and assigned task. This paper describes the technological support designed to alleviate this complexity. The result is a Computer Supported Collaborative Blended Learning (CSCBL) script and its associated orchestration system that allows the replication of these practices at a minimum cost. The CSCBL script has been evaluated in a case study with 35 students and 5 teachers. Based on this orchestration system and the findings of the experiment, we also propose an architecture based on IMS Learning Design and Generic Service Integration in combination with other web based tools to support the enactment of other similar CSCBL scripts. The findings of this experiment offer interesting insights into the extend to which different technologies and multiple spaces can be combined for orchestrating integrated complex collaborative practices.
ER  - 

TY  - JOUR
T1  - Sequenced release of privacy-accurate information in a forensic investigation
JO  - Digital Investigation
VL  - 7
IS  - 1–2
SP  - 95
EP  - 101
PY  - 2010/10//
T2  - 
AU  - Croft, N.J.
AU  - Olivier, M.S.
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2010.01.002
UR  - http://www.sciencedirect.com/science/article/pii/S1742287610000046
KW  - Privacy
KW  - Forensic investigation
KW  - Sequence
KW  - Release
KW  - Privacy-accurate
KW  - Privacy-preserving object
AB  - When a decision is taken to forensically investigate information concerning an individual, it is currently not possible to preserve the privacy enjoyed by that individual and to ‘undo’ the effects of the investigation should he or she prove to be innocent. In other words, once private information deviates from an expected flow and is disclosed, it cannot always be rendered private again. Thus there is a need to balance the efficacy of the investigation against infringements of privacy.

This paper shows how a balance can be reached between privacy and forensics through the release of private information in a sequential manner. Access to information is based on prior knowledge and proof of a hypothesis which, if proved correct, releases information sequentially.

The solution focuses on the classification and ordering of information by creating a privacy-preserving object. Using cryptographic techniques and blind signatures, we demonstrate the technical feasibility of protecting information up until the point of release.
ER  - 

TY  - JOUR
T1  - Options in Computer Forensic Tools
JO  - Computer Fraud & Security
VL  - 2002
IS  - 11
SP  - 8
EP  - 11
PY  - 2002/11//
T2  - 

SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(02)01108-9
UR  - http://www.sciencedirect.com/science/article/pii/S1361372302011089
ER  - 

TY  - JOUR
T1  - A Quantitative Approach for Intrusions Detection and Prevention based on Statistical N-Gram Models
JO  - Procedia Computer Science
VL  - 10
IS  - 
SP  - 450
EP  - 457
PY  - 2012///
T2  - ANT 2012 and MobiWIS 2012
AU  - Boulaiche, Ammar
AU  - Bouzayani, Hatem
AU  - Adi, Kamel
SN  - 1877-0509
DO  - http://dx.doi.org/10.1016/j.procs.2012.06.058
UR  - http://www.sciencedirect.com/science/article/pii/S1877050912004152
KW  - IDS
KW  - Honeypots
KW  - attack scenarios
KW  - automatic generation of attack scenarios
KW  - Honeypot-IDS integration
KW  - Markov models
AB  - In this paper we propose a new, quantitative-based approach for the detection and the prevention of intrusions. Our model is able to probabilistically predict attacks before their completion by using a quantitative Markov model built from a corpus of network traffic collected on a honeypot. Moreover, the proposed collaborative architecture honeypot intrusion detection system provides a fully autonomous system with self-learning capabilities. To validate our approach, we built a software prototype and compared its performance with the well known Snort tool. The results clearly show that our system outperforms Snort on multiple criteria including autonomy, accuracy, detection and prediction rates
ER  - 

TY  - JOUR
T1  - WebTrafMon: Web-based Internet/Intranet network traffic monitoring and analysis system
JO  - Computer Communications
VL  - 22
IS  - 14
SP  - 1333
EP  - 1342
PY  - 1999/9/15/
T2  - 
AU  - Hong, J.W.-K
AU  - Kwon, S.-S
AU  - Kim, J.-Y
SN  - 0140-3664
DO  - http://dx.doi.org/10.1016/S0140-3664(99)00130-9
UR  - http://www.sciencedirect.com/science/article/pii/S0140366499001309
KW  - Network traffic monitoring and analysis
KW  - Traffic management
KW  - Traffic type and source analysis
KW  - Web-based management
KW  - Enterprise network management
AB  - As enterprise computing environments become more network-oriented, the importance of network traffic monitoring and analysis intensifies. Most existing traffic monitoring and analysis tools focus on measuring the traffic loads of individual network segments. Further, they typically have complicated user interfaces. As Internet and Intranet traffic increases due to the increase in the use of the World-Wide Web and other applications, determining which host and which application generates how much network traffic is becoming critical in managing and using network resources effectively. This paper presents the design and implementation of a portable, Web-based network traffic monitoring and analysis system called WebTrafMon. Web-based technology enables users to be free from complex user interfaces, while allowing monitoring and analysis results to be viewed from any site, using widely available Web browsers. WebTrafMon provides monitoring and analysis capabilities not only for traffic loads but also for traffic types, sources and destinations. WebTrafMon consists of two parts: a probe and a viewer. The probe extracts raw traffic information from the network, and the viewer provides the user with analyzed traffic information via Web browsers. The effectiveness of WebTrafMon has been verified by applying it to an enterprise network environment.
ER  - 

TY  - JOUR
T1  - How secure is data over the internet?
JO  - Network Security
VL  - 1994
IS  - 6
SP  - 9
EP  - 11
PY  - 1994/6//
T2  - 
AU  - Highland, Harold Joseph
SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/1353-4858(94)90046-9
UR  - http://www.sciencedirect.com/science/article/pii/1353485894900469
AB  - The growing commercial use of the Internet is resulting in increasing requirements for secure communication. We take a look at some of the basic threats and consider some of the preventative measures that can be taken.
ER  - 

TY  - JOUR
T1  - Intrusion detection using a fuzzy genetics-based learning algorithm
JO  - Journal of Network and Computer Applications
VL  - 30
IS  - 1
SP  - 414
EP  - 428
PY  - 2007/1//
T2  - Network and Information Security: A Computational Intelligence Approach
AU  - Abadeh, M. Saniee
AU  - Habibi, J.
AU  - Lucas, C.
SN  - 1084-8045
DO  - http://dx.doi.org/10.1016/j.jnca.2005.05.002
UR  - http://www.sciencedirect.com/science/article/pii/S1084804505000305
KW  - Intrusion detection
KW  - Fuzzy logic
KW  - Genetic algorithm
KW  - Rule learning
AB  - Fuzzy systems have demonstrated their ability to solve different kinds of problems in various applications domains. Currently, there is an increasing interest to augment fuzzy systems with learning and adaptation capabilities. Two of the most successful approaches to hybridize fuzzy systems with learning and adaptation methods have been made in the realm of soft computing. Neural fuzzy systems and genetic fuzzy systems hybridize the approximate reasoning method of fuzzy systems with the learning capabilities of neural networks and evolutionary algorithms. The objective of this paper is to describe a fuzzy genetics-based learning algorithm and discuss its usage to detect intrusion in a computer network. Experiments were performed with DARPA data sets [KDD-cup data set. http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html], which have information on computer networks, during normal behaviour and intrusive behaviour. This paper presents some results and reports the performance of generated fuzzy rules in detecting intrusion in a computer network.
ER  - 

TY  - JOUR
T1  - Digital evidence accreditation in the corporate and business environment
JO  - Digital Investigation
VL  - 2
IS  - 2
SP  - 137
EP  - 146
PY  - 2005/6//
T2  - 
AU  - Barbara, John J.
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2005.04.005
UR  - http://www.sciencedirect.com/science/article/pii/S1742287605000344
KW  - Accreditation
KW  - ASCLD/LAB
KW  - Audio analysis
KW  - Computer forensics
KW  - Digital evidence
KW  - Video analysis
AB  - A prominent banking institution in the United States has submitted an application to have its Computer Forensics unit inspected as the first step towards attaining accreditation. Several other corporations and businesses that operate Computer Forensics units are also considering submitting their applications. This is in response to the American Society of Crime Laboratory Directors/Laboratory Accreditation Board's (ASCLD/LAB) accreditation program which began offering accreditation in the Digital Evidence Discipline in 2003. As defined in the ASCLD/LAB accreditation manual, any laboratory conducting forensic analysis in any of the four sub-disciplines of Digital Evidence (Audio Analysis, Computer Forensics, Digital Imaging Analysis, or Video Analysis) can apply for accreditation. This information is widely known in the forensic crime laboratory community, but most executives and examiners in the corporate and business sector are not aware that they also can apply for accreditation in the Digital Evidence discipline.
ER  - 

TY  - JOUR
T1  - Protecting the Network: NIDS: the logical first step in intrusion detection deployment
JO  - Network Security
VL  - 2001
IS  - 12
SP  - 10
EP  - 11
PY  - 2001/12/1/
T2  - 
AU  - Packer, Ryon
SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/S1353-4858(01)01218-1
UR  - http://www.sciencedirect.com/science/article/pii/S1353485801012181
AB  - Every effort to secure a network starts with policy decisions. These decisions may be simply a discussion in the corridor, or they can be a well thought out corporate security policy. Like all things, the more preparation that goes into the security policy, the more cost effectively and quickly it can be deployed.
ER  - 

TY  - JOUR
T1  - Nowhere to Hide
JO  - Computer Fraud & Security
VL  - 2002
IS  - 12
SP  - 13
EP  - 15
PY  - 2002/12//
T2  - 
AU  - Dwan, Berni
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(02)01213-7
UR  - http://www.sciencedirect.com/science/article/pii/S1361372302012137
AB  - There are so many good reasons to call in a computer forensics investigator. Leaking of trade secrets, sexual harassment, embezzlement, cooking the books to hide a bad situation, leaking of trade secrets or customer information, or a dismissal challenge. You want to get the hard evidence to prove that these misdemeanours really occurred, and you want the evidence to put a name to the perpetrator. Dan Verton invites us to think of computer forensics as we would an autopsy, its purpose being to determine whether or not a crime has been committed, and if so, proving it.1 The single, consistent piece of advice from all the literature though is that you must leave the potential sources of evidence untouched until the computer forensics experts, and only they, handle the equipment. Otherwise, it is highly likely that the evidence will be inadvertently destroyed and therefore inadmissible in court.
ER  - 

TY  - JOUR
T1  - Digital droplets: Microsoft SkyDrive forensic data remnants
JO  - Future Generation Computer Systems
VL  - 29
IS  - 6
SP  - 1378
EP  - 1394
PY  - 2013/8//
T2  - Including Special sections: High Performance Computing in the Cloud &amp; Resource Discovery Mechanisms for P2P Systems
AU  - Quick, Darren
AU  - Choo, Kim-Kwang Raymond
SN  - 0167-739X
DO  - http://dx.doi.org/10.1016/j.future.2013.02.001
UR  - http://www.sciencedirect.com/science/article/pii/S0167739X13000265
KW  - Cloud storage
KW  - Cloud forensics
KW  - Digital forensic framework
KW  - Microsoft SkyDrive analysis
AB  - Cloud storage services such as the popular Microsoft© SkyDrive© provide both organisational and individual users a cost-effective, and in some cases free, way of accessing, storing and disseminating data. The identification of digital evidence relating to cloud storage services can, however, be a challenge in a digital forensic investigation. Using SkyDrive as a case study, we identified the types of terrestrial artefacts that are likely to remain on a client’s machine (in the context of our experiments; computer hard drive and iPhone), and where the access point(s) for digital forensics examiners are, that will allow them to undertake steps to secure evidence in a timely fashion.
ER  - 

TY  - JOUR
T1  - Fostering group norm development and orientation while creating awareness contents for improving net-based collaborative problem solving
JO  - Computers in Human Behavior
VL  - 37
IS  - 
SP  - 298
EP  - 306
PY  - 2014/8//
T2  - 
AU  - Engelmann, Tanja
AU  - Kozlov, Michail D.
AU  - Kolodziej, Richard
AU  - Clariana, Roy B.
SN  - 0747-5632
DO  - http://dx.doi.org/10.1016/j.chb.2014.04.052
UR  - http://www.sciencedirect.com/science/article/pii/S0747563214002738
KW  - Computer-supported collaborative problem solving
KW  - Knowledge and information awareness
KW  - Group awareness
KW  - Norm development
KW  - Norm orientation
AB  - Abstract
Empirical studies have demonstrated that being aware of the knowledge structures and of the underlying information of other group members improves computer-supported collaborative problem solving. While such studies used pre-made individual concept maps as awareness tools, empirical studies that used individual concept maps created by the group members themselves have not shown an advantage for group performance. An assumed reason is that individual members’ concept maps differ too much structurally so that using them would need a lot of effort. This experimental study compares 20 triads whose members can observe the map creation process of the other members in their group with 20 triads without this possibility. The results demonstrated that access to the map creation process of the other group members while building one’s own concept map led to a group norm of how to create such a map. As a result, group members created more structurally similar maps, which led, as in prior studies with pre-made maps, to improved group performance.
ER  - 

TY  - JOUR
T1  - Risk sensitive digital evidence collection
JO  - Digital Investigation
VL  - 2
IS  - 2
SP  - 101
EP  - 119
PY  - 2005/6//
T2  - 
AU  - Kenneally, Erin E.
AU  - Brown, Christopher L.T.
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2005.02.001
UR  - http://www.sciencedirect.com/science/article/pii/S1742287605000290
KW  - Computer forensics
KW  - Digital evidence collection
KW  - Evidence acquisition
KW  - Digital evidence admissibility
AB  - Over the past decade or so, well-understood procedures and methodologies have evolved within computer forensics digital evidence collection. Correspondingly, many organizations such as the HTCIA (High Technology Criminal Investigators Association) and IACIS (International Association of Computer Investigative Specialists) have emphasized disk imaging procedures which ensure reliability, completeness, accuracy, and verifiability of computer disk evidence. The rapidly increasing and changing volume of data within corporate network information systems and personal computers are driving the need to revisit current evidence collection methodologies. These methodologies must evolve to maintain the balance between electronic environmental pressures and legal standards.

This paper posits that the current methodology which focuses on collecting entire bit-stream images of original evidence disk is increasing legal and financial risks.11The authors emphasize that the proposed Risk Sensitive Evidence Collection Methodology is intended to complement traditional bit-stream methodology in circumstances that necessitate a more efficient and cost-sensitive approach to digital evidence collection. Those types of contexts are addressed herein. The authors do not suggest an abdication of the bit-stream methodology in contexts where the cost-benefit assessment suggests it is reasonable to adhere to this traditional approach. For example, when a search warrant application (affidavit) establishes that the computer is an “instrumentality” or “fruit” of the crime(s), then seizure and retention of the entire machine is permitted (and advisable) under the law because the computer per se becomes evidence of the criminal conduct, like a gun used in furtherance of a robbery. See, e.g., United States v. Farrell, 606 F.2d 1341, 1347 (D.C. Cir. 1979) (noting that the government is entitled “to seize the instrumentalities of crime and hold them until the trial is completed.”
 The first section frames the debate and change drivers for a Risk Sensitive approach to digital evidence collection, which is followed by the current methods of evidence collection along with a cost-benefit analysis. Then the methodology components of the Risk Sensitive approach to collection, and then concludes with a legal and resource risk assessment of this approach. Anticipated legal arguments are explored and countered, as well. The authors suggest an evolved evidence collection methodology which is more responsive to voluminous data cases while balancing the legal requirements for reliability, completeness, accuracy, and verifiability of evidence.
ER  - 

TY  - JOUR
T1  - Digital forensic investigation of cloud storage services
JO  - Digital Investigation
VL  - 9
IS  - 2
SP  - 81
EP  - 95
PY  - 2012/11//
T2  - 
AU  - Chung, Hyunji
AU  - Park, Jungheum
AU  - Lee, Sangjin
AU  - Kang, Cheulhoon
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2012.05.015
UR  - http://www.sciencedirect.com/science/article/pii/S1742287612000400
KW  - Digital forensics
KW  - Cloud storage services
KW  - Artifacts
KW  - Windows system
KW  - Mac system
KW  - iOS
KW  - Android
AB  - The demand for cloud computing is increasing because of the popularity of digital devices and the wide use of the Internet. Among cloud computing services, most consumers use cloud storage services that provide mass storage. This is because these services give them various additional functions as well as storage. It is easy to access cloud storage services using smartphones. With increasing utilization, it is possible for malicious users to abuse cloud storage services. Therefore, a study on digital forensic investigation of cloud storage services is necessary. This paper proposes new procedure for investigating and analyzing the artifacts of all accessible devices, such as Windows system, Mac system, iPhone, and Android smartphone.
ER  - 

TY  - JOUR
T1  - Digital forensics research: The next 10 years
JO  - Digital Investigation
VL  - 7, Supplement
IS  - 
SP  - S64
EP  - S73
PY  - 2010/8//
T2  - The Proceedings of the Tenth Annual DFRWS Conference
AU  - Garfinkel, Simson L.
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2010.05.009
UR  - http://www.sciencedirect.com/science/article/pii/S1742287610000368
KW  - Forensics
KW  - Human subjects research
KW  - Corpora
KW  - Real data corpus
KW  - Realistic data
AB  - Today’s Golden Age of computer forensics is quickly coming to an end. Without a clear strategy for enabling research efforts that build upon one another, forensic research will fall behind the market, tools will become increasingly obsolete, and law enforcement, military and other users of computer forensics products will be unable to rely on the results of forensic analysis. This article summarizes current forensic research directions and argues that to move forward the community needs to adopt standardized, modular approaches for data representation and forensic processing.
ER  - 

TY  - JOUR
T1  - Self-optimization of secure web services
JO  - Computer Communications
VL  - 31
IS  - 18
SP  - 4312
EP  - 4323
PY  - 2008/12/18/
T2  - Secure Multi-Mode Systems and their Applications for Pervasive Computing
AU  - Casola, Valentina
AU  - Mancini, Emilio P.
AU  - Mazzocca, Nicola
AU  - Rak, Massimiliano
AU  - Villano, Umberto
SN  - 0140-3664
DO  - http://dx.doi.org/10.1016/j.comcom.2008.05.045
UR  - http://www.sciencedirect.com/science/article/pii/S0140366408003538
KW  - Self-optimization
KW  - Web services
KW  - Performance prediction
KW  - Security evaluation
KW  - Autonomic
AB  - Service-oriented architectures (SOA) and, in particular, web services technologies are widely adopted for the development of interoperable systems. In a dynamic scenario, a composed service may exploit component services in order to complete its task; composed services are variously distributed, and offered by different providers in different security domains and under different load conditions. So, the development of services and their integration entails a huge number of design choices. Obtaining optimality for all of the involved parameters for composed services is a challenging and open issue. In this paper we present MAWeS, an autonomic framework that makes it possible to auto-configure and to auto-tune the composition of services, guaranteeing optimal performance and the fulfillment of given security requirements. We will illustrate the framework architecture and how it is able to support the development of self-optimizing autonomic services on the basis of two evaluation services, the first one able to predict the performance of different services execution, the second one able to evaluate the security level provided by a service.
ER  - 

TY  - JOUR
T1  - Usage analysis of a primary care medical resource on the internet1
JO  - Computers in Biology and Medicine
VL  - 28
IS  - 5
SP  - 581
EP  - 588
PY  - 1998/9//
T2  - 
AU  - Graber, Mark A.
AU  - D'Alessandro, Donna M.
AU  - D'Alessandro, Michael P.
AU  - Bergus, George R.
AU  - Levy, Barcey
AU  - Ostrem, Steven F.
SN  - 0010-4825
DO  - http://dx.doi.org/10.1016/S0010-4825(98)00035-3
UR  - http://www.sciencedirect.com/science/article/pii/S0010482598000353
KW  - Medical education
KW  - Digital libraries
KW  - Digital information resources
AB  - Physicians and patients need convenient access to quality medical information. This study's goal was to place a medical resource on the World-Wide Web (WWW), allow access to it through a simple to use interface, and analyze the usage of such a resource. The Family Practice Handbook (TFPH) was digitized and placed onto the WWW. Usage data was obtained from June 1995–June 1996. 118,804 individuals accessed TFPH viewing 409,711 pages of information. A broad spectrum of topics was accessed. TFPH proved to be an extremely popular resource, servicing the broad information needs of an international audience. These preliminary findings suggest the future promise of Internet medical resources.
ER  - 

TY  - JOUR
T1  - Non-significant intention–behavior effects in educational technology acceptance: A case of competing cognitive scripts?
JO  - Computers in Human Behavior
VL  - 34
IS  - 
SP  - 333
EP  - 338
PY  - 2014/5//
T2  - 
AU  - Murillo Montes de Oca, Ambar
AU  - Nistor, Nicolae
SN  - 0747-5632
DO  - http://dx.doi.org/10.1016/j.chb.2014.01.026
UR  - http://www.sciencedirect.com/science/article/pii/S0747563214000314
KW  - Educational technology acceptance
KW  - Use behavior
KW  - Academic help seeking
KW  - Informal learning
KW  - Scripts
AB  - Abstract
Current technology acceptance research insufficiently considers complex educational settings. Recent research in educational technology acceptance has found weak or non-significant intention–behavior effects. To understand this finding, this paper presents a learning scripts approach to acceptance. A mixed methods approach is used to examine the intention–behavior effect in the context of informal learning in the workplace, focusing on the use of a virtual community of practice (vCoP) where participants share knowledge about the technical use of a software used in daily work tasks. Alternatively, users can access expert knowledge by contacting a Help Desk. As expected, the quantitative results show that the participants develop an intention to use the vCoP, however this intention has a limited effect on the actual vCoP use behavior. Qualitative results reveal that users have two cognitive scripts: an acceptance script, resulting in intention formation, and a help-seeking script, a well-established script in users which is leading them away from the technology and toward alternative help-seeking strategies. The help-seeking script is therefore interfering with the acceptance script, thus explaining weak or non-significant intention–behavior effects. Further research is needed to explore additional scripts that play a role in educational technology acceptance.
ER  - 

TY  - JOUR
T1  - Advanced topics of a computer center audit
JO  - Computers & Security
VL  - 3
IS  - 3
SP  - 171
EP  - 185
PY  - 1984/8//
T2  - 
AU  - van den Berg, Bram
AU  - Leenaars, Hans
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/0167-4048(84)90039-7
UR  - http://www.sciencedirect.com/science/article/pii/0167404884900397
KW  - Auditing
KW  - data communication networks
KW  - RACF
KW  - access control
KW  - data dictionary/directory systems
AB  - An auditor's review is concerned mostly with the faithfulness of accounts. The opinion is always achieved after carrying out an audit in which various types of appropriate control measures are applied, depending on the actual situation. An investigation into the organization and procedures of the computer center (hereafter called the computer center audit) forms a professionally necessary complement to activities traditionally carried out by the auditor in environments where the opinion is not merely achieved on the basis of the application of control measures directed towards the results of data processing, but where the opinion is also based on the conditions under which automatic data processing takes place.

After a concise introduction about the place of a computer center audit in the whole spectrum of internal control measures and related factor, this article will deal with two specific subjects which may be covered in a computer center audit: the security of data communication networks and data dictionary/directory systems. This article is aimed especially at auditors who carry out computer center audits. The authors hope that it will lead to further discussion.
ER  - 

TY  - JOUR
T1  - An advanced security scheme based on clustering and key distribution in vehicular ad-hoc networks
JO  - Computers & Electrical Engineering
VL  - 40
IS  - 2
SP  - 517
EP  - 529
PY  - 2014/2//
T2  - 
AU  - Daeinabi, Ameneh
AU  - Rahbar, Akbar Ghaffarpour
SN  - 0045-7906
DO  - http://dx.doi.org/10.1016/j.compeleceng.2013.10.003
UR  - http://www.sciencedirect.com/science/article/pii/S0045790613002516
AB  - Abstract
Preservation of security is an essential requirement in vehicular ad hoc networks (VANETs) as vehicular communication is vulnerable to attacks. Attackers may exploit VANETs to send bogus information to deceive other vehicles which leads to serious issues. In this paper, we describe an advanced Secure scheme based on Clustering and Key Distribution (SCKD) among members and cluster-heads in VANET. The SCKD is a coordination based algorithm in which nodes are located within different clusters and their cluster heads are chosen from trusty nodes. For a secure end-to-end communication, our scheme deploys the proxy signature, blind proxy signature, hashed message authentication code, and symmetric cryptography. Results show that our scheme preserves security requirements including authentication, confidentiality, data-integrity, non-repudiation, and unforgeability. Since the cost and time computation of key generation and distribution decreases by SCKD compared with other algorithms, our algorithm will be applicable for VANETs.
ER  - 

TY  - JOUR
T1  - Roving bugnet: Distributed surveillance threat and mitigation
JO  - Computers & Security
VL  - 29
IS  - 5
SP  - 592
EP  - 602
PY  - 2010/7//
T2  - Challenges for Security, Privacy and Trust
AU  - Farley, Ryan
AU  - Wang, Xinyuan
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/j.cose.2009.12.002
UR  - http://www.sciencedirect.com/science/article/pii/S0167404809001400
KW  - Surveillance
KW  - Spyware
KW  - Anti-spyware
KW  - Roving bug
KW  - Microphone hijack
KW  - Bot
KW  - Botnet
KW  - Mobile devices
KW  - Win XP
KW  - Mac OS X
AB  - Advanced mobile devices such as laptops and smartphones make convenient hiding places for surveillance spyware. They commonly have a microphone and camera built-in, are increasingly network accessible, frequently within close proximity of their users, and almost always lack mechanisms designed to prevent unauthorized microphone or camera access.

In order to explore surveillance intrusion and detection methods, we present a modernized version of a microphone hijacker for Windows and Mac OS X. The Windows attack can be executed as soon as the target connects to the Internet from anywhere in the world without requiring interaction from victimized users and the Mac OS X attack involves a trojaned installation routine. As the attacker compromises additional machines they are organized into a botnet so the attacker can maintain stealthy control of the systems and launch later surveillance attacks.

We then use the attack to show how common elements of microphone hijacker programs can be used against them. From there we present a mechanism to detect the threat on Windows, as well as a novel method to deceive an attacker in order to permit traceback. As a result of the detection mechanism we address a missing segment of resource control, decreasing the complexity of privacy concerns as exploitable devices become more pervasive.
ER  - 

TY  - JOUR
T1  - The difficult art of managing logs
JO  - Computer Fraud & Security
VL  - 2007
IS  - 10
SP  - 5
EP  - 7
PY  - 2007/10//
T2  - 
AU  - Pasquinucci, Andrea
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(07)70131-8
UR  - http://www.sciencedirect.com/science/article/pii/S1361372307701318
AB  - Andrea Pasquinucci looks at how logs can make life much easier.
ER  - 

TY  - JOUR
T1  - Intrusion Detection Systems: What is intrusion detection and why do we need it?
JO  - Computer Fraud & Security
VL  - 2001
IS  - 6
SP  - 9
EP  - 12
PY  - 2001/6/1/
T2  - 
AU  - Barber, Richard
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(01)00614-5
UR  - http://www.sciencedirect.com/science/article/pii/S1361372301006145
AB  - In order to answer the opening question, it may help to consider the following scenario. Once upon a time there were isolated villages where everyone trusted everyone else. Little by little, people started to travel from village to village to exchange goods, news and information and life generally got better. A few people used the opportunity to travel from village to village to look inside other people’s houses and take things without permission. As this behaviour became more frequent, villagers began putting locks on their doors. But the criminal element (or in some cases the rabidly curious) was inspired to find ways around these locks. So the villages developed new ways of detecting intruders and monitoring what was taken. These methods included security guards, infra-red motion detectors and security cameras.
ER  - 

TY  - JOUR
T1  - A healthy prognosis for biometric technologies
JO  - Biometric Technology Today
VL  - 10
IS  - 3
SP  - 9
EP  - 11
PY  - 2002/3/31/
T2  - 

SN  - 0969-4765
DO  - http://dx.doi.org/10.1016/S0969-4765(02)00318-1
UR  - http://www.sciencedirect.com/science/article/pii/S0969476502003181
AB  - In an increasingly complex healthcare environment, where information can be shared by multiple organizations and users, biometric technology is gradually gaining acceptance as a vital part of the security equation. New legislation is pushing this biometric adoption, but lead times still remain lengthy.
ER  - 

TY  - JOUR
T1  - A practical and robust approach to coping with large volumes of data submitted for digital forensic examination
JO  - Digital Investigation
VL  - 10
IS  - 2
SP  - 116
EP  - 128
PY  - 2013/9//
T2  - Triage in Digital Forensics
AU  - Shaw, Adrian
AU  - Browne, Alan
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2013.04.003
UR  - http://www.sciencedirect.com/science/article/pii/S1742287613000327
KW  - Digital forensics
KW  - Triage
KW  - Linux
KW  - Open source
KW  - Enhanced previewing
AB  - Abstract
Digital forensic triage is poorly defined and poorly understood. The lack of clarity surrounding the process of triage has given rise to legitimate concerns. By trying to define what triage actually is, one can properly engage with the concerns surrounding the process. This paper argues that digital forensic triage has been conducted on an informal basis for a number of years in digital forensic laboratories, even where there are legitimate objections to the process. Nevertheless, there are clear risks associated with the process of technical triage, as currently practised. The author has developed and deployed a technical digital forensic previewing process that negates many of the current concerns regarding the triage process and that can be deployed in any digital forensic laboratory at very little cost. This paper gives a high-level overview of how the system works and how it can be deployed in the digital forensic laboratory.
ER  - 

TY  - JOUR
T1  - Embedded holonic fault diagnosis of complex transportation systems
JO  - Engineering Applications of Artificial Intelligence
VL  - 26
IS  - 1
SP  - 227
EP  - 240
PY  - 2013/1//
T2  - 
AU  - Le Mortellec, Antoine
AU  - Clarhaut, Joffrey
AU  - Sallez, Yves
AU  - Berger, Thierry
AU  - Trentesaux, Damien
SN  - 0952-1976
DO  - http://dx.doi.org/10.1016/j.engappai.2012.09.008
UR  - http://www.sciencedirect.com/science/article/pii/S0952197612002242
KW  - Embedded diagnosis
KW  - Holonic architecture
KW  - Cooperative fault diagnosis
KW  - Model-based diagnosis
KW  - Corrective maintenance
KW  - Railway transportation system
AB  - The use of electronic equipment and embedded computing technologies in modern complex transportation systems continues to grow in a highly competitive market, in which product maintainability and availability is vital. These technological advances also make fault diagnosis and maintenance interventions much more challenging, since these operations require a deep understanding of the entire system. This paper proposes a holonic cooperative fault diagnosis approach, along with a generic architecture, to increase the embedded diagnosis capabilities of complex transportation systems. This concept is applied to the fault diagnosis of door systems of a railway transportation system.
ER  - 

TY  - JOUR
T1  - Security policy
JO  - Computers & Security
VL  - 9
IS  - 7
SP  - 605
EP  - 610
PY  - 1990/11//
T2  - 
AU  - Banks, Simon
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/0167-4048(90)90058-2
UR  - http://www.sciencedirect.com/science/article/pii/0167404890900582
AB  - We frequently hear a lot about the need for security, indeed there are now security products on the market to more than protect most systems whether they be on mainframe, mini or microcomputer systems. While each of these products is usually designed to carry out a specific task, the way in which these controls are introduced is of prime importance as they are only a means to an end. All too often it is thought that all that is required is to introduce some piece of software or hardware and everything will be a bed of roses.

In order to ensure that security is implemented in a controlled manner it is essential that there is a formal policy. What follows is my representations of what this policy should contain.
ER  - 

TY  - JOUR
T1  - The value of Retrospective and Concurrent Think Aloud in formative usability testing of a physician data query tool
JO  - Journal of Biomedical Informatics
VL  - 55
IS  - 
SP  - 1
EP  - 10
PY  - 2015/6//
T2  - 
AU  - Peute, Linda W.P.
AU  - de Keizer, Nicolette F.
AU  - Jaspers, Monique W.M.
SN  - 1532-0464
DO  - http://dx.doi.org/10.1016/j.jbi.2015.02.006
UR  - http://www.sciencedirect.com/science/article/pii/S1532046415000386
KW  - User–computer interface
KW  - Usability
KW  - Methodologies
KW  - Think Aloud
KW  - ICU
KW  - Databases
AB  - AbstractObjective
To compare the performance of the Concurrent (CTA) and Retrospective (RTA) Think Aloud method and to assess their value in a formative usability evaluation of an Intensive Care Registry-physician data query tool designed to support ICU quality improvement processes.
Methods
Sixteen representative intensive care physicians participated in the usability evaluation study. Subjects were allocated to either the CTA or RTA method by a matched randomized design. Each subject performed six usability-testing tasks of varying complexity in the query tool in a real-working context. Methods were compared with regard to number and type of problems detected. Verbal protocols of CTA and RTA were analyzed in depth to assess differences in verbal output. Standardized measures were applied to assess thoroughness in usability problem detection weighted per problem severity level and method overall effectiveness in detecting usability problems with regard to the time subjects spent per method.
Results
The usability evaluation of the data query tool revealed a total of 43 unique usability problems that the intensive care physicians encountered. CTA detected unique usability problems with regard to graphics/symbols, navigation issues, error messages, and the organization of information on the query tool’s screens. RTA detected unique issues concerning system match with subjects’ language and applied terminology. The in-depth verbal protocol analysis of CTA provided information on intensive care physicians’ query design strategies. Overall, CTA performed significantly better than RTA in detecting usability problems. CTA usability problem detection effectiveness was 0.80 vs. 0.62 (p &lt; 0.05) respectively, with an average difference of 42% less time spent per subject compared to RTA. In addition, CTA was more thorough in detecting usability problems of a moderate (0.85 vs. 0.7) and severe nature (0.71 vs. 0.57).
Conclusion
In this study, the CTA is more effective in usability-problem detection and provided clarification of intensive care physician query design strategies to inform redesign of the query tool. However, CTA does not outperform RTA. The RTA additionally elucidated unique usability problems and new user requirements. Based on the results of this study, we recommend the use of CTA in formative usability evaluation studies of health information technology. However, we recommend further research on the application of RTA in usability studies with regard to user expertise and experience when focusing on user profile customized (re)design.
ER  - 

TY  - JOUR
T1  - Advanced probabilistic approach for network intrusion forecasting and detection
JO  - Expert Systems with Applications
VL  - 40
IS  - 1
SP  - 315
EP  - 322
PY  - 2013/1//
T2  - 
AU  - Shin, Seongjun
AU  - Lee, Seungmin
AU  - Kim, Hyunwoo
AU  - Kim, Sehun
SN  - 0957-4174
DO  - http://dx.doi.org/10.1016/j.eswa.2012.07.057
UR  - http://www.sciencedirect.com/science/article/pii/S0957417412009128
KW  - Intrusion forecasting
KW  - Markov chain
KW  - Anomaly detection
KW  - DDoS detection
AB  - Recently, as damage caused by Internet threats has increased significantly, one of the major challenges is to accurately predict the period and severity of threats. In this study, a novel probabilistic approach is proposed effectively to forecast and detect network intrusions. It uses a Markov chain for probabilistic modeling of abnormal events in network systems. First, to define the network states, we perform K-means clustering, and then we introduce the concept of an outlier factor. Based on the defined states, the degree of abnormality of the incoming data is stochastically measured in real-time. The performance of the proposed approach is evaluated through experiments using the well-known DARPA 2000 data set and further analyzes. The proposed approach achieves high detection performance while representing the level of attacks in stages. In particular, our approach is shown to be very robust to training data sets and the number of states in the Markov model.
ER  - 

TY  - JOUR
T1  - From access and integration to mining of secure genomic data sets across the Grid
JO  - Future Generation Computer Systems
VL  - 23
IS  - 3
SP  - 447
EP  - 456
PY  - 2007/3//
T2  - 
AU  - Sinnott, Richard O.
SN  - 0167-739X
DO  - http://dx.doi.org/10.1016/j.future.2006.07.007
UR  - http://www.sciencedirect.com/science/article/pii/S0167739X06001361
KW  - Grid
KW  - Security
KW  - Genomics
KW  - Data Grid
KW  - Microarray data
AB  - The UK Department of Trade and Industry (DTI) funded BRIDGES project (Biomedical Research Informatics Delivered by Grid Enabled Services) has developed a Grid infrastructure to support cardiovascular research. This includes the provision of a compute Grid and a data Grid infrastructure with security at its heart. In this paper we focus on the BRIDGES data Grid. A primary aim of the BRIDGES data Grid is to help control the complexity in access to and integration of a myriad of genomic data sets through simple Grid-based tools. We outline these tools, how they are delivered to the end user scientists. We also describe how these tools are to be extended in the BBSRC funded Grid Enabled Microarray Expression Profile Search (GEMEPS) to support a richer vocabulary of search capabilities to support mining of microarray data sets. As with BRIDGES, fine grain Grid security underpins GEMEPS.
ER  - 

TY  - JOUR
T1  - Two-phase clustering process for outliers detection
JO  - Pattern Recognition Letters
VL  - 22
IS  - 6–7
SP  - 691
EP  - 700
PY  - 2001/5//
T2  - 
AU  - Jiang, M.F
AU  - Tseng, S.S
AU  - Su, C.M
SN  - 0167-8655
DO  - http://dx.doi.org/10.1016/S0167-8655(00)00131-8
UR  - http://www.sciencedirect.com/science/article/pii/S0167865500001318
KW  - Outliers
KW  - k-means clustering
KW  - Two-phase clustering
KW  - MST
AB  - In this paper, a two-phase clustering algorithm for outliers detection is proposed. We first modify the traditional k-means algorithm in Phase 1 by using a heuristic “if one new input pattern is far enough away from all clusters' centers, then assign it as a new cluster center”. It results that the data points in the same cluster may be most likely all outliers or all non-outliers. And then we construct a minimum spanning tree (MST) in Phase 2 and remove the longest edge. The small clusters, the tree with less number of nodes, are selected and regarded as outlier. The experimental results show that our process works well.
ER  - 

TY  - JOUR
T1  - An inexpensive computer-assisted psychometric system (the computer programs)
JO  - International Journal of Bio-Medical Computing
VL  - 3
IS  - 3
SP  - 223
EP  - 236
PY  - 1972/7//
T2  - 
AU  - Birtles, C.J.
SN  - 0020-7101
DO  - http://dx.doi.org/10.1016/0020-7101(72)90016-5
UR  - http://www.sciencedirect.com/science/article/pii/0020710172900165
AB  - The use of computers in the study of human behaviour has greatly simplified the task of psychometric testing; however, the cost of on-line computer facilities is prohibitive in many situations. This paper describes computer programs used in conjunction with an off-line system described by Birtles, Sambrooks, MacCulloch and Holland (1971). Software is described for processing recorded data from various types of tape and also for permanent storage of data on magnetic tape. Security of data is ensured under a variety of situations, but where these safeguards fail, error messages are provided to assist in rapid fault tracing. Finally, a number of points arising from studies using the system are discussed.
ER  - 

TY  - JOUR
T1  - A proposal for automating investigations in live forensics
JO  - Computer Standards & Interfaces
VL  - 32
IS  - 5–6
SP  - 246
EP  - 255
PY  - 2010/10//
T2  - Information and communications security, privacy and trust: Standards and Regulations
AU  - Lee, Seokhee
AU  - Savoldi, Antonio
AU  - Lim, Kyoung Soo
AU  - Park, Jong Hyuk
AU  - Lee, Sangjin
SN  - 0920-5489
DO  - http://dx.doi.org/10.1016/j.csi.2009.09.001
UR  - http://www.sciencedirect.com/science/article/pii/S0920548909000762
KW  - Digital evidence collection
KW  - Live forensics
KW  - Automated digital investigation process
KW  - XML technology
AB  - In this paper we present an XML-based framework, called XLIVE, which provides an efficient way to collect data in live forensic cases, according to well-known crime categories. XLIVE is a forensic automated framework that can be used in live forensic investigations for gathering live data on a Windows-based system. In addition, we have also implemented a proof-of-concept, called LRDS (Live Resource Detection System). This approach of examination will be used extensively to deal with terabyte/petabyte digital systems, where other approaches, such as a post-mortem analysis, cannot be adopted.
ER  - 

TY  - JOUR
T1  - A lightweight web-based vulnerability scanner for small-scale computer network security assessment
JO  - Journal of Network and Computer Applications
VL  - 32
IS  - 1
SP  - 78
EP  - 95
PY  - 2009/1//
T2  - 
AU  - Davies, Pete
AU  - Tryfonas, Theodore
SN  - 1084-8045
DO  - http://dx.doi.org/10.1016/j.jnca.2008.04.007
UR  - http://www.sciencedirect.com/science/article/pii/S1084804508000416
KW  - Vulnerability scanner
KW  - Web-based security assessment
AB  - There appears to be a common perception amongst average computer users pointing towards a global lack of trust when using the Internet. The resolution of this lack of trust relating to the use of the Internet, particularly orientated towards its commercial use and online purchasing, requires partly from website developers to create and maintain web applications that are robust and provide a certain degree of resilience to attack from outside threats. This project intends to contribute to this particular aspect by providing site developers and system testers, as well as simple site users, with a tool for reconnaissance, vulnerability scanning and remote network mapping that is easily accessible and useable due to its web-based and visual, event-driven interface. It is anticipated that the cumbersome task of learning to use a number of command line tools and their exact functionality and parameters can be avoided through this and similar developments, and hence that this will potentially widen the access to security testing, particularly to small and medium businesses.
ER  - 

TY  - JOUR
T1  - Achieving an effective, scalable and privacy-preserving data sharing service in cloud computing
JO  - Computers & Security
VL  - 42
IS  - 
SP  - 151
EP  - 164
PY  - 2014/5//
T2  - 
AU  - Dong, Xin
AU  - Yu, Jiadi
AU  - Luo, Yuan
AU  - Chen, Yingying
AU  - Xue, Guangtao
AU  - Li, Minglu
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/j.cose.2013.12.002
UR  - http://www.sciencedirect.com/science/article/pii/S0167404813001703
KW  - Data security
KW  - Data sharing
KW  - Privacy-preserving
KW  - Attribute-based encryption
KW  - Scalability
KW  - Cloud computing
AB  - Abstract
Data sharing in the cloud, fueled by favorable trends in cloud technology, is emerging as a promising technique for allowing users to conveniently access data. However, the growing number of enterprises and customers who stores their data in cloud servers is increasingly challenging users' privacy and the security of data. This paper focuses on providing a dependable and secure cloud data sharing service that allows users dynamic access to their data. In order to achieve this, we propose an effective, scalable and flexible privacy-preserving data policy with semantic security, by utilizing ciphertext policy attribute-based encryption (CP-ABE) combined with identity-based encryption (IBE) techniques. In addition to ensuring robust data sharing security, our policy succeeds in preserving the privacy of cloud users and supports efficient and secure dynamic operations including, but not limited to, file creation, user revocation and modification of user attributes. Security analysis indicates that the proposed policy is secure under the generic bilinear group model in the random oracle model and enforces fine-grained access control, full collusion resistance and backward secrecy. Furthermore, performance analysis and experimental results show that the overheads are as light as possible.
ER  - 

TY  - JOUR
T1  - On using PROMPT for the automatic implementation of the ISO ACSE protocol
JO  - Journal of Systems and Software
VL  - 28
IS  - 2
SP  - 143
EP  - 155
PY  - 1995/2//
T2  - 
AU  - Lai, R.
SN  - 0164-1212
DO  - http://dx.doi.org/10.1016/0164-1212(94)00052-O
UR  - http://www.sciencedirect.com/science/article/pii/016412129400052O
AB  - Implementation of communication protocols requires a huge amount of programmers' effort. New protocols are being introduced and old ones are being updated. More effective methods are needed for fast implementation of new protocol standards. As formal description techniques mature, formal specification can facilitate automatic implementation of protocol standards through the use of a high-level compiler. This has the advantages of eliminating bugs introduced during programming development and producing fast implementation of the standards. This article describes use of the automated tool, PROMPT, developed by Telecom Australia, for the automatic implementation of an application layer protocol, the ISO association control service element protocol.
ER  - 

TY  - JOUR
T1  - Extending the advanced forensic format to accommodate multiple data sources, logical evidence, arbitrary information and forensic workflow
JO  - Digital Investigation
VL  - 6, Supplement
IS  - 
SP  - S57
EP  - S68
PY  - 2009/9//
T2  - The Proceedings of the Ninth Annual DFRWS Conference
AU  - Cohen, Michael
AU  - Garfinkel, Simson
AU  - Schatz, Bradley
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2009.06.010
UR  - http://www.sciencedirect.com/science/article/pii/S1742287609000401
KW  - Digital forensics
KW  - Image
KW  - Hard disk Imaging
KW  - Digital Evidence Management
KW  - Distributed Storage
KW  - Distributed Forensic Analysis
KW  - Forensic File Format
KW  - Evidence Archiving
KW  - Cryptography
KW  - Forensic Integrity
AB  - Forensic analysis requires the acquisition and management of many different types of evidence, including individual disk drives, RAID sets, network packets, memory images, and extracted files. Often the same evidence is reviewed by several different tools or examiners in different locations. We propose a backwards-compatible redesign of the Advanced Forensic Format—an open, extensible file format for storing and sharing of evidence, arbitrary case related information and analysis results among different tools. The new specification, termed AFF4, is designed to be simple to implement, built upon the well supported ZIP file format specification. Furthermore, the AFF4 implementation has downward comparability with existing AFF files.
ER  - 

TY  - JOUR
T1  - Usability flaws of medication-related alerting functions: A systematic qualitative review
JO  - Journal of Biomedical Informatics
VL  - 55
IS  - 
SP  - 260
EP  - 271
PY  - 2015/6//
T2  - 
AU  - Marcilly, Romaric
AU  - Ammenwerth, Elske
AU  - Vasseur, Francis
AU  - Roehrer, Erin
AU  - Beuscart-Zéphir, Marie-Catherine
SN  - 1532-0464
DO  - http://dx.doi.org/10.1016/j.jbi.2015.03.006
UR  - http://www.sciencedirect.com/science/article/pii/S1532046415000490
KW  - User–computer interface
KW  - Human engineering
KW  - Decision support systems
KW  - Clinical
KW  - Review
KW  - Systematic
KW  - Usability
KW  - Alerting functions
AB  - AbstractIntroduction
Medication-related alerting functions may include usability flaws that limit their optimal use. A first step on the way to preventing usability flaws is to understand the characteristics of these usability flaws. This systematic qualitative review aims to analyze the type of usability flaws found in medication-related alerting functions.
Method
Papers were searched via PubMed, Scopus and Ergonomics Abstracts databases, along with references lists. Paper selection, data extraction and data analysis was performed by two to three Human Factors experts. Meaningful semantic units representing instances of usability flaws were the main data extracted. They were analyzed through qualitative methods: categorization following general usability heuristics and through an inductive process for the flaws specific to medication-related alerting functions.
Main results
From the 6380 papers initially identified, 26 met all eligibility criteria. The analysis of the papers identified a total of 168 instances of usability flaws that could be classified into 13 categories of usability flaws representing either violations of general usability principles (i.e. they could be found in any system, e.g. guidance and workload issues) or infractions specific to medication-related alerting functions. The latter refer to issues of low signal-to-noise ratio, incomplete content of alerts, transparency, presentation mode and timing, missing alert features, tasks and control distribution.
Main conclusion
The list of 168 instances of usability flaws of medication-related alerting functions provides a source of knowledge for checking the usability of medication-related alerting functions during their design and evaluation process and ultimately constructs evidence-based usability design principles for these functions.
ER  - 

TY  - JOUR
T1  - An efficient data exchange scheme for semiconductor engineering chain management system
JO  - Robotics and Computer-Integrated Manufacturing
VL  - 26
IS  - 5
SP  - 507
EP  - 516
PY  - 2010/10//
T2  - 
AU  - Hung, Min-Hsiung
AU  - Wu, Ssu-Wei
AU  - Wang, Tsung-Li
AU  - Cheng, Fan-Tien
AU  - Feng, Yen-Yun
SN  - 0736-5845
DO  - http://dx.doi.org/10.1016/j.rcim.2010.03.014
UR  - http://www.sciencedirect.com/science/article/pii/S0736584510000244
KW  - Engineering chain management system (ECMS)
KW  - Data exchange scheme
KW  - Web services
KW  - Message transmission optimization mechanism (MTOM)
KW  - Multi-threading
AB  - When IC production enters into the nano-meter generation, many yield problems are related to design. The semiconductor industry is eager to have engineering chain management systems (ECMSs) to tightly share engineering data among cooperative semiconductor companies, such as IC Design House, Mask-Fabrication Company, Foundry-Service Company, and Assembly/Test Company, via Internet for increasing the yield, reducing production cost, and decreasing time to market for a new IC. Traditionally, cooperative semiconductor companies exchange data through FTP that is activated manually. In recent years, the Web Services technology has provided a new and excellent approach for automatically exchanging and integrating data among heterogeneous systems on the Internet. In this paper, an ECMS framework for semiconductor industry is presented. Also, an efficient Web-Services-based data exchange scheme is developed to solve three core problems of data exchange in ECMS: the convenience of data exchange and integration, the security protection of data transmission, and the efficiency of transmitting data, in particular large binary data. Experimental test results show that the proposed EC data exchange scheme can fulfill the desired functional requirements and demonstrate a superior performance over the traditional data transfer methods. It is believed that the proposed data exchange scheme can be an effective solution to the data exchange problem of ECMS.
ER  - 

TY  - JOUR
T1  - The role of criminal profiling in the computer forensics process
JO  - Computers & Security
VL  - 22
IS  - 4
SP  - 292
EP  - 298
PY  - 2003/5//
T2  - 
AU  - Rogers, Marc
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/S0167-4048(03)00405-X
UR  - http://www.sciencedirect.com/science/article/pii/S016740480300405X
AB  - In today’s increasingly complex world, we find ourselves at a rather unique societal and cultural cross roads. At no other time in history has society been so dependent on technology and its various offshoots and incarnations11There may be an argument that the dawn of the Industrial Revolution placed society in a similar situation.
. Almost every facet of our day-to-day lives is impacted to some extent by technology (e.g., email, Internet, online banking, digital music, etc.). This reliance and to some extent dependence on technology, has had a ripple effect on other less obvious areas of society (Rogers, 2001; Schneier, 2002; Schwartau, 2000). One such area is law enforcement and, more specifically, criminal investigations (Kruse and Heiser, 2002). Historically, criminal investigations relied on such concepts as physical evidence, eyewitnesses, and confessions. Today, the criminal investigator must recognize that a vast amount of evidence will be in the electronic or digital form. The crime scene may consist of a computer system or network as opposed to the traditional ‘physical’ scene (Kruse and Heiser, 2002). The eyewitness of today and tomorrow may be a computer generated ‘log file’.
ER  - 

TY  - JOUR
T1  - Intrusion detection systems as evidence
JO  - Computer Networks
VL  - 31
IS  - 23–24
SP  - 2477
EP  - 2487
PY  - 1999/12/14/
T2  - 
AU  - Sommer, Peter
SN  - 1389-1286
DO  - http://dx.doi.org/10.1016/S1389-1286(99)00113-9
UR  - http://www.sciencedirect.com/science/article/pii/S1389128699001139
KW  - Evidence
KW  - Proof
KW  - Hacker
AB  - Although the main aim of Intrusion Detection Systems (IDSs) is to detect intrusions to prompt evasive measures, a further aim can be to supply evidence in criminal and civil legal proceedings. However the features that make an ID product good at providing early warning may render it less useful as an evidence-acquisition tool. An explanation is provided of admissibility and weight, the two determinants in the legal acceptability of evidence. The problems the courts have in dealing with novel scientific evidence and the differences between `scientific' and `legal' proof are discussed. Criteria for the evaluation of IDSs as sources of legal evidence are proposed, including preservation of evidence, continuity of evidence and transparency of forensic method. It is suggested that the key to successful prosecution of complex intrusions is the finding of multiple independent streams of evidence which corroborate one another. The USAF Rome Labs intrusion of early 1994 is used as a case-study to show how defence experts and lawyers can undermine investigators’ evidence.
ER  - 

TY  - JOUR
T1  - Building agents for rule-based intrusion detection system
JO  - Computer Communications
VL  - 25
IS  - 15
SP  - 1366
EP  - 1373
PY  - 2002/9/15/
T2  - 
AU  - Jha, S
AU  - Hassan, M
SN  - 0140-3664
DO  - http://dx.doi.org/10.1016/S0140-3664(02)00038-5
UR  - http://www.sciencedirect.com/science/article/pii/S0140366402000385
KW  - Intrusion detection system
KW  - Linux platform
KW  - Graphical warning interface
AB  - In this paper we describe the development and testing of an agent-based intrusion detection system for Linux platform. We take a dual-approach to intrusion detection: pre-emptory and reactionary. With the pre-emptory approach, a network-based agent is implemented to monitor all packets entering the network and detect a known attack-based on a pre-defined rule. The reactionary approach is realized through a separate host-based agent to routinely check specific log files in order to detect system anomalies caused by successful attacks. Once a possible intrusion attempt has been detected by either one of the agents, it attempts to block the attack, records the attack details in a system log file, E-mails the system administrator, displays a warning through a graphical warning window. The agents operate in the background of user applications and system software without any noticeable performance effect on them.
ER  - 

TY  - JOUR
T1  - PRINDA: Architecture and design of non-disclosure agreements in privacy policy framework
JO  - Data & Knowledge Engineering
VL  - 63
IS  - 3
SP  - 684
EP  - 698
PY  - 2007/12//
T2  - ’Four of the best papers presented
AU  - Gupta, S.K.
AU  - Goyal, Vikram
AU  - Gupta, Anand
AU  - Meshram, Indira
SN  - 0169-023X
DO  - http://dx.doi.org/10.1016/j.datak.2007.03.007
UR  - http://www.sciencedirect.com/science/article/pii/S0169023X07000493
KW  - PRINDA
KW  - Non-disclosure Agreements
KW  - Privacy policies
AB  - Non-disclosure agreements (NDAs) in real life are typically used whenever there is transfer of private or confidential information from one organization to another. The provider organization cannot have any control over privacy mechanisms of the receiving organization. The privacy policy work so far has addressed itself for preventing privacy violations within an organization. We aim to give an architecture of PRINDA (PRIvacy NDA) system which incorporates NDA’s in privacy policy framework. Advantages of PRINDA system will be the following: (i) as there can be traces of malicious activity (invasion of privacy) either at provider-end or at recipient-end, if detected/reported, NDA can help to fix the responsibility to the organization violating the agreement, and will strengthen control in the privacy arena and (ii) owner of data can be provided with detailed description of accesses to the data from every organization to which the data has been transferred (strengthening the principle of compliance).
ER  - 

TY  - JOUR
T1  - Procedure guidance for Internet forensics coping with copyright arguments of client-server-based P2P models
JO  - Computer Standards & Interfaces
VL  - 31
IS  - 4
SP  - 795
EP  - 800
PY  - 2009/6//
T2  - 
AU  - Wang, Shiuh-Jeng
AU  - Kao, Da-Yu
AU  - Huang, Frank Fu-Yuan
SN  - 0920-5489
DO  - http://dx.doi.org/10.1016/j.csi.2008.09.009
UR  - http://www.sciencedirect.com/science/article/pii/S0920548908001190
KW  - Copyright protection
KW  - P2P models
KW  - Cybercrime
KW  - Digital content
KW  - Forensic analysis
AB  - Digital technology for transferring and controlling data has made substantial advances in recent years. It is important to protect innovations and to curb the copyright infringements in computer-based systems. Copyright is a legal framework of basic rights, allowing the owner to control or permit someone else to reproduce copyrighted works with commercial value. In recent decades, copyright violations have been moving into the criminal realm. This paper focuses on the procedure guidance of a fictitious P2P model, and discusses whether it contributes to the crime of copyright infringement in dealing with the distribution of digital content. From the perspective of internet forensics, the action research and the whole control mechanism, it is shown that a commercial server has full control over the P2P model.
ER  - 

TY  - JOUR
T1  - The influence of task and gender on search and evaluation behavior using Google
JO  - Information Processing & Management
VL  - 42
IS  - 4
SP  - 1123
EP  - 1131
PY  - 2006/7//
T2  - 
AU  - Lorigo, Lori
AU  - Pan, Bing
AU  - Hembrooke, Helene
AU  - Joachims, Thorsten
AU  - Granka, Laura
AU  - Gay, Geri
SN  - 0306-4573
DO  - http://dx.doi.org/10.1016/j.ipm.2005.10.001
UR  - http://www.sciencedirect.com/science/article/pii/S0306457305001366
KW  - Web search
KW  - Eye tracking
KW  - Information retrieval
KW  - Human–computer interaction
KW  - Information seeking
AB  - To improve search engine effectiveness, we have observed an increased interest in gathering additional feedback about users’ information needs that goes beyond the queries they type in. Adaptive search engines use explicit and implicit feedback indicators to model users or search tasks. In order to create appropriate models, it is essential to understand how users interact with search engines, including the determining factors of their actions. Using eye tracking, we extend this understanding by analyzing the sequences and patterns with which users evaluate query result returned to them when using Google. We find that the query result abstracts are viewed in the order of their ranking in only about one fifth of the cases, and only an average of about three abstracts per result page are viewed at all. We also compare search behavior variability with respect to different classes of users and different classes of search tasks to reveal whether user models or task models may be greater predictors of behavior. We discover that gender and task significantly influence different kinds of search behaviors discussed here. The results are suggestive of improvements to query-based search interface designs with respect to both their use of space and workflow.
ER  - 

TY  - JOUR
T1  - Mining significant factors affecting the adoption of SaaS using the rough set approach
JO  - Journal of Systems and Software
VL  - 84
IS  - 3
SP  - 435
EP  - 441
PY  - 2011/3//
T2  - 
AU  - Wu, Wei-Wen
SN  - 0164-1212
DO  - http://dx.doi.org/10.1016/j.jss.2010.11.890
UR  - http://www.sciencedirect.com/science/article/pii/S0164121210003122
KW  - Software as a Service
KW  - Technology acceptance model
KW  - Rough set theory
AB  - Despite that Software as a Service (SaaS) seems to be the most tempting solution among different types of cloud services, yet it has not been adopted to-date with as much alacrity as was originally expected. A variety of factors may influence the adoption of SaaS solutions. The objective of this study is thus to explore the significant factors affecting the adoption of SaaS for vendors and enterprise users. An analytical framework is proposed containing two approaches—Technology Acceptance Model (TAM) and Rough Set Theory (RST). An empirical study on the IT/MIS enterprises in Taiwan is carried out. The results have revealed a considerable amount of meaningful information, which not only facilitates the SaaS vendors to grasp users’ needs and concerns about SaaS adoption, but also helps the managers to introduce effective marketing strategies and actions to promote the growth of SaaS market. Based on the findings, some managerial implications are discussed.
ER  - 

TY  - JOUR
T1  - Microcomputer security techniques
JO  - Computer Audit Update
VL  - 1989
IS  - 1
SP  - 11
EP  - 15
PY  - 1988/7//
Y2  - 1988/8//
T2  - 
AU  - Pattenden, Nick
SN  - 0960-2593
DO  - http://dx.doi.org/10.1016/S0960-2593(88)80023-5
UR  - http://www.sciencedirect.com/science/article/pii/S0960259388800235
ER  - 

TY  - JOUR
T1  - Data security and confidentiality in Europe
JO  - Computers & Security
VL  - 4
IS  - 3
SP  - 207
EP  - 210
PY  - 1985/9//
T2  - 
AU  - Chamoux, J.P.
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/0167-4048(85)90029-X
UR  - http://www.sciencedirect.com/science/article/pii/016740488590029X
KW  - Electronic funds transfer
KW  - electronic mail service
KW  - local area network
KW  - logical security
KW  - physical security
KW  - privacy.
AB  - The European Economic Community study covers the problems raised by new information systems of the 1980s and 1990s. The main findings demonstrate that the spreading use of computers with versatile networking facilities has led to the obsolescence of some of the established control systems. The logical security provided by operating systems and basic software is generally inadequate. The study stresses the need for better data protection in public systems as well as in the private sector. It also stresses the need for wider and more efficient use of available means for the physical protection of computer centers and media.
ER  - 

TY  - JOUR
T1  - Usability evaluation methods for the web: A systematic mapping study
JO  - Information and Software Technology
VL  - 53
IS  - 8
SP  - 789
EP  - 817
PY  - 2011/8//
T2  - Advances in functional size measurement and effort estimation - Extended best papers
AU  - Fernandez, Adrian
AU  - Insfran, Emilio
AU  - Abrahão, Silvia
SN  - 0950-5849
DO  - http://dx.doi.org/10.1016/j.infsof.2011.02.007
UR  - http://www.sciencedirect.com/science/article/pii/S0950584911000607
KW  - Usability evaluation methods
KW  - Web development
KW  - Systematic mapping
AB  - Context
In recent years, many usability evaluation methods (UEMs) have been employed to evaluate Web applications. However, many of these applications still do not meet most customers’ usability expectations and many companies have folded as a result of not considering Web usability issues. No studies currently exist with regard to either the use of usability evaluation methods for the Web or the benefits they bring.
Objective
The objective of this paper is to summarize the current knowledge that is available as regards the usability evaluation methods (UEMs) that have been employed to evaluate Web applications over the last 14 years.
Method
A systematic mapping study was performed to assess the UEMs that have been used by researchers to evaluate Web applications and their relation to the Web development process. Systematic mapping studies are useful for categorizing and summarizing the existing information concerning a research question in an unbiased manner.
Results
The results show that around 39% of the papers reviewed reported the use of evaluation methods that had been specifically crafted for the Web. The results also show that the type of method most widely used was that of User Testing. The results identify several research gaps, such as the fact that around 90% of the studies applied evaluations during the implementation phase of the Web application development, which is the most costly phase in which to perform changes. A list of the UEMs that were found is also provided in order to guide novice usability practitioners.
Conclusions
From an initial set of 2703 papers, a total of 206 research papers were selected for the mapping study. The results obtained allowed us to reach conclusions concerning the state-of-the-art of UEMs for evaluating Web applications. This allowed us to identify several research gaps, which subsequently provided us with a framework in which new research activities can be more appropriately positioned, and from which useful information for novice usability practitioners can be extracted.
ER  - 

TY  - JOUR
T1  - Assisting the design of a groupware system
JO  - The Journal of Logic and Algebraic Programming
VL  - 78
IS  - 4
SP  - 191
EP  - 232
PY  - 2009/4//
T2  - IFIP WG1.8 Workshop on Applying Concurrency Research in Industry
AU  - ter Beek, Maurice H.
AU  - Gnesi, Stefania
AU  - Latella, Diego
AU  - Massink, Mieke
AU  - Sebastianis, Maurizio
AU  - Trentanni, Gianluca
SN  - 1567-8326
DO  - http://dx.doi.org/10.1016/j.jlap.2008.11.004
UR  - http://www.sciencedirect.com/science/article/pii/S1567832608000945
KW  - Groupware
KW  - Concurrency
KW  - Formal methods
KW  - Verification
KW  - Model checking
AB  - Product Data Management (PDM) systems support the product/document management of design processes such as those typically used in the manufacturing industry. They allow enterprises to capture, organise, automate and share engineering information in an efficient way. The efficient handling of queries on product information and the uploading and downloading of families of related files for modification by designers are essential aspects of such systems. The efficiency of the system as perceived by its clients depends on its correct functioning, but also for a significant part on its performance aspects. In this article, we apply both qualitative and stochastic model-checking techniques to evaluate various usability and performance aspects of the thinkteam PDM system, and of several proposed extensions, thereby assisting the design phase of an industrial groupware system.
ER  - 

TY  - JOUR
T1  - Public participation in proprietary software development through user roles and discourse
JO  - International Journal of Human-Computer Studies
VL  - 66
IS  - 7
SP  - 545
EP  - 557
PY  - 2008/7//
T2  - Collaborative and social aspects of software development
AU  - Hendry, David G.
SN  - 1071-5819
DO  - http://dx.doi.org/10.1016/j.ijhcs.2007.12.002
UR  - http://www.sciencedirect.com/science/article/pii/S107158190700170X
KW  - Design informatics
KW  - Information management
KW  - Design knowledge management
KW  - Design information systems
KW  - Social creativity
KW  - Reflective practice
KW  - Free/open source software
KW  - Socio-technical analysis
KW  - Innovation
KW  - Social bookmarking
KW  - del.icio.us
AB  - The opportunity for users to participate in design and development processes has expanded in recent years through such communication and information technologies as mailing lists, bug trackers, usage monitoring, rich interactions between users and service-center staff, remote usability testing, and so on. A key question, therefore, is deciding how to engage users in design and development through such technologies. This paper addresses this question by reviewing literature on end-user programming and open source development to develop a framework concerning user roles and discourse. The framework makes two claims: (1) user roles and a social structure emerge after the introduction of a software application (role differentiation); and (2) different roles demand different kinds of discourse for deciding what to do and for reflecting upon intended and unintended consequences (role discourse demands). To show its application, the framework is used to analyze the development of del.icio.us, a breakthrough application for social bookmarking. This development process is notable because it is a characteristic of open source software development in some respects, but the code is not made available publicly. This hybridization appears to be widely applicable and suggests how design and development processes can be structured as a service where the design and development of the system proceeds simultaneously with the formation and nurturing of a community of users.
ER  - 

TY  - JOUR
T1  - Digital Wiretap Warrant: Improving the security of ETSI Lawful Interception
JO  - Digital Investigation
VL  - 14
IS  - 
SP  - 1
EP  - 16
PY  - 2015/9//
T2  - 
AU  - Muñoz, Alfonso
AU  - Urueña, Manuel
AU  - Aparicio, Raquel
AU  - Rodríguez de los Santos, Gerson
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2015.04.005
UR  - http://www.sciencedirect.com/science/article/pii/S1742287615000456
KW  - Digital Wiretap Warrant (DWW)
KW  - Lawful Interception (LI)
KW  - ETSI LI Technical Committee
KW  - Civil rights
KW  - Security
KW  - Privacy
KW  - Digital evidence
KW  - Chain of custody
AB  - Abstract
Lawful Interception (LI) of data communications is an essential tool for Law Enforcement Agencies (LEA) in order to investigate criminal activities carried out or coordinated by means of Internet. However, the ability to secretly monitor the activities of citizens also has a great impact on civil rights. Therefore, democratic societies must prevent abuse and ensure that LI is only employed in specific cases with justifiable grounds or a probable cause. Nowadays, in many countries each interception must be authorized by a wiretap warrant, usually issued by a judge. However, this wiretap warrant is merely an administrative document that should be checked by the network or service operator before enabling the monitoring of its customers, whose communications are later handed over to a LEA in plaintext. This paper proposes the idea of employing a Digital Wiretap Warrant (DWW), which further protects the civil liberties, security and privacy of LI by ensuring that monitoring devices can only be enabled with a valid DWW, and by encrypting the captured data so only the authorized LEA is able to decrypt those communications. Moreover, in the proposed DWW framework all digital evidence is securely time-stamped and signed, thus guaranteeing that it has not been tampered with, and that a proper chain of custody has been met. In particular this paper proposes how to apply the DWW concept to the lawful interception framework defined by the ETSI LI Technical Committee, and evaluates how the additional security mechanisms could impact the performance and storage costs of a LI platform.
ER  - 

TY  - JOUR
T1  - What matters in help-seeking? A study of help effectiveness and learner-related factors
JO  - Computers in Human Behavior
VL  - 22
IS  - 1
SP  - 113
EP  - 129
PY  - 2006/1//
T2  - Instructional Design for Effective and Enjoyable Computer-Supported Learning
AU  - Bartholomé, Tobias
AU  - Stahl, Elmar
AU  - Pieschl, Stephanie
AU  - Bromme, Rainer
SN  - 0747-5632
DO  - http://dx.doi.org/10.1016/j.chb.2005.01.007
UR  - http://www.sciencedirect.com/science/article/pii/S0747563205000166
KW  - Help-seeking
KW  - Interactive learning environment
KW  - Learner-related factors
AB  - Offering help functions is a standard feature of computer-based interactive learning environments (ILEs). Nevertheless, the effectiveness of learners’ use of help facilities has been subject to extensive debate. Recent research indicates an inefficient use or even ignorance of help functions. This article addresses the issues of effectiveness of help and the impact of learner-related factors in an ILE for plant identification. Students from three regular university courses in plant identification worked in a dyadic setting. The effects of two different types of help facilities (context-sensitive help and glossary function) on task performance in plant identification are analyzed. In addition, a broad set of potential learner-related factors is explored with respect to their effects on help-seeking, including prior knowledge, motivational orientation, interest, self-estimated competence, and epistemological beliefs. Results yield a positive effect of help-seeking on task performance. In addition, most learner-related factors affect help-seeking behavior and performance. Results are discussed with respect to their implications for future research on help-seeking in ILEs.
ER  - 

TY  - JOUR
T1  - The MINWii project: Renarcissization of patients suffering from Alzheimer’s disease through video game-based music therapy
JO  - Entertainment Computing
VL  - 3
IS  - 4
SP  - 111
EP  - 120
PY  - 2012/12//
T2  - 9th International Conference on Entertainment Computing (ICEC)
AU  - Benveniste, S.
AU  - Jouvelot, P.
AU  - Pin, B.
AU  - Péquignot, R.
SN  - 1875-9521
DO  - http://dx.doi.org/10.1016/j.entcom.2011.12.004
UR  - http://www.sciencedirect.com/science/article/pii/S1875952112000043
KW  - Alzheimer
KW  - Dementia
KW  - Music therapy
KW  - Wiimote
KW  - Renarcissization
AB  - MINWii, a new serious video game targeting Alzheimer and demented patients, is a simple music therapy tool usable by untrained care givers. Its goal is to improve patients’ self-image (renarcissization) to reduce behavioral symptoms, which are an important cause of institutionalization. With MINWii, elderly gamers use Wiimotes to improvise or play predefined songs on a virtual keyboard. We detail our design process, which addresses the specific features of dementia: this iterative refinement scheme, built upon qualitative, small scale experiments in a therapeutic environment, led to a shift of MINWii’s original focus from creativity to reminiscence. A large majority of our patients, with mild to moderate dementia, expressed a strong interest in our system, which was confirmed by feedback from the care givers. A controlled therapeutic study of MINWii is currently under way, which investigates its impact on behavior and quality of life in a hospital setting.
ER  - 

TY  - JOUR
T1  - Subtitled interaction: complementary support as an alternative to localization
JO  - International Journal of Human-Computer Studies
VL  - 59
IS  - 6
SP  - 941
EP  - 957
PY  - 2003/12//
T2  - 
AU  - Lepouras, Giorgos
AU  - Weir, George R.S.
SN  - 1071-5819
DO  - http://dx.doi.org/10.1016/j.ijhcs.2003.06.001
UR  - http://www.sciencedirect.com/science/article/pii/S1071581903001332
AB  - Many computer users face problems in their interaction as a result of the native language employed by the application. The language of the application is often at variance with the native language of its users. This issue is frequently addressed through localization. In turn, localization generates a range of new problems. We propose an alternative to localization that is analogous to cinematic subtitles. This has the potential to reduce the user interaction defects that otherwise arise with localization whilst benefiting users through an additional channel of information in their own language. This paper outlines a prototype implementation and describes our initial evaluation of this approach. We suggest that our complementary ‘subtitles’ promise consistent support for all applications in the user's computing environment and yield a system that is expandable and much easier to maintain than pre-localized software.
ER  - 

TY  - JOUR
T1  - Design and implementation of a fine-grained menu control processor for web-based information systems
JO  - Future Generation Computer Systems
VL  - 19
IS  - 7
SP  - 1105
EP  - 1119
PY  - 2003/10//
T2  - Selected papers on Theoretical and Computational Aspects of Structural Dynamical Systems in Linear Algebra and Control
AU  - Lu, Eric Jui-Lin
AU  - Chen, Rai-Fu
SN  - 0167-739X
DO  - http://dx.doi.org/10.1016/S0167-739X(03)00108-0
UR  - http://www.sciencedirect.com/science/article/pii/S0167739X03001080
KW  - WIS
KW  - Access control
KW  - XML
KW  - Fine-grained control
AB  - Web-based information systems (WISs) have become mainstream systems on the Internet and are widely deployed by enterprises worldwide. Although it is extremely important to secure access to WISs, the development of access control for WISs is still in its infancy stage. In addition, existing access control models for web applications are not suitable for WISs. In this paper, we proposed an access control model, called X-Menu, for WISs. Also, a prototype had been designed and implemented. The proposed model provides fine-grained control up to the element level of documents and is flexible and secured. The maintenance cost of the proposed model is low, and the proposed model can prevent users from performing any unauthorized task.
ER  - 

TY  - JOUR
T1  - The effect of temporal adaptation granularity and game genre on the time-balancing abilities of adaptive time-varying minigames
JO  - Entertainment Computing
VL  - 5
IS  - 1
SP  - 43
EP  - 54
PY  - 2014/1//
T2  - 
AU  - Tavassolian, Amin
AU  - Stanley, Kevin G.
AU  - Gutwin, Carl
SN  - 1875-9521
DO  - http://dx.doi.org/10.1016/j.entcom.2013.06.003
UR  - http://www.sciencedirect.com/science/article/pii/S1875952113000098
KW  - Game balance
KW  - Time balancing
KW  - Minigames
KW  - Adaptation
AB  - Abstract
Game designers spend a great deal of time developing well-balanced game experiences. However, differences in player ability, hardware capacity (e.g. network connections) or game mechanic constraints make it difficult to balance games for all players in all conditions. Adaptive balancing systems have been employed in an attempt to automatically compensate for these differences in real time as the game is being played. However, due to the complex non-linear mechanics underlying modern games, automated balancing systems can be highly unstable for all but the simplest mechanics, restricting the design space. In prior work we advanced the concept of using adaptive minigames deployed from within a larger game to decouple the adaptive mechanics from the main game mechanics. In particular, we looked at time-adaptive minigames (ATMs) which attempt to control the time to completion of a minigame. In this paper, we extend the ATM framework with additional time-adaptation algorithms and analyze the interaction between adaptive algorithm, game mechanic, and game difficulty in a controlled experiment. We find significant effects and interactions for all three factors, confirming our intuition that these processes are important and linked. We further find that finer temporal granularity leads to less-perceptible adaptation and smaller deviations in game completion times. This work provides an empirically-grounded algorithmic foundation for the design and practical deployment of ATMs in larger games, a foundation that can improve the balance and experience in these games.
ER  - 

TY  - JOUR
T1  - Falling in love with online games: The uses and gratifications perspective
JO  - Computers in Human Behavior
VL  - 26
IS  - 6
SP  - 1862
EP  - 1871
PY  - 2010/11//
T2  - Online Interactivity: Role of Technology in Behavior Change
AU  - Wu, Jen-Her
AU  - Wang, Shu-Ching
AU  - Tsai, Ho-Huang
SN  - 0747-5632
DO  - http://dx.doi.org/10.1016/j.chb.2010.07.033
UR  - http://www.sciencedirect.com/science/article/pii/S0747563210002293
KW  - Proactive stickiness
KW  - Uses and Gratifications theory
KW  - Service mechanisms
KW  - Presence
KW  - Continuance motivation
KW  - Online games
AB  - Playing online games is experience-oriented but few studies have explored the user’s initial (trial) reaction to game playing and how this further influences a player’s behavior. Drawing upon the Uses and Gratifications theory, we investigated players’ multiple gratifications for playing (i.e. achievement, enjoyment and social interaction) and their experience with the service mechanisms offered after they had played an online game. This study explores the important antecedents of players’ proactive “stickiness” to a specific online game and examines the relationships among these antecedents. The results show that both the gratifications and service mechanisms significantly affect a player’s continued motivation to play, which is crucial to a player’s proactive stickiness to an online game.
ER  - 

TY  - JOUR
T1  - Fault Detection in Multi-Threaded C++ Server Applications
JO  - Electronic Notes in Theoretical Computer Science
VL  - 174
IS  - 9
SP  - 5
EP  - 22
PY  - 2007/6/22/
T2  - Proceedings of the Thread Verification Workshop (TV 2006)
AU  - Mühlenfeld, Arndt
AU  - Wotawa, Franz
SN  - 1571-0661
DO  - http://dx.doi.org/10.1016/j.entcs.2007.04.004
UR  - http://www.sciencedirect.com/science/article/pii/S1571066107003568
KW  - data races
KW  - race conditions
KW  - debugging
KW  - parallel programs
KW  - synchronization
KW  - multi-threaded programming
KW  - object-oriented programming
KW  - static-dynamic co-analysis
AB  - Due to increasing demands in processing power on the one hand, but the physical limit on CPU clock speed on the other hand, multi-threaded programming is becoming more important in current applications. Unfortunately, multi-threaded programs are prone to programming mistakes that result in hard to find defects, mainly race-conditions and deadlocks. The need for tools that help finding these faults is immanent, but currently available tools are either difficult to use because of the need for annotations, unable to cope with more than a few 10 kLOC, or issue too many false warnings. This paper describes experiments with the freely available tool Helgrind and results obtained by using it for debugging a server application comprising 500 kLOC. We present improvements to the runtime analysis of C++ programs that result in a dramatic reduction of false warnings.
ER  - 

TY  - JOUR
T1  - Forensic feature extraction and cross-drive analysis
JO  - Digital Investigation
VL  - 3, Supplement
IS  - 
SP  - 71
EP  - 81
PY  - 2006/9//
T2  - The Proceedings of the 6th Annual Digital Forensic Research Workshop (DFRWS '06)
AU  - Garfinkel, Simson L.
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2006.06.007
UR  - http://www.sciencedirect.com/science/article/pii/S1742287606000697
KW  - Computer forensics
KW  - Forensic feature extraction
KW  - Cross-drive analysis
KW  - Data analysis
KW  - Information extraction
AB  - This paper introduces Forensic Feature Extraction (FFE) and Cross-Drive Analysis (CDA), two new approaches for analyzing large data sets of disk images and other forensic data. FFE uses a variety of lexigraphic techniques for extracting information from bulk data; CDA uses statistical techniques for correlating this information within a single disk image and across multiple disk images. An architecture for these techniques is presented that consists of five discrete steps: imaging, feature extraction, first-order cross-drive analysis, cross-drive correlation, and report generation. CDA was used to analyze 750 images of drives acquired on the secondary market; it automatically identified drives containing a high concentration of confidential financial records as well as clusters of drives that came from the same organization. FFE and CDA are promising techniques for prioritizing work and automatically identifying members of social networks under investigation. We believe it is likely to have other uses as well.
ER  - 

TY  - JOUR
T1  - MEMOIR — an open framework for enhanced navigation of distributed information
JO  - Information Processing & Management
VL  - 37
IS  - 1
SP  - 53
EP  - 74
PY  - 2001/1/1/
T2  - 
AU  - De Roure, D.
AU  - Hall, W.
AU  - Reich, S.
AU  - Hill, G.
AU  - Pikrakis, A.
AU  - Stairmand, M.
SN  - 0306-4573
DO  - http://dx.doi.org/10.1016/S0306-4573(00)00019-4
UR  - http://www.sciencedirect.com/science/article/pii/S0306457300000194
KW  - User trails
KW  - Open hypermedia framework
KW  - Software agents
KW  - Evaluation
KW  - Corporate memory
AB  - In large companies, whose business is critically dependent on the effectiveness of their R&amp;D function, the provision of effective means to access and share all forms of technical information is an acute problem. It is often easier to repeat an activity than it is to determine whether work has been carried out before.

In this paper we present experiences in implementing and evaluating the MEMOIR system. MEMOIR is an open framework, i.e., it is extensible and adaptable to an organization’s infrastructure and applications, and it provides its user interface via standard Web browsers. It uses trails, open hypermedia link services and a set of software agents to assist users in accessing and navigating vast amounts of information in Intranet environments. Additionally, MEMOIR exploits trail data to support users in finding colleagues with similar interests. The MEMOIR system has been installed and evaluated by two end-user organizations. This paper describes the results obtained in this evaluation.
ER  - 

TY  - JOUR
T1  - Web traffic characterization: an assessment of the impact of caching documents from NCSA's web server
JO  - Computer Networks and ISDN Systems
VL  - 28
IS  - 1–2
SP  - 37
EP  - 51
PY  - 1995/12//
T2  - Selected Papers from the Second World-Wide Web Conference
AU  - Braun, Hans-Werner
AU  - Claffy, Kimberly C.
SN  - 0169-7552
DO  - http://dx.doi.org/10.1016/0169-7552(95)00105-X
UR  - http://www.sciencedirect.com/science/article/pii/016975529500105X
KW  - Traffic analysis
KW  - Geographic distribution
KW  - Server workload
KW  - Caching
KW  - Accounting
AB  - We analyze two days of queries to the popular NCSA Mosaic server to assess the geographic distribution of transaction requests. The wide geographic diversity of query sources and popularity of a relatively small portion of the web server file set present a strong case for deployment of geographically distributed caching mechanisms to improve server and network efficiency.

The NCSA web server consists of four servers in a cluster. We show time series of bandwidth and transaction demands for the server cluster and break these demands down into components according to geographical source of the query. We analyze the impact of caching the results of queries within the geographic zone from which the request was sourced, in terms of reduction of transactions with and bandwidth volume from the main server. We find that a cache document timeout even as low as 1024 seconds (about 17 minutes) during the two days that we analyzed would have saved between 40% and 70% of the bytes transferred from the central server. We investigate a range of timeouts for flushing documents from the cache, outlining the tradeoff between bandwidth savings and memory/cache management costs. We discuss the implications of this tradeoff in the face of possible future usage-based pricing of backbone services that may connect several cache sites.

We also discuss other issues that caching inevitably poses, such as how to redirect queries initially destined for a central server to a preferred cache site. The preference of a cache site may be a function of not only geographic proximity, but also current load on nearby servers or network links. Such refinements in the web architecture will be essential to the stability of the network as the web continues to grow, and operational geographic analysis of queries to archive and library servers will be fundamental to its effective evolution.
ER  - 

TY  - JOUR
T1  - Spam, scams, chains, hoaxes and other junk mail
JO  - Computers & Security
VL  - 21
IS  - 7
SP  - 592
EP  - 606
PY  - 2002/11//
T2  - 
AU  - Hinde, Stephen
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/S0167-4048(02)01104-5
UR  - http://www.sciencedirect.com/science/article/pii/S0167404802011045
ER  - 

TY  - JOUR
T1  - AIDA's fourth-generation software functionality
JO  - Computer Methods and Programs in Biomedicine
VL  - 25
IS  - 3
SP  - 245
EP  - 258
PY  - 1987/11//
Y2  - 1987/12//
T2  - 
AU  - Fraken, Berend
AU  - Duisterhout, Joop S.
AU  - Witte, Frans S.C.
AU  - van Bemmel, Jan H.
SN  - 0169-2607
DO  - http://dx.doi.org/10.1016/0169-2607(87)90082-4
UR  - http://www.sciencedirect.com/science/article/pii/0169260787900824
KW  - DBMS
KW  - Information system
KW  - Fourth-generation software
KW  - MUMPS
AB  - Aspects characterizing typical, office-like environments are described and common procedures are extracted. These are used to derive the requirements of fourth-generation software packages. Typical fourth-generation software packages are discussed, and a presentation is given of a fourth-generation software package called AIDA, including all its functional aspects.
ER  - 

TY  - JOUR
T1  - International trade procedures
JO  - Computer Law & Security Review
VL  - 3
IS  - 6
SP  - 23
EP  - 29
PY  - 1988/3//
Y2  - 1988/4//
T2  - 
AU  - Bergsten, Eric
SN  - 0267-3649
DO  - http://dx.doi.org/10.1016/0267-3649(88)90132-X
UR  - http://www.sciencedirect.com/science/article/pii/026736498890132X
ER  - 

TY  - JOUR
T1  - Historical bits &amp; bytes
JO  - Computers & Security
VL  - 16
IS  - 5
SP  - 387
EP  - 411
PY  - 1997///
T2  - 
AU  - Highland, Harold Joseph
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/S0167-4048(97)82244-4
UR  - http://www.sciencedirect.com/science/article/pii/S0167404897822444
AB  - Over the next few pages we have included several section taken from Harold's Random Bits &amp; Bytes columns spanning the last three volumes of the journal. The material covers a range of security issues and contains detailed advice and information that is still very relevant today.
ER  - 

TY  - JOUR
T1  - A survey of intrusion detection in wireless network applications
JO  - Computer Communications
VL  - 42
IS  - 
SP  - 1
EP  - 23
PY  - 2014/4/1/
T2  - 
AU  - Mitchell, Robert
AU  - Chen, Ing-Ray
SN  - 0140-3664
DO  - http://dx.doi.org/10.1016/j.comcom.2014.01.012
UR  - http://www.sciencedirect.com/science/article/pii/S0140366414000280
KW  - Classification
KW  - Intrusion detection
KW  - Security
KW  - Wireless networks
AB  - Abstract
Information systems are becoming more integrated into our lives. As this integration deepens, the importance of securing these systems increases. Because of lower installation and maintenance costs, many of these systems are largely networked by wireless means. In order to identify gaps and propose research directions in wireless network intrusion detection research, we survey the literature of this area. Our approach is to classify existing contemporary wireless intrusion detection system (IDS) techniques based on target wireless network, detection technique, collection process, trust model and analysis technique. We summarize pros and cons of the same or different types of concerns and considerations for wireless intrusion detection with respect to specific attributes of target wireless networks including wireless local area networks (WLANs), wireless personal area networks (WPANs), wireless sensor networks (WSNs), ad hoc networks, mobile telephony, wireless mesh networks (WMNs) and cyber physical systems (CPSs). Next, we summarize the most and least studied wireless IDS techniques in the literature, identify research gaps, and analyze the rationale for the degree of their treatment. Finally, we identify worthy but little explored topics and provide suggestions for ways to conduct research.
ER  - 

TY  - JOUR
T1  - Analyzing multiple logs for forensic evidence
JO  - Digital Investigation
VL  - 4, Supplement
IS  - 
SP  - 82
EP  - 91
PY  - 2007/9//
T2  - 
AU  - Arasteh, Ali Reza
AU  - Debbabi, Mourad
AU  - Sakha, Assaad
AU  - Saleh, Mohamed
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2007.06.013
UR  - http://www.sciencedirect.com/science/article/pii/S1742287607000448
KW  - Forensic analysis
KW  - Log analysis
KW  - Formal methods
KW  - Model checking
KW  - Logging systems
KW  - Log correlation
AB  - Information stored in logs of a computer system is of crucial importance to gather forensic evidence of investigated actions or attacks. Analysis of this information should be rigorous and credible, hence it lends itself to formal methods. We propose a model checking approach to the formalization of the forensic analysis of logs. A set of logs is modeled as a tree whose labels are events extracted from the logs. In order to provide a structure to these events, we express each event as a term of algebra. The signature of the algebra is carefully chosen to include all relevant information necessary to conduct the analysis. Properties of the model, attack scenarios, and event sequences are expressed as formulas of a logic having dynamic, linear, temporal, and modal characteristics. Moreover, we provide a tableau-based proof system for this logic upon which a model checking algorithm can be developed. We use our model in a case study to demonstrate how events leading to an SYN attack can be reconstructed from a number of system logs.
ER  - 

TY  - JOUR
T1  - Evaluation of Simulation Games for Teaching Engineering and Manufacturing
JO  - Procedia Computer Science
VL  - 15
IS  - 
SP  - 210
EP  - 220
PY  - 2012///
T2  - 4th International Conference on Games and Virtual Worlds for Serious Applications(VS-GAMES’12)
AU  - Hauge, Jannicke Baalsrud
AU  - Riedel, Johann C.K.H.
SN  - 1877-0509
DO  - http://dx.doi.org/10.1016/j.procs.2012.10.073
UR  - http://www.sciencedirect.com/science/article/pii/S1877050912008356
KW  - Serious Games
KW  - Evaluation
KW  - Evaluation Methods
KW  - Engineering
KW  - Manufacturing.
AB  - This paper reports on the evaluation methods and findings from serious games for teaching engineering and manufacturing. Two serious games are considered: Cosiga, a new product development simulation game and Beware, a risk management simulation game. These two games cover the front and middle parts of the engineering process – from design to manufacture to sale. For the Cosiga simulation evaluations of the communication and cognitive change were performed. For the Beware game evaluation of communication, risk awareness and improvement of risk management skills were performed The findings from the evaluations showed that serious games deliver learning outcomes. However, there are drawbacks to their use that need to be taken into account. Principally the high cost of development and the need for expert facilitators for running game sessions.
ER  - 

TY  - JOUR
T1  - Effects of visual feedback on medical students’ procrastination within web-based planning and reflection protocols
JO  - Computers in Human Behavior
VL  - 41
IS  - 
SP  - 120
EP  - 136
PY  - 2014/12//
T2  - 
AU  - Wäschle, Kristin
AU  - Lachner, Andreas
AU  - Stucke, Björn
AU  - Rey, Sabine
AU  - Frömmel, Cornelius
AU  - Nückles, Matthias
SN  - 0747-5632
DO  - http://dx.doi.org/10.1016/j.chb.2014.09.022
UR  - http://www.sciencedirect.com/science/article/pii/S0747563214004749
KW  - Web-based learning protocols
KW  - Procrastination
KW  - Self-monitoring
KW  - Visual feedback
KW  - Self-regulated learning
AB  - Abstract
Procrastination is a very common problem among students that results from ineffective selfregulation. In two field-experimental studies (N = 18 and N = 49), we investigated whether visual feedback on students’ previous procrastination was effective in provoking a decrease in students’ future procrastination as well as improvements in self-regulated learning. The visual feedback was implemented as a dynamic line chart in a web-based planning and reflection protocol used once a week by medical students to record their class preparation and homework once a week. In the protocols, the students planned and reflected on their personal learning processes and they estimated retrospectively their inclination to procrastinate. The results of both studies consistently showed that presenting students a line chart that adaptively visualizes the course and extent of their self-reported previous procrastination led to a statistically significant and practically relevant decrease in their future procrastination. Furthermore, the visualization had positive effects on other variables central to self-regulated learning. The studies provide converging evidence that the inclination to procrastinate can successfully be counteracted both by a parsimonious and easy-to-implement method. They are suggestive of ways how Internet technology can be used support students’ self-regulated learning.
ER  - 

TY  - JOUR
T1  - VPNs: Only Part of the Remote Access Security Solution
JO  - Network Security
VL  - 2001
IS  - 1
SP  - 12
EP  - 14
PY  - 2001/1/1/
T2  - 
AU  - Brown, Arlene
SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/S1353-4858(01)00115-5
UR  - http://www.sciencedirect.com/science/article/pii/S1353485801001155
AB  - Today’s mobile workforce has found that having remote access to network resources and data can substantially increase productivity and is becoming increasingly popular. Employees can access information, network resources, and the Internet, regardless of their geographic location. Remote access offers workers cost savings on commuting and flexible work schedules, whilst enabling businesses to economize on valuable office space.
ER  - 

TY  - JOUR
T1  - Efficient audit service outsourcing for data integrity in clouds
JO  - Journal of Systems and Software
VL  - 85
IS  - 5
SP  - 1083
EP  - 1095
PY  - 2012/5//
T2  - 
AU  - Zhu, Yan
AU  - Hu, Hongxin
AU  - Ahn, Gail-Joon
AU  - Yau, Stephen S.
SN  - 0164-1212
DO  - http://dx.doi.org/10.1016/j.jss.2011.12.024
UR  - http://www.sciencedirect.com/science/article/pii/S0164121211003189
KW  - Security
KW  - Cloud storage
KW  - Interactive proof system
KW  - Provable data possession
KW  - Audit service
AB  - Cloud-based outsourced storage relieves the client's burden for storage management and maintenance by providing a comparably low-cost, scalable, location-independent platform. However, the fact that clients no longer have physical possession of data indicates that they are facing a potentially formidable risk for missing or corrupted data. To avoid the security risks, audit services are critical to ensure the integrity and availability of outsourced data and to achieve digital forensics and credibility on cloud computing. Provable data possession (PDP), which is a cryptographic technique for verifying the integrity of data without retrieving it at an untrusted server, can be used to realize audit services.

In this paper, profiting from the interactive zero-knowledge proof system, we address the construction of an interactive PDP protocol to prevent the fraudulence of prover (soundness property) and the leakage of verified data (zero-knowledge property). We prove that our construction holds these properties based on the computation Diffie–Hellman assumption and the rewindable black-box knowledge extractor. We also propose an efficient mechanism with respect to probabilistic queries and periodic verification to reduce the audit costs per verification and implement abnormal detection timely. In addition, we present an efficient method for selecting an optimal parameter value to minimize computational overheads of cloud audit services. Our experimental results demonstrate the effectiveness of our approach.
ER  - 

TY  - JOUR
T1  - A topology for secure MVS systems
JO  - Computers & Security
VL  - 3
IS  - 4
SP  - 278
EP  - 285
PY  - 1984/11//
T2  - 
AU  - Paans, R.
AU  - Herschberg, I.S.
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/0167-4048(84)90006-3
UR  - http://www.sciencedirect.com/science/article/pii/0167404884900063
KW  - MVS systems
KW  - security topology
KW  - separation of security levels
KW  - inter-system connections
KW  - security statements
KW  - shared DASD
AB  - Ultimate protection of computers against programming users appears unachievable. True security seems within reach only within systems without programming users. However, programming has to be done within each computing centre. To meet these conflicting ends, this paper proposes a means of isolating any enterprise's vital data from abuse by fully mutually isolating systems at three security levels from one another.

The approach proposed is already partly implemented in major computing centres, though with an effectiveness far from that required. Specifically, it is shown that shared DASD degrades the overall security level to that of the least secure system connected. A higher degree of security, as this paper suggests, is reachable in current systems by defining and implementing a three-level (minimal) topology as part of an overall security strategy.
ER  - 

TY  - JOUR
T1  - An experimental assessment of module documentation-based testing
JO  - Information and Software Technology
VL  - 53
IS  - 7
SP  - 747
EP  - 760
PY  - 2011/7//
T2  - 
AU  - Baharom, Salmi
AU  - Shukur, Zarina
SN  - 0950-5849
DO  - http://dx.doi.org/10.1016/j.infsof.2011.01.005
UR  - http://www.sciencedirect.com/science/article/pii/S0950584911000164
KW  - Specification-based testing
KW  - Grey-box testing
KW  - Automated module testing
KW  - Mutation-testing
AB  - Context
Testing a module that has memory using the black-box approach has been found to be expensive and relatively ineffective. Instead, testing without knowledge of the specifications (white-box approach) may not be effective in showing whether a program has been properly implemented as stated in its specifications. We propose instead a grey-box approach called Module Documentation-based Testing or MD-Test, the heart of which is an automatic generation of the test oracle from the external and internal views of the module.
Objective
This paper presents an empirical analysis and comparison of MD-Test against three existing testing tools.
Method
The experiment was conducted using a mutation-testing approach, in two phases that assess the capability of MD-Test in general and its capability of evaluating test results in particular.
Results
The results of the general assessment indicate that MD-Test is more effective than the other three tools under comparison, where it is able to detect all faults. The second phase of the experiment, which is significant to this study, compares the capabilities of MD-Test and JUnit-black using the test evaluation results. Likewise, an analysis of the test evaluation results shows that MD-Test is more effective and efficient, where MD-Test is able to detect at least the same number of faults as, or is at par with, the black-box approach.
Conclusion
It is concluded that test evaluation using grey-box approach is more effective and efficient that the black-box approach when testing a module that has memory.
ER  - 

TY  - JOUR
T1  - What are participants doing while filling in an online questionnaire: A paradata collection tool and an empirical study
JO  - Computers in Human Behavior
VL  - 26
IS  - 6
SP  - 1488
EP  - 1495
PY  - 2010/11//
T2  - Online Interactivity: Role of Technology in Behavior Change
AU  - Stieger, Stefan
AU  - Reips, Ulf-Dietrich
SN  - 0747-5632
DO  - http://dx.doi.org/10.1016/j.chb.2010.05.013
UR  - http://www.sciencedirect.com/science/article/pii/S0747563210001512
KW  - Response behavior
KW  - Response latency
KW  - Response visualization
KW  - Non-reactive data
KW  - Online questionnaire
KW  - Paradata
AB  - The use of online questionnaires is rapidly increasing. Contrary to manifold advantages, not much is known about user behavior that can be measured outside the boundaries set by standard web technologies like HTML form elements. To show how the lack of knowledge about the user setting in web studies can be accounted for, we present a tool called UserActionTracer, with which it is possible to collect more behavior information than with any other paradata gathering tool, in order to (1) gather additional data unobtrusively from the process of answering questions and (2) to visualize individual user behavior on web pages. In an empirical study on a large web sample (N = 1046) we observed and analysed online behaviors (e.g., clicking through). We found that only 10.5% of participants showed more than five single behaviors with highly negative influence on data quality in the whole online questionnaire (out of 132 possible single behavior judgments). Furthermore, results were validated by comparison with data from online address books. With the UserActionTracer it is possible to gain further insight into the process of answering online questionnaires.
ER  - 

TY  - JOUR
T1  - Application of growing hierarchical SOM for visualisation of network forensics traffic data
JO  - Neural Networks
VL  - 32
IS  - 
SP  - 275
EP  - 284
PY  - 2012/8//
T2  - Selected Papers from IJCNN 2011
AU  - Palomo, E.J.
AU  - North, J.
AU  - Elizondo, D.
AU  - Luque, R.M.
AU  - Watson, T.
SN  - 0893-6080
DO  - http://dx.doi.org/10.1016/j.neunet.2012.02.021
UR  - http://www.sciencedirect.com/science/article/pii/S0893608012000500
KW  - Network forensics
KW  - Hierarchical self-organisation
KW  - Data clustering
KW  - Data visualisation
KW  - Feature extraction
AB  - Digital investigation methods are becoming more and more important due to the proliferation of digital crimes and crimes involving digital evidence. Network forensics is a research area that gathers evidence by collecting and analysing network traffic data logs. This analysis can be a difficult process, especially because of the high variability of these attacks and large amount of data. Therefore, software tools that can help with these digital investigations are in great demand. In this paper, a novel approach to analysing and visualising network traffic data based on growing hierarchical self-organising maps (GHSOM) is presented. The self-organising map (SOM) has been shown to be successful for the analysis of highly-dimensional input data in data mining applications as well as for data visualisation in a more intuitive and understandable manner. However, the SOM has some problems related to its static topology and its inability to represent hierarchical relationships in the input data. The GHSOM tries to overcome these limitations by generating a hierarchical architecture that is automatically determined according to the input data and reflects the inherent hierarchical relationships among them. Moreover, the proposed GHSOM has been modified to correctly treat the qualitative features that are present in the traffic data in addition to the quantitative features. Experimental results show that this approach can be very useful for a better understanding of network traffic data, making it easier to search for evidence of attacks or anomalous behaviour in a network environment.
ER  - 

TY  - JOUR
T1  - Stateful relational database gateways for the World Wide Web
JO  - Journal of Systems and Software
VL  - 48
IS  - 3
SP  - 177
EP  - 187
PY  - 1999/11/1/
T2  - 
AU  - Hadjiefthymiades, Stathes
AU  - Martakos, Drakoulis
AU  - Petrou, Costas
SN  - 0164-1212
DO  - http://dx.doi.org/10.1016/S0164-1212(99)00056-4
UR  - http://www.sciencedirect.com/science/article/pii/S0164121299000564
KW  - WWW
KW  - RDBMS
KW  - Gateways
KW  - Session
KW  - Stateless HTTP
KW  - Stateful applications
KW  - Cookies
AB  - The World Wide Web is currently considered as the most appropriate software platform for the deployment of applications in wide area networks (telematics) as well as corporate intranets. Such applications are, in the majority of cases, tightly coupled with legacy databases hosted by relational management systems. However, the nature of the database enabled WWW systems is quite different from that of classical database applications developed with tools like 4GLs, Forms, Menus, etc. The basic difference lies in the stateless character of the WWW. The `user session' concept, encountered in conventional database applications, does not apply in the WWW environment. Instead, interaction with the information server is accomplished through a series of hits (request–response interactions) which are treated independently. This paper presents an architecture for the deployment of stateful database gateways for WWW servers. Although the server still treats each individual hit independently, state information maintained in the WWW browser as well as in specialized agents that operate behind the WWW server renders the WWW appropriate for a `session-aware' database application. The effort required to port an existing `session-aware' database application to the WWW environment is minimal.
ER  - 

TY  - JOUR
T1  - Problems of data protection law for private multinational communication networks
JO  - Computer Networks (1976)
VL  - 3
IS  - 3
SP  - 205
EP  - 214
PY  - 1979/6//
T2  - 
AU  - Rooms, Peter L.P
AU  - Dexter, John
SN  - 0376-5075
DO  - http://dx.doi.org/10.1016/0376-5075(79)90042-4
UR  - http://www.sciencedirect.com/science/article/pii/0376507579900424
KW  - Network design
KW  - distributed processing
KW  - data protection
KW  - encryption
KW  - radiation and induction
KW  - authentication and authorisation
KW  - physical security
KW  - security systems
AB  - The emergence of data protection legislation will place additional constraints on computer network managers, designers and operators. Networks may be centralised, decentralised, or distributed; choice is determined by degree of interaction and corporate control policy. Laws on crossborder data flow add additional factors into the design process; this may lead to major reassessments of processing strategies. However, traditional methods of network optimisation using computer programs may be capable of modification to cope.

To maintain security, communications will require to be protected to an appropriate degree. Bulk encryption, random data flow and super-encryption may be required; also electromagnetic radiation and induction should be avoided. Access to computer facilities should be controlled physically and by authentication, and due account will need to be taken of the commerically available hardware and software security safeguards. Management security measures should be formalised, and an overall balanced approach to maintaining privacy and security stands the best chance of success.
ER  - 

TY  - JOUR
T1  - Development of an e-Diagnostics/Maintenance framework for semiconductor factories with security considerations
JO  - Advanced Engineering Informatics
VL  - 17
IS  - 3–4
SP  - 165
EP  - 178
PY  - 2003/7//
Y2  - 2003/10//
T2  - Intelligent Maintenance Systems
AU  - Hung, Min-Hsiung
AU  - Chen, Kuan-Yii
AU  - Ho, Rui-Wen
AU  - Cheng, Fan-Tien
SN  - 1474-0346
DO  - http://dx.doi.org/10.1016/j.aei.2004.07.004
UR  - http://www.sciencedirect.com/science/article/pii/S1474034604000096
KW  - e-Diagnostics
KW  - e-Maintenance
KW  - Semiconductor Equipment
KW  - Web Services
KW  - Information integration framework
AB  - In recent years, the concept of e-Diagnostics and e-Maintenance is proposed in the semiconductor industry. By using Internet and information technologies, e-Diagnostics and e-Maintenance intend to provide equipment specialists with the remote capabilities of connectivity, manipulation, configuration, performance monitoring, and data collection and analysis on equipment to achieve the goal of promptly diagnosing, repairing, and maintaining equipment. In this paper, new-generation information software technologies and object-oriented technologies, such as Web Services, XML signature, XML encryption, UML, etc. are used to develop an e-Diagnostics/Maintenance framework with security considerations. The proposed framework can achieve the automaton of diagnostic processes and the integration of diagnostics and maintenance information under a secure communication infrastructure.
ER  - 

TY  - JOUR
T1  - Web search strategies: The influence of Web experience and task type
JO  - Information Processing & Management
VL  - 44
IS  - 3
SP  - 1308
EP  - 1329
PY  - 2008/5//
T2  - 
AU  - Thatcher, Andrew
SN  - 0306-4573
DO  - http://dx.doi.org/10.1016/j.ipm.2007.09.004
UR  - http://www.sciencedirect.com/science/article/pii/S0306457307001719
KW  - Web experience
KW  - Cognitive search strategies
KW  - Task type
KW  - Web experts
KW  - Web novices
KW  - Participant-defined tasks
AB  - Despite a number of studies looking at Web experience and Web searching tactics and behaviours, the specific relationships between experience and cognitive search strategies have not been widely researched. This study investigates how the cognitive search strategies of 80 participants might vary with Web experience as they engaged in two researcher-defined tasks and two participant-defined information seeking tasks. Each of the two researcher-defined tasks and participant-defined tasks included a directed search task and a general-purpose browsing task. While there were almost no significant performance differences between experience levels on any of the four tasks, there were significant differences in the use of cognitive search strategies. Participants with higher levels of Web experience were more likely to use “Parallel player”, “Parallel hub-and-spoke”, “Known address search domain” and “Known address” strategies, whereas participants with lower levels of Web experience were more likely to use “Virtual tourist”, “Link-dependent”, “To-the-point”, “Sequential player”, “Search engine narrowing”, and “Broad first” strategies. The patterns of use and differences between researcher-defined and participant-defined tasks and between directed search tasks and general-purpose browsing tasks are also discussed, although the distribution of search strategies by Web experience were not statistically significant for each individual task.
ER  - 

TY  - JOUR
T1  - E-commerce oriented software agents: Towards legal programming: a legal analysis of ecommerce and personal assistant agents using a process/IT view of the firm
JO  - Computer Law & Security Review
VL  - 19
IS  - 3
SP  - 201
EP  - 211
PY  - 2003/5//
T2  - 
AU  - Bain, Malcolm
AU  - Subirana, Brian
SN  - 0267-3649
DO  - http://dx.doi.org/10.1016/S0267-3649(03)00303-0
UR  - http://www.sciencedirect.com/science/article/pii/S0267364903003030
AB  - Agent-based technologies and processing may answer some of the legal difficulties raised by traditional online commerce, introducing elements of compliance, control, flexibility and personalisation. However as they mediate commercial relationships with third parties, software agents in turn raise new legal difficulties, while potentially heightening user fears and mistrust. The autonomy, adaptivity and interactivity of agents, combined with the advent of ubiquitous computing, introduce a new set of legal dimensions including the liability of agent users, the automation of notification and consent or the attribution of responsibility, as well as new fears for users. This article presents a process oriented analysis of agent activities, within the context of augmented reality: the application of Internet technologies to the real world, specifically in this case to supermarket shopping. Specific areas of difficulty are contract and consumer protection law, as well as privacy. These topics are highlighted, together with trust issues raised in Multi-Agent Systems which will be discussed in a later article in this series.
ER  - 

TY  - JOUR
T1  - User interface development for interactive television: extending a commercial DTV platform to the virtual channel API
JO  - Computers & Graphics
VL  - 28
IS  - 2
SP  - 157
EP  - 166
PY  - 2004/4//
T2  - 
AU  - Chorianopoulos, Konstantinos
AU  - Spinellis, Diomidis
SN  - 0097-8493
DO  - http://dx.doi.org/10.1016/j.cag.2003.12.004
UR  - http://www.sciencedirect.com/science/article/pii/S0097849303002590
KW  - Digital set-top box
KW  - User interface
KW  - Animated character
KW  - Music video clip
KW  - TiVo
AB  - We explore the generation of interactive computer graphics at digital set-top boxes in place of the fixed graphics that were embedded to the television video before the broadcast. This direction raises new requirements for user interface development, since the graphics are merged with video at each set-top box dynamically, without the traditional quality control from the television producers. Besides the technical issues, interactive computer graphics for television should be evaluated by television viewers. We employ an animated character in an interactive music television application that was evaluated by consumers, and was developed using the Virtual Channel Control Library, a custom high-level API, that was built using Microsoft Windows and TV technologies.
ER  - 

TY  - JOUR
T1  - Random bits &amp; bytes
JO  - Computers & Security
VL  - 13
IS  - 3
SP  - 192
EP  - 205
PY  - 1994/5//
T2  - 
AU  - Highland,, HaroldJoseph
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/0167-4048(94)90069-8
UR  - http://www.sciencedirect.com/science/article/pii/0167404894900698
ER  - 

TY  - JOUR
T1  - Random bits &amp; bytes
JO  - Computers & Security
VL  - 11
IS  - 1
SP  - 4
EP  - 15
PY  - 1992/3//
T2  - 
AU  - Highland, HaroldJoseph
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/0167-4048(92)90213-B
UR  - http://www.sciencedirect.com/science/article/pii/016740489290213B
ER  - 

TY  - JOUR
T1  - On social Web sites
JO  - Information Systems
VL  - 35
IS  - 2
SP  - 215
EP  - 236
PY  - 2010/4//
T2  - Special Section: Context-Oriented Information Integration
AU  - Kim, Won
AU  - Jeong, Ok-Ran
AU  - Lee, Sang-Won
SN  - 0306-4379
DO  - http://dx.doi.org/10.1016/j.is.2009.08.003
UR  - http://www.sciencedirect.com/science/article/pii/S0306437909000866
KW  - Social networking web sites
KW  - Social media web sites
AB  - Today hundreds of millions of Internet users are using thousands of social Web sites to stay connected with their friends, discover new “friends,” and to share user-created contents, such as photos, videos, social bookmarks, and blogs. There are so many social Web sites, and their features are evolving rapidly. There is controversy about the benefits of these sites, and there are social issues these sites have given rise to. There are lots of press articles, Wikipedia articles, and blogs—in varying degrees of authoritativeness, clarity and accuracy—about some of the social Web sites, uses of the sites, and some social problems, and business challenges faced by the sites. In this paper, we attempt to organize the status, uses, and issues of social Web sites into a comprehensive framework for discussing, understanding, using, building, and forecasting the future of social Web sites.
ER  - 

TY  - JOUR
T1  - Semantic scaffolds in hypermedia learning environments
JO  - Computers in Human Behavior
VL  - 25
IS  - 2
SP  - 371
EP  - 380
PY  - 2009/3//
T2  - Including the Special Issue: State of the Art Research into Cognitive Load Theory
AU  - Schnotz, Wolfgang
AU  - Heiß, Andrea
SN  - 0747-5632
DO  - http://dx.doi.org/10.1016/j.chb.2008.12.016
UR  - http://www.sciencedirect.com/science/article/pii/S0747563208002276
KW  - Hypermedia learning
KW  - Semantic scaffolds
KW  - Prior knowledge
AB  - In knowledge acquisition from hypermedia, learners have to orient themselves in a non-linear information space, navigate in this space and build a knowledge structure from the information there. Research on learning from hypermedia has focused primarily on enhancing orientation and navigation, with only minor attention on possibilities for supporting semantic processing. In a first experiment, 98 students from senior high school and university undergraduates learned about a complex subject matter either without or with semantic scaffolds, which were presented either in an obligatory (non-removable) or in an optional mode. High prior knowledge learners profited from adequately used semantic scaffolds, whereas low prior knowledge learners did not. In a second experiment, 53 senior high school students and university undergraduates received the learning material with semantic scaffolds presented either in an obligatory mode (but removable on demand) or in an optional mode. Learners with low prior knowledge performed better with optional presentation. Learners with high prior knowledge performed better with obligatory presentation, which allowed removing scaffolds on demand. Learners generally preferred the optional presentation of learning scaffolds.
ER  - 

TY  - JOUR
T1  - Routing centralization across domains via SDN: A model and emulation framework for BGP evolution
JO  - Computer Networks
VL  - 
IS  - 
SP  - 
EP  - 
PY  - 
T2  - 
AU  - Kotronis, Vasileios
AU  - Gämperli, Adrian
AU  - Dimitropoulos, Xenofontas
SN  - 1389-1286
DO  - http://dx.doi.org/10.1016/j.comnet.2015.07.015
UR  - http://www.sciencedirect.com/science/article/pii/S1389128615002480
KW  - SDN
KW  - Routing
KW  - BGP
KW  - Network emulation
KW  - Controllers
KW  - Convergence
AB  - Abstract
The Border Gateway Protocol (BGP) was designed almost three decades ago and has many limitations relating to its fully distributed nature, policy enforcement capabilities, scalability, security and complexity. For example, the control plane can take several minutes to converge after a routing change; this may be unacceptable for real-time network services. Despite many research proposals for incremental improvements and clean-slate redesigns of how inter-domain routing should work, BGP is likely one of the most ossified protocols of the Internet architecture and it has not retrofitted the proposed ideas. In this work, we propose a radical, incrementally deployable Internet routing paradigm in which the control plane of multiple networks is logically centralized. This follows the Software Defined Networking (SDN) paradigm, although at the inter-domain level involving multiple Autonomous Systems (AS). Multi-domain SDN centralization can be realized by outsourcing routing functions to an external contractor, which provides inter-domain routing services facilitated through a multi-AS network controller. The proposed model promises to become a vehicle for evolving BGP and uses the bird’s eye view over several networks to benefit aspects of inter-domain routing, such as convergence properties, policy conflict resolution, inter-domain troubleshooting, and collaborative security. In addition to the proposed paradigm, we introduce a publicly available emulation platform built on top of Mininet and the Quagga routing software, for experimenting on hybrid BGP–SDN AS-level networks. As a proof of concept, we focus specifically on exploiting multi-domain centralization to improve BGP’s slow convergence. We build and make publicly available a first multi-AS controller tailored to this use case and demonstrate experimentally that SDN centralization helps to linearly reduce BGP convergence times and churn rates with expanding SDN deployments.
ER  - 

TY  - JOUR
T1  - Multimedia, hypermedia, and hypertext: Motivation considered and reconsidered
JO  - Computers in Human Behavior
VL  - 26
IS  - 3
SP  - 265
EP  - 276
PY  - 2010/5//
T2  - 
AU  - Moos, Daniel C.
AU  - Marroquin, Elizabeth
SN  - 0747-5632
DO  - http://dx.doi.org/10.1016/j.chb.2009.11.004
UR  - http://www.sciencedirect.com/science/article/pii/S0747563209001757
KW  - Motivation
KW  - Multimedia
KW  - Hypermedia
KW  - Hypertext
KW  - Literature review
AB  - Computer-based instruction (CBI) is becoming increasingly popular in the classroom, particularly because the latest technological advancements allow for visually rich and interactive environments. While the inherent nature of CBIs is often thought to engage learners, research examining the role of motivation in learning with these environments has resulted in mixed findings. These findings are further complicated by unique design characteristics of distinct CBIs. This literature review synthesizes research that has examined the role of theoretically-grounded constructs of motivation in the context of three popular CBIs, multimedia, hypermedia, and hypertext. Specifically, this literature review considered empirical studies that examined the effect of these CBIs on motivation, in addition to the effect of motivation on learning outcomes and the learning process within the context of these environments. The literature review concludes with a theoretical consideration of previous research and a discussion of a framework for future directions.
ER  - 

TY  - JOUR
T1  - In search of reliable usage data on the WWW
JO  - Computer Networks and ISDN Systems
VL  - 29
IS  - 8–13
SP  - 1343
EP  - 1355
PY  - 1997/9//
T2  - Papers from the Sixth International World Wide Web Conference
AU  - Pitkow, James
SN  - 0169-7552
DO  - http://dx.doi.org/10.1016/S0169-7552(97)00021-4
UR  - http://www.sciencedirect.com/science/article/pii/S0169755297000214
AB  - The WWW is currently the hottest testbed for future interactive digital systems. While much is understood technically about how the WWW functions, substantially less is known about how this technology is used collectively and on an individual basis. This disparity of knowledge exists largely as a direct consequence of the decentralized nature of Web. Since each user of the Web is not uniquely identifiable across the system and the system employs various levels of caching, measurement of actual usage is problematic. This paper establishes terminology to frame the problem of reliably determining usage of WWW resources while reviewing current practice and their shortcomings. A review of the various metrics and analyses that can be performed to determine usage is then presented. This is followed by a discussion of the strengths and weaknesses of the hit-metering proposal [8] currently in consideration by the HTTP working group. Lastly, new proposals, based upon server-side sampling are introduced and assessed against the other proposal. It is argued that server-side sampling provides more reliable and useful usage data while requiring no change to the current HTTP protocol and enhancing user privacy.
ER  - 

TY  - JOUR
T1  - What makes mobile computer supported cooperative work mobile? Towards a better understanding of cooperative mobile interactions
JO  - International Journal of Human-Computer Studies
VL  - 60
IS  - 5–6
SP  - 737
EP  - 752
PY  - 2004/5//
T2  - HCI Issues in Mobile Computing
AU  - Schrott, Gregor
AU  - Glückler, Johannes
SN  - 1071-5819
DO  - http://dx.doi.org/10.1016/j.ijhcs.2003.11.006
UR  - http://www.sciencedirect.com/science/article/pii/S1071581903002155
AB  - Despite the high availability of mobile phones and personal digital assistants with online capabilities, mobile computer supported cooperative work is still in its infancy. So far, only little is known about the distinct attributes of mobile cooperative work in comparison to its stationary counterpart. Across which dimensions does cooperation via mobile devices differ from traditional hard-wired settings and what implications have to be drawn for future research? To bring more light to this question, we conducted an experimental business-case at Frankfurt University with 16 graduate students and analysed their collaborative behaviour across mobile and non-mobile channels of communication over a 5 week period. We find that mobile messages differed from stationary messages in terms of size and that the use of mobile emails prevailed over stationary emails under conditions of stress. Finally, we found that the social structure of mobile communication corresponded with the structure of stationary communication. This indicates that mobile communication technologies support existing communication relations rather than creating new relations. From the perspective of system designers, these results may serve as practical insights into the user behaviour of mobile technologies and might support the future development of mobile computer supported cooperative work environments.
ER  - 

TY  - JOUR
T1  - Persistent system architectures: a comparison
JO  - Microprocessors and Microsystems
VL  - 17
IS  - 3
SP  - 183
EP  - 192
PY  - 1993///
T2  - 
AU  - Reitenspieß, Manfred
SN  - 0141-9331
DO  - http://dx.doi.org/10.1016/0141-9331(93)90048-C
UR  - http://www.sciencedirect.com/science/article/pii/014193319390048C
KW  - persistent storage
KW  - security
KW  - distribution support
KW  - UNIX availability
AB  - Persistent storage, security, distribution support and high availability are basic properties in persistent system architectures. UNIXTM and BiiN systems support these properties in different ways. BiiN systems are based on a proprietary architecture, which was specifically designed to provide a secure, distributed, object oriented operating environment. On the other hand, UNIX systems went through many evolutionary steps and are now a de facto standard for open systems. Within this evolution, many important features have been added, which are also relevant from the persistent system architecture point of view. A comparison shows that the enhancements in UNIX provide a functionality similar to proprietary systems in addition to the widely required advantages of open system compatibility.
ER  - 

TY  - JOUR
T1  - Who holds the key to IT security?
JO  - Information Security Technical Report
VL  - 7
IS  - 4
SP  - 10
EP  - 22
PY  - 2002/12//
T2  - 
AU  - Flink, Yona
SN  - 1363-4127
DO  - http://dx.doi.org/10.1016/S1363-4127(02)00403-X
UR  - http://www.sciencedirect.com/science/article/pii/S136341270200403X
ER  - 

TY  - JOUR
T1  - The impact of the 1990s on EDP audit
JO  - Computer Audit Update
VL  - 1991
IS  - 1
SP  - 2
EP  - 11
PY  - 1991/1//
T2  - 
AU  - S. Oliphant, Alan
SN  - 0960-2593
DO  - http://dx.doi.org/10.1016/0960-2593(91)90074-J
UR  - http://www.sciencedirect.com/science/article/pii/096025939190074J
ER  - 

TY  - JOUR
T1  - Group awareness tools: It’s what you do with it that matters
JO  - Computers in Human Behavior
VL  - 27
IS  - 3
SP  - 1046
EP  - 1058
PY  - 2011/5//
T2  - Group Awareness in CSCL Environments
AU  - Janssen, Jeroen
AU  - Erkens, Gijsbert
AU  - Kirschner, Paul A.
SN  - 0747-5632
DO  - http://dx.doi.org/10.1016/j.chb.2010.06.002
UR  - http://www.sciencedirect.com/science/article/pii/S0747563210001743
KW  - Group awareness
KW  - Computer-supported collaborative learning
KW  - Tool use
KW  - Group performance
KW  - Coordination and regulation
AB  - This study examined the effect of using a group awareness tool on online collaboration. Furthermore, we examined whether the effect of using a group awareness tool on online collaboration is mediated by group awareness (i.e., students’ awareness of their group members’ levels of participation). To answer these questions, we determined how often and how long 107 secondary education students used the Participation-tool (PT), a group awareness tool designed to visualize group members’ relative contribution to the online collaborative process. Our analyses show that duration of PT use (how long students displayed the tool on their screens) significantly predicted group members’ participation in the online dialogue, their participation when writing collaborative texts, equality of participation within the group, and coordination and regulation of activities in the relational space (i.e., discussing the collaboration process with group members). No effect of using the PT on group performance was found. Mediation analyses showed that the effect of using the PT is only partially mediated by group awareness: an indirect effect of using the PT, via enhanced awareness of participation, on student participation during chat discussions and the collaborative writing process was found.
ER  - 

TY  - JOUR
T1  - Web navigation and the behavioral effects of constantly visible site maps
JO  - Interacting with Computers
VL  - 14
IS  - 5
SP  - 601
EP  - 618
PY  - 2002/10//
T2  - 
AU  - Danielson, David R
SN  - 0953-5438
DO  - http://dx.doi.org/10.1016/S0953-5438(02)00024-3
UR  - http://www.sciencedirect.com/science/article/pii/S0953543802000243
KW  - Navigation
KW  - Site map
KW  - Orientation
KW  - Usability
KW  - World Wide Web
KW  - Information seeking
AB  - Knowledge regarding how Web information-seekers behave with respect to the structures and cues they are provided with may shed light on general principles of navigation in electronic spaces, and assist designers in making more informed structural decisions. This study examines user movement through hierarchically structured Web sites and the behavioral effects of a constantly visible, textual contents list for relatively small sites or more extensive local views than are generally used on the Web today. The site overview resulted in users abandoning fewer information-seeking tasks. Users with such context dig deeper into the site structure, make less use of the browser's Back button, and frequently make navigational movements of great hierarchical distances. Navigational correlates of success and reported confidence for users with the overview differ from those without such context. Both with and without a constant overview, the relationship between the source and destination pages may help predict the amount of time spent at the destination. Experimental reports are preceded by a review of click-stream navigation behavior research.
ER  - 

TY  - JOUR
T1  - A survey on Advanced Metering Infrastructure
JO  - International Journal of Electrical Power & Energy Systems
VL  - 63
IS  - 
SP  - 473
EP  - 484
PY  - 2014/12//
T2  - 
AU  - Rashed Mohassel, Ramyar
AU  - Fung, Alan
AU  - Mohammadi, Farah
AU  - Raahemifar, Kaamran
SN  - 0142-0615
DO  - http://dx.doi.org/10.1016/j.ijepes.2014.06.025
UR  - http://www.sciencedirect.com/science/article/pii/S0142061514003743
KW  - Advanced Metering Infrastructure
KW  - Smart metering
KW  - Smart Grid
AB  - Abstract
This survey paper is an excerpt of a more comprehensive study on Smart Grid (SG) and the role of Advanced Metering Infrastructure (AMI) in SG. The survey was carried out as part of a feasibility study for creation of a Net-Zero community in a city in Ontario, Canada. SG is not a single technology; rather it is a combination of different areas of engineering, communication and management. This paper introduces AMI technology and its current status, as the foundation of SG, which is responsible for collecting all the data and information from loads and consumers. AMI is also responsible for implementing control signals and commands to perform necessary control actions as well as Demand Side Management (DSM). In this paper we introduce SG and its features, establish the relation between SG and AMI, explain the three main subsystems of AMI and discuss related security issues.
ER  - 

TY  - JOUR
T1  - Flashlight – Recording information acquisition online
JO  - Computers in Human Behavior
VL  - 27
IS  - 5
SP  - 1771
EP  - 1782
PY  - 2011/9//
T2  - 2009 Fifth International Conference on Intelligent ComputingICIC 20092009 Fifth International Conference on Intelligent Computing
AU  - Schulte-Mecklenbeck, Michael
AU  - Murphy, Ryan O.
AU  - Hutzler, Florian
SN  - 0747-5632
DO  - http://dx.doi.org/10.1016/j.chb.2011.03.004
UR  - http://www.sciencedirect.com/science/article/pii/S0747563211000628
KW  - Process tracing
KW  - Information acquisition
KW  - Online research
KW  - Decision making
KW  - Open source
AB  - A flashlight enables a person to see part of the world in the dark. As a person directs a flashlight beam to certain places in the environment, it serves as a manifestation of their attention, interest and focus. In this paper we introduce Flashlight, an open-source (free) web-based software package that can be used to collect continuous and non-obtrusive measures of users’ information acquisition behavior. Flashlight offers a cost effective and rapid way to collect data on how long and how often a participant reviews information in different areas of visual stimuli. It provides the functionality of other open source process tracing tools, like MouselabWeb, and adds the capability to present any static visual stimulus. We report the results from three different types of stimuli presented with both the Flashlight tool and a traditional eye-tracker. We found no differences measuring simple outcome data (e.g., choices in gambles or performance on algebraic tasks) between the two methods. However, due to the nature of the more complicated information acquisition, task completion takes longer with Flashlight than with an eye-tracking system. Other differences and commonalities between the two recording methods are reported and discussed. Additionally we provide detailed instructions on the installation and setup of Flashlight, the construction of stimuli, and the analysis of collected data.
ER  - 

TY  - JOUR
T1  - Automated Web issue analysis: A nurse prescribing case study
JO  - Information Processing & Management
VL  - 42
IS  - 6
SP  - 1471
EP  - 1483
PY  - 2006/12//
T2  - Special Issue on Informetrics
AU  - Thelwall, Mike
AU  - Thelwall, Saheeda
AU  - Fairclough, Ruth
SN  - 0306-4573
DO  - http://dx.doi.org/10.1016/j.ipm.2006.03.011
UR  - http://www.sciencedirect.com/science/article/pii/S030645730600032X
KW  - Web
KW  - Automated Web issue analysis
KW  - Link analysis
KW  - Nurse prescribing
KW  - Medical informatics
AB  - Web issue analysis, a new automated technique designed to rapidly give timely management intelligence about a topic from an automated large-scale analysis of relevant pages from the Web, is introduced and demonstrated. The technique includes hyperlink and URL analysis to identify common direct and indirect sources of Web information. In addition, text analysis through natural language processing techniques is used identify relevant common nouns and noun phrases. A case study approach is taken, applying Web issue analysis to the topic of nurse prescribing. The results are presented in descriptive form and a qualitative analysis is used to argue that new information has been found. The nurse prescribing results demonstrate interesting new findings, such as the parochial nature of the topic in the UK, an apparent absence of similar concepts internationally, at least in the English-speaking world, and a significant concern with mental health issues. These demonstrate that automated Web issue analysis is capable of quickly delivering new insights into a problem. General limitations are that the success of Web issue analysis is dependant upon the particular topic chosen and the ability to find a phrase that accurately captures the topic and is not used in other contexts, as well as being language-specific.
ER  - 

TY  - JOUR
T1  - A systematic approach to data security
JO  - Computers & Security
VL  - 1
IS  - 2
SP  - 99
EP  - 112
PY  - 1982/6//
T2  - 
AU  - Courtney Jr., Robert H.
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/0167-4048(82)90003-7
UR  - http://www.sciencedirect.com/science/article/pii/0167404882900037
KW  - Cost on control
KW  - computer crime
KW  - data security
KW  - privacy
KW  - data integrity
KW  - contingency planning
KW  - backup techniques
KW  - risk analysis
KW  - physical security controls
KW  - authorization
KW  - identification
KW  - systems integrity
AB  - The problem of selecting internal controls, or that subset of those controls we call security measures, in a data processing environment yields rather readily to an orderly, systematic approach. Such an approach requires recognition that a control should not be implemented if it costs more than tolerating the problem. Further, no control should be implemented which is more costly or less effective or displaces less potential loss than does some other control. The basic concept and outline of the systematic approach is described and references to supplemental papers for guidance in specific areas is provided through the bibliography.
ER  - 

TY  - JOUR
T1  - Enhancing usability testing through datamining techniques: A novel approach to detecting usability problem patterns for a context of use
JO  - Information and Software Technology
VL  - 50
IS  - 6
SP  - 547
EP  - 568
PY  - 2008/5//
T2  - 
AU  - González, María Paula
AU  - Lorés, Jesús
AU  - Granollers, Antoni
SN  - 0950-5849
DO  - http://dx.doi.org/10.1016/j.infsof.2007.06.001
UR  - http://www.sciencedirect.com/science/article/pii/S0950584907000614
KW  - Usability engineering
KW  - Qualitative usability testing
KW  - Context of use
KW  - Usability problem patterns
KW  - Datamining
KW  - Association rules
KW  - Decision trees
AB  - Usability is a software attribute usually associated with the “ease of use and to learn” of a given interactive system. Nowadays usability evaluation is becoming an important part of software development, providing results based on quantitative and qualitative estimations. In this context, qualitative results are usually obtained through a Qualitative Usability Testing process which includes a number of different methods focused on analyzing the interface of a particular interactive system. These methods become complex when a large number of interactive systems belonging to the same context of use have to be jointly considered to provide a general diagnosis, as a considerable amount of information must be visualized and treated simultaneously. However, diagnosing the most general usability problems of a context of use as a whole from a qualitative viewpoint is a challenge for UE nowadays. Identifying such problems can help to evaluate a new interface belonging to this context, and to prevent usability errors when a novel interactive system is being developed. From a quantitative viewpoint, condensing results in singles scores, metrics or statistical functions is an acceptable solution for processing huge amounts of usability related information. Nevertheless, QUT processes need to keep their richness by prioritizing the “what” over the “how much/how many” questions related to the detection of usability problems.

To cope with the above situation, this paper presents a new approach in which two datamining techniques (association rules and decision trees) are used to extend the existing Qualitative Usability Testing process in order to provide a general usability diagnosis of a given context of use from a qualitative viewpoint. In order to validate our proposal, usability problems patterns belonging to academic webpages in Spanish-speaking countries are assessed by processing 3450 records which store qualitative information collected by means of a Heuristic Evaluation.
ER  - 

TY  - JOUR
T1  - Technology evolution drives need for greater information technology security
JO  - Computers & Security
VL  - 24
IS  - 5
SP  - 359
EP  - 361
PY  - 2005/8//
T2  - 
AU  - LeVine, Richard
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/j.cose.2005.06.004
UR  - http://www.sciencedirect.com/science/article/pii/S0167404805000982
KW  - Enterprise rights management
KW  - Data security at the point of use
KW  - Content security
KW  - Perimeter security
KW  - Encryption
KW  - Identity management
AB  - Securing the enterprise perimeter is not enough; we must secure the data itself. This simple idea has taken decades to become a practical technology, and is built upon a foundation of policies and processes.
ER  - 

TY  - JOUR
T1  - Bridging the gap between organizational and user perspectives of security in the clinical domain
JO  - International Journal of Human-Computer Studies
VL  - 63
IS  - 1–2
SP  - 175
EP  - 202
PY  - 2005/7//
T2  - HCI research in privacy and security
AU  - Adams, Anne
AU  - Blandford, Ann
SN  - 1071-5819
DO  - http://dx.doi.org/10.1016/j.ijhcs.2005.04.022
UR  - http://www.sciencedirect.com/science/article/pii/S1071581905000728
KW  - Security
KW  - Privacy
KW  - Communities of practice
AB  - An understanding of ‘communities of practice’ can help to make sense of existing security and privacy issues within organizations; the same understanding can be used proactively to help bridge the gap between organizational and end-user perspectives on these matters. Findings from two studies within the health domain reveal contrasting perspectives on the ‘enemy within’ approach to organizational security. Ethnographic evaluations involving in-depth interviews, focus groups and observations with 93 participants (clinical staff, managers, library staff and IT department members) were conducted in two hospitals. All of the data was analysed using the social science methodology ‘grounded theory’. In one hospital, a community and user-centred approach to the development of an organizational privacy and security application produced a new communication medium that improved corporate awareness across the organization. User involvement in the development of this application increased the perceived importance, for the designers, of application usability, quality and aesthetics. However, other initiatives within this organization produced clashes with informal working practices and communities of practice. Within the second hospital, poor communication from IT about security mechanisms resulted in their misuse by some employees, who viewed them as a socially controlling force. Authentication mechanisms were used to socially exclude users who were formally authorized to access systems but whose access was unacceptable within some local communities of practice. The importance of users’ security awareness and control are reviewed within the context of communities of practice.
ER  - 

TY  - JOUR
T1  - User experiences with a MICROPAD
JO  - Journal of Microcomputer Applications
VL  - 7
IS  - 1
SP  - 19
EP  - 39
PY  - 1984/1//
T2  - 
AU  - Barker, P.G.
AU  - Najah, M.
AU  - Roper, J.S.
SN  - 0745-7138
DO  - http://dx.doi.org/10.1016/0745-7138(84)90085-X
UR  - http://www.sciencedirect.com/science/article/pii/074571388490085X
AB  - This paper examines the utility of a hand-print terminal (The MICROPAD) for implementing human interaction with a computer data base. Three issues are addressed: 
its usefulness for textual and numerical input;

the feasibility of its supporting line diagram images via ‘peck boxes’; and

its use within a multi-media workstation.
ER  - 

TY  - JOUR
T1  - Model of QoS Management in a Distributed Data Sharing and Archiving System
JO  - Procedia Computer Science
VL  - 18
IS  - 
SP  - 100
EP  - 109
PY  - 2013///
T2  - 2013 International Conference on Computational Science
AU  - Nikolow, Darin
AU  - S?ota, Renata
AU  - Polak, Stanis?aw
AU  - Mitera, Danilo
AU  - Pogoda, Marek
AU  - Winiarczyk, Pawe?
AU  - Kitowski, Jacek
SN  - 1877-0509
DO  - http://dx.doi.org/10.1016/j.procs.2013.05.173
UR  - http://www.sciencedirect.com/science/article/pii/S1877050913003165
KW  - Distributed storage systems
KW  - Service level agreement
KW  - Quality of service,
AB  - Abstract
The scientific applications of the fourth paradigm of science deal with large amounts of data stored in various storage devices or systems. Distributed storage systems are often chosen for storing these data. Some of the requirements posed to those storage systems may concern Quality of Service (QoS) aspects formally expressed in a Service Level Agreement. The QoS management in distributed storage systems is a challenging task given the possible storage device heterogeneity, the dynami- cally changing data access patterns, the client's concurrency and storage resource sharing. The problem becomes even more complicated when distributed computing environments with virtualized and shared resources like Clouds are considered. In this paper we present our research concerning the methods of storage performance management with respect of QoS in dis- tributed environments which has been done within the National Data Storage 2 (NDS2) project. A new model of storage QoS management is proposed and its implementation within NDS2 is described.
ER  - 

TY  - JOUR
T1  - Remote working: managing the balancing act between network access and data security
JO  - Computer Fraud & Security
VL  - 2009
IS  - 11
SP  - 14
EP  - 17
PY  - 2009/11//
T2  - 
AU  - Hart, Jason
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(09)70141-1
UR  - http://www.sciencedirect.com/science/article/pii/S1361372309701411
AB  - These days, it is generally accepted that economic, regulatory and social pressures are coming together to drive growth in remote working in businesses of all sizes and in all sectors. At the same time, organisations have recognised a parallel increase in hacking attacks and other forms of unauthorised access relating to corporate networks.
ER  - 

TY  - JOUR
T1  - A review on feature selection in mobile malware detection
JO  - Digital Investigation
VL  - 13
IS  - 
SP  - 22
EP  - 37
PY  - 2015/6//
T2  - 
AU  - Feizollah, Ali
AU  - Anuar, Nor Badrul
AU  - Salleh, Rosli
AU  - Wahab, Ainuddin Wahid Abdul
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2015.02.001
UR  - http://www.sciencedirect.com/science/article/pii/S1742287615000195
KW  - Mobile malware
KW  - Android
KW  - Feature selection
KW  - Review paper
KW  - Mobile operating system
AB  - Abstract
The widespread use of mobile devices in comparison to personal computers has led to a new era of information exchange. The purchase trends of personal computers have started decreasing whereas the shipment of mobile devices is increasing. In addition, the increasing power of mobile devices along with portability characteristics has attracted the attention of users. Not only are such devices popular among users, but they are favorite targets of attackers. The number of mobile malware is rapidly on the rise with malicious activities, such as stealing users data, sending premium messages and making phone call to premium numbers that users have no knowledge. Numerous studies have developed methods to thwart such attacks. In order to develop an effective detection system, we have to select a subset of features from hundreds of available features. In this paper, we studied 100 research works published between 2010 and 2014 with the perspective of feature selection in mobile malware detection. We categorize available features into four groups, namely, static features, dynamic features, hybrid features and applications metadata. Additionally, we discuss datasets used in the recent research studies as well as analyzing evaluation measures utilized.
ER  - 

TY  - JOUR
T1  - Some suggested reading
JO  - Computers & Security
VL  - 5
IS  - 2
SP  - 97
EP  - 
PY  - 1986/6//
T2  - 

SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/0167-4048(86)90128-8
UR  - http://www.sciencedirect.com/science/article/pii/0167404886901288
ER  - 

TY  - JOUR
T1  - The security phase of software development
JO  - Computer Fraud & Security Bulletin
VL  - 1992
IS  - 7
SP  - 12
EP  - 17
PY  - 1992/7//
T2  - 
AU  - Robertson, Bernard
SN  - 0142-0496
DO  - http://dx.doi.org/10.1016/0142-0496(92)90014-A
UR  - http://www.sciencedirect.com/science/article/pii/014204969290014A
ER  - 

TY  - JOUR
T1  - Load and traffic balancing in large scale cache meshes
JO  - Computer Networks and ISDN Systems
VL  - 30
IS  - 16–18
SP  - 1687
EP  - 1695
PY  - 1998/9/30/
T2  - 
AU  - Grimm, C
AU  - Vöckler, J.-S
AU  - Pralle, H
SN  - 0169-7552
DO  - http://dx.doi.org/10.1016/S0169-7552(98)00197-4
UR  - http://www.sciencedirect.com/science/article/pii/S0169755298001974
KW  - Cache hierarchy
KW  - Cache mesh
KW  - DFN
KW  - B-WiN
KW  - Squid
AB  - The current document summarizes the experiences obtained during the maintenance and operations of the cache hierarchy within the German broadband research network. Installed at central nodes, the cache service is an integral part of the backbone infrastructure. Ten distributed cache servers are the building blocks of a large scale top-level cache hierarchy. Since the beginning in January 1997, the DFN cache service was subjected to different mesh designs and conceptions. Various approaches were aimed at improvements in the load and traffic balance among the caches. At the same time, the benefit of the mesh as whole was to be increased, as well. Due to the fact that the caches are used in a production-like environment, few of the variations thought of manifested themselves in practical configurations. Maintainers of similar cache meshes may benefit from our ideas and experiences related in this document for their own conceptual design, configuration, and hardware selection. The reader might want to investigate further publications describing the design of single web caching systems [1–4].
ER  - 

TY  - JOUR
T1  - Network printing security – getting to grips with the multifunction device
JO  - Network Security
VL  - 2006
IS  - 2
SP  - 19
EP  - 20
PY  - 2006/2//
T2  - 
AU  - Cassidy, Darren
SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/S1353-4858(06)70338-5
UR  - http://www.sciencedirect.com/science/article/pii/S1353485806703385
AB  - Today most companies no longer use individual products for printing, copying, faxing and scanning. In the modern office environment, these tasks are handled by network-capable multifunction devices. These complex products help optimise work processes and reduce costs, but also present new challenges for IT administrators in terms of security. The article explores the threats of insecure use of multifunction devices, and offers tips for secure network printing.
ER  - 

TY  - JOUR
T1  - STARE-HI—Statement on reporting of evaluation studies in Health Informatics
JO  - International Journal of Medical Informatics
VL  - 78
IS  - 1
SP  - 1
EP  - 9
PY  - 2009/1//
T2  - 
AU  - Talmon, Jan
AU  - Ammenwerth, Elske
AU  - Brender, Jytte
AU  - de Keizer, Nicolette
AU  - Nykänen, Pirkko
AU  - Rigby, Michael
SN  - 1386-5056
DO  - http://dx.doi.org/10.1016/j.ijmedinf.2008.09.002
UR  - http://www.sciencedirect.com/science/article/pii/S1386505608001640
KW  - Medical Informatics
KW  - Guidelines
KW  - Publishing standards
KW  - Research design
AB  - Objective
Development of guidelines for publication of evaluation studies of Health Informatics applications.
Methods
An initial list of issues to be addressed in reports on evaluation studies was drafted based on experiences as editors and reviewers of journals in Health Informatics and as authors of systematic reviews of Health Informatics studies, taking into account guidelines for reporting of medical research. This list has been discussed in several rounds by an increasing number of experts in Health Informatics evaluation during conferences and by using e-mail and has been put up for comments on the web.
Results
A set of STARE-HI principles to be addressed in papers describing evaluations of Health Informatics interventions is presented. These principles include formulation of title and abstract, of introduction (e.g. scientific background, study objectives), study context (e.g. organizational setting, system details), methods (e.g. study design, outcome measures), results (e.g. study findings, unexpected observations) and discussion and conclusion of an IT evaluation paper.
Conclusion
A comprehensive list of principles relevant for properly describing Health Informatics evaluations has been developed. When manuscripts submitted to Health Informatics journals and general medical journals adhere to these aspects, readers will be better positioned to place the studies in a proper context and judge their validity and generalisability. It will also be possible to judge better whether papers will fit in the scope of meta-analyses of Health Informatics interventions. STARE-HI may also be used for study planning and hence positively influence the quality of evaluation studies in Health Informatics. We believe that better publication of both quantitative and qualitative evaluation studies is an important step toward the vision of evidence-based Health Informatics.
Limitations
This study is based on experiences from editors, reviewers, authors of systematic reviews and readers of the scientific literature. The applicability of the principles has not been evaluated in real practice. Only when authors start to use these principles for reporting, shortcomings in the principles will emerge.
ER  - 

TY  - JOUR
T1  - The holistic approach to security
JO  - Network Security
VL  - 2013
IS  - 3
SP  - 14
EP  - 17
PY  - 2013/3//
T2  - 
AU  - Bassill, Peter
SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/S1353-4858(13)70042-4
UR  - http://www.sciencedirect.com/science/article/pii/S1353485813700424
AB  - When evaluating the risk of a data or system security breach, it's essential that one addresses the consequences in a comprehensive way. One of the most significant threats is the devaluation of the business itself and the impact on the company's reputation. The data breach or technical compromise is actually only the mechanism through which the threat is realised.
ER  - 

TY  - JOUR
T1  - A combined approach to ensure data security in cloud computing
JO  - Journal of Network and Computer Applications
VL  - 35
IS  - 6
SP  - 1831
EP  - 1838
PY  - 2012/11//
T2  - 
AU  - Sood, Sandeep K.
SN  - 1084-8045
DO  - http://dx.doi.org/10.1016/j.jnca.2012.07.007
UR  - http://www.sciencedirect.com/science/article/pii/S1084804512001592
KW  - Cloud security
KW  - Encryption
KW  - Message authentication code
KW  - Virtualization
KW  - Secured socket layer
AB  - Cloud computing is a forthcoming revolution in information technology (IT) industry because of its performance, accessibility, low cost and many other luxuries. It is an approach to maximize the capacity or step up capabilities vigorously without investing in new infrastructure, nurturing new personnel or licensing new software. It provides gigantic storage for data and faster computing to customers over the internet. It essentially shifts the database and application software to the large data centers, i.e., cloud, where management of data and services may not be completely trustworthy. That is why companies are reluctant to deploy their business in the cloud even cloud computing offers a wide range of luxuries. Security of data in cloud is one of the major issues which acts as an obstacle in the implementation of cloud computing. In this paper, a frame work comprising of different techniques and specialized procedures is proposed that can efficiently protect the data from the beginning to the end, i.e., from the owner to the cloud and then to the user. We commence with the classification of data on the basis of three cryptographic parameters presented by the user, i.e., Confidentiality (C), Availability (A) and Integrity (I).The strategy followed to protect the data utilizes various measures such as the SSL (Secure Socket Layer) 128-bit encryption and can also be raised to 256-bit encryption if needed, MAC (Message Authentication Code) is used for integrity check of data, searchable encryption and division of data into three sections in cloud for storage. The division of data into three sections renders supplementary protection and simple access to the data. The user who wishes to access the data is required to provide the owner login identity and password, before admittance is given to the encrypted data in Section 1, Section 2, and Section 3.
ER  - 

TY  - JOUR
T1  - Catchsoft catches laptop thieves
JO  - Computer Fraud & Security
VL  - 1997
IS  - 3
SP  - 6
EP  - 
PY  - 1997/3//
T2  - 
AU  - Szweda, Roy
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(97)83573-7
UR  - http://www.sciencedirect.com/science/article/pii/S1361372397835737
ER  - 

TY  - JOUR
T1  - Free insurance with chip theft solution
JO  - Computer Fraud & Security
VL  - 1997
IS  - 3
SP  - 6
EP  - 
PY  - 1997/3//
T2  - 

SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(97)83571-3
UR  - http://www.sciencedirect.com/science/article/pii/S1361372397835713
ER  - 

TY  - JOUR
T1  - New product says security is a “snap”
JO  - Computer Fraud & Security
VL  - 1997
IS  - 3
SP  - 6
EP  - 
PY  - 1997/3//
T2  - 
AU  - Bee, Adrianne
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(97)83572-5
UR  - http://www.sciencedirect.com/science/article/pii/S1361372397835725
ER  - 

TY  - JOUR
T1  - Usability in the real world: assessing medical information technologies in patients’ homes
JO  - Journal of Biomedical Informatics
VL  - 36
IS  - 1–2
SP  - 45
EP  - 60
PY  - 2003/2//
Y2  - 2003/4//
T2  - Patient Safety
AU  - Kaufman, David R
AU  - Patel, Vimla L
AU  - Hilliman, Charlyn
AU  - Morin, Philip C
AU  - Pevzner, Jenia
AU  - Weinstock, Ruth S
AU  - Goland, Robin
AU  - Shea, Steven
AU  - Starren, Justin
SN  - 1532-0464
DO  - http://dx.doi.org/10.1016/S1532-0464(03)00056-X
UR  - http://www.sciencedirect.com/science/article/pii/S153204640300056X
KW  - Usability evaluation
KW  - Field usability testing
KW  - Cognitive methods
KW  - Telemedicine
KW  - Diabetes
KW  - Chronic illness
AB  - Objective: This paper presents an approach to usability evaluation of computer-based health care systems designed for patient use in their homes. Although such devices are becoming more prevalent, there is very little known about their usability.

Design: The theoretical foundations for the methods are discussed. The approach incorporates a cognitive walkthrough usability evaluation and new methods for usability testing that can be conducted in patient’s homes. The method was applied to the IDEATel intervention, a multi-institution randomized controlled trial of the feasibility, acceptability, and clinical utility of a home-based telemedicine system for diabetic Medicare population. The usability study was designed to assess barriers to optimal use of the system. The focus was both on dimensions of the interface and on dimensions of patient skills and competency. The usability field research involved testing 25 patients in their homes using the system. The analysis included a range of video-analytic methods of varying levels of granularity.

Results: The usability evaluation revealed aspects of the interface that were sub-optimal and impeded the performance of certain tasks. It also found a range of patient-related factors such as numeracy and psychomotor skills that constituted barriers to productive use.

Conclusions: A multifaceted usability approach provided important insight regarding use of technology by an elderly chronic-care patient population and more generally, for understanding how home health initiatives can more effectively use such technology.
ER  - 

TY  - JOUR
T1  - Guideline for good evaluation practice in health informatics (GEP-HI)
JO  - International Journal of Medical Informatics
VL  - 80
IS  - 12
SP  - 815
EP  - 827
PY  - 2011/12//
T2  - Designing for Healthy Living
AU  - Nykänen, Pirkko
AU  - Brender, Jytte
AU  - Talmon, Jan
AU  - de Keizer, Nicolette
AU  - Rigby, Michael
AU  - Beuscart-Zephir, Marie-Catherine
AU  - Ammenwerth, Elske
SN  - 1386-5056
DO  - http://dx.doi.org/10.1016/j.ijmedinf.2011.08.004
UR  - http://www.sciencedirect.com/science/article/pii/S1386505611001687
KW  - Health informatics
KW  - Evaluation
KW  - Guideline
KW  - Research design
AB  - Objective
Development of a good practice guideline to plan and perform scientifically robust evaluation studies in health informatics.
Methods
Issues to be addressed in evaluation studies were identified and guidance drafted based on the evaluation literature and on experiences by key players. Successive drafts of the guideline were discussed in several rounds by an increasing number of experts during conferences and by e-mail. At a fairly early point the guideline was put up for comments on the web.
Results
Sixty issues were identified that are of potential relevance for planning, implementation and execution of an evaluation study in the health informatics domain. These issues cover all phases of an evaluation study: Preliminary outline, study design, operationalization of methods, project planning, execution and completion of the evaluation study. Issues of risk management and project control as well as reporting and publication of the evaluation results are also addressed.
Conclusion
A comprehensive list of issues is presented as a guideline for good evaluation practice in health informatics (GEP-HI). The strengths and weaknesses of the guideline are discussed. Application of this guideline will support better handling of an evaluation study, potentially leading to a higher quality of evaluation studies. This guideline is an important step towards building stronger evidence and thus to progress towards evidence-based health informatics.
ER  - 

TY  - JOUR
T1  - A survey on security issues in service delivery models of cloud computing
JO  - Journal of Network and Computer Applications
VL  - 34
IS  - 1
SP  - 1
EP  - 11
PY  - 2011/1//
T2  - 
AU  - Subashini, S.
AU  - Kavitha, V.
SN  - 1084-8045
DO  - http://dx.doi.org/10.1016/j.jnca.2010.07.006
UR  - http://www.sciencedirect.com/science/article/pii/S1084804510001281
KW  - Cloud computing
KW  - Data privacy
KW  - Data protection
KW  - Security
KW  - Virtualization
AB  - Cloud computing is a way to increase the capacity or add capabilities dynamically without investing in new infrastructure, training new personnel, or licensing new software. It extends Information Technology’s (IT) existing capabilities. In the last few years, cloud computing has grown from being a promising business concept to one of the fast growing segments of the IT industry. But as more and more information on individuals and companies are placed in the cloud, concerns are beginning to grow about just how safe an environment it is. Despite of all the hype surrounding the cloud, enterprise customers are still reluctant to deploy their business in the cloud. Security is one of the major issues which reduces the growth of cloud computing and complications with data privacy and data protection continue to plague the market. The advent of an advanced model should not negotiate with the required functionalities and capabilities present in the current model. A new model targeting at improving features of an existing model must not risk or threaten other important features of the current model. The architecture of cloud poses such a threat to the security of the existing technologies when deployed in a cloud environment. Cloud service users need to be vigilant in understanding the risks of data breaches in this new environment. In this paper, a survey of the different security risks that pose a threat to the cloud is presented. This paper is a survey more specific to the different security issues that has emanated due to the nature of the service delivery models of a cloud computing system.
ER  - 

TY  - JOUR
T1  - Computational investigations of maximum flow algorithms
JO  - European Journal of Operational Research
VL  - 97
IS  - 3
SP  - 509
EP  - 542
PY  - 1997/3/16/
T2  - 
AU  - Ahuja, Ravindra K.
AU  - Kodialam, Murali
AU  - Mishra, Ajay K.
AU  - Orlin, James B.
SN  - 0377-2217
DO  - http://dx.doi.org/10.1016/S0377-2217(96)00269-X
UR  - http://www.sciencedirect.com/science/article/pii/S037722179600269X
AB  - The maximum flow algorithm is distinguished by the long line of successive contributions researchers have made in obtaining algorithms with incrementally better worst-case complexity. Some, but not all, of these theoretical improvements have produced improvements in practice. The purpose of this paper is to test some of the major algorithmic ideas developed in the recent years and to assess their utility on the empirical front. However, our study differs from previous studies in several ways. Whereas previous studies focus primarily on CPU time analysis, our analysis goes further and provides detailed insight into algorithmic behavior. It not only observes how algorithms behave but also tries to explain why algorithms behave that way. We have limited our study to the best previous maximum flow algorithms and some of the recent algorithms that are likely to be efficient in practice. Our study encompasses ten maximum flow algorithms and five classes of networks. The augmenting path algorithms tested by us include Dinic's algorithm, the shortest augmenting path algorithm, and the capacity-scaling algorithm. The preflow-push algorithms tested by us include Karzanov's algorithm, three implementations of Goldberg-Tarjan's algorithm, and three versions of Ahuja-Orlin-Tarjan's excess-scaling algorithms. Among many findings, our study concludes that the preflow-push algorithms are substantially faster than other classes of algorithms, and the highest-label preflow-push algorithm is the fastest maximum flow algorithm for which the growth rate in the computational time is O(n1.5) on four out of five of our problem classes. Further, in contrast to the results of the worst-case analysis of maximum flow algorithms, our study finds that the time to perform relabel operations (or constructing the layered networks) takes at least as much computation time as that taken by augmentations and/or pushes.
ER  - 

TY  - JOUR
T1  - Some practical advice
JO  - Computer Fraud & Security Bulletin
VL  - 1992
IS  - 7
SP  - 6
EP  - 12
PY  - 1992/7//
T2  - 

SN  - 0142-0496
DO  - http://dx.doi.org/10.1016/0142-0496(92)90013-9
UR  - http://www.sciencedirect.com/science/article/pii/0142049692900139
ER  - 

TY  - JOUR
T1  - Cryptanalysis and improvement of a password-based user authentication scheme for the integrated EPR information system
JO  - Journal of King Saud University - Computer and Information Sciences
VL  - 27
IS  - 2
SP  - 211
EP  - 221
PY  - 2015/4//
T2  - 
AU  - Islam, SK Hafizul
AU  - Biswas, G.P.
SN  - 1319-1578
DO  - http://dx.doi.org/10.1016/j.jksuci.2014.03.018
UR  - http://www.sciencedirect.com/science/article/pii/S1319157815000142
KW  - EPR information system
KW  - Two-factor user authentication
KW  - Password
KW  - Healthcare
KW  - Smartcard
KW  - Anonymity
AB  - Abstract
Recently, Wu et al. proposed a password-based remote user authentication scheme for the integrated Electronic Patient Record (EPR) information system to achieve mutual authentication and session key agreement over the Internet. They claimed that the scheme resists various attacks and offers lower computation cost, data integrity, confidentiality and authenticity. However, we observed that the scheme cannot withstand lost smartcard/off-line password guessing, privileged-insider and known session-specific temporary information attacks, and lacks the requirements of lost smartcard revocation and users’ anonymity. Besides, the password change phase is inconvenient to use because a user cannot change his password independently. Thus, we proposed a new password-based user authentication scheme for the integrated EPR information system that would be able to resist detected security flaws of Wu et al.’s scheme.
ER  - 

TY  - JOUR
T1  - Technology watch
JO  - Computers & Security
VL  - 5
IS  - 2
SP  - 94
EP  - 97
PY  - 1986/6//
T2  - 

SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/0167-4048(86)90127-6
UR  - http://www.sciencedirect.com/science/article/pii/0167404886901276
ER  - 

TY  - JOUR
T1  - Security and control issues in local area network design
JO  - Computers & Security
VL  - 8
IS  - 4
SP  - 283
EP  - 290
PY  - 1989/6//
T2  - 
AU  - Jamieson, Rodger
AU  - Low, Graham
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/0167-4048(89)90087-4
UR  - http://www.sciencedirect.com/science/article/pii/0167404889900874
KW  - Local area network (LAN)
KW  - Network risks
KW  - Control
KW  - Audit
KW  - Network planning
KW  - Network management
KW  - Network data and program access
KW  - Level of LAN service
KW  - Encryption
KW  - Network monitoring
AB  - This paper presents security and control issues and concerns associated with network design within a local area network (LAN) environment. Five key issues are identified for more detailed investigation: adequate network planning, sound network management, dangers from external LAN connections, access to data and programs, and level of LAN service. For each of these issues control and security measures are proposed as guidelines to counter these risks or concerns. To complement this issues approach, an alternative strategy based on LAN components is presented in the form of a summary table. Here network design is subdivided into several functional areas or components, together with the security and control measures providing protection for each component.

The identification and discussion of these issues, concurrently with suggested security measures, provides guidance to personnel involved in the design and review of LAN security during the LAN design phase. Information on LAN risks, exposures and protective measures will be valuable to information systems management when planning resource commitments for the security budget.
ER  - 

TY  - JOUR
T1  - User-generated contents and reasoning-based personalization: Ingredients for a novel model of mobile TV
JO  - Expert Systems with Applications
VL  - 38
IS  - 5
SP  - 5289
EP  - 5298
PY  - 2011/5//
T2  - 
AU  - Blanco-Fernández, Yolanda
AU  - López-Nores, Martín
AU  - Gil-Solla, Alberto
AU  - Ramos-Cabrer, Manuel
AU  - Pazos-Arias, José J.
SN  - 0957-4174
DO  - http://dx.doi.org/10.1016/j.eswa.2010.10.029
UR  - http://www.sciencedirect.com/science/article/pii/S0957417410011735
KW  - Personalization
KW  - Mobile TV
KW  - User-driven content generation
KW  - Web 2.0
AB  - During the last years, we have witnessed the boom of the digital market due to the proliferation of emergent audiovisual services and the increasing number of broadband networks. In this scenario, users insistently demand innovative services for exchanging and sharing their own audiovisual contents. In order to meet these needs, in this paper we propose a system that broadcasts user-generated audiovisual contents for handheld devices in a mobile network based on the DVB-H broadcasting standard. Besides, our system offers diverse added-value services to these new active users, such as: (i) multi modal access (via Web or by client applications running locally in handheld devices) to digital contents, (ii) exploitation of return channels to transmit interactive contents that enhance the user’s experience, and (iii) annotation, sharing and personalized distribution of audiovisual contents. To achieve these goals, our system adopts well-known technologies for broadcasting and semantic annotation of audiovisual contents, as well as emergent technology from the so-called Web 2.0. A prototype of our system has been experimentally evaluated with a group of students from the University of Vigo, who were enthusiastic about the personalization capabilities offered by our TV system for a mobile setting.
ER  - 

TY  - JOUR
T1  - On discrete stochastic congruences in communications: The one- and two-dimensional problems
JO  - Signal Processing
VL  - 11
IS  - 3
SP  - 237
EP  - 248
PY  - 1986/10//
T2  - 
AU  - Morgera, Salvatore D
SN  - 0165-1684
DO  - http://dx.doi.org/10.1016/0165-1684(86)90003-4
UR  - http://www.sciencedirect.com/science/article/pii/0165168486900034
KW  - Congruence
KW  - modulo
KW  - homomorphic mapping
KW  - quantization
KW  - likelihood ratio test
KW  - critical region
KW  - Bayesian error
KW  - digital communications
AB  - In this paper, a transformation is described which establishes residue classes on a discrete stochastic variable. We describe the transformation as a discrete stochastic congruence. The properties of congruences have been studied in number theory, but their behavior in a stochastic setting is not well known. The probability distribution associated with the range of the congruence is derived, with examples given when the domain is distributed as discretized Gaussian. A simple hypothesis test utilizing congruences is described with Bayesian error compared to the classical approach. A unique difference between the two is the manner in which the critical regions are defined. The problem studied has direct application to digital communications theory and numerous other areas.
ER  - 

TY  - JOUR
T1  - Using the Web as a survey tool: results from the second WWW user survey
JO  - Computer Networks and ISDN Systems
VL  - 27
IS  - 6
SP  - 809
EP  - 822
PY  - 1995/4//
T2  - Proceedings of the Third International World-Wide Web Conference
AU  - Pitkow, James E.
AU  - Recker, Margaret M.
SN  - 0169-7552
DO  - http://dx.doi.org/10.1016/0169-7552(95)00018-3
UR  - http://www.sciencedirect.com/science/article/pii/0169755295000183
KW  - New applications
KW  - Surveys
KW  - Demographics
KW  - Tools
AB  - This paper presents the initial results from the second World-Wide Web User Survey, which was advertised and made available to the Web user population for 38 days during October and November 1994. The survey is built on our architecture and Web technologies, which together offer a number of technical and surveying advantages. In particular, our architecture supports the use of adaptive questions, and supports methods for tracking users' responses across different surveys, allowing more in-depth analyses of survey responses. The present survey was composed of three question categories: general demographic questions, browsing usage, and questions for Web information authors. In addition, we added an additional, experimental category addressing users' attitudes toward commercial use of the Web and the Internet. In just over one month, we received over 18000 total responses to the combined surveys. To the best of our knowledge, the number of respondents and range of questions make this survey the most reliable and comprehensive characterization of WWW users to date. It will be interesting to see if and how the user trends shown in our results change as the Web gains in global access and popularity.
ER  - 

TY  - JOUR
T1  - The ‘phasing-in’ of security governance in the SDLC
JO  - Network Security
VL  - 2008
IS  - 12
SP  - 15
EP  - 17
PY  - 2008/12//
T2  - 
AU  - Danahy, Jack
SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/S1353-4858(08)70142-9
UR  - http://www.sciencedirect.com/science/article/pii/S1353485808701429
AB  - For too long security has been an afterthought in the software development lifecycle (SDLC) with critical security flaws being uncovered either just prior to deployment when they endanger promised release dates, or worse, after an application has been deployed. This is no longer acceptable. Security can be affordable and achievable if integrated properly at each key phase of the SDLC.
ER  - 

TY  - JOUR
T1  - White House unveils key escrow export criteria
JO  - Computer Fraud & Security Bulletin
VL  - 1995
IS  - 10
SP  - 8
EP  - 9
PY  - 1995/10//
T2  - 
AU  - Madsen, Wayne
SN  - 0142-0496
DO  - http://dx.doi.org/10.1016/0142-0496(95)80042-5
UR  - http://www.sciencedirect.com/science/article/pii/0142049695800425
ER  - 

TY  - JOUR
T1  - Managing the EDP audit and security functions
JO  - Computers & Security
VL  - 5
IS  - 3
SP  - 201
EP  - 206
PY  - 1986/9//
T2  - 
AU  - Beatson, John G.
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/0167-4048(86)90011-8
UR  - http://www.sciencedirect.com/science/article/pii/0167404886900118
KW  - Banking
KW  - Controls
KW  - EDP audit
KW  - Data security
KW  - Integrity
AB  - As data security and EDP audit assume greater importance in organizational matters, there will be an equally important need for the functions to be properly managed. Data security responsibilities should be delegated to a separate group outside the data processing department and, with EDP audit, must be adequately staffed to meet the requirements of the rapidly changing environment in which personnel are required to work.
ER  - 

TY  - JOUR
T1  - Data security and protection in cross-institutional electronic patient records
JO  - International Journal of Medical Informatics
VL  - 70
IS  - 2–3
SP  - 117
EP  - 130
PY  - 2003/7//
T2  - MIE 2002 Special Issue
AU  - van der Haak, M
AU  - Wolff, A.C
AU  - Brandner, R
AU  - Drings, P
AU  - Wannenmacher, M
AU  - Wetter, Th
SN  - 1386-5056
DO  - http://dx.doi.org/10.1016/S1386-5056(03)00033-9
UR  - http://www.sciencedirect.com/science/article/pii/S1386505603000339
KW  - Data security
KW  - Electronic patient record
KW  - Electronic signature
KW  - Internet
KW  - Shared care
AB  - This paper aims at identifying the specific legal requirements concerning data security and data protection of patient health data that apply to a cross-institutional electronic patient record (EPR) and describes possible solutions for meeting these requirements. In Germany, the legal framework for such records provide that disclosure of patient health information to physicians of third-party institutions is only allowed in case that it is necessary for the joint treatment of the patient, i.e. in case of a “treatment connection”. As a first step, the functionality of a remote-access architecture was proven allowing a one-way connection between the EPR systems of two health institutions in Germany, which jointly treat tumor patients. Besides, a signature system model for ensuring the integrity and authenticity of medical documents was developed and implemented in the existing information system architecture of the University Medical Center of Heidelberg. Especially in Germany, the legal framework for cross-institutional EPRs is very complex and has a considerable influence on the development and implementation of cross-institutional EPRs. However, its introduction is thought to be valuable, since a cross-institutional EPR will improve communication within shared care processes, and thus improve the quality of patient care.
ER  - 

TY  - JOUR
T1  - The dangers facing data on the move
JO  - Computer Fraud & Security
VL  - 2012
IS  - 12
SP  - 5
EP  - 10
PY  - 2012/12//
T2  - 
AU  - Caldwell, Tracey
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(12)70120-3
UR  - http://www.sciencedirect.com/science/article/pii/S1361372312701203
AB  - Data on the move is vulnerable, as the usual security measures may not apply. Yet 95% of organisations move data around at least once per year, and 44% move data more than five times per year.

Organisations are generating more and more data and are looking creatively at how to handle and store it. In addition, some organisations are moving at least some of their data to the cloud. And M&amp;A activity and departmental mergers are also driving data migration. Tracey Caldwell highlights the issues around migrating data securely.

Data on the move is vulnerable, as the usual security measures may not apply. Yet 95% of organisations move data around at least once a year, and 44% move data more than five times per year, according to recent research by Varonis.1
ER  - 

TY  - JOUR
T1  - Protecting PC systems
JO  - Computer Audit Update
VL  - 1992
IS  - 2
SP  - 5
EP  - 8
PY  - 1992/2//
T2  - 
AU  - Hiles, Andrew
SN  - 0960-2593
DO  - http://dx.doi.org/10.1016/0960-2593(92)90087-4
UR  - http://www.sciencedirect.com/science/article/pii/0960259392900874
ER  - 

TY  - JOUR
T1  - Stream cipher for binary image encryption using Ant Colony Optimization based key generation
JO  - Applied Soft Computing
VL  - 12
IS  - 9
SP  - 2879
EP  - 2895
PY  - 2012/9//
T2  - 
AU  - Sreelaja, N.K.
AU  - Vijayalakshmi Pai, G.A.
SN  - 1568-4946
DO  - http://dx.doi.org/10.1016/j.asoc.2012.04.002
UR  - http://www.sciencedirect.com/science/article/pii/S1568494612001640
KW  - Binary image encryption
KW  - Ant Colony Optimization
KW  - Stream cipher
AB  - Encryption of binary images is essential since it is vulnerable to eavesdropping in wired and wireless networks. The security of data becomes important since the communications over open network occur frequently. This paper focuses on encryption of binary image using a stream cipher method. In this paper we propose an Ant Colony Optimization (ACO) based approach of generating keys for encryption. The binary image is represented in a text form and encrypted using a stream cipher method. A novel technique termed Ant Colony Optimization Key Generation Binary Image Encryption (AKGBE) algorithm employs a character code table for encoding the keys and the plain text representing the binary image. The main advantage of this approach is that it reduces the number of keys to be stored and distributed. Experimental results demonstrating AKGBE's encrypting binary images of different sizes and the comparison of its performance with other stream cipher methods are presented.
ER  - 

TY  - JOUR
T1  - On the resource utilization and traffic distribution of multipath transmission control
JO  - Performance Evaluation
VL  - 68
IS  - 11
SP  - 1175
EP  - 1192
PY  - 2011/11//
T2  - Special Issue: Performance 2011
AU  - Jiang, Bo
AU  - Cai, Yan
AU  - Towsley, Don
SN  - 0166-5316
DO  - http://dx.doi.org/10.1016/j.peva.2011.07.013
UR  - http://www.sciencedirect.com/science/article/pii/S0166531611001040
KW  - Multipath TCP
KW  - Link utilization
KW  - Traffic distribution
KW  - Flappiness
AB  - There is growing interest in the development and deployment of multipath rate and route control mechanisms for the Internet, due to their ability to exploit bandwidth resources, alleviate network congestion, and provide robustness against failures. However, two performance issues have been uncovered: low link utilization when the number of flows is small, and route flappiness, namely the traffic of a flow tends to concentrate on one path and then another. In this paper we study these issues with respect to several variations of multipath rate and route control algorithms. We demonstrate the qualitatively different impacts that the couplings of the increase and decrease phases have on link utilization. We also demonstrate how the coupling strength affects both the long-term and short-term traffic distributions among different paths. In particular, we show that the flappy behavior is prominent only when there is strong coupling in both the increase and decrease phases, and when the number of good paths is small.
ER  - 

TY  - JOUR
T1  - Security reminders
JO  - Computers & Security
VL  - 5
IS  - 2
SP  - 97
EP  - 98
PY  - 1986/6//
T2  - 

SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/0167-4048(86)90129-X
UR  - http://www.sciencedirect.com/science/article/pii/016740488690129X
ER  - 

TY  - JOUR
T1  - Efficient attribute-based data sharing in mobile clouds
JO  - Pervasive and Mobile Computing
VL  - 
IS  - 
SP  - 
EP  - 
PY  - 
T2  - 
AU  - Zhang, Yinghui
AU  - Zheng, Dong
AU  - Chen, Xiaofeng
AU  - Li, Jin
AU  - Li, Hui
SN  - 1574-1192
DO  - http://dx.doi.org/10.1016/j.pmcj.2015.06.009
UR  - http://www.sciencedirect.com/science/article/pii/S1574119215001169
KW  - Attribute-based encryption
KW  - Data sharing
KW  - Constant cost
KW  - Mobile clouds
AB  - Abstract
Ciphertext-policy attribute-based encryption (CP-ABE) is extremely suitable for cloud computing environment in that it enables data owners to make and enforce access policies themselves. However, most of existing CP-ABE schemes suffer severe efficiency drawbacks due to large ciphertext size and computation cost, and hence are not suitable for mobile clouds, where users are usually resource-limited. In this paper, we first present a generic attribute-based data sharing system based on a hybrid mechanism of CP-ABE and a symmetric encryption scheme. Then, we propose a CP-ABE scheme which features constant computation cost and constant-size ciphertexts. The proposed CP-ABE scheme is proven selective-secure in the random oracle model under the decision n -BDHE assumption, where n represents the total number of attributes in universe. It can efficiently support AND-gate access policies with multiple attribute values and wildcards. Theoretical analysis and experimental results indicate that the proposed scheme is extremely suitable for data sharing in mobile clouds.
ER  - 

TY  - JOUR
T1  - The control of computer-based fraud
JO  - Computers & Security
VL  - 1
IS  - 2
SP  - 123
EP  - 138
PY  - 1982/6//
T2  - 
AU  - Carroll, John M.
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/0167-4048(82)90005-0
UR  - http://www.sciencedirect.com/science/article/pii/0167404882900050
KW  - Computer fraud
KW  - passwords
KW  - systems integrity
KW  - data accuracy
KW  - contingency planning
KW  - hardware controls
KW  - authorization
KW  - environmental controls
KW  - audit trails
KW  - program controls
AB  - Computer crime, the glamor crime of the 1970's, will become in the 1980's one of the greatest sources of preventable business loss. As businessmen entrust more and more of their assets to computer systems, eventually all business crime will become computer crime. Hard times in a permissive society will prompt greater numbers of knowledgeable, but unprincipled, persons to steal from their employers. Meanwhile, a hasty and largely unplanned plunge into computer networking, electronic funds transfer, and distributed data processing has greatly exacerbated existing security exposures in computer systems. Against this backdrop, the forces of law and order can rely only on inadequate legislation and largely untrained personnel to stem a potential hemorrhage of business profits. Control of computer-based fraud is first and foremost a do-it-yourself task. This article surveys the ways to go about it.
ER  - 

TY  - JOUR
T1  - The Malaysian Telehealth Flagship Application: a national approach to health data protection and utilisation and consumer rights
JO  - International Journal of Medical Informatics
VL  - 73
IS  - 3
SP  - 217
EP  - 227
PY  - 2004/3/31/
T2  - Realizing Security into the Electronic Health Record
AU  - Mohan, Jai
AU  - Razali Raja Yaacob, Raja
SN  - 1386-5056
DO  - http://dx.doi.org/10.1016/j.ijmedinf.2003.11.023
UR  - http://www.sciencedirect.com/science/article/pii/S1386505603001837
KW  - Malaysia
KW  - Telemedicine
KW  - Data protection
KW  - Confidentiality
AB  - Telehealth refers to the integration of information, telecommunication, human–machine interface technologies and health technologies to deliver health care, to promote the heath status of the people and to create health.

The Malaysian Telehealth Application [Proceedings of the Ninth World Congress on Medical Informatics, 1998, p. 1282] will, on completion, provide every resident of the country an electronic Lifetime Health Record (LHR) and Lifetime Health Plan (LHP). He or she will also hold a smart card that will contain a subset of the data in the Lifetime Health Record. These will be the means by which Malaysians will receive “seamless continuous quality care” across a range of health facilities and health care providers, and by which Malaysia’s health goal of a nation of “healthy individuals, families and communities” is achieved. The challenges to security and privacy in providing access to an electronic Lifetime Health Record at private and government health facilities and to the electronic Lifetime Health Plan at homes of consumers require not only technical mechanisms but also national policies and practices addressing threats while facilitating access to health data during health encounters in different care settings. Organisational policies establish the goals that technical mechanisms serve. They should outline appropriate uses and access to information, create mechanisms for preventing and detecting violations, and set sanctions for violations. Some interesting innovations have been used to address these issues against the background of the launching of the multimedia supercorridor (MSC) in Malaysia.
ER  - 

TY  - JOUR
T1  - A scalable and dynamic application-level secure communication framework for inter-cloud services
JO  - Future Generation Computer Systems
VL  - 48
IS  - 
SP  - 19
EP  - 27
PY  - 2015/7//
T2  - Special Section: Business and Industry Specific Cloud
AU  - Sajjad, Ali
AU  - Rajarajan, Muttukrishnan
AU  - Zisman, Andrea
AU  - Dimitrakos, Theo
SN  - 0167-739X
DO  - http://dx.doi.org/10.1016/j.future.2015.01.018
UR  - http://www.sciencedirect.com/science/article/pii/S0167739X15000357
KW  - Cloud computing
KW  - Secure communication
KW  - Virtual private networks
AB  - Abstract
Most of the current cloud computing platforms offer Infrastructure as a Service (IaaS) model, which aims to provision basic virtualized computing resources as on-demand and dynamic services. Nevertheless, a single cloud does not have limitless resources to offer to its users, hence the notion of an Inter-Cloud environment where a cloud can use the infrastructure resources of other clouds. However, there is no common framework in existence that allows the service owners to seamlessly provision even some basic services across multiple cloud service providers, albeit not due to any inherent incompatibility or proprietary nature of the foundation technologies on which these cloud platforms is built. In this paper we present a novel solution which aims to cover a gap in a subsection of this problem domain. Our solution offers a security architecture that enables service owners to provision a dynamic and service-oriented secure virtual private network on top of multiple cloud IaaS providers. It does this by leveraging the scalability, robustness and flexibility of peer-to-peer overlay techniques to eliminate the manual configuration, key management and peer churn problems encountered in setting up the secure communication channels dynamically, between different components of a typical service that is deployed on multiple clouds. We present the implementation details of our solution as well as experimental results carried out on two commercial clouds.
ER  - 

TY  - JOUR
T1  - A report from the United States — part II
JO  - Computer Law & Security Review
VL  - 13
IS  - 2
SP  - 87
EP  - 95
PY  - 1997/3//
Y2  - 1997/4//
T2  - 
AU  - Bigelow, Robert P.
SN  - 0267-3649
DO  - http://dx.doi.org/10.1016/S0267-3649(97)89742-7
UR  - http://www.sciencedirect.com/science/article/pii/S0267364997897427
AB  - Summary
This article has outlined some of the legal considerations with which data security officers and other management personnel should be familiar: a checklist at the end of the article covers some of the points discussed. Particularly important from the company's point of view is the need to examine legal obligations and contractual commitments undertaken by the company as to security of data processing systems programs and databases.

There are many situations that management must analyze from a security viewpoint. For example:•
When acquiring software from a new vendor, review the licence agreement for clauses with specific requirements about security.
•
When licence agreements with previous software vendors are amended, review the amendments to see if there are any changes in the security requirements.
•
When the company licenses others to use its software, review to see whether there are any security requirements imposed on the licensee that the company as licensor should supervise.
•
Consult with the company's tax accountants to be sure that security precautions required by the IRS are being followed.
•
Review security procedures to protect the company in situations where there is labour unrest or layoffs are planned.
•
If the company has undertaken government contracts that involve records on people, review security procedures to be sure that they comply with privacy acts.
•
Consult with the personnel department to be sure that data processing security of personnel records is appropriate.
•
Compare security techniques used by the company with those promulgated by the National Institute of Standards and Technology; if there are differences, document the reason for not following the federal standard.
•
Review contracts between the company and governmental agencies to determine whether they include any specific data security requirements.


While following the foregoing outline will not guarantee that the company — or management — will escape from legal liability, compliance with the various suggestions will make such liability much less likely.58
ER  - 

TY  - JOUR
T1  - Abstracts of recent articles and literature
JO  - Computers & Security
VL  - 1
IS  - 2
SP  - 193
EP  - 199
PY  - 1982/6//
T2  - 

SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/0167-4048(82)90015-3
UR  - http://www.sciencedirect.com/science/article/pii/0167404882900153
ER  - 

TY  - JOUR
T1  - Treasury management systems — security system overview
JO  - Computer Audit Update
VL  - 1991
IS  - 4
SP  - 2
EP  - 12
PY  - 1991/4//
T2  - 
AU  - Bhaskar, Krish
AU  - Stamper, David
SN  - 0960-2593
DO  - http://dx.doi.org/10.1016/0960-2593(91)90002-Q
UR  - http://www.sciencedirect.com/science/article/pii/096025939190002Q
AB  - This is the second of two articles on the security of Treasury Management Systems, the first of which appeared in the February issue of Computer Audit Update. The articles are based on a case study carried out as part of the MARS project, further details of which are available from the author. The first article dealt with the analysis of the system and the second with the development of the security system.
ER  - 

TY  - JOUR
T1  - Tutorial on telecommunications and security
JO  - Computers & Security
VL  - 3
IS  - 3
SP  - 215
EP  - 224
PY  - 1984/8//
T2  - 
AU  - Nestman, Chadwick H.
AU  - Windsor, John C.
AU  - Hinson, Mary C.
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/0167-4048(84)90042-7
UR  - http://www.sciencedirect.com/science/article/pii/0167404884900427
KW  - Security
KW  - telecommunications
KW  - authorization
KW  - authentication
KW  - encryption
AB  - Telecommunications, although around for many years, is finally beginning to get the attention it deserves. There are more applications utilizing telecommunication principles than ever before and many of these applications are facing security problems much like those faced by batch applications of a few years ago. Although this paper is not a definitive work about telecommunication security, it is a tutorial which will help “jog” the minds of the reader to remember key elements and components of telecommunication systems. By remembering these elements and components, system designers will be better able to utilize existing security functions to make current and planned telecommunication systems more secure. This paper describes general security issues in telecommunications and then uses those generalities to discuss two specific areas of concern: access and authorization.
ER  - 

TY  - JOUR
T1  - Analyzing best practices on Web development frameworks: The lift approach
JO  - Science of Computer Programming
VL  - 102
IS  - 
SP  - 1
EP  - 19
PY  - 2015/5/1/
T2  - 
AU  - Salas-Zárate, María del Pilar
AU  - Alor-Hernández, Giner
AU  - Valencia-García, Rafael
AU  - Rodríguez-Mazahua, Lisbeth
AU  - Rodríguez-González, Alejandro
AU  - López Cuadrado, José Luis
SN  - 0167-6423
DO  - http://dx.doi.org/10.1016/j.scico.2014.12.004
UR  - http://www.sciencedirect.com/science/article/pii/S0167642314005735
KW  - Best practices
KW  - Lift
KW  - Scala
KW  - Web frameworks
AB  - Abstract
Choosing the Web framework that best fits the requirements is not an easy task for developers. Several frameworks now exist to develop Web applications, such as Struts, JSF, Ruby on Rails, Grails, CakePHP, Django, and Catalyst. However, Lift is a relatively new framework that emerged in 2007 for the Scala programming language and which promises a great number of advantages and additional features. Companies such as Siemens©?and IBM®, as well as social networks such as Twitter® and Foursquare®, have now begun to develop their applications by using Scala and Lift. Best practices are activities, technical or important issues identified by users in a specific context, and which have rendered excellent service and are expected to achieve similar results in similar situations. Each framework has its own best practices whose aim is to facilitate the development of Web applications. However, there is no current comparative analysis that identifies the best practices for Web frameworks. Thus, as its salient contribution, this paper identifies a set of best practices for Web frameworks. Afterwards, these best practices were analyzed and discussed in terms of developing Lift-based Web applications. The identification of these best practices would allow developers to construct more interactive and efficient Lift-based Web applications, integrating features of Web 2.0 technologies with less effort and exploiting the frameworks' benefits. In addition, this paper contains a comparative analysis with Web frameworks such as JSF, Struts, CakePHP, Ruby on Rails, Lift, Django, and Catalyst. Finally, as proof of concept, a set of Lift-based Web applications were developed for this paper by applying best practices such as actors, lazy loading, Comet support, SiteMap, Wiring, HyperText Markup Language, version 5 (HTML5) support, and parallel rendering.
ER  - 

TY  - JOUR
T1  - Computer fraud &amp; abuse survey 1990
JO  - Computer Audit Update
VL  - 1991
IS  - 7
SP  - 3
EP  - 10
PY  - 1991/7//
T2  - 
AU  - Hurford, Chris
SN  - 0960-2593
DO  - http://dx.doi.org/10.1016/0960-2593(91)90007-V
UR  - http://www.sciencedirect.com/science/article/pii/096025939190007V
ER  - 

TY  - JOUR
T1  - Abstracts of recent articles and literature
JO  - Computers & Security
VL  - 2
IS  - 2
SP  - 194
EP  - 205
PY  - 1983/6//
T2  - 

SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/0167-4048(83)90059-7
UR  - http://www.sciencedirect.com/science/article/pii/0167404883900597
ER  - 

TY  - JOUR
T1  - A web-based approach for electrocardiogram monitoring in the home
JO  - International Journal of Medical Informatics
VL  - 54
IS  - 2
SP  - 145
EP  - 153
PY  - 1999/5//
T2  - 
AU  - Magrabi, Farah
AU  - Lovell, Nigel H.
AU  - Celler, Branko G.
SN  - 1386-5056
DO  - http://dx.doi.org/10.1016/S1386-5056(98)00177-4
UR  - http://www.sciencedirect.com/science/article/pii/S1386505698001774
KW  - Telemedicine
KW  - Home monitoring
KW  - Web
KW  - Internet
KW  - Electrocardiogram
AB  - A Web-based electrocardiogram (ECG) monitoring service in which a longitudinal clinical record is used for management of patients, is described. The Web application is used to collect clinical data from the patient's home. A database on the server acts as a central repository where this clinical information is stored. A Web browser provides access to the patient's records and ECG data. We discuss the technologies used to automate the retrieval and storage of clinical data from a patient database, and the recording and reviewing of clinical measurement data. On the client's Web browser, ActiveX controls embedded in the Web pages provide a link between the various components including the Web server, Web page, the specialised client side ECG review and acquisition software, and the local file system. The ActiveX controls also implement FTP functions to retrieve and submit clinical data to and from the server. An intelligent software agent on the server is activated whenever new ECG data is sent from the home. The agent compares historical data with newly acquired data. Using this method, an optimum patient care strategy can be evaluated, a summarised report along with reminders and suggestions for action is sent to the doctor and patient by email.
ER  - 

TY  - JOUR
T1  - A review on remote data auditing in single cloud server: Taxonomy and open issues
JO  - Journal of Network and Computer Applications
VL  - 43
IS  - 
SP  - 121
EP  - 141
PY  - 2014/8//
T2  - 
AU  - Sookhak, Mehdi
AU  - Talebian, Hamid
AU  - Ahmed, Ejaz
AU  - Gani, Abdullah
AU  - Khan, Muhammad Khurram
SN  - 1084-8045
DO  - http://dx.doi.org/10.1016/j.jnca.2014.04.011
UR  - http://www.sciencedirect.com/science/article/pii/S1084804514000940
KW  - Cloud computing
KW  - Remote data auditing
KW  - Provable data possession
KW  - Proof of retrievability
KW  - Proof of ownership
AB  - Abstract
Cloud computing has emerged as a computational paradigm and an alternative to the conventional computing with the aim of providing reliable, resilient infrastructure, and with high quality of services for cloud users in both academic and business environments. However, the outsourced data in the cloud and the computation results are not always trustworthy because of the lack of physical possession and control over the data for data owners as a result of using to virtualization, replication and migration techniques. Since that the security protection the threats to outsourced data have become a very challenging and potentially formidable task in cloud computing, many researchers have focused on ameliorating this problem and enabling public auditability for cloud data storage security using remote data auditing (RDA) techniques. This paper presents a comprehensive survey on the remote data storage auditing in single cloud server domain and presents taxonomy of RDA approaches. The objective of this paper is to highlight issues and challenges to current RDA protocols in the cloud and the mobile cloud computing. We discuss the thematic taxonomy of RDA based on significant parameters such as security requirements, security metrics, security level, auditing mode, and update mode. The state-of-the-art RDA approaches that have not received much coverage in the literature are also critically analyzed and classified into three groups of provable data possession, proof of retrievability, and proof of ownership to present a taxonomy. It also investigates similarities and differences in such framework and discusses open research issues as the future directions in RDA research.
ER  - 

TY  - JOUR
T1  - Managing microcomputers and end-user computing: Some critical issues
JO  - Telematics and Informatics
VL  - 2
IS  - 2
SP  - 133
EP  - 140
PY  - 1985///
T2  - 
AU  - Sisson, Roger L.
SN  - 0736-5853
DO  - http://dx.doi.org/10.1016/S0736-5853(85)80006-X
UR  - http://www.sciencedirect.com/science/article/pii/S073658538580006X
AB  - The end-user computing is typically a non-data processing professional who accesses and performs analyses on data to support decision making. End-user computing is a prevalent wave in today's society. Software is being developed that is easy to use and does not require one to have a programming background. This paper examines managing microcomputers and end-user computing.
ER  - 

TY  - JOUR
T1  - Cloud computing: A new business paradigm for biomedical information sharing
JO  - Journal of Biomedical Informatics
VL  - 43
IS  - 2
SP  - 342
EP  - 353
PY  - 2010/4//
T2  - 
AU  - Rosenthal, Arnon
AU  - Mork, Peter
AU  - Li, Maya Hao
AU  - Stanford, Jean
AU  - Koester, David
AU  - Reynolds, Patti
SN  - 1532-0464
DO  - http://dx.doi.org/10.1016/j.jbi.2009.08.014
UR  - http://www.sciencedirect.com/science/article/pii/S1532046409001154
KW  - Cloud computing
KW  - Data sharing
KW  - Bioinformatics
KW  - Security
KW  - Distributed computing
KW  - Cost-benefit analysis
AB  - We examine how the biomedical informatics (BMI) community, especially consortia that share data and applications, can take advantage of a new resource called “cloud computing”. Clouds generally offer resources on demand. In most clouds, charges are pay per use, based on large farms of inexpensive, dedicated servers, sometimes supporting parallel computing. Substantial economies of scale potentially yield costs much lower than dedicated laboratory systems or even institutional data centers. Overall, even with conservative assumptions, for applications that are not I/O intensive and do not demand a fully mature environment, the numbers suggested that clouds can sometimes provide major improvements, and should be seriously considered for BMI. Methodologically, it was very advantageous to formulate analyses in terms of component technologies; focusing on these specifics enabled us to bypass the cacophony of alternative definitions (e.g., exactly what does a cloud include) and to analyze alternatives that employ some of the component technologies (e.g., an institution’s data center). Relative analyses were another great simplifier. Rather than listing the absolute strengths and weaknesses of cloud-based systems (e.g., for security or data preservation), we focus on the changes from a particular starting point, e.g., individual lab systems. We often find a rough parity (in principle), but one needs to examine individual acquisitions—is a loosely managed lab moving to a well managed cloud, or a tightly managed hospital data center moving to a poorly safeguarded cloud?
ER  - 

TY  - JOUR
T1  - Looking at clouds from both sides: The advantages and disadvantages of placing personal narratives in the cloud
JO  - Information Security Technical Report
VL  - 16
IS  - 3–4
SP  - 115
EP  - 122
PY  - 2011/8//
Y2  - 2011/11//
T2  - Cloud Security
AU  - Coles-Kemp, Lizzie
AU  - Reddington, Joseph
AU  - Williams, Patricia A.H.
SN  - 1363-4127
DO  - http://dx.doi.org/10.1016/j.istr.2011.09.001
UR  - http://www.sciencedirect.com/science/article/pii/S1363412711000525
KW  - Cloud
KW  - Healthcare
KW  - Disability
KW  - AAC
KW  - VOCA
AB  - This article explores the nature of cloud computing in the context of processing sensitive personal data as part of a personal narrative. In so doing, it identifies general security concerns about cloud computing and presents examples of cloud technologies used to process such data. The use of personal narratives in electronic patient records and in voice output communication aids is compared and contrasted and the implications of the advent of cloud computing for these two scenarios are considered.
ER  - 

TY  - JOUR
T1  - A perception-based model for EDI adoption in small businesses using a technology–organization–environment framework
JO  - Information & Management
VL  - 38
IS  - 8
SP  - 507
EP  - 521
PY  - 2001/10//
T2  - 
AU  - Kuan, Kevin K.Y.
AU  - Chau, Patrick Y.K.
SN  - 0378-7206
DO  - http://dx.doi.org/10.1016/S0378-7206(01)00073-8
UR  - http://www.sciencedirect.com/science/article/pii/S0378720601000738
KW  - Electronic data interchange (EDI)
KW  - Technology adoption
KW  - Small business
KW  - Perception-based model
AB  - The wide adoption of electronic data interchange (EDI) has been argued to be important for the success of the technology. Past studies on EDI have focused mainly on large firms, as they were the major users at the time. With the advance of technology, however, EDI applications that used to require mainframe computers can be used on PCs at a much lower cost. At a result, small businesses are now able to enjoy the benefits of EDI. Using a technology–organization–environment framework, this study proposes a perception-based small business EDI adoption model that is tested against data collected from 575 small firms in Hong Kong. Six factors are tested using logistic regression and five are found to be significant in distinguishing adopter firms from non-adopter firms. The results suggest the perception-based model using a technology–organization–environment framework is a useful approach for examining factors affecting the adoption decision. For small businesses, while direct benefits are perceived to be higher by adopter firms than by non-adopter firms, indirect benefits are not perceived differently by either adopter firms or non-adopter firms, contrary to the findings in studies on large business. In addition, adopter firms perceive lower financial costs and higher technical competence than non-adopter firms do. Also, adopter firms perceive higher government pressure but lower industry pressure than non-adopter firms do. Implications of the findings and future research areas are discussed.
ER  - 

TY  - JOUR
T1  - Collaborative use of individual search histories
JO  - Interacting with Computers
VL  - 20
IS  - 1
SP  - 184
EP  - 198
PY  - 2008/1//
T2  - 
AU  - Komlodi, Anita
AU  - Lutters, Wayne G.
SN  - 0953-5438
DO  - http://dx.doi.org/10.1016/j.intcom.2007.10.003
UR  - http://www.sciencedirect.com/science/article/pii/S0953543807000811
KW  - Search histories
KW  - Interaction histories
KW  - Awareness
KW  - Coordination
KW  - HCI
KW  - CSCW
AB  - Interaction history tools record interactions between users and systems, allowing users to annotate, edit, and replay their activities. Search history tools, a class of interaction history recorders, preserve search, browse, and other information-seeking steps. These tools include web browser histories and history lists in online full-text databases. Although search history tools were developed to support individuals in their information seeking, individuals often share their histories with one another collaboratively. This paper examines such sharing behaviors in two field studies of knowledge workers who routinely shared their individual search histories with their colleagues. While this practice is widespread, it is not supported by the design of contemporary interaction history tools. The results of the field research highlight core dimensions of this activity and inform considerations for the next generation of collaboration-sensitive interaction history tools.
ER  - 

TY  - JOUR
T1  - Security and integrity controls for microcomputers: A summary analysis
JO  - Information & Management
VL  - 13
IS  - 1
SP  - 33
EP  - 41
PY  - 1987/8//
T2  - 
AU  - Boockholdt, J.L.
SN  - 0378-7206
DO  - http://dx.doi.org/10.1016/0378-7206(87)90028-0
UR  - http://www.sciencedirect.com/science/article/pii/0378720687900280
KW  - Data security
KW  - Data integrity
KW  - Microcomputers
KW  - Personal computers
KW  - Internal controls
AB  - This article addresses the impact of microcomputers on information system integrity and security and suggests modifications to system controls appropriate to both standalone and networked microcomputers. Common business applications are identified and associated threats described. Appropriate control measures are discussed, including ways of creating an environment for microcomputing and methods of achieving data integrity and security.
ER  - 

TY  - JOUR
T1  - The Jigsaw secure distributed file system
JO  - Computers & Electrical Engineering
VL  - 39
IS  - 4
SP  - 1142
EP  - 1152
PY  - 2013/5//
T2  - 
AU  - Bian, Jiang
AU  - Seker, Remzi
SN  - 0045-7906
DO  - http://dx.doi.org/10.1016/j.compeleceng.2013.01.018
UR  - http://www.sciencedirect.com/science/article/pii/S0045790613000244
AB  - Abstract
The Jigsaw Distributed File System (JigDFS) aims to securely store and retrieve files on large scale networks. The design of JigDFS is driven by the privacy needs of its users. Files in JigDFS are sliced into small segments using an Information Dispersal Algorithm (IDA) and distributed onto different nodes recursively. JigDFS provides fault-tolerance against node failures while assuring confidentiality, integrity, and availability of the stored data. Layered encryption is applied to each file segment with keys produced by a hashed-key chain algorithm. Recursive IDA and layered encryption enhance users’ anonymity and provide a degree of plausible deniability. JigDFS is envisioned to be an ideal long-term storage solution for developing secure data archiving systems.
ER  - 

TY  - JOUR
T1  - Applying means-end chain theory to eliciting system requirements and understanding users perceptual orientations
JO  - Information & Management
VL  - 42
IS  - 3
SP  - 455
EP  - 468
PY  - 2005/3//
T2  - 
AU  - Chiu, Chao-Min
SN  - 0378-7206
DO  - http://dx.doi.org/10.1016/j.im.2004.02.002
UR  - http://www.sciencedirect.com/science/article/pii/S0378720604000461
KW  - Means-end chain
KW  - Perceptual orientations
KW  - System requirements
KW  - Technology acceptance model
AB  - This paper discusses how the use of a means-end approach in eliciting user requirements for a system results in a better understanding of the user’s perceptual orientation toward the Web-based document management system under design. Our findings imply that, from an overall perceptive, respondents are most concerned with the sense of being comfortable with the system, while respondents are more aware of the need for a security mechanism. However, none sought a sense of accomplishment or of belonging, self-respect, self-fulfillment, excitement, fun or enjoyment of the system. I provide a model that fuses the attribute–consequence–value (A–C–V) model and technology acceptance model (TAM). This model posits that factors at the consequence level lead to other factors at the value level, which in turn lead to behavioral intention to use the system. The model explains how attribute, consequence, and value factors ultimately lead to system use.
ER  - 

TY  - JOUR
T1  - Confining data and processes in global computing applications
JO  - Science of Computer Programming
VL  - 63
IS  - 1
SP  - 57
EP  - 87
PY  - 2006/11//
T2  - Special issue on security issues in coordination models, languages, and systems
AU  - De Nicola, Rocco
AU  - Gorla, Daniele
AU  - Pugliese, Rosario
SN  - 0167-6423
DO  - http://dx.doi.org/10.1016/j.scico.2005.07.013
UR  - http://www.sciencedirect.com/science/article/pii/S0167642306001018
KW  - Global computing
KW  - Formal methods
KW  - Type systems
KW  - Data secrecy
AB  - A programming notation is introduced that can be used for protecting secrecy and integrity of data in global computing applications. The approach is based on the explicit annotations of data and network nodes. Data are tagged with information about the allowed movements, network nodes are tagged with information about the nodes that can send data and spawn processes to them. The annotations are used to confine movements of data and processes. The approach is illustrated by applying it to three paradigmatic calculi for global computing, namely cKlaim (a calculus at the basis of cKlaim), D ? (a distributed version of the ? -calculus) and Mobile Ambients Calculus. For all of these formalisms, it is shown that their semantics guarantees that computations proceed only while respecting confinement constraints. Namely, it is proven that, after successful static type checking, data can reside at and cross only authorised nodes. “Local” formulations of this property where only relevant subnets type check are also presented. Finally, the theory is tested by using it to model secure behaviours of a UNIX-like multiuser system.
ER  - 

TY  - JOUR
T1  - A management information system for a Chinese university
JO  - Information & Management
VL  - 24
IS  - 5
SP  - 283
EP  - 288
PY  - 1993///
T2  - 
AU  - Qirui, Ying
AU  - Mingxue, Zhu
AU  - Bailey, Therold E.
SN  - 0378-7206
DO  - http://dx.doi.org/10.1016/0378-7206(93)90005-E
UR  - http://www.sciencedirect.com/science/article/pii/037872069390005E
KW  - China
KW  - Data base management systems
KW  - Decision support systems
KW  - Developing countries
KW  - Information management
KW  - Management information systems
KW  - People's Republic of China
KW  - University administration
AB  - This paper describes a comprehensive Management Information System (MIS) designed and developed at Shenzhen University, People's Republic of China, using off-the-shelf hardware and software. The data base provides information on all aspects of the university, including, but not limited to, operations, maintenance, programs, enrollments, students, faculty, and financial records. Management of all aspects of the university depends heavily on the MIS. The MIS has had enormous administrative consequences and has potential for expansion.
ER  - 

TY  - JOUR
T1  - Recent developments in cryptographic hash functions: Security implications and future directions
JO  - Information Security Technical Report
VL  - 11
IS  - 2
SP  - 100
EP  - 107
PY  - 2006///
T2  - 
AU  - Cid, Carlos
SN  - 1363-4127
DO  - http://dx.doi.org/10.1016/j.istr.2006.03.007
UR  - http://www.sciencedirect.com/science/article/pii/S1363412706000203
AB  - One of the most important classes of cryptographic algorithms in current use is the class of cryptographic hash functions. Hash functions are ubiquitous in today's IT systems and have a wide range of applications in security protocols and schemes, such as providing software integrity, digital signatures, message authentication and password protection. Among their many security requirements, cryptographic hash function algorithms need to feature a property known as collision resistance, that is, it must be infeasible to construct two distinct inputs with the same hash output. This article provides an overview of cryptographic hash functions and some of the recent developments affecting their security, namely the discovery of efficient methods for constructing collisions for algorithms such as MD5 and SHA-1. We also discuss the many implications of these recent attacks, and the possible directions for the development of the theory of hash functions.
ER  - 

TY  - JOUR
T1  - Protecting your confidential information
JO  - Computer Law & Security Review
VL  - 4
IS  - 4
SP  - 28
EP  - 31
PY  - 1988/11//
Y2  - 1988/12//
T2  - 
AU  - Lewis, David P.
SN  - 0267-3649
DO  - http://dx.doi.org/10.1016/0267-3649(88)90150-1
UR  - http://www.sciencedirect.com/science/article/pii/0267364988901501
AB  - DATA PROTECTION There are two sides to the issue of Data Protection namely: &amp;#x02022;
? Protection of Company Information; and
&amp;#x02022;
? Protection of Personal Information.


The proliferation of computers within government, business and the public generally has provided certain individuals with enormous potential power over the people and the information gathered in the many databases in existence throughout the world. The cross matching capabilities of these systems at terrific speeds has given rise to genuine concerns as to the purposes to which this information will be put and the possible abuses.

In Australia (unlike the US) there is not constitutional protection of privacy and the only remedy available to either companies or individuals is via the common law for breach of confidence or the unauthorised disclosure of a trade secret.

In this article I intend to raise some of the issues faced by both the corporation attempting to restrict access and abuse of confidential information together with the protection of individual privacy in the face of increased central data collection.
ER  - 

TY  - JOUR
T1  - Remote monitoring system helps keep traffic under control
JO  - Microprocessors and Microsystems
VL  - 9
IS  - 3
SP  - 133
EP  - 137
PY  - 1985/4//
T2  - 
AU  - Hudson, NAJ
SN  - 0141-9331
DO  - http://dx.doi.org/10.1016/0141-9331(85)90361-8
UR  - http://www.sciencedirect.com/science/article/pii/0141933185903618
KW  - microsystems
KW  - switched networks
KW  - emulation
AB  - Remote monitoring equipment for traffic junctions is described. The system, denoted REMAC, comprises an unspecified number of outstations which communicate with the central computer via the public switched telephone network. Malfunctions or other problems are diagnosed using a portable microcomputer, the Epson HX-20, to emulate outstations and/or instations. The equipment also has potential for other applications where the key element is monitoring and control of flows, eg in the water or petrochemical industries.
ER  - 

TY  - JOUR
T1  - Cloud computing — The business perspective
JO  - Decision Support Systems
VL  - 51
IS  - 1
SP  - 176
EP  - 189
PY  - 2011/4//
T2  - 
AU  - Marston, Sean
AU  - Li, Zhi
AU  - Bandyopadhyay, Subhajyoti
AU  - Zhang, Juheng
AU  - Ghalsasi, Anand
SN  - 0167-9236
DO  - http://dx.doi.org/10.1016/j.dss.2010.12.006
UR  - http://www.sciencedirect.com/science/article/pii/S0167923610002393
KW  - Cloud computing
KW  - Virtualization
KW  - Software as a service
KW  - Platform as a service
KW  - Infrastructure as a service
KW  - On-demand computing
KW  - Cloud computing regulation
AB  - The evolution of cloud computing over the past few years is potentially one of the major advances in the history of computing. However, if cloud computing is to achieve its potential, there needs to be a clear understanding of the various issues involved, both from the perspectives of the providers and the consumers of the technology. While a lot of research is currently taking place in the technology itself, there is an equally urgent need for understanding the business-related issues surrounding cloud computing. In this article, we identify the strengths, weaknesses, opportunities and threats for the cloud computing industry. We then identify the various issues that will affect the different stakeholders of cloud computing. We also issue a set of recommendations for the practitioners who will provide and manage this technology. For IS researchers, we outline the different areas of research that need attention so that we are in a position to advice the industry in the years to come. Finally, we outline some of the key issues facing governmental agencies who, due to the unique nature of the technology, will have to become intimately involved in the regulation of cloud computing.
ER  - 

TY  - JOUR
T1  - Development of an expert system prototype for determining software functional requirements for command management activities at NASA goddard
JO  - Telematics and Informatics
VL  - 3
IS  - 1
SP  - 47
EP  - 79
PY  - 1986///
T2  - 
AU  - Liebowitz, Jay
SN  - 0736-5853
DO  - http://dx.doi.org/10.1016/S0736-5853(86)80037-5
UR  - http://www.sciencedirect.com/science/article/pii/S0736585386800375
AB  - At NASA Goddard, the role of the command management system (CMS) is to transform general requests for spacecraft operations into detailed operational plans to be uplinked to the spacecraft. The CMS is part of the NASA Data System which entails the downlink of science and engineering data from NASA near-earth satellites to the user, and the uplink of command and control data to the spacecraft. Presently, it takes one to three years, with meetings once or twice a week, to determine functional requirements for CMS software design. As an alternative approach to the present technique of developing CMS software functional requirements, an expert system prototype was developed to aid in this function. Specifically, the knowledge base was formulated through interactions with domain experts, and was then linked to an existing expert system application generator called “Knowledge Engineering System.” Knowledge base development focused on four major steps: (1) develop the problem-oriented attribute hierarchy; (2) determine the knowledge management approach; (3) encode the knowledge base; and (4) validate, test, certify, and evaluate the knowledge base and the expert system prototype as a whole. Backcasting was accomplished for validating and testing the expert system prototype. Knowledge refinement, evaluation, and implementation procedures of the expert system prototype were then transacted.
ER  - 

TY  - JOUR
T1  - ECGK: An efficient clustering scheme for group key management in MANETs
JO  - Computer Communications
VL  - 33
IS  - 9
SP  - 1094
EP  - 1107
PY  - 2010/6/1/
T2  - 
AU  - Drira, K.
AU  - Seba, H.
AU  - Kheddouci, H.
SN  - 0140-3664
DO  - http://dx.doi.org/10.1016/j.comcom.2010.02.007
UR  - http://www.sciencedirect.com/science/article/pii/S0140366410000733
KW  - Group key management
KW  - Mobile ad hoc networks
KW  - Trust
KW  - Clustering
AB  - Mobile Ad hoc NETworks (or MANETs) are flexible networks that are expected to support emerging group applications such as spontaneous collaborative activities and rescue operations. In order to provide secrecy to these applications, a common encryption key has to be established between group members of the application. This task is critical in MANETs because these networks have no fixed infrastructure, frequent node and link failures and a dynamic topology. The proposed approaches to cope with these characteristics aim to avoid centralized solutions and organize the network into clusters. However, the clustering criteria used in the literature are not always adequate for key management and security. In, this paper, we propose, a group key management framework based on a trust oriented clustering scheme. We show that trust is a relevant clustering criterion for group key management in MANETs. Trust information enforce authentication and is disseminated by the mobility of nodes. Furthermore, it helps to evict malicious nodes from the multicast session even if they are authorized members of the group. Simulation results show that our solution is efficient and typically adapted to mobility of nodes.
ER  - 

TY  - JOUR
T1  - Computing a curriculum: descriptor-based domain analysis for educators
JO  - Information Processing & Management
VL  - 37
IS  - 1
SP  - 91
EP  - 117
PY  - 2001/1/1/
T2  - 
AU  - White, H.D
SN  - 0306-4573
DO  - http://dx.doi.org/10.1016/S0306-4573(00)00013-3
UR  - http://www.sciencedirect.com/science/article/pii/S0306457300000133
KW  - Information analysis
KW  - Data mining
KW  - Curriculum analysis
KW  - Curriculum evaluation
KW  - Higher education
KW  - Bibliographic databases
AB  - College educators need objective ways of assessing coverage, overlaps, and gaps in courses in their curricula, of validating their present offerings, and of monitoring subject-matter trends. This article presents a new methodology for attaining these goals through the use of descriptors from commercial bibliographic databases. A small set of master terms is chosen to model a college, department, or academic degree program, and then large numbers of descriptors that co-occur with the master terms are downloaded in online retrievals. The number of master terms with which descriptors intersect, and the number of documents these intersections produce, yield weights by which the descriptors’ relevance to the curriculum can be prioritized. Curricula are thus grounded in the subject indexing of evolving literatures. Suitably arranged, the descriptors form a rich outline of the subject matter, both central and peripheral, that coursework in a field might cover. From this outline, the descriptors with the highest weights are extracted as a “Virtual Curriculum,” against which the subject-matter of existing courses can be validated. If individual courses are assigned duplicate high-weight terms, overlaps in course content become visible. High-weight terms that cannot be matched to any existing courses reveal possible curricular gaps. Because online bibliographic databases are dynamic, domain analyses such as this can be repeated periodically to monitor trends and update judgments. A single analyst can carry out all or much of the work; the main costs are for online searching and the analyst’s time. The results are comparable to those produced by a national committee of experts. The study reported here used nine master terms to model the curricula for Drexel University’s graduate programs in information systems and library and information science. Descriptors from the INSPEC and ERIC databases were processed with Dialog search software (principally the RANK command) and SPSS.
ER  - 

TY  - JOUR
T1  - Baseline controls in some vital but often-overlooked areas of your information protection programme
JO  - Computer Fraud & Security
VL  - 2007
IS  - 12
SP  - 17
EP  - 20
PY  - 2007/12//
T2  - 
AU  - Forte, Dario
AU  - Power, Richard
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(07)70170-7
UR  - http://www.sciencedirect.com/science/article/pii/S1361372307701707
AB  - Dario Forte &amp; Richard Power look at the baseline.
ER  - 

TY  - JOUR
T1  - A human approach to the technological challenges in data security
JO  - Computers & Security
VL  - 5
IS  - 4
SP  - 328
EP  - 335
PY  - 1986/12//
T2  - 
AU  - Alagar, V.S.
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/0167-4048(86)90054-4
UR  - http://www.sciencedirect.com/science/article/pii/0167404886900544
KW  - Human factors
KW  - Risks and controls
KW  - Legal aspects
KW  - Education
KW  - Software security
AB  - A great variety of methods based on sophisticated mathematical theories are being studied recently as a means of protecting confidential data. It is argued in this paper that security is by no means an entirely technical issue. The human factors affecting data security issues are discussed and emphasized; the understanding of these factors should be reflected in the design of a system and its management.
ER  - 

TY  - JOUR
T1  - A method for forensic analysis of control
JO  - Computers & Security
VL  - 29
IS  - 8
SP  - 891
EP  - 902
PY  - 2010/11//
T2  - 
AU  - Cohen, Fred
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/j.cose.2010.05.003
UR  - http://www.sciencedirect.com/science/article/pii/S0167404810000374
KW  - Turing capability
KW  - Control
KW  - Digital forensics
KW  - Attribution
KW  - Analysis of control
AB  - This paper examines technical underpinnings for the notion of control as identified in laws and regulations in order to provide a technical basis for performing forensic analysis of digital forensic evidence in cases where taking control over systems or mechanisms is the issue.
ER  - 

TY  - JOUR
T1  - Processing of confidential information in distributed systems by fragmentation1
JO  - Computer Communications
VL  - 20
IS  - 3
SP  - 177
EP  - 188
PY  - 1997/5/1/
T2  - 
AU  - Fabre, J.-C.
AU  - Pérennou, T.
SN  - 0140-3664
DO  - http://dx.doi.org/10.1016/S0140-3664(97)00005-4
UR  - http://www.sciencedirect.com/science/article/pii/S0140366497000054
KW  - Confidential objects
KW  - Fragmentation-Redundancy-Scattering technique
KW  - Fragmentation
AB  - This paper discusses how object orientation in application design enables confidentiality aspects to be handled more easily than in conventional approaches. The approach is based on the Fragmentation-Redundancy-Scattering technique developed at LAAS–CNRS for several years. This technique and previous developments are briefly summarized. The idea developed in this paper is based on object fragmentation at design time for reducing data processing in confidential objects; the more non confidential objects can be produced at design-time, the more application objects can be processed on untrusted shared computers. Still confidential objects must be processed on non shared trusted workstations. Rules and limits of object fragmentation are discussed together with some criteria evaluating tradeoffs between fragmentation and performance. Finally, a distributed object-oriented support especially fitted for fragmented applications is briefly described.
ER  - 

TY  - JOUR
T1  - Abstracts of recent articles and literature
JO  - Computers & Security
VL  - 2
IS  - 1
SP  - 88
EP  - 96
PY  - 1983/1//
T2  - 

SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/0167-4048(83)90045-7
UR  - http://www.sciencedirect.com/science/article/pii/0167404883900457
ER  - 

TY  - JOUR
T1  - Location-based design for secure and efficient wireless sensor networks
JO  - Computer Networks
VL  - 52
IS  - 16
SP  - 3119
EP  - 3129
PY  - 2008/11/13/
T2  - 
AU  - Yang, Cungang
AU  - Li, Celia
AU  - Xiao, Jie
SN  - 1389-1286
DO  - http://dx.doi.org/10.1016/j.comnet.2008.07.011
UR  - http://www.sciencedirect.com/science/article/pii/S1389128608002508
KW  - Location-based system
KW  - Data aggregation
KW  - Node compromise
KW  - Threshold behavior
KW  - En-route data authentication
AB  - This paper studies a novel location-based design for secure and efficient wireless sensor networks. With this scheme, the sensing area is divided into a number of location cells and a logical group consists of four location cells. A pairwise key is established for each pair of sensor nodes in a logical group based on grid-based bivariate t-degree polynomials. An efficient en-route data authentication method is proposed to limit data authentication within a small neighboring range. In addition, a greedy location-based secure and energy-efficient data aggregation approach is integrated with the design. It further utilizes data aggregation by setting up control groups, applying pattern codes, selecting and switching control head nodes dynamically and periodically. The analysis demonstrate that our design is resilient to node compromise and decrease storage and communication overhead when compared with non location-based designs.
ER  - 

TY  - JOUR
T1  - Modula and a vision laboratory
JO  - International Journal of Man-Machine Studies
VL  - 14
IS  - 3
SP  - 371
EP  - 386
PY  - 1981/4//
T2  - 
AU  - Runciman, Colin
SN  - 0020-7373
DO  - http://dx.doi.org/10.1016/S0020-7373(81)80064-8
UR  - http://www.sciencedirect.com/science/article/pii/S0020737381800648
AB  - A short description of Modula, the high-level language for real-time parallel programming, concentrates on its distinctive features as compared with Pascal; in particular the processs, signal and three types of module are considered.

VRW, a vision laboratory control program written in Modula is introduced. Its complete module and process structure is presented in support of the argument that Modula allows a most attractive program architecture which matches that of the laboratory and -the experimental control problem. Detailed fragments of VRW are presented to illustrate the capabilities of Modula with special attention to device handling.

Further benefits of the Modula discipline such as the inherent confidence possible in solutions and the merits of the module as a unit for software construction are discussed. In examining means of control over the use of machine-store, scalar types and, more particularly, the timing of events, weaknesses in Modula are noted and discussed. But these do not prevent the conclusion that it is a most capable and attractive language for laboratory control.
ER  - 

TY  - JOUR
T1  - The regulation of transborder data flows
JO  - Telecommunications Policy
VL  - 3
IS  - 3
SP  - 176
EP  - 191
PY  - 1979/9//
T2  - 
AU  - de Sola Pool, Ithiel
AU  - Solomon, Richard J.
SN  - 0308-5961
DO  - http://dx.doi.org/10.1016/0308-5961(79)90002-8
UR  - http://www.sciencedirect.com/science/article/pii/0308596179900028
AB  - Restrictions on transmission of data across borders are sometimes proposed as means to control abuses such as invasions of privacy. The authors explore the thesis that controls on cross-border data flows are rarely effective in helping to achieve such ends. They argue that the technology of modern telecommunications does not allow easy control over message content; and that there are other, more strategically effective techniques to restrict abuses committed in international electronic transactions.
ER  - 

TY  - JOUR
T1  - Self-sustaining, efficient and forward-secure cryptographic constructions for Unattended Wireless Sensor Networks
JO  - Ad Hoc Networks
VL  - 10
IS  - 7
SP  - 1204
EP  - 1220
PY  - 2012/9//
T2  - 
AU  - Yavuz, Attila Altay
AU  - Ning, Peng
SN  - 1570-8705
DO  - http://dx.doi.org/10.1016/j.adhoc.2012.03.006
UR  - http://www.sciencedirect.com/science/article/pii/S1570870512000479
KW  - Applied cryptography
KW  - Unattended Wireless Sensor Networks (UWSNs)
KW  - Digital signatures
KW  - Forward security
KW  - Aggregate signatures
AB  - Unattended Wireless Sensor Networks (UWSNs) operating in hostile environments face great security and performance challenges due to the lack of continuous real-time communication with the final data receivers (e.g., mobile data collectors). The lack of real-time communication forces sensors to accumulate sensed data possibly for long time periods, along with the corresponding authentication tags. It also makes UWSNs vulnerable to active adversaries, which can compromise sensors and manipulate the collected data. Hence, it is critical to have forward security property such that even if the adversary can compromise the current keying materials, she cannot forge authentication tags generated before the compromise. Forward secure and aggregate signature schemes are developed to address these issues. Unfortunately, existing schemes either impose substantial overhead, or do not allow public verifiability, thereby impractical for resource-constrained UWSNs.

In this paper, we propose a new class of cryptographic schemes, referred to as Hash-BasedSequentialAggregate andForwardSecureSignature (HaSAFSS), which allows a signer to sequentially generate a compact, fixed-size, and publicly verifiable signature efficiently. We develop three HaSAFSS schemes, Symmetric HaSAFSS (Sym-HaSAFSS), Elliptic Curve Cryptography (ECC) based HaSAFSS (ECC-HaSAFSS) and self-SUstaining HaSAFSS (SU-HaSAFSS). These schemes integrate the efficiency of MAC-based aggregate signatures and the public verifiability of Public Key Cryptography (PKC)-based signatures by preserving forward security via Timed-Release Encryption (TRE). We demonstrate that our schemes are secure and also significantly more efficient than previous approaches.
ER  - 

TY  - JOUR
T1  - MD3M: The master data management maturity model
JO  - Computers in Human Behavior
VL  - 51, Part B
IS  - 
SP  - 1068
EP  - 1076
PY  - 2015/10//
T2  - Computing for Human Learning, Behaviour and Collaboration in the Social and Mobile Networks Era
AU  - Spruit, Marco
AU  - Pietzka, Katharina
SN  - 0747-5632
DO  - http://dx.doi.org/10.1016/j.chb.2014.09.030
UR  - http://www.sciencedirect.com/science/article/pii/S0747563214004828
KW  - Master data management
KW  - Maturity assessment
KW  - Information management
AB  - Abstract
This research aims to assess the master data maturity of an organization. It is based on thorough literature study to derive the main concepts and best practices in master data maturity assessment. A maturity matrix relating 13 focus areas and 65 capabilities was designed and validated. Furthermore, an assessment questionnaire was developed which can be used to assess the master data management maturity. Emphasis is laid on the academic validity of the model development process. Our extensive case study provides an example of iterative human learning, behavior and collaboration resulting from technological needs in a large-scale infrastructural network. Concludingly, this research uncovers reasons and incentives for prudent master data management and provides a benchmarking tool with which different organizations can compare their levels.
ER  - 


