TY  - CONF
JO  - Computer Science and Information Systems (FedCSIS), 2013 Federated Conference on
TI  - FAL: A forensics aware language for secure logging
T2  - Computer Science and Information Systems (FedCSIS), 2013 Federated Conference on
IS  - 
SN  - 
VO  - 
SP  - 1579
EP  - 1586
AU  - Zawoad, S.
AU  - Mernik, M.
AU  - Hasan, R.
Y1  - 8-11 Sept. 2013
PY  - 2013
KW  - data integrity
KW  - data privacy
KW  - digital forensics
KW  - specification languages
KW  - trusted computing
KW  - FAL
KW  - application logging
KW  - application logs
KW  - digital forensics
KW  - domain-specific language
KW  - forensics aware language
KW  - heterogeneous formats
KW  - log confidentiality
KW  - log integrity
KW  - log structure
KW  - logging security
KW  - source code generation
KW  - trustworthy system logs
KW  - DSL
KW  - Encryption
KW  - Indexes
KW  - Semantics
KW  - Syntactics
KW  - Audit Trail
KW  - DSL
KW  - Digital Forensics
KW  - Secure Logging
VL  - 
JA  - Computer Science and Information Systems (FedCSIS), 2013 Federated Conference on
DO  - 
AB  - Trustworthy system logs and application logs are crucial for digital forensics. Researchers have proposed different security mechanisms to ensure the integrity and confidentiality of logs. However, applying current secure logging schemes on heterogeneous formats of logs is tedious. Here, we propose FAL, a domain-specific language (DSL) through which we can apply a secure logging mechanism on any format of logs. Using FAL, we can define log structure, which represents the format of logs and ensures the security properties of a chosen secure logging scheme. This log structure can be later used by FAL to serve two purposes: it can be used to store system logs securely, and it will help application developers for secure application logging by generating required source code.
ER  - 

TY  - CONF
JO  - Computer Security Applications Conference, 2009. ACSAC '09. Annual
TI  - BAF: An Efficient Publicly Verifiable Secure Audit Logging Scheme for Distributed Systems
T2  - Computer Security Applications Conference, 2009. ACSAC '09. Annual
IS  - 
SN  - 1063-9527
VO  - 
SP  - 219
EP  - 228
AU  - Yavuz, A.A.
AU  - Peng Ning
Y1  - 7-11 Dec. 2009
PY  - 2009
KW  - cryptography
KW  - distributed processing
KW  - blind-aggregate-forward logging scheme
KW  - distributed system
KW  - public key cryptography
KW  - secure audit logging scheme
KW  - Aggregates
KW  - Application software
KW  - Computer security
KW  - Data security
KW  - Digital forensics
KW  - Distributed computing
KW  - Hardware
KW  - Information security
KW  - Public key cryptography
KW  - Secure storage
KW  - Applied cryptography
KW  - digital forensics
KW  - forward security
KW  - secure audit logging
KW  - signature aggregation
VL  - 
JA  - Computer Security Applications Conference, 2009. ACSAC '09. Annual
DO  - 10.1109/ACSAC.2009.28
AB  - Audit logs, providing information about the current and past states of systems, are one of the most important parts of modern computer systems. Providing security for audit logs on an untrusted machine in a large distributed system is a challenging task, especially in the presence of active adversaries. In such a system, it is critical to have forward security such that when an adversary compromises a machine, she cannot modify or forge the log entries accumulated before the compromise. Unfortunately, existing secure audit logging schemes have significant limitations that make them impractical for real-life applications: existing public key cryptography (PKC) based schemes are computationally expensive for logging in task intensive or resource-constrained systems, while existing symmetric schemes are not publicly verifiable and incur significant storage and communication overheads. In this paper, we propose a novel forward secure and aggregate logging scheme called blind-aggregate-forward (BAF) logging scheme, which is suitable for large distributed systems. BAF can produce publicly verifiable forward secure and aggregate signatures with near-zero computational, storage, and communication costs for the loggers, without requiring any online trusted third party (TTP) support. We prove that BAF is secure under appropriate computational assumptions, and demonstrate that BAF is significantly more efficient and scalable than the previous schemes. Therefore, BAF is an ideal solution for secure logging in both task intensive and resource-constrained systems.
ER  - 

TY  - CONF
JO  - Trusted Infrastructure Technologies Conference, 2008. APTC '08. Third Asia-Pacific
TI  - Trusted Logging for Grid Computing
T2  - Trusted Infrastructure Technologies Conference, 2008. APTC '08. Third Asia-Pacific
IS  - 
SN  - 
VO  - 
SP  - 30
EP  - 42
AU  - Jun Ho Huh
AU  - Martin, A.
Y1  - 14-17 Oct. 2008
PY  - 2008
KW  - grid computing
KW  - security of data
KW  - system monitoring
KW  - administrative domains
KW  - audit
KW  - distributed services
KW  - grid computing
KW  - grid systems
KW  - log data
KW  - log security manager
KW  - logging architecture
KW  - logging security
KW  - secure logging service
KW  - security configurations
KW  - security threats
KW  - trusted computing
KW  - trusted logging
KW  - trustworthy services
KW  - virtual machine isolation
KW  - Access control
KW  - Computer architecture
KW  - Data security
KW  - Grid computing
KW  - Information analysis
KW  - Information security
KW  - Laboratories
KW  - Protection
KW  - Virtual machining
KW  - Voice mail
KW  - Grid Computing
KW  - Secure Logging Requirements
KW  - Trusted Computing
KW  - Trusted Logging
KW  - Virtualization
VL  - 
JA  - Trusted Infrastructure Technologies Conference, 2008. APTC '08. Third Asia-Pacific
DO  - 10.1109/APTC.2008.9
AB  - The rise of many kinds of grid systems, and associated security threats, makes very necessary the provision of trustworthy services for audit and logging. However, existing solutions tend to put little emphasis on the security of logging. We present a number of use cases where the logs have security properties in their own rights, and so the logs themselves are highly privileged: hence, these need to be integrity and confidentiality protected while being generated, accessed, reconciled and analysed with distributed services spanning across multiple administrative domains. We derive a common set of secure logging requirements to address the security gaps which exist between these use cases and existing solutions.From the requirements, we propose a novel logging architecture for the grid based on virtual machine (VM) isolation and trusted computing capabilities: a small number of privileged driver VMs trigger all trusted logging requests and forward them to the secure logging service running within the log security manager. The logging service verifies the integrity of the log data and the security configurations of these driver VMs (log generators) before storing the logs.
ER  - 

TY  - CONF
JO  - Advance Computing Conference (IACC), 2014 IEEE International
TI  - &#x201C;Role of metadata in forensic analysis of database attacks&#x201C;
T2  - Advance Computing Conference (IACC), 2014 IEEE International
IS  - 
SN  - 
VO  - 
SP  - 457
EP  - 462
AU  - Khanuja, H.
AU  - Suratkar, S.S.
Y1  - 21-22 Feb. 2014
PY  - 2014
KW  - data privacy
KW  - digital forensics
KW  - law
KW  - meta data
KW  - antiforensics attacks
KW  - audit logs
KW  - cache
KW  - court of law
KW  - database attacks
KW  - database security breaches
KW  - database server artifacts
KW  - digital evidence
KW  - e-transactions
KW  - forensic analysis
KW  - fraudulent transaction
KW  - information analysis
KW  - information retrieval
KW  - metadata
KW  - online activities
KW  - open source database forensics tool
KW  - privacy issue
KW  - security issue
KW  - table storage
KW  - Conferences
KW  - Handheld computers
KW  - Database forensics
KW  - SQL injection
KW  - anti-forensics attacks
KW  - digital notarization
KW  - linked hash technique
KW  - metadata
KW  - reconnaissance attack
KW  - trail obfuscation
VL  - 
JA  - Advance Computing Conference (IACC), 2014 IEEE International
DO  - 10.1109/IAdCC.2014.6779367
AB  - With the spectacular increase in online activities like e-transactions, security and privacy issues are at the peak with respect to their significance. Large numbers of database security breaches are occurring at a very high rate on daily basis. So, there is a crucial need in the field of database forensics to make several redundant copies of sensitive data found in database server artifacts, audit logs, cache, table storage etc. for analysis purposes. Large volume of metadata is available in database infrastructure for investigation purposes but most of the effort lies in the retrieval and analysis of that information from computing systems. Thus, in this paper we mainly focus on the significance of metadata in database forensics. We proposed a system here to perform forensics analysis of database by generating its metadata file independent of the DBMS system used. We also aim to generate the digital evidence against criminals for presenting it in the court of law in the form of who, when, why, what, how and where did the fraudulent transaction occur. Thus, we are presenting a system to detect major database attacks as well as anti-forensics attacks by developing an open source database forensics tool. Eventually, we are pointing out the challenges in the field of forensics and how these challenges can be used as opportunities to stimulate the areas of database forensics.
ER  - 

TY  - CONF
JO  - Software Engineering (ICSE), 2013 35th International Conference on
TI  - Measuring the forensic-ability of audit logs for nonrepudiation
T2  - Software Engineering (ICSE), 2013 35th International Conference on
IS  - 
SN  - 
VO  - 
SP  - 1419
EP  - 1422
AU  - King, J.
Y1  - 18-26 May 2013
PY  - 2013
KW  - auditing
KW  - digital forensics
KW  - educational computing
KW  - financial data processing
KW  - fraud
KW  - health care
KW  - software metrics
KW  - system monitoring
KW  - activity logging mechanism
KW  - application performance monitoring
KW  - audit log
KW  - compliance checking
KW  - data field
KW  - debugging
KW  - education
KW  - finance
KW  - forensic ability measurement
KW  - forensic analysis
KW  - fraud detection
KW  - grounded theory method
KW  - healthcare
KW  - log file attribute
KW  - software log files
KW  - software security events
KW  - software security metrics
KW  - software system
KW  - system resources
KW  - unique user identifier
KW  - user access tracking
KW  - user behavior profile extraction
KW  - user nonrepudiation
KW  - user privilege revocation
KW  - Forensics
KW  - Measurement
KW  - Medical services
KW  - Security
KW  - Software systems
KW  - Standards
KW  - forensics
KW  - grounded theory
KW  - logging
KW  - metric
KW  - nonrepudiation
KW  - security
KW  - software logs
VL  - 
JA  - Software Engineering (ICSE), 2013 35th International Conference on
DO  - 10.1109/ICSE.2013.6606732
AB  - Forensic analysis of software log files is used to extract user behavior profiles, detect fraud, and check compliance with policies and regulations. Software systems maintain several types of log files for different purposes. For example, a system may maintain logs for debugging, monitoring application performance, and/or tracking user access to system resources. The objective of my research is to develop and validate a minimum set of log file attributes and software security metrics for user nonrepudiation by measuring the degree to which a given audit log file captures the data necessary to allow for meaningful forensic analysis of user behavior within the software system. For a log to enable user nonrepudiation, the log file must record certain data fields, such as a unique user identifier. The log must also record relevant user activity, such as creating, viewing, updating, and deleting system resources, as well as software security events, such as the addition or revocation of user privileges. Using a grounded theory method, I propose a methodology for observing the current state of activity logging mechanisms in healthcare, education, and finance, then I quantify differences between activity logs and logs not specifically intended to capture user activity. I will then propose software security metrics for quantifying the forensic-ability of log files. I will evaluate my work with empirical analysis by comparing the performance of my metrics on several types of log files, including both activity logs and logs not directly intended to record user activity. My research will help software developers strengthen user activity logs for facilitating forensic analysis for user nonrepudiation.
ER  - 

TY  - CONF
JO  - Advanced Information Networking and Applications, 2005. AINA 2005. 19th International Conference on
TI  - Distributed agent-based real time network intrusion forensics system architecture design
T2  - Advanced Information Networking and Applications, 2005. AINA 2005. 19th International Conference on
IS  - 
SN  - 1550-445X
VO  - 1
SP  - 177
EP  - 182 vol.1
AU  - Wei Ren
AU  - Hai Jin
Y1  - 28-30 March 2005
PY  - 2005
KW  - real-time systems
KW  - security of data
KW  - software agents
KW  - audit system
KW  - distributed agent-based real time system
KW  - network intrusion forensics system
KW  - network security
KW  - network traffic
KW  - Computer architecture
KW  - Computerized monitoring
KW  - Data security
KW  - Forensics
KW  - Grid computing
KW  - Intrusion detection
KW  - Network servers
KW  - Protection
KW  - Real time systems
KW  - Telecommunication traffic
VL  - 1
JA  - Advanced Information Networking and Applications, 2005. AINA 2005. 19th International Conference on
DO  - 10.1109/AINA.2005.164
AB  - Network forensics is a new approach for the network security, because the firewall and IDS cannot always stop and discover the misuse in the network. Once the system is compromised, the forensics and investigation always after the attacks and lose some useful instant evidence. The integrated analysis of the log and audit system and network traffic can lead to an efficient navigation of the traffic. The current network forensics approaches only focus on the network traffic capture and traffic replay, which always result in the performance bottleneck or forensics analysis difficulties. However, the adaptive capture without lose the potential sensitive traffic and real time investigation are seldom discussed. In this paper, we discuss the frameworks of distributed agent-based real time network intrusion forensics system, which is deployed in local area network environment. Some novel approaches for network forensics are discussed for the first time, such as network forensics server, network forensics database, network forensics agents, forensics data integration and active real time network forensic.
ER  - 

TY  - CONF
JO  - Communication Networks and Services Research Conference, 2009. CNSR '09. Seventh Annual
TI  - A Prioritized Retransmission Mechanism for Reliable and Efficient Delivery of Syslog Messages
T2  - Communication Networks and Services Research Conference, 2009. CNSR '09. Seventh Annual
IS  - 
SN  - 
VO  - 
SP  - 158
EP  - 165
AU  - Tsunoda, H.
AU  - Maruyama, T.
AU  - Ohta, K.
AU  - Waizumi, Y.
AU  - Keeni, G.M.
AU  - Nemoto, Y.
Y1  - 11-13 May 2009
PY  - 2009
KW  - computer network management
KW  - computer network reliability
KW  - intranets
KW  - operating systems (computers)
KW  - security of data
KW  - telecommunication security
KW  - transport protocols
KW  - NS-2 simulation
KW  - TCP
KW  - application program
KW  - auditing
KW  - forensics
KW  - intranet
KW  - network administrator
KW  - network logging
KW  - operating system
KW  - prioritized retransmission mechanism
KW  - reliable syslog message delivery
KW  - security management
KW  - syslog protocol
KW  - Communication networks
KW  - Electronic mail
KW  - Forensics
KW  - Information security
KW  - Internet
KW  - Operating systems
KW  - Relays
KW  - Reliability engineering
KW  - Telecommunication network reliability
KW  - Transport protocols
KW  - Network management
KW  - audit
KW  - logging
KW  - syslog
VL  - 
JA  - Communication Networks and Services Research Conference, 2009. CNSR '09. Seventh Annual
DO  - 10.1109/CNSR.2009.33
AB  - Summary form only given. Logs generated by operating systems and application programs provide important information to a network administrator. Logs are used for various purposes including security management, audit, and forensics of intranet. To use logs for such purposes, it is important that logs are reliably retrieved from hosts in the intranet. But the syslog protocol which is widely used for network logging does not meet this requirement. Thus, the use of TCP for improving the reliability is being standardized at the IETF. However, TCP is not effective for providing the reliability in terms of cost and delay. In this paper, we examine the issues and requirements of network logging based on experiments in a real network environment and point out problems of TCP. Then we propose an efficient mechanism for the reliable delivery of syslog messages and validate its effectiveness thorough NS-2 simulations.
ER  - 

TY  - CONF
JO  - Integrated Network Management (IM), 2011 IFIP/IEEE International Symposium on
TI  - On the design of Virtual machine Intrusion detection system
T2  - Integrated Network Management (IM), 2011 IFIP/IEEE International Symposium on
IS  - 
SN  - 
VO  - 
SP  - 682
EP  - 685
AU  - Tupakula, U.
AU  - Varadharajan, V.
Y1  - 23-27 May 2011
PY  - 2011
KW  - security of data
KW  - virtual machines
KW  - VICTOR
KW  - dynamic analyzer
KW  - fine granular isolation
KW  - intrusion detection engine
KW  - secure logging
KW  - spoofed source address
KW  - suspicious behaviour
KW  - traffic
KW  - virtual machine
KW  - Australia
KW  - Hardware
KW  - IP networks
KW  - Irrigation
KW  - Levee
KW  - Trojan horses
KW  - Intrusion detection
KW  - Security architecture
KW  - Virtual machine monitorsy
VL  - 
JA  - Integrated Network Management (IM), 2011 IFIP/IEEE International Symposium on
DO  - 10.1109/INM.2011.5990655
AB  - In this paper we propose comprehensive security architecture called VICTOR to deal with different types of attacks on virtual machines. Our model takes into account the specific characteristics of operating system and applications running in each virtual machine (VM) at a fine granular level to deal with the attacks. Our architecture has several components such as entity validation, intrusion detection engine and dynamic analyzer. The entity validation component is used in the detection of attack traffic with spoofed source address, secure logging, and capturing information of the operating system and applications running in the virtual machines. The intrusion detection engine component is used for detection of known attacks and suspicious behaviour by monitoring the incoming and outgoing traffic of virtual machines. The dynamic analyzer is used for detection and validation of suspicious processes, detection of zero day attacks and fine granular isolation of malicious process or application that is generating the attack traffic.
ER  - 

TY  - CONF
JO  - Machine Learning and Cybernetics, 2003 International Conference on
TI  - Improving one-class SVM for anomaly detection
T2  - Machine Learning and Cybernetics, 2003 International Conference on
IS  - 
SN  - 
VO  - 5
SP  - 3077
EP  - 3081 Vol.5
AU  - Kun-Lun Li
AU  - Hou-Kuan Huang
AU  - Shen-Feng Tian
AU  - Wei Xu
Y1  - 2-5 Nov. 2003
PY  - 2003
KW  - pattern classification
KW  - security of data
KW  - support vector machines
KW  - system monitoring
KW  - DARPA
KW  - Internet
KW  - SVM
KW  - anomaly detection
KW  - computer system security
KW  - information system security
KW  - intrusion detection
KW  - pattern analysis
KW  - support vector machines
KW  - system audit log files
KW  - Computer errors
KW  - Computer security
KW  - Cybernetics
KW  - Data security
KW  - Information security
KW  - Information systems
KW  - Intrusion detection
KW  - Protection
KW  - Support vector machine classification
KW  - Support vector machines
VL  - 5
JA  - Machine Learning and Cybernetics, 2003 International Conference on
DO  - 10.1109/ICMLC.2003.1260106
AB  - With the tremendous growth of the Internet, information system security has become an issue of serious global concern due to the rapid connection and accessibility. Developing effective methods for intrusion detection, therefore, is an urgent task for assuring computer & information system security. Since most attacks and misuses can be recognized through the examination of system audit log files and pattern analysis therein, an approach for intrusion detection can be built on them. First we have made deep analysis on attacks and misuses patterns in log files; and then proposed an approach using support vector machines for anomaly detection. It is a one-class SVM based approach, trained with abstracted user audit logs data from 1999 DARPA.
ER  - 

TY  - CONF
JO  - Security and Privacy, 1989. Proceedings., 1989 IEEE Symposium on
TI  - Detection of anomalous computer session activity
T2  - Security and Privacy, 1989. Proceedings., 1989 IEEE Symposium on
IS  - 
SN  - 
VO  - 
SP  - 280
EP  - 289
AU  - Vaccaro, H.S.
AU  - Liepins, G.E.
Y1  - 1-3 May 1989
PY  - 1989
KW  - DP management
KW  - security of data
KW  - Wisdom and Sense
KW  - anomalous computer session activity
KW  - audit logs
KW  - categorical data
KW  - historical data
KW  - rules
KW  - system security officers
KW  - usage patterns
KW  - Computer security
KW  - Computer viruses
KW  - Data security
KW  - Event detection
KW  - Humans
KW  - Information security
KW  - Invasive software
KW  - Laboratories
KW  - National security
KW  - Physics computing
VL  - 
JA  - Security and Privacy, 1989. Proceedings., 1989 IEEE Symposium on
DO  - 10.1109/SECPRI.1989.36302
AB  - The authors discusses Wisdom and Sense (W&amp;S), a computer security anomaly detection system. W&amp;S is statistically based. It automatically generates rules from historical data and, in terms of those rules, identifies computer transactions that are at variance with historically established usage patterns. Issues addressed include how W&amp;S generates rules from a necessarily small sample of all possible transactions, how W&amp;S deals with inherently categorical data, and how W&amp;S assists system security officers in their review of audit logs. Preliminary results with W&amp;S show that the software does periodically detect anomalies of high interest even in data though to be free of such events
ER  - 

TY  - CONF
JO  - Military Communications Conference, 2004. MILCOM 2004. 2004 IEEE
TI  - Optical control and management security standards for the GIG-BE
T2  - Military Communications Conference, 2004. MILCOM 2004. 2004 IEEE
IS  - 
SN  - 
VO  - 2
SP  - 968
EP  - 974 Vol. 2
AU  - Esposito, R.
AU  - Frankel, S.
AU  - Graveman, R.
AU  - McNown, S.
Y1  - 31 Oct.-3 Nov. 2004
PY  - 2004
KW  - auditing
KW  - military standards
KW  - optical fibre networks
KW  - routing protocols
KW  - security of data
KW  - telecommunication network management
KW  - telecommunication signalling
KW  - ATIS-T1M1
KW  - Department of Defense
KW  - GIG-BE
KW  - Global Information Grid-Bandwidth Expansion
KW  - IETF
KW  - OAM and P interfaces
KW  - Optical Internetworking Forum
KW  - RFC
KW  - Security Extension for UNI and NNI
KW  - Security for Management Interfaces to Transport Network Elements
KW  - audit log capability
KW  - control plane functionality
KW  - control protocols
KW  - discovery
KW  - end-to-end security
KW  - management security standards
KW  - operations administration maintenance and provisioning
KW  - optical networking protocols
KW  - optical networks
KW  - routing
KW  - signaling
KW  - Communication system security
KW  - High speed optical techniques
KW  - Information security
KW  - National security
KW  - Optical control
KW  - Optical fiber networks
KW  - Optical interconnections
KW  - Protocols
KW  - Routing
KW  - Standards development
VL  - 2
JA  - Military Communications Conference, 2004. MILCOM 2004. 2004 IEEE
DO  - 10.1109/MILCOM.2004.1494967
AB  - This paper presents an overview of requirements and standards development activities for securing the control and management infrastructure protocols for optical networking protocols used in the Global Information Grid-Bandwidth Expansion (GIG-BE). Our approaches to hardening these protocols are: (1) to develop open standards that encompass the Department of Defense's needs; and (2) to encourage vendors to supply products that support these standards and other appropriate security functionality for GIG-BE signaling, routing, discovery, and management. At MILCOM 2001, Buda et al. reported on commercial-off-the-shelf security standards being developed for the GIG; they covered asynchronous transfer mode, multi-protocol label switching, and newly emerging optical networking. We have now completed control plane security and management plane security implementation agreements at the Optical Internetworking Forum (OIF), coordinated and aligned these with ATIS-T1M1 and the IETF, and begun efforts to implement and demonstrate these agreements. This paper briefly describes the OIF's work on control plane functionality in optical networks and the security requirements for these control protocols. It then explains why additional security was required for signaling, routing, and discovery; shows what alternatives were considered; and describes the choices made in the OIF's Security Extension for UNI and NNI. Securing an optical switch depends on much more than secure control protocols, so the paper next covers the OIF's Security for Management Interfaces to Transport Network Elements, which describes security objectives and choices for securing operations, administration, maintenance, and provisioning (OAM and P) interfaces to these network elements. Specifications and recommendations are given along with a mapping of how following the specifications satisfies the initial objectives. The relationship of this work to the security standards developed by T1M1 is also described. Beyond these two implementation agreements, on-going efforts are focused on demonstrating the practicality of this approach, addressing end-to-end security, adding an audit log capability, continuing cooperation with T1M1 on OAM and P security, and keeping these im- plementation agreements aligned with new drafts and RFC on signaling, routing, discovery, and security at the IETF.
ER  - 

TY  - CONF
JO  - Computational Aspects of Social Networks (CASoN), 2011 International Conference on
TI  - Log management comprehensive architecture in Security Operation Center (SOC)
T2  - Computational Aspects of Social Networks (CASoN), 2011 International Conference on
IS  - 
SN  - 
VO  - 
SP  - 284
EP  - 289
AU  - Madani, A.
AU  - Rezayi, S.
AU  - Gharaee, H.
Y1  - 19-21 Oct. 2011
PY  - 2011
KW  - security of data
KW  - SOC
KW  - correlation module
KW  - fraudulent activities
KW  - log management comprehensive architecture
KW  - operational network problems
KW  - policy violations
KW  - security incidents
KW  - security log data
KW  - security log management
KW  - security operation center
KW  - storage module
KW  - Computer architecture
KW  - Correlation
KW  - Monitoring
KW  - Security
KW  - Servers
KW  - Software
KW  - System-on-a-chip
KW  - SOC
KW  - event fiields
KW  - log management
KW  - normalizing
KW  - storage
VL  - 
JA  - Computational Aspects of Social Networks (CASoN), 2011 International Conference on
DO  - 10.1109/CASON.2011.6085959
AB  - With the widespread use of information, variety of security logs have increased greatly, which due need for security log management. Organizations requirements have imposed to collect, store, and analyze tremendous volumes of log data across entire infrastructure for extended durations and at increasingly granular levels. It is the process of generating, transmitting, storing, analyzing, and disposing security log data from network to databases. Due to the wide variety of logs, storing comprises different methods. Recorded events in collection module are processed, normalized and classified. Logs are stored in storage module in order to use in forensic, reviewing, auditing and providing further necessities of correlation module. Routine log correlation analysis is beneficial for identifying security incidents, policy violations, fraudulent activities, troubleshooting and operational network problems. So log management is an important and efficient activity in network monitoring. Finding an effective log management functional architecture for network events analysis is the main goal of this paper. In this paper, we aim to suggest log management architecture with more common functions that are used by vendors. By studying logging architectures the main functions are administration of log collection, normalizing, categorization, queuing prioritizing and storing logged events/alarms by sensors. Log functions are different but the suitable architecture must justify the functions to send a normative, synchronized and prioritized log in an efficient way. The mentioned functions are gathered from SIEM products characteristics. Suggested architecture includes functions and activities in log collection server and storage server.
ER  - 

TY  - CONF
JO  - Security Technology, 2003. Proceedings. IEEE 37th Annual 2003 International Carnahan Conference on
TI  - A study of information and communication security forensic technology capability in Taiwan
T2  - Security Technology, 2003. Proceedings. IEEE 37th Annual 2003 International Carnahan Conference on
IS  - 
SN  - 
VO  - 
SP  - 386
EP  - 393
AU  - I-Long Lin
AU  - Hong-Cheng Yang
AU  - Guo-Long Gu
AU  - Lin, A.C.
Y1  - 14-16 Oct. 2003
PY  - 2003
KW  - Internet
KW  - security of data
KW  - telecommunication security
KW  - Internet security problems
KW  - Taiwan
KW  - communication security
KW  - computer forensics
KW  - forensic technology capability
KW  - information security
KW  - intellectual property right
KW  - Communication system security
KW  - Communications technology
KW  - Computer security
KW  - Forensics
KW  - Government
KW  - Hard disks
KW  - Information security
KW  - Internet
KW  - Intrusion detection
KW  - Research and development
VL  - 
JA  - Security Technology, 2003. Proceedings. IEEE 37th Annual 2003 International Carnahan Conference on
DO  - 10.1109/CCST.2003.1297591
AB  - There are constant occurrences of Internet security problems due to its rapid development. It is important to maintain Internet security system during and after the occurrences to collect evidence and forensics essences by various devices, such as hard disk, system log, firewall, IDS log, processes, as well as Internet connections. It would be even more beneficial to the forensic process if evidence could be established and kept well before such an incident happened. Most government organizations lack sufficient knowledge of security system techniques and they still believe that this technical part of the work belongs to the police. In fact, we cannot guarantee a perfect stage, but at least we can figure out what the problem is and trace where the attack is from, and that is the mission of computer forensics. Schools or institutes that are engaged in research and development of relative security system techniques are doing their own work; thus, capabilities of computer forensics have been dispersed. Therefore, in order to enhance the forensic skill of information and communication security and ability, what we really need is a complete plan so as to integrate the forensic skill of information and communication security and ability.
ER  - 

TY  - CONF
JO  - Management and Service Science, 2009. MASS '09. International Conference on
TI  - Research and Realization of Secure Audit Mechanism Based on LSM
T2  - Management and Service Science, 2009. MASS '09. International Conference on
IS  - 
SN  - 
VO  - 
SP  - 1
EP  - 5
AU  - Wu Jie
AU  - Qu Kun
Y1  - 20-22 Sept. 2009
PY  - 2009
KW  - Linux
KW  - dynamic response
KW  - operating systems (computers)
KW  - security of data
KW  - LSM
KW  - Linux security modules
KW  - audit hooks
KW  - audit log format
KW  - buffer
KW  - constraint control algorithm
KW  - data structure
KW  - double-linked list structure
KW  - hook functions
KW  - log storage
KW  - query
KW  - real-time security warning
KW  - register functions
KW  - secure audit mechanism
KW  - Computer science
KW  - Computer security
KW  - Data security
KW  - Data structures
KW  - Information security
KW  - Kernel
KW  - Linux
KW  - Monitoring
KW  - Operating systems
KW  - Safety
VL  - 
JA  - Management and Service Science, 2009. MASS '09. International Conference on
DO  - 10.1109/ICMSS.2009.5304261
AB  - The secure audit mechanism based on Linux security modules (LSM) is presented. The SAM enhances LSM in audit, makes security domain of process task structure point to the specified data structure, and adds audit hooks and hook functions to capture comprehensive audit information. Additionally, register functions and unregistered functions are provided to implement dynamic addition and deletion of security audit modules. Buffer with double-linked list structure is designed to solve easy lost of audit information and buffer overflow. Normal activities rule base, RVA, and its dynamic response algorithm are presented. And real-time security warning and punishment mechanisms are achieved by constraint control algorithm and the set of warning threshold and punish threshold. Audit log format based on efficiency is designed. And five kinds of basic query are provided to achieve effective log storage and query. The tested results indicate the mechanism has better security and running performance, less influence on the kernel, and adaptive kernel's upgrade.
ER  - 

TY  - CONF
JO  - Systematic Approaches to Digital Forensic Engineering (SADFE), 2010 Fifth IEEE International Workshop on
TI  - Explorative Visualization of Log Data to Support Forensic Analysis and Signature Development
T2  - Systematic Approaches to Digital Forensic Engineering (SADFE), 2010 Fifth IEEE International Workshop on
IS  - 
SN  - 
VO  - 
SP  - 109
EP  - 118
AU  - Schmerl, S.
AU  - Vogel, M.
AU  - Rietz, R.
AU  - Ko&#x0308;nig, H.
Y1  - 20-20 May 2010
PY  - 2010
KW  - computer forensics
KW  - data visualisation
KW  - digital signatures
KW  - computers security threats
KW  - explorative log data visualization
KW  - forensic analysis
KW  - intrusion detection systems
KW  - signature development
KW  - Character generation
KW  - Communication networks
KW  - Communication system security
KW  - Data security
KW  - Data visualization
KW  - Digital forensics
KW  - Event detection
KW  - Information analysis
KW  - Information security
KW  - Proposals
KW  - Attack Signatures
KW  - Audit Data Analysis
KW  - Computer Forensic
KW  - Computer Security
KW  - Data Visualization
KW  - Intrusion Detection
KW  - Misuse Detection
VL  - 
JA  - Systematic Approaches to Digital Forensic Engineering (SADFE), 2010 Fifth IEEE International Workshop on
DO  - 10.1109/SADFE.2010.10
AB  - Today's growing number of security threats to computers and networks also increase the importance of log inspections to support the detection of possible breaches. The investigation and assessment of security incidents becomes more and more a daily business. Further, the manual log analysis is essentially in the context of developing signatures for intrusion detection systems (IDS), which allow for an automated defense against security attacks or incidents. But the analysis of log data in the context of fo-rensic investigations and IDS signature development is a tedious and time-consuming task, due to the large amount of textual data. Moreover, this task requires a skilled knowledge to differentiate between the important and the non-relevant information. In this paper, we propose an approach for log resp. audit data representation, which aims at simplifying the analysis process for the security officer. For this purpose audit data and existing relations between audit events are represented graphically in a three-dimensional space. We describe a general approach for analyzing and exploring audit or log data in the context of this presentation paradigm. Further, we introduce our tool, which implements this approach and demonstrate the strengths and benefits of this presentation and exploration form.
ER  - 

TY  - CONF
JO  - Information Technology in Medicine and Education (ITME), 2012 International Symposium on
TI  - Research and design of security audit system for compliance
T2  - Information Technology in Medicine and Education (ITME), 2012 International Symposium on
IS  - 
SN  - 
VO  - 2
SP  - 905
EP  - 909
AU  - Jing Liu
AU  - Xiaoni Wang
AU  - Dongliang Jiao
AU  - Chen Wang
Y1  - 3-5 Aug. 2012
PY  - 2012
KW  - auditing
KW  - design engineering
KW  - security of data
KW  - compliance audit
KW  - information system
KW  - log-based network security audit system
KW  - security audit system design
KW  - security audit system research
KW  - system architecture
KW  - Databases
KW  - Java
KW  - Terminology
KW  - audit analysis
KW  - audit system
KW  - compliance audit
KW  - log
VL  - 2
JA  - Information Technology in Medicine and Education (ITME), 2012 International Symposium on
DO  - 10.1109/ITiME.2012.6291450
AB  - Security audit plays an important role in information system, but currently there are various deficiencies in the security audit systems. This paper elaborates the concept of compliance audit, and then introduces the system architecture and components of a log-based network security audit system for compliance. In the end, the detailed design of key components and technologies used are emphasized. Application results indicate that this system achieves the compliance requirements and can effectively enhance the security of the system being audited.
ER  - 

TY  - CONF
JO  - Artificial Intelligence Applications, 1990., Sixth Conference on
TI  - Security audit trail analysis using inductively generated predictive rules
T2  - Artificial Intelligence Applications, 1990., Sixth Conference on
IS  - 
SN  - 
VO  - 
SP  - 24
EP  - 29 vol.1
AU  - Teng, H.S.
AU  - Chen, K.
AU  - Lu, S.C.-Y.
Y1  - 5-9 May 1990
PY  - 1990
KW  - DP management
KW  - auditing
KW  - computer aided analysis
KW  - inference mechanisms
KW  - learning systems
KW  - security of data
KW  - user modelling
KW  - anomaly detection system
KW  - discriminating capability
KW  - hostile security events
KW  - inductive engine
KW  - inductively generated predictive rules
KW  - rule-based sequential patterns
KW  - security audit trail analysis
KW  - security management
KW  - time-based inductive learning
KW  - user behaviour characterization
KW  - user log-in session
KW  - Computer security
KW  - Data security
KW  - Industrial engineering
KW  - Information security
KW  - Intelligent systems
KW  - Knowledge engineering
KW  - Laboratories
KW  - Manufacturing automation
KW  - Protection
KW  - Systems engineering and theory
VL  - 
JA  - Artificial Intelligence Applications, 1990., Sixth Conference on
DO  - 10.1109/CAIA.1990.89167
AB  - A time-based inductive learning approach to security audit trail analysis is presented. The approach uses a time-based inductive engine to generate rule-based sequential patterns that characterize the behavior of a user. The time-based inductive approach substantially increases the discriminating capability of an anomaly detection system due to the added dimension of information given in the sequential relationships between security events. It is shown that the use of rule-based sequential patterns allows a security auditing system to capture characteristics of user behavior that may be otherwise intractable using traditional statistical approaches. The approach also may help security management to focus on a few potentially hostile security events inside an entire user log-in session
ER  - 

TY  - CONF
JO  - Computer Security Applications Conference, 1989., Fifth Annual
TI  - A model of security monitoring
T2  - Computer Security Applications Conference, 1989., Fifth Annual
IS  - 
SN  - 
VO  - 
SP  - 46
EP  - 52
AU  - Bishop, M.
Y1  - 4-8 Dec 1989
PY  - 1989
KW  - security of data
KW  - auditing
KW  - backups
KW  - file access
KW  - formal model
KW  - logging
KW  - monitoring mechanisms
KW  - query control
KW  - recording information
KW  - security monitoring
KW  - statistical databases
KW  - Computer science
KW  - Computer security
KW  - Computerized monitoring
KW  - Data security
KW  - Educational institutions
KW  - Information analysis
KW  - Information security
KW  - Mathematical model
KW  - Mathematics
KW  - National security
VL  - 
JA  - Computer Security Applications Conference, 1989., Fifth Annual
DO  - 10.1109/CSAC.1989.81024
AB  - A formal model of security monitoring that distinguishes two different methods of recording information (logging) and two different methods of analyzing information (auditing) is presented. From this model, implications for the design and use of security monitoring mechanisms are drawn. The model is then applied to security mechanisms for statistical databases, monitoring mechanisms for computer systems, and backups, in order to demonstrate its usefulness. It is concluded that the proposed model of logging and auditing is comprehensive enough to encompass very different schemes used in a variety of contexts. For example. Statistical database query control and file access monitoring systems do not seem to be related, and yet they create closely related security problems, and the mechanisms designed to improve the security of one will also improve the security of the other
ER  - 

TY  - JOUR
JO  - Security & Privacy, IEEE
TI  - How to Do Application Logging Right
T2  - Security & Privacy, IEEE
IS  - 4
SN  - 1540-7993
VO  - 8
SP  - 82
EP  - 85
AU  - Chuvakin, A.
AU  - Peterson, Gunnar
Y1  - July-Aug. 2010
PY  - 2010
KW  - security of data
KW  - access control
KW  - application logging right
KW  - comprehensive application logging
KW  - protection technology
KW  - security professionals
KW  - Access control
KW  - Application virtualization
KW  - Cloud computing
KW  - Computer crime
KW  - Computer security
KW  - Protection
KW  - Security
KW  - application logging
KW  - audit logs
KW  - cloud computing
KW  - debugging logs
KW  - security and privacy
VL  - 8
JA  - Security & Privacy, IEEE
DO  - 10.1109/MSP.2010.127
AB  - As threats shift toward applications and as more companies struggle with compliance mandates and the limitation of protection technologies such as access control, the need for useful, comprehensive application logging can only increase. This article provides guidance on application logging to application developers and architects and to security professionals.
ER  - 

TY  - CONF
JO  - Trust, Security and Privacy in Computing and Communications (TrustCom), 2013 12th IEEE International Conference on
TI  - Towards Building an Automated Security Compliance Tool for the Cloud
T2  - Trust, Security and Privacy in Computing and Communications (TrustCom), 2013 12th IEEE International Conference on
IS  - 
SN  - 
VO  - 
SP  - 1587
EP  - 1593
AU  - Ullah, K.W.
AU  - Ahmed, A.S.
AU  - Ylitalo, J.
Y1  - 16-18 July 2013
PY  - 2013
KW  - application program interfaces
KW  - business data processing
KW  - cloud computing
KW  - security of data
KW  - trusted computing
KW  - ASCT
KW  - CloudAudit API
KW  - OpenStack cloud platform
KW  - auditing information
KW  - automated security compliance tool
KW  - business requirements
KW  - cloud computing platform
KW  - data collection mechanisms
KW  - enterprise environment
KW  - governmental regulations
KW  - log analysis
KW  - manual entry
KW  - trust
KW  - vulnerability scanning
KW  - Cloud computing
KW  - Engines
KW  - Manuals
KW  - Ports (Computers)
KW  - Security
KW  - Servers
KW  - Standards
KW  - Cloud Audit
KW  - Cloud Control Matrix (CCM)
KW  - OpenStack
KW  - Security Compliance
VL  - 
JA  - Trust, Security and Privacy in Computing and Communications (TrustCom), 2013 12th IEEE International Conference on
DO  - 10.1109/TrustCom.2013.195
AB  - Security, especially security compliance, is a major concern that is slowing down large scale adoption of cloud computing in the enterprise environment. Governmental regulations, business requirements and trust are among the reasons why enterprises require certain levels of security compliance from cloud providers. So far, security compliance or auditing information has been generated manually by security specialists. This involves manual data collection and assessment, which is slow and expensive. Thus, there is a need for an automated security compliance tool (ASCT) to verify and express the compliance of various cloud providers. Such a tool can reduce the human intervention and eventually reduce the cost and time by verifying the compliance automatically. Also, the tool will enable transparency of the cloud vendors to the customers which in turn will help grow confidence on the cloud vendors. Having these goals in mind, we have developed an architecture to build an ASCT for a cloud computing platform. We have also outlined four possible approaches to achieve this automation. These possible four approaches refer to four data collection mechanisms to collect data from the cloud systems and these are: API, vulnerability scanning, log analysis and manual entry. Finally, we have implemented a proof-of-concept prototype of this ASCT based on the proposed architecture. The prototype is integrated with OpenStack cloud platform and the results are exposed using the CloudAudit API.
ER  - 

TY  - CONF
JO  - Circuit, Power and Computing Technologies (ICCPCT), 2015 International Conference on
TI  - Cloud based security solution for android smartphones
T2  - Circuit, Power and Computing Technologies (ICCPCT), 2015 International Conference on
IS  - 
SN  - 
VO  - 
SP  - 1
EP  - 6
AU  - Marengereke, T.M.
AU  - Sornalakshmi, K.
Y1  - 19-20 March 2015
PY  - 2015
KW  - Android (operating system)
KW  - cloud computing
KW  - data visualisation
KW  - firewalls
KW  - portals
KW  - security of data
KW  - smart phones
KW  - statistical analysis
KW  - system monitoring
KW  - Android security monitoring
KW  - Android smartphones
KW  - IPTABLES firewall
KW  - NETFILTER firewall
KW  - SIEM
KW  - Web interface
KW  - Web portal auditing
KW  - abnormal behavior detection
KW  - abnormal network traffic
KW  - application logs
KW  - cloud based security solution
KW  - firewall rules
KW  - intrusion mitigation solution
KW  - log visualization
KW  - statistics
KW  - Androids
KW  - Humanoid robots
KW  - IP networks
KW  - Malware
KW  - Servers
KW  - Smart phones
KW  - Android
KW  - Cloud offloading
KW  - Mobile Security Audit
KW  - Protocol analysis
KW  - Security Information Management
KW  - Security Monitoring
VL  - 
JA  - Circuit, Power and Computing Technologies (ICCPCT), 2015 International Conference on
DO  - 10.1109/ICCPCT.2015.7159512
AB  - In this paper, we define SIEM and we discuss Android security monitoring as well as recent research in Android security systems. Then, we propose a cloud based security system for collection, visualization, analysis and correlation of application logs, statistics and determining abnormal application and network behavior on the device. If abnormal behavior is detected an appropriate alert is sent back to the device for remedial action. In the case of abnormal network traffic, then firewall rules to be updated on an implementation of an IPTABLES/ NETFILTER firewall to block unwarranted network traffic. Furthermore a web interface is created to enable visualization of logs and all data collected from the device. So it serves as an intrusion mitigation solution coupled with security information audit web portal. This paper highlights the architecture of the proposed system.
ER  - 

TY  - CONF
JO  - Advance Computing Conference (IACC), 2013 IEEE 3rd International
TI  - MapReduce based log file analysis for system threats and problem identification
T2  - Advance Computing Conference (IACC), 2013 IEEE 3rd International
IS  - 
SN  - 
VO  - 
SP  - 831
EP  - 835
AU  - Vernekar, S.S.
AU  - Buchade, A.
Y1  - 22-23 Feb. 2013
PY  - 2013
KW  - data analysis
KW  - parallel processing
KW  - security of data
KW  - MapReduce based log file analysis
KW  - problem identification
KW  - security alert
KW  - security threat
KW  - security warning
KW  - system threat
KW  - Algorithm design and analysis
KW  - Clustering algorithms
KW  - Computers
KW  - Conferences
KW  - Context
KW  - Correlation
KW  - Security
KW  - Event Correlation
KW  - Log File analysis
KW  - MapReduce
VL  - 
JA  - Advance Computing Conference (IACC), 2013 IEEE 3rd International
DO  - 10.1109/IAdCC.2013.6514334
AB  - Log files are primary source of information for identifying the System threats and problems that occur in the System at any point of time. These threats and problem in the system can be identified by analyzing the log file and finding the patterns for possible suspicious behavior. The concern administrator can then be provided with appropriate alter or warning regarding these security threats and problems in the system, which are generated after the log files are analyzed. Based upon this alters or warnings the administrator can take appropriate actions. Many tools or approaches are available for this purpose, some are proprietary and some are open source. This paper presents a new approach which uses a MapReduce algorithm for the purpose of log file analysis, providing appropriate security alerts or warning. The results of this system can then be compared with the tools available.
ER  - 

TY  - CONF
JO  - Availability, Reliability and Security, 2008. ARES 08. Third International Conference on
TI  - Histogram Matrix: Log File Visualization for Anomaly Detection
T2  - Availability, Reliability and Security, 2008. ARES 08. Third International Conference on
IS  - 
SN  - 
VO  - 
SP  - 610
EP  - 617
AU  - Frei, A.
AU  - Rennhard, M.
Y1  - 4-7 March 2008
PY  - 2008
KW  - data visualisation
KW  - matrix algebra
KW  - security of data
KW  - statistical analysis
KW  - anomaly detection
KW  - histogram matrix
KW  - interactive search
KW  - intrusion detection system
KW  - log file visualization
KW  - statistical technique
KW  - Availability
KW  - Data security
KW  - Data visualization
KW  - Event detection
KW  - Histograms
KW  - Humans
KW  - Information analysis
KW  - Information security
KW  - Information technology
KW  - Intrusion detection
VL  - 
JA  - Availability, Reliability and Security, 2008. ARES 08. Third International Conference on
DO  - 10.1109/ARES.2008.148
AB  - In today's IT environments, there is an ever increasing demand for log file analysis solutions. Log files often contain important information about possible incidents, but inspecting the often large amounts of textual data is too time-consuming and tedious a task to perform manually. To address this issue, we propose a novel log file visualization technique called Histogram Matrix (HMAT). HMAT visualizes the content of a log file in order to enable a security administrator to efficiently spot anomalies. The system uses a combination of graphical and statistical techniques and allows even non-experts to interactively search for anomalous log messages. Contrary to other approaches, our proposal does not only work on certain special kinds of log files, but instead works on almost every textual log file. Additionally, the system allows to automatically generate security events if an anomaly is detected, similar to anomaly-based intrusion detection systems. This paper introduces HMAT, demonstrates its functionality using log files from a variety of services in real environments, and identifies strengths and limitations of the technique.
ER  - 

TY  - CONF
JO  - Measurement Systems for Homeland Security, Contraband Detection and Personal Safety Workshop, 2005. (IMS 2005) Proceedings of the 2005 IEEE International Workshop on
TI  - Implementation of an electronic media security system
T2  - Measurement Systems for Homeland Security, Contraband Detection and Personal Safety Workshop, 2005. (IMS 2005) Proceedings of the 2005 IEEE International Workshop on
IS  - 
SN  - 
VO  - 
SP  - 35
EP  - 41
AU  - Silvers, K.L.
AU  - Burghard, B.J.
AU  - Skorpik, J.R.
AU  - Clark, D.A.
Y1  - 29-30 March 2005
PY  - 2005
KW  - identification technology
KW  - security of data
KW  - storage management
KW  - storage media
KW  - Department of Energy Laboratories
KW  - access events
KW  - classified removable electronic media storage
KW  - electronic media security system
KW  - electronic tag
KW  - electronically readable identification code
KW  - security lapses
KW  - weekly inventory collection
KW  - weekly inventory reporting system
KW  - Alarm systems
KW  - Containers
KW  - Costs
KW  - Data security
KW  - Documentation
KW  - Energy storage
KW  - Humans
KW  - Laboratories
KW  - National security
KW  - Secure storage
VL  - 
JA  - Measurement Systems for Homeland Security, Contraband Detection and Personal Safety Workshop, 2005. (IMS 2005) Proceedings of the 2005 IEEE International Workshop on
DO  - 10.1109/MSHS.2005.1502552
AB  - Recent security lapses within the Department of Energy Laboratories prompted the establishment and implementation of additional procedures and training for operations involving classified removable electronic media (CREM) storage. In addition, the definition of CREM has been expanded and the number of CREM has increased significantly. Procedures now require that all CREM be inventoried and accounted for on a weekly basis. Weekly inventories consist of a physical comparison of each item against the reportable inventory listing. Securing and accounting for CREM is a continuous challenge for existing security systems. To address the challenge, Pacific Northwest National Laboratory (PNNL) has developed an automated electronic media security system (EMSS) for a weekly CREM inventory collection and reporting system. The EMSS approach is to tag the CREM with an electronically readable unique identification code and automatically collect data on the inventory in each security container or vault at a user-defined interval and upon detection of an access event, thus eliminating the need for hand-written inventory sheets while allowing automated transfer of the collected inventory data to an electronic reporting system. An electronic log of CREM access events is maintained, providing enhanced accountability for daily/weekly checks, routine audits, and follow-up investigations. The key attributes of the EMSS include improved accountability, reduced risk of human error, improved accuracy and timeliness of inventory data, and reduced costs as a result of man-hour reductions.
ER  - 

TY  - CONF
JO  - Computer Science and Engineering, 2009. WCSE '09. Second International Workshop on
TI  - A Logging Scheme for Database Audit
T2  - Computer Science and Engineering, 2009. WCSE '09. Second International Workshop on
IS  - 
SN  - 
VO  - 2
SP  - 390
EP  - 393
AU  - Qiang Huang
AU  - Lianzhong Liu
Y1  - 28-30 Oct. 2009
PY  - 2009
KW  - auditing
KW  - computer networks
KW  - data recording
KW  - database management systems
KW  - protocols
KW  - security of data
KW  - system monitoring
KW  - data storage
KW  - database audit
KW  - database communication protocols
KW  - database security
KW  - logging scheme
KW  - network traffic analysis
KW  - packets capturing
KW  - packets parsing
KW  - Computer science
KW  - Data analysis
KW  - Data engineering
KW  - Data security
KW  - Database systems
KW  - Monitoring
KW  - Protocols
KW  - Relational databases
KW  - Spatial databases
KW  - Telecommunication traffic
KW  - database audit
KW  - logging
KW  - network traffic analyzing
VL  - 2
JA  - Computer Science and Engineering, 2009. WCSE '09. Second International Workshop on
DO  - 10.1109/WCSE.2009.837
AB  - Database audit can strengthen the security of database. Logging database activities is usually the first step of implementing database audit. In this paper, we present a logging scheme for database audit. Unlike native database logging and auditing mechanism, our scheme is to monitor and log database activities through analyzing network traffic. The architecture of our scheme contains three principal components: packets capturing, packets parsing and data storage. First capture the packets to and from the database; then, by analyzing database communication protocols, parse the captured packets; finally, use the parsed results to support database audit.
ER  - 

TY  - CONF
JO  - Services Computing (SCC), 2014 IEEE International Conference on
TI  - CDCAS: A Novel Cloud Data Center Security Auditing System
T2  - Services Computing (SCC), 2014 IEEE International Conference on
IS  - 
SN  - 
VO  - 
SP  - 605
EP  - 612
AU  - Xueying Wang
AU  - Jun Zhang
AU  - Mingbo Wang
AU  - Lijun Zu
AU  - Zhihui Lu
AU  - Jie Wu
Y1  - June 27 2014-July 2 2014
PY  - 2014
KW  - auditing
KW  - cloud computing
KW  - computer centres
KW  - security of data
KW  - virtualisation
KW  - CDCAS
KW  - cloud computing
KW  - cloud data center security auditing system
KW  - correlative analysis algorithm
KW  - log analysis model
KW  - security event extraction
KW  - signature based method
KW  - virtualization technology
KW  - Algorithm design and analysis
KW  - Cloud computing
KW  - Distributed databases
KW  - Security
KW  - Servers
KW  - Transforms
KW  - autonomous rule
KW  - cloud data center
KW  - cloud security auditing
KW  - log collection and analysis
VL  - 
JA  - Services Computing (SCC), 2014 IEEE International Conference on
DO  - 10.1109/SCC.2014.85
AB  - With the increasing acceptance of cloud data center and virtualization technology by enterprises and industries, the security concern becomes the key hindrance to the development and deployment of cloud computing. Security auditing is a good way to deal with the threats faced by a cloud data center. But traditional auditing is no longer suitable for the new cloud environment. In this paper, we design, implement and evaluate the CDCAS, a novel cloud data center auditing system, which matches the demand of the scalability and efficiency of a cloud data center. In this system, we design one distributed and autonomous agent model which can be controlled by a set of rules dynamically generated to fit its use scenario. We then build the log analysis model which uses the signature based method and correlative analysis algorithm to extract security events from collected log with agreeable false positives. We evaluate our system both on real world and simulation to validate its efficiency. And our system is also deployed by the cloud data center of a well-known financial institution, and performs well.
ER  - 

TY  - CONF
JO  - Computer Security Applications Conference, 21st Annual
TI  - A user-level framework for auditing and monitoring
T2  - Computer Security Applications Conference, 21st Annual
IS  - 
SN  - 1063-9527
VO  - 
SP  - 11 pp.
EP  - 105
AU  - Wu Yongzheng
AU  - Yap, R.H.C.
Y1  - 5-9 Dec. 2005
PY  - 2005
KW  - security of data
KW  - supervisory programs
KW  - system monitoring
KW  - user interfaces
KW  - arbitrary user-level monitor
KW  - monitor use policy
KW  - potential security problem detection
KW  - programmable user-level monitor
KW  - setuid process confidentiality
KW  - system auditing
KW  - system facility
KW  - system logging
KW  - system operation monitoring
KW  - system security
KW  - user-level auditing application
KW  - Application software
KW  - Computer architecture
KW  - Computer displays
KW  - Computer security
KW  - Computerized monitoring
KW  - Costs
KW  - Intrusion detection
KW  - Kernel
KW  - Maintenance
KW  - National security
VL  - 
JA  - Computer Security Applications Conference, 21st Annual
DO  - 10.1109/CSAC.2005.8
AB  - Logging and auditing is an important system facility for monitoring correct system operation and for detecting potential security problems. We present an architecture for implementing user-level auditing monitors which: (i) does not require superuser privileges; (ii) makes it simple to create user defined monitors which are transparent; and (iii) provides security guarantees such as mandatory and reliable monitoring while maintaining confidentiality of setuid processes. We avoid problems of self-referential monitoring. Monitor use policies can be specified to increase flexibility. We show that our framework can be tailored so that it is very efficient with low overhead on macro and micro benchmarks. This demonstrates that it is feasible to make use of arbitrary and programmable user-level monitors for system security and auditing applications
ER  - 

TY  - CONF
JO  - Computer Sciences and Convergence Information Technology, 2009. ICCIT '09. Fourth International Conference on
TI  - A Framework for Database Auditing
T2  - Computer Sciences and Convergence Information Technology, 2009. ICCIT '09. Fourth International Conference on
IS  - 
SN  - 
VO  - 
SP  - 982
EP  - 986
AU  - Lianzhong Liu
AU  - Qiang Huang
Y1  - 24-26 Nov. 2009
PY  - 2009
KW  - database management systems
KW  - security of data
KW  - audit analysis
KW  - database auditing
KW  - database security
KW  - event correlation
KW  - network traffic
KW  - security regulation detection
KW  - third-party auditing component
KW  - Access control
KW  - Data analysis
KW  - Data engineering
KW  - Data security
KW  - Database systems
KW  - Information security
KW  - Monitoring
KW  - Relational databases
KW  - Spatial databases
KW  - Telecommunication traffic
KW  - audit analysis
KW  - database auditing
KW  - logging
VL  - 
JA  - Computer Sciences and Convergence Information Technology, 2009. ICCIT '09. Fourth International Conference on
DO  - 10.1109/ICCIT.2009.207
AB  - Database auditing can help strengthen the security of database. In this paper, we present a framework of database auditing, which log the database activities through analyzing network traffic, execute audit analysis through event correlation and generate alarms if an anomaly or a violation of security regulations is detected. Compared with native auditing mechanism in database, our approach has an obvious advantage of providing zero-impact to the performance of the database or the applications that access it. In addition, using third-party auditing component complies with the principle of separation of duties.
ER  - 

TY  - CONF
JO  - Machine Learning and Cybernetics, 2003 International Conference on
TI  - Wavelet based data mining and querying in network security databases
T2  - Machine Learning and Cybernetics, 2003 International Conference on
IS  - 
SN  - 
VO  - 1
SP  - 178
EP  - 182 Vol.1
AU  - Wu Liu
AU  - Hai-Xin Duan
AU  - Ping Ren
AU  - Xing Li
AU  - Jian-Ping Wu
Y1  - 2-5 Nov. 2003
PY  - 2003
KW  - computer networks
KW  - data mining
KW  - database management systems
KW  - security of data
KW  - wavelet transforms
KW  - computer forensics
KW  - content-based queries
KW  - intrusion detection
KW  - network security databases
KW  - time-serial database
KW  - wavelet based data mining
KW  - wavelet transform analysis algorithm
KW  - Computer hacking
KW  - Computer networks
KW  - Computer security
KW  - Computer worms
KW  - Data mining
KW  - Data security
KW  - Databases
KW  - Forensics
KW  - Intrusion detection
KW  - Wavelet analysis
VL  - 1
JA  - Machine Learning and Cybernetics, 2003 International Conference on
DO  - 10.1109/ICMLC.2003.1264466
AB  - The phenomenal increase in the amounts of network security data are due to the hacker attacks, virus, worm and Slapper etc. Network security log databases are very important in intrusion detection and computer forensics. A lot of data mining methods to research it have been found. Fast and accurate retrievals for content-based queries are crucial for such numerous database systems to be useful. In this paper, a new method is provided to analyze and mine this kind of time-serial database. After signalize the NSD databases, we first represent a DWT wavelet transform analysis algorithm, then present two wavelet-based algorithms GET&#095;INDICES and QUERY for querying the complex and numerous NSD, and finally give the experimental result using these algorithms.
ER  - 

TY  - CONF
JO  - Information Security and Assurance, 2008. ISA 2008. International Conference on
TI  - A Secure Virtualized Logging Scheme for Digital Forensics in Comparison with Kernel Module Approach
T2  - Information Security and Assurance, 2008. ISA 2008. International Conference on
IS  - 
SN  - 
VO  - 
SP  - 421
EP  - 426
AU  - Bin-Hui Chou
AU  - Tatara, K.
AU  - Sakuraba, T.
AU  - Hori, Y.
AU  - Sakurai, K.
Y1  - 24-26 April 2008
PY  - 2008
KW  - computer crime
KW  - security of data
KW  - system monitoring
KW  - Log files
KW  - criminal method
KW  - digital forensics
KW  - integrity property
KW  - kernel module approach
KW  - perpetrator
KW  - virtualized logging scheme
KW  - Digital forensics
KW  - File systems
KW  - History
KW  - Information analysis
KW  - Information science
KW  - Information security
KW  - Kernel
KW  - Laboratories
KW  - Proposals
KW  - Virtual machining
KW  - digital forensics
KW  - log integrity
KW  - virtualization
VL  - 
JA  - Information Security and Assurance, 2008. ISA 2008. International Conference on
DO  - 10.1109/ISA.2008.96
AB  - Digital forensics encompasses the process of identifying the perpetrator and the criminal method by analyzing the logs generated in the computer. Log files record the activities of the computer and by reading them one can know what kind of event happened at a certain time. Therefore, secure logs with the integrity property are essential. In this paper, we discuss two approaches to achieve the integrity of logs- the kernel module and virtualization, and compare them. Although virtualization is more inefficient in performance than the kernel module, it provides more security properties for logs. Thus, we then focus on the virtualization approach with a detailed proposal, which describes ways to preserve logs without tampering and deletion.
ER  - 

TY  - CONF
JO  - Dependable Systems and Networks (DSN), 2014 44th Annual IEEE/IFIP International Conference on
TI  - Reliability and Security Monitoring of Virtual Machines Using Hardware Architectural Invariants
T2  - Dependable Systems and Networks (DSN), 2014 44th Annual IEEE/IFIP International Conference on
IS  - 
SN  - 
VO  - 
SP  - 13
EP  - 24
AU  - Cuong Pham
AU  - Estrada, Z.
AU  - Phuong Cao
AU  - Kalbarczyk, Z.
AU  - Iyer, R.K.
Y1  - 23-26 June 2014
PY  - 2014
KW  - monitoring
KW  - reliability
KW  - security of data
KW  - virtual machines
KW  - GOSHD
KW  - Guest OS Hang Detection
KW  - HRKD
KW  - Hyper Tap
KW  - PED
KW  - active monitoring
KW  - fault injection
KW  - hardware architectural invariants
KW  - hidden root kit detection
KW  - hyper visor-level framework
KW  - privilege escalation detection
KW  - reliability
KW  - robust monitoring
KW  - security monitoring framework
KW  - virtual machines
KW  - virtualization environments
KW  - Data structures
KW  - Hardware
KW  - Kernel
KW  - Monitoring
KW  - Reliability
KW  - Security
KW  - Virtual machine monitors
KW  - Fault Injection
KW  - Hypervisor
KW  - Invariant
KW  - Monitoring
KW  - Reliability
KW  - Rootkit
KW  - Security
VL  - 
JA  - Dependable Systems and Networks (DSN), 2014 44th Annual IEEE/IFIP International Conference on
DO  - 10.1109/DSN.2014.19
AB  - This paper presents a solution that simultaneously addresses both reliability and security (RnS) in a monitoring framework. We identify the commonalities between reliability and security to guide the design of Hyper Tap, a hyper visor-level framework that efficiently supports both types of monitoring in virtualization environments. In Hyper Tap, the logging of system events and states is common across monitors and constitutes the core of the framework. The audit phase of each monitor is implemented and operated independently. In addition, Hyper Tap relies on hardware invariants to provide a strongly isolated root of trust. Hyper Tap uses active monitoring, which can be adapted to enforce a wide spectrum of RnS policies. We validate Hyper Tap by introducing three example monitors: Guest OS Hang Detection (GOSHD), Hidden Root Kit Detection (HRKD), and Privilege Escalation Detection (PED). Our experiments with fault injection and real root kits/exploits demonstrate that Hyper Tap provides robust monitoring with low performance overhead.
ER  - 

TY  - CONF
JO  - Cloud Computing Technologies, Applications and Management (ICCCTAM), 2012 International Conference on
TI  - Security issues and control mechanisms in Cloud
T2  - Cloud Computing Technologies, Applications and Management (ICCCTAM), 2012 International Conference on
IS  - 
SN  - 
VO  - 
SP  - 74
EP  - 76
AU  - Surianarayanan, S.
AU  - Santhanam, T.
Y1  - 8-10 Dec. 2012
PY  - 2012
KW  - cloud computing
KW  - security of data
KW  - storage management
KW  - cloud service provider
KW  - control mechanism
KW  - infinite resource availability
KW  - log files
KW  - private cloud
KW  - security issue
KW  - storage resources
KW  - virtualization technology
KW  - Cloud Computing
KW  - Compliance
KW  - Control Mechanisms
KW  - Private cloud
KW  - Regulations
KW  - Security
VL  - 
JA  - Cloud Computing Technologies, Applications and Management (ICCCTAM), 2012 International Conference on
DO  - 10.1109/ICCCTAM.2012.6488075
AB  - The responsibility of providing cost-effective, elastic and on-demand access to computing and storage resources lies with the Cloud Service Providers. As for the end-user, it provides an abstraction of reliable and infinite resource availability. Infinite resource availability is based on virtualization technology and availability of a shared pool of computational and storage resources. It is this very nature of the cloud - of being a black box to the end user - that is posing a threat to its faster adoption and growth in the market. Several issues have been raised on the aspects of legal, security and compliance requirements in the public cloud. Legal issues concern with the applicable laws according to the physical location of data. Regulatory issues such as provision of log files provided by the Cloud Service Provider (CSP) for audits. Security issues are concerned with the safety of cloud environment against any form of security attacks. Other major concerns include, but are not limited to - availability, recovery and privacy. Identifying measures and control mechanisms to address and resolve such issues could help in standardizing the Cloud environment from the Cloud Service Provider's end. This paper explores the major issues identified in the Private cloud and the existing control mechanisms.
ER  - 

TY  - CONF
JO  - Geoinformatics, 2010 18th International Conference on
TI  - Log management approach in three-dimensional spatial data management system
T2  - Geoinformatics, 2010 18th International Conference on
IS  - 
SN  - 
VO  - 
SP  - 1
EP  - 5
AU  - Jing Li
AU  - Xing Li
AU  - Gang Liu
AU  - Zhenwen He
Y1  - 18-20 June 2010
PY  - 2010
KW  - security of data
KW  - visual databases
KW  - 3D spatial data management system
KW  - Oracle database tables
KW  - data auditing
KW  - data monitoring
KW  - developer productivity
KW  - log management approach
KW  - spatial data management security
KW  - Concrete
KW  - File systems
KW  - Geology
KW  - Geospatial analysis
KW  - Registers
KW  - Spatial databases
KW  - Oracle
KW  - file
KW  - log management
KW  - three-dimensional spatial data
VL  - 
JA  - Geoinformatics, 2010 18th International Conference on
DO  - 10.1109/GEOINFORMATICS.2010.5568028
AB  - Log management is essential to register the three-dimensional spatial data. It plays the role of auditing and monitoring in the system. The traditional method of log management is to set up a management module which records the users' manipulations respectively for each function. Though it's easy to apply such a management system, it consumes more time and energy. Another method is only to use the triggers to record the users' operations. But if this method is adopted, the information would be lost before the triggers were created manually. Thus it's necessary to design an efficient system to improve this situation. This log management designed in this paper introduces the stored procedures and triggers to design a log system to store and manage the three-dimensional spatial data. The stored procedure is used to create triggers and Oracle database tables which are applied for recording information. Triggers would be fired when users do anything to the records stored in the Oracle database tables. It also sets the switches to disable and enable triggers timely to release resources. All the tables and triggers are created dynamically by procedures. Once users created a user account on the Oracle database, the system would create all tables and triggers by calling the procedure which written in the system package. It can save a lot of time for the user and record all the operations in case users damage the data before the tables were created. Oracle database records the concrete operations, including operate-times, operate-types, Ips, usernames, etc. And the log files of this system are designed especially for recording each detail of the operations. The administrator is able to do the recovery operation according to the log management system. So it is essential to use the log management system to insure the safety of the three-dimensional spatial data. This system cuts down the programmers' workload for writing duplicate codes. It is not only an improvement to th- - e Oracle database application performance for the three-dimensional spatial data management and the developer productivity, but also a consolidation to the three-dimensional spatial data management security.
ER  - 

TY  - CONF
JO  - Network and Parallel Computing Workshops, 2007. NPC Workshops. IFIP International Conference on
TI  - A Log Analysis Audit Model Based on Optimized Clustering Algorithm
T2  - Network and Parallel Computing Workshops, 2007. NPC Workshops. IFIP International Conference on
IS  - 
SN  - 
VO  - 
SP  - 841
EP  - 848
AU  - Hui Yu
AU  - Xingjian Shi
Y1  - 18-21 Sept. 2007
PY  - 2007
KW  - auditing
KW  - pattern clustering
KW  - security of data
KW  - cluster number
KW  - network attack type
KW  - optimized clustering algorithm
KW  - security log analysis audit model
KW  - unknown intrusion detection
KW  - Algorithm design and analysis
KW  - Application software
KW  - Automatic control
KW  - Clustering algorithms
KW  - Computer science
KW  - Computer security
KW  - Data mining
KW  - Intrusion detection
KW  - Parallel processing
KW  - Protection
VL  - 
JA  - Network and Parallel Computing Workshops, 2007. NPC Workshops. IFIP International Conference on
DO  - 10.1109/NPC.2007.116
AB  - In view of the problem how to detect the network unknown attacks, a security log analysis audit model based on optimized clustering algorithm is proposed in this paper. Since the main question which influence the clustering algorithm application in the log analysis is uneasy to determine the network attack type and the cluster number, so we bring forward an optimized cluster algorithm to solve this problem. By means of simulated experiments, this algorithm is proved feasible, efficient and extensible for unknown intrusion detection.
ER  - 

TY  - CONF
JO  - Information Visualisation, 2002. Proceedings. Sixth International Conference on
TI  - Tudumi: information visualization system for monitoring and auditing computer logs
T2  - Information Visualisation, 2002. Proceedings. Sixth International Conference on
IS  - 
SN  - 1093-9547
VO  - 
SP  - 570
EP  - 576
AU  - Takada, T.
AU  - Koike, H.
Y1  - 2002
PY  - 2002
KW  - data visualisation
KW  - security of data
KW  - Tudumi
KW  - computer logs auditing
KW  - computer logs monitoring
KW  - computer security breaches
KW  - information visualization system
KW  - log summarization
KW  - log visualization system
KW  - textual data
KW  - Computer security
KW  - Computerized monitoring
KW  - Data visualization
KW  - Humans
KW  - Information security
KW  - Information systems
KW  - Satellites
VL  - 
JA  - Information Visualisation, 2002. Proceedings. Sixth International Conference on
DO  - 10.1109/IV.2002.1028831
AB  - Computer security breaches are already a major problem in using computers. The most basic defense against it is to monitor and audit the computer logs. Computer logs, however have a huge amount of textual data. It is, therefore, almost impossible to inspect them manually using current systems. We propose a log visualization system called "Tudumi". Tudumi consists of several functions which assist system administrators to perform such tasks manually. These functions are information visualization, log summarization and reflecting known rules into the visualization method. Tudumi makes it easier to detect anomalous user activities, such as intrusion, from a huge amount of computer logs.
ER  - 

TY  - CONF
JO  - Parallel and Distributed Processing with Applications, 2009 IEEE International Symposium on
TI  - Secure Logging for Auditable File System Using Separate Virtual Machines
T2  - Parallel and Distributed Processing with Applications, 2009 IEEE International Symposium on
IS  - 
SN  - 
VO  - 
SP  - 153
EP  - 160
AU  - Siqin Zhao
AU  - Kang Chen
AU  - Weimin Zheng
Y1  - 10-12 Aug. 2009
PY  - 2009
KW  - file organisation
KW  - security of data
KW  - system monitoring
KW  - virtual machines
KW  - auditable file system
KW  - logging virtual machine
KW  - malware
KW  - on-disk structure
KW  - operating system
KW  - secure logging
KW  - working virtual machine
KW  - Application software
KW  - Computer science
KW  - Distributed processing
KW  - File systems
KW  - Frequency
KW  - Hardware
KW  - Information systems
KW  - Operating systems
KW  - Virtual machining
KW  - Web pages
KW  - Auditable File System
KW  - File System Logging
KW  - Virtual Machines
VL  - 
JA  - Parallel and Distributed Processing with Applications, 2009 IEEE International Symposium on
DO  - 10.1109/ISPA.2009.32
AB  - Auditable file system is used to track the usage of the file system including the operations like read and write. Auditable file system keeps the trails of userspsila action and the trails are kept faithfully for future auditing. However, as the logs are still kept within the same file system, it will be quite vulnerable to be exposed as malware penetrating the system. Even with the file system hiding the logs, the skillful attacker can still analyze the on-disk structure to get and modify the logs. Thus the logs should be kept separate from the working system. Virtual machines can provide such separation as virtual machines can hold the whole operating system while still keep the system apart from the metal hardware. We propose a method of secure logging for auditable file system using a logging virtual machine. The logs are kept in another virtual machine safely. Even the working virtual machine is broken; the logs are not exposed to the outside. By the isolation provided by virtual machines, the logs can be kept safe and valid. The high privileged user can not modify the logs contents, or forge the logs and data to keep consistency, or pretend to be another user for doing un-authorized actions. We have done several works as well as a prototype system to show the feasibility of such approach. Experiments show that the logging virtual machine will not bring too much overhead.
ER  - 

TY  - CONF
JO  - Systematic Approaches to Digital Forensic Engineering, 2008. SADFE '08. Third International Workshop on
TI  - Computer Forensics in Forensis
T2  - Systematic Approaches to Digital Forensic Engineering, 2008. SADFE '08. Third International Workshop on
IS  - 
SN  - 
VO  - 
SP  - 102
EP  - 122
AU  - Peisert, S.
AU  - Bishop, M.
AU  - Marzullo, K.
Y1  - 22-22 May 2008
PY  - 2008
KW  - security of data
KW  - computer forensic systems
KW  - forensic data
KW  - forensis
KW  - valid forensic analysis
KW  - Chemical analysis
KW  - Computer errors
KW  - Computer science
KW  - DNA computing
KW  - Digital forensics
KW  - Educational institutions
KW  - Law
KW  - Legal factors
KW  - Terrorism
KW  - Testing
KW  - Auditing
KW  - Laocoon
KW  - data measurement
KW  - forensic analysis
KW  - forensic systems
KW  - law
KW  - legal procedure
KW  - logging
KW  - models
KW  - scientific method
KW  - security
VL  - 
JA  - Systematic Approaches to Digital Forensic Engineering, 2008. SADFE '08. Third International Workshop on
DO  - 10.1109/SADFE.2008.18
AB  - Different users apply computer forensic systems, models, and terminology in very different ways. They often make incompatible assumptions and reach different conclusions about the validity and accuracy of the methods they use to log, audit, and present forensic data. This is problematic, because these fields are related, and results from one can be meaningful to the others. We present several forensic systems and discuss situations in which they produce valid and accurate conclusions and also situations in which their accuracy is suspect. We also present forensic models and discuss areas in which they are useful and areas in which they could be augmented. Finally, we present some recommendations about how computer scientists, forensic practitioners, lawyers, and judges could build more complete models of forensics that take into account appropriate legal details and lead to scientifically valid forensic analysis.
ER  - 

TY  - CONF
JO  - Information Communication and Embedded Systems (ICICES), 2013 International Conference on
TI  - Network activity classification schema in IDS and log audit for cloud computing
T2  - Information Communication and Embedded Systems (ICICES), 2013 International Conference on
IS  - 
SN  - 
VO  - 
SP  - 502
EP  - 506
AU  - Sathya, G.
AU  - Vasanthraj, K.
Y1  - 21-22 Feb. 2013
PY  - 2013
KW  - cloud computing
KW  - data mining
KW  - pattern classification
KW  - security of data
KW  - virtual machines
KW  - IDS
KW  - Internet computing
KW  - anomaly detection method
KW  - cloud computing
KW  - data mining technique
KW  - log audit
KW  - multilevel intrusion detection system
KW  - network activity classification schema
KW  - virtual machine
KW  - Cloud computing
KW  - Hardware
KW  - Intrusion detection
KW  - Monitoring
KW  - Servers
KW  - Cloud Computing
KW  - Cooperative IDS
KW  - IDS
KW  - Intrusion Detection
KW  - Multi- level IDS
VL  - 
JA  - Information Communication and Embedded Systems (ICICES), 2013 International Conference on
DO  - 10.1109/ICICES.2013.6508322
AB  - Cloud computing plays an important role in web .The term cloud computing is based on internet computing. Cloud computing is a service to perform tasks through internet. It provides large scale of resource to each user using virtual machine. Cloud computing is easily threatened by the users because all users are not trustworthy. This is the first problem. The second problem in cloud computing is number of users can use the service but it makes the admin to analyze the log. In this paper ,we propose the multilevel intrusion detection system and log audit for detecting the attacks and maintain the security level and effectiveness of the system. The intrusion detection system is based on data mining techniques and anomaly detection method.
ER  - 

TY  - CONF
JO  - Data Engineering (ICDE), 2013 IEEE 29th International Conference on
TI  - SELECT triggers for data auditing
T2  - Data Engineering (ICDE), 2013 IEEE 29th International Conference on
IS  - 
SN  - 1063-6382
VO  - 
SP  - 1141
EP  - 1152
AU  - Fabbri, D.
AU  - Ramamurthy, R.
AU  - Kaushik, R.
Y1  - 8-12 April 2013
PY  - 2013
KW  - SQL
KW  - data analysis
KW  - database management systems
KW  - query processing
KW  - security of data
KW  - HIPAA
KW  - SELECT query
KW  - SELECT triggers
KW  - SQL query
KW  - TPC-H benchmark
KW  - UPDATE query
KW  - commercial database systems
KW  - data auditing
KW  - experimental evaluation
KW  - implementation techniques
KW  - security infrastructure
KW  - sensitive data
KW  - tracking accesses
KW  - Cancer
KW  - Database systems
KW  - Diseases
KW  - Security
KW  - Semantics
VL  - 
JA  - Data Engineering (ICDE), 2013 IEEE 29th International Conference on
DO  - 10.1109/ICDE.2013.6544904
AB  - Auditing is a key part of the security infrastructure in a database system. While commercial database systems provide mechanisms such as triggers that can be used to track and log any changes made to &#x201C;sensitive&#x201D; data using UPDATE queries, they are not useful for tracking accesses to sensitive data using complex SQL queries, which is important for many applications given recent laws such as HIPAA. In this paper, we propose the notion of SELECT triggers that extends triggers to work for SELECT queries in order to facilitate data auditing. We discuss the challenges in integrating SELECT triggers in a database system including specification, semantics as well as efficient implementation techniques. We have prototyped our framework in a commercial database system and present an experimental evaluation of our framework using the TPC-H benchmark.
ER  - 

TY  - CONF
JO  - Security and Privacy, 2008. SP 2008. IEEE Symposium on
TI  - Secure Web Browsing with the OP Web Browser
T2  - Security and Privacy, 2008. SP 2008. IEEE Symposium on
IS  - 
SN  - 1081-6011
VO  - 
SP  - 402
EP  - 416
AU  - Grier, C.
AU  - Shuo Tang
AU  - King, S.T.
Y1  - 18-22 May 2008
PY  - 2008
KW  - Internet
KW  - formal verification
KW  - operating system kernels
KW  - security of data
KW  - user interfaces
KW  - OP Web browser security
KW  - browser-level information-flow tracking system
KW  - formal method
KW  - innovative network architecture
KW  - operating system design principle
KW  - user interface
KW  - Communication system security
KW  - Computer hacking
KW  - Delay
KW  - Design methodology
KW  - Information analysis
KW  - Kernel
KW  - Operating systems
KW  - Testing
KW  - User interfaces
KW  - Web pages
KW  - browser
KW  - security
KW  - web security
VL  - 
JA  - Security and Privacy, 2008. SP 2008. IEEE Symposium on
DO  - 10.1109/SP.2008.19
AB  - Current Web browsers are plagued with vulnerabilities, providing hackers with easy access to computer systems via browser-based attacks. Browser security efforts that retrofit existing browsers have had limited success because the design of modern browsers is fundamentally flawed. To enable more secure web browsing, we design and implement a new browser, called the OP Web browser, that attempts to improve the state-of-the-art in browser security. Our overall design approach is to combine operating system design principles with formal methods to design a more secure Web browser by drawing on the expertise of both communities. Our overall design philosophy is to partition the browser into smaller subsystems and make all communication between subsystems simple and explicit. At the core of our design is a small browser kernel that manages the browser subsystems and interposes on all communications between them to enforce our new browser security features. To show the utility of our browser architecture, we design and implement three novel security features. First, we develop novel and flexible security policies that allows us to include plugins within our security framework. Our policy removes the burden of security from plugin writers, and gives plugins the flexibility to use innovative network architectures to deliver content while still maintaining the confidentiality and integrity of our browser, even if attackers compromise the plugin. Second, we use formal methods to prove that the address bar displayed within our browser user interface always shows the correct address for the current Web page. Third, we design and implement a browser-level information-flow tracking system to enable post-mortem analysis of browser-based attacks. If an attacker is able to compromise our browser, we highlight the subset of total activity that is causally related to the attack, thus allowing users and system administrators to determine easily which Web site lead to the compromise and- to assess the damage of a successful attack. To evaluate our design, we implemented OP and tested both performance and filesystem impact. To test performance, we measure latency to verify OP's performance penalty from security features are be minimal from a users perspective. Our experiments show that on average the speed of the OP browser is comparable to Firefox and the audit log occupies around 80 KB per page on average.
ER  - 

TY  - CONF
JO  - Cloud Computing (CLOUD), 2014 IEEE 7th International Conference on
TI  - Progger: An Efficient, Tamper-Evident Kernel-Space Logger for Cloud Data Provenance Tracking
T2  - Cloud Computing (CLOUD), 2014 IEEE 7th International Conference on
IS  - 
SN  - 
VO  - 
SP  - 881
EP  - 889
AU  - Ko, R.K.L.
AU  - Will, M.A.
Y1  - June 27 2014-July 2 2014
PY  - 2014
KW  - cloud computing
KW  - security of data
KW  - system monitoring
KW  - Progger
KW  - cloud computing systems
KW  - cloud data provenance tracking
KW  - data accountability
KW  - data activity audit
KW  - data activity tracking
KW  - data security component
KW  - kernel-space logger
KW  - Cloud computing
KW  - Data security
KW  - Kernel
KW  - Sockets
KW  - Synchronization
KW  - Virtual machining
KW  - Accountability
KW  - Cloud Computing
KW  - Data Provenance
KW  - Data Security
KW  - Tamper-evident logging
KW  - Time Synchronisation
VL  - 
JA  - Cloud Computing (CLOUD), 2014 IEEE 7th International Conference on
DO  - 10.1109/CLOUD.2014.121
AB  - Cloud data provenance, or "what has happened to my data in the cloud", is a critical data security component which addresses pressing data accountability and data governance issues in cloud computing systems. In this paper, we present Progger (Provenance Logger), a kernel-space logger which potentially empowers all cloud stakeholders to trace their data. Logging from the kernel space empowers security analysts to collect provenance from the lowest possible atomic data actions, and enables several higher-level tools to be built for effective end-to-end tracking of data provenance. Within the last few years, there has been an increasing number of proposed kernel space provenance tools but they faced several critical data security and integrity problems. Some of these prior tools' limitations include (1) the inability to provide log tamper-evidence and prevention of fake/manual entries, (2) accurate and granular timestamp synchronisation across several machines, (3) log space requirements and growth, and (4) the efficient logging of root usage of the system. Progger has resolved all these critical issues, and as such, provides high assurance of data security and data activity audit. With this in mind, the paper will discuss these elements of high-assurance cloud data provenance, describe the design of Progger and its efficiency, and present compelling results which paves the way for Progger being a foundation tool used for data activity tracking across all cloud systems.
ER  - 

TY  - CONF
JO  - Cloud Computing Technology and Science (CloudCom), 2014 IEEE 6th International Conference on
TI  - Open Stack Secure Enterprise File Sync and Share Turnkey Solution
T2  - Cloud Computing Technology and Science (CloudCom), 2014 IEEE 6th International Conference on
IS  - 
SN  - 
VO  - 
SP  - 1015
EP  - 1020
AU  - Yen-Hung Kuo
AU  - Tzu-Wei Yeh
AU  - Guang-Yan Zheng
AU  - Jyun-Kai Wu
AU  - Chao-Chin Yang
AU  - Jia-Ming Lin
Y1  - 15-18 Dec. 2014
PY  - 2014
KW  - cloud computing
KW  - security of data
KW  - data encryption
KW  - employee privacy protection
KW  - encryption key management
KW  - integrated security approach
KW  - open stack secure enterprise file sync and share turnkey Solution
KW  - out-of-band authentication method
KW  - sandbox-based cloud file synchronization
KW  - scalable secure EFSS service
KW  - secure enterprise directory integration
KW  - security issues
KW  - share link utility
KW  - share links
KW  - synchronized cloud files
KW  - Authentication
KW  - Databases
KW  - Encryption
KW  - File systems
KW  - Synchronization
KW  - Open Stack
KW  - enterprise file sync and share
KW  - security
VL  - 
JA  - Cloud Computing Technology and Science (CloudCom), 2014 IEEE 6th International Conference on
DO  - 10.1109/CloudCom.2014.17
AB  - The Enterprise File Sync and Share (EFSS) is one of the most important services to provide enterprises' employees with cloud file sync, share, and collaboration services. To take enterprises' concerns into account, such as security, privacy, compliance, and regulation, the existing EFSS solutions are either using private (on-premise) or hybrid cloud service model to provide their services. They usually emphasize that files stored in the solutions are encrypted on transfer and at rest and events occurred in the service are logged as the audit trail. However, support of data encryption and audit trail are not capable of protecting enterprise sensitive data from not well addressed security issues of the EFSS service. The security issues, including employee privacy protection, management of share links and synchronized cloud files, and the secure enterprise directory integration, are pointed out in this article. To address these issues, this work proposes and develops a scalable Secure EFSS service which can be deployed on the on-premise Open Stack cloud infrastructure to securely provide employees with EFSS service. Designs of an integrated security approach are introduced in this article, including data and metadata isolations, Distinct Share Link utility, encryption key management for personal and shared files, sandbox-based cloud file synchronization, and out-of-band authentication method.
ER  - 

TY  - CONF
JO  - Computer Science & Service System (CSSS), 2012 International Conference on
TI  - Research on the Technology of Secure Access to Online Geographic Information Services Based on SSL-based Token
T2  - Computer Science & Service System (CSSS), 2012 International Conference on
IS  - 
SN  - 
VO  - 
SP  - 624
EP  - 626
AU  - Xiaoya Lu
AU  - Jianchuan Liu
Y1  - 11-13 Aug. 2012
PY  - 2012
KW  - auditing
KW  - geographic information systems
KW  - security of data
KW  - SSL-based token
KW  - audit techniques
KW  - identity authentication technology
KW  - illegal geographic information resource downloading
KW  - log management technology
KW  - secure online geographic information service access technology
KW  - security problems
KW  - Authentication
KW  - Computer science
KW  - Cryptography
KW  - IP networks
KW  - Information services
KW  - Servers
KW  - SSL
KW  - authentication
KW  - online geographic information services
KW  - secure access
KW  - strong log management
KW  - token
VL  - 
JA  - Computer Science & Service System (CSSS), 2012 International Conference on
DO  - 10.1109/CSSS.2012.161
AB  - This paper explored the security problems in the field of online geographic information services, proposed the solution of secure access for online geographic information services based on SSL-based token, realized identity authentication technology for user secure access online geographic information services, strong log management technology and audit techniques, and solved the problems of secure access for online geographic information services, illegal downloading geographic information resources and etc.
ER  - 

TY  - CONF
JO  - Green Computing, Communication and Conservation of Energy (ICGCE), 2013 International Conference on
TI  - Trust management approach for secure and privacy data access in cloud computing
T2  - Green Computing, Communication and Conservation of Energy (ICGCE), 2013 International Conference on
IS  - 
SN  - 
VO  - 
SP  - 923
EP  - 927
AU  - Mythili, K.
AU  - Anandakumar, H.
Y1  - 12-14 Dec. 2013
PY  - 2013
KW  - cloud computing
KW  - data privacy
KW  - security of data
KW  - trusted computing
KW  - Base64 algorithm
KW  - Internet services
KW  - cloud computing
KW  - confidential data
KW  - data access privacy
KW  - data access security
KW  - policy-based approach
KW  - secured CIA framework
KW  - secured detecting mechanisms
KW  - trust based cloud
KW  - trust management approach
KW  - Authentication
KW  - Cloud computing
KW  - Data privacy
KW  - Encoding
KW  - Monitoring
KW  - Servers
KW  - BASE64
KW  - CIA
KW  - Cloud Computing
KW  - Logging
KW  - auditing
VL  - 
JA  - Green Computing, Communication and Conservation of Energy (ICGCE), 2013 International Conference on
DO  - 10.1109/ICGCE.2013.6823567
AB  - Cloud computing enables the use of Internet services as - needed basis. An important issue in the cloud is assuring privacy for the data stored in the cloud. There is a great challenge in providing security for the data shared in cloud and also prevent leakage of confidential data. To overcome this problem we introduce a novel approach with more secured CIA framework. We introduce auditing mechanisms and for providing trust for cloud users. Logs are maintained and sent periodically to the data owner. To provide security for the data file we encode the file using Base64 algorithm. This paper discusses key challenges in achieving privacy and trust based cloud through the use of secured detecting mechanisms which monitors the actions in cloud via technical and policy-based approaches.
ER  - 

TY  - CONF
JO  - Communications, 2003. APCC 2003. The 9th Asia-Pacific Conference on
TI  - Web server scanner: scanning on IIS CGI and HTTP
T2  - Communications, 2003. APCC 2003. The 9th Asia-Pacific Conference on
IS  - 
SN  - 
VO  - 3
SP  - 919
EP  - 923 Vol.3
AU  - Selamat, S.R.
Y1  - 21-24 Sept. 2003
PY  - 2003
KW  - Internet
KW  - file servers
KW  - hypermedia
KW  - security of data
KW  - transport protocols
KW  - HTTP
KW  - IIS CGI
KW  - Internet information server
KW  - Internet security
KW  - Web server scanner
KW  - common gateway interface
KW  - hypertext transfer protocol
KW  - security weaknesses
KW  - Computer bugs
KW  - Computer hacking
KW  - File systems
KW  - Information science
KW  - Information security
KW  - Internet
KW  - Network servers
KW  - Protocols
KW  - Testing
KW  - Web server
VL  - 3
JA  - Communications, 2003. APCC 2003. The 9th Asia-Pacific Conference on
DO  - 10.1109/APCC.2003.1274232
AB  - This paper explains about the design and implementation of Web server scanner. The scanner detected the security weaknesses on IIS, CGI and HTTP. A report is produced for audit log purposes to help decrease the security weaknesses. In Internet security, no hacking tool is more celebrated than the scanner. The scanner is a program that automatically detects security weaknesses in a remote or local host. It might reveal certain inherent weakness within the target host. The primary attributes of the scanner includes the capability to find a machine or network, the capability to find out what services are being run on the host, and the capability to test those services for known holes.
ER  - 

TY  - CONF
JO  - Communications, 2004 IEEE International Conference on
TI  - Scalable packet digesting schemes for IP traceback
T2  - Communications, 2004 IEEE International Conference on
IS  - 
SN  - 
VO  - 2
SP  - 1008
EP  - 1013 Vol.2
AU  - Tsern-Huei Lee
AU  - Wei-Kai Wu
AU  - Huang, T.-Y.W.
Y1  - 20-24 June 2004
PY  - 2004
KW  - IP networks
KW  - Internet
KW  - security of data
KW  - telecommunication links
KW  - telecommunication security
KW  - telecommunication traffic
KW  - Bloom filters
KW  - IP traceback
KW  - Internet security
KW  - Internet traffic
KW  - destination addresses
KW  - distributed denial of service
KW  - link capacity
KW  - packet digesting schemes
KW  - packet sharing
KW  - packet streams
KW  - source addresses
KW  - transit packets
KW  - Communication system security
KW  - Computer crime
KW  - Computer science
KW  - Computer security
KW  - Electronic mail
KW  - Filtering
KW  - IP networks
KW  - Internet
KW  - Protocols
KW  - Telecommunication traffic
VL  - 2
JA  - Communications, 2004 IEEE International Conference on
DO  - 10.1109/ICC.2004.1312653
AB  - Identifying the sources of an attack is an important task in the Internet security area. An attack could consist of a large number of packet streams generated by many compromised slaves that consume resources associated with various network elements to deny normal services or a few offending packets to disable a system. Several techniques based on probabilistic samples of transit packets have been developed, to determine the sources of large packet flows. It seems that logging of packet digests is necessary for traceback of an individual packet. A clever technique based on Bloom filters has recently been proposed to generate the audit trails for each individual packet within the network. The scheme is effective. However, the storage requirement is approximately 0.5% of the link capacity, which becomes a problem as link capacity increases. In this paper, we propose packet digesting schemes for flows and sets of packets sharing the same source and destination addresses. Compared with the individual packet digesting scheme, these schemes can achieve similar goals and are much more scalable. Simulations with real Internet traffic show that the storage requirements of our proposed schemes are one to two orders of magnitude lower.
ER  - 

TY  - CONF
JO  - Computer Information Systems and Industrial Management Applications, 2008. CISIM '08. 7th
TI  - A Keystroke Dynamics Based System for User Identification
T2  - Computer Information Systems and Industrial Management Applications, 2008. CISIM '08. 7th
IS  - 
SN  - 
VO  - 
SP  - 225
EP  - 230
AU  - Rybnik, M.
AU  - Tabedzki, M.
AU  - Saeed, K.
Y1  - 26-28 June 2008
PY  - 2008
KW  - authorisation
KW  - authorization methods
KW  - high-security computer systems
KW  - keystroke dynamics
KW  - keystrokes feature extraction
KW  - leaving system
KW  - logged-in workstation
KW  - medium-sized company
KW  - on-the-fly keyboard user authorization
KW  - security procedures
KW  - user identification
KW  - user replacement
KW  - Authorization
KW  - Biometrics
KW  - Data security
KW  - Financial management
KW  - Fingerprint recognition
KW  - Hardware
KW  - Keyboards
KW  - Mice
KW  - Operating systems
KW  - Workstations
KW  - authorization
KW  - biometrics
KW  - classification
KW  - dwell
KW  - flight
KW  - keystrokes
VL  - 
JA  - Computer Information Systems and Industrial Management Applications, 2008. CISIM '08. 7th
DO  - 10.1109/CISIM.2008.8
AB  - On-the-fly keyboard user authorization is certainly an interesting option for standard security procedures for high-security computer systems. Unauthorized access to logged-in workstation could threaten the security of data and systems. Conventional authorization methods (passwords, fingerprint scans) verify user identity only during logging process, leaving system vulnerable to user replacement afterwards. Procedures overcoming this peril are often invasive (constant visual monitoring) or uncomfortable (frequent identity verification with user cooperation). A possible solution for constant authorization without these drawbacks is to identify the keyboard user while typing. The approach we propose is based on two extracted keystrokes features: 'flight' and 'dwell'. We have tested the suggested solutions on individual group resembling a medium-sized company. The obtained results are promising.
ER  - 

TY  - CONF
JO  - Computer, Communication and Electrical Technology (ICCCET), 2011 International Conference on
TI  - Secured communication for MANETS in military
T2  - Computer, Communication and Electrical Technology (ICCCET), 2011 International Conference on
IS  - 
SN  - 
VO  - 
SP  - 146
EP  - 151
AU  - Sivakami, R.
AU  - Nawaz, G.M.K.
Y1  - 18-19 March 2011
PY  - 2011
KW  - defence industry
KW  - military communication
KW  - mobile ad hoc networks
KW  - protocols
KW  - telecommunication security
KW  - MANET
KW  - SMT protocol
KW  - binary search probing technique
KW  - defense communications environment
KW  - end-to-end secure data forwarding protocol
KW  - mobile ad hoc networks
KW  - nodes location
KW  - secure message transmission protocol
KW  - secured communication
KW  - Ad hoc networks
KW  - Delay
KW  - Mobile computing
KW  - Probes
KW  - Protocols
KW  - Reliability
KW  - Security
KW  - Binary Search Probing
KW  - Byzantine Faults
KW  - Location Prediction
KW  - Military
KW  - Mobile Ad Hoc Networks
KW  - Multipath Message Transmission
KW  - Reliability
KW  - SMT- Secure Message Transmission
KW  - Secure Transmission
KW  - Spatial and Temporal Mining
VL  - 
JA  - Computer, Communication and Electrical Technology (ICCCET), 2011 International Conference on
DO  - 10.1109/ICCCET.2011.5762456
AB  - A new way to increase the security of data transmission of mobile ad hoc networks [MANETS] is presented in this work. There is a massive increase in using MANETS for unmanned army system for both surveillance and future combat operations. This has necessitated the development of innovative MANET solutions catering to the reliability, security and scalability needs of the defense communications environment [15]. Security and reliability are crucial aspects of MANET, especially in security sensitive applications like military. Secure Message Transmission SMT[1] protocol secure the data transmission phase by tailoring an end-to-end secure data forwarding protocol to the MANET communication requirements and increases the reliability through transmitting the messages in multiple paths with minimal redundancy. This work increases the through the removal of Byzantine Faults [8] in the multiple paths. A binary search probing technique which is resilient to Byzantine failures caused by individual or colluding nodes is incorporated in the SMT protocol to provide more secured transmission. The fault detection algorithm bounds logarithmically (log n -n the number of nodes in the path), so the delay is reduced drastically. The simulated implementation of the work in NS2 shows the marginal increase in the throughput. The delay and jitter variants can also be improved if the nodes location can be predicted. Predicting the nodes location and reducing the unnecessary traffic with the aid of Spatial and Temporal mining is the second phase of this work.
ER  - 

TY  - CONF
JO  - System Sciences (HICSS), 2010 43rd Hawaii International Conference on
TI  - Securing E-Government Assets through Automating Deployment of Honeynets for IDS Support
T2  - System Sciences (HICSS), 2010 43rd Hawaii International Conference on
IS  - 
SN  - 1530-1605
VO  - 
SP  - 1
EP  - 10
AU  - Hecker, C.
AU  - Hay, Brian
Y1  - 5-8 Jan. 2010
PY  - 2010
KW  - government data processing
KW  - security of data
KW  - Honeynets deployment
KW  - IDS support
KW  - forensics analysis
KW  - malicious traffic
KW  - production system environment
KW  - securing e-government assets
KW  - Computer science
KW  - Data security
KW  - Electronic government
KW  - Humans
KW  - Intrusion detection
KW  - National security
KW  - Power capacitors
KW  - Production systems
KW  - Telecommunication traffic
KW  - US Government
VL  - 
JA  - System Sciences (HICSS), 2010 43rd Hawaii International Conference on
DO  - 10.1109/HICSS.2010.327
AB  - One of the challenges facing system e-government security professionals is the laborious task of sifting through numerous log files in an attempt to identify malicious traffic and conduct a forensics analysis to determine an appropriate course of action. This process is complicated significantly by the volume of traffic that can be associated with a production system environment. A honeynet can provide a mechanism to identify much of the forensically interesting traffic by creating a representative system to collect traffic data. However, it is challenging to maintain an accurate representation of a dynamic system in order to consistently collect the appropriate data of interest. This research effort addresses a current challenge identified by researchers at the Honeynet Project by describing a methodology for automatically creating and dynamically updating a honeynet in order to facilitate IDS support.
ER  - 

TY  - CONF
JO  - Computational Intelligence and Software Engineering, 2009. CiSE 2009. International Conference on
TI  - An Improved Sealing Scheme for Trusted Storage
T2  - Computational Intelligence and Software Engineering, 2009. CiSE 2009. International Conference on
IS  - 
SN  - 
VO  - 
SP  - 1
EP  - 4
AU  - Chi YaPing
AU  - Ju Lei
AU  - Shen Xiaodong
AU  - Fang Yong
Y1  - 11-13 Dec. 2009
PY  - 2009
KW  - security of data
KW  - improved sealing method
KW  - root of trust for reporting
KW  - stored measurement log
KW  - trusted computing standard
KW  - trusted storage
KW  - Communication standards
KW  - Cryptography
KW  - Data engineering
KW  - Data security
KW  - Hydrogen
KW  - Information security
KW  - Physics computing
KW  - Protection
KW  - Registers
KW  - Secure storage
VL  - 
JA  - Computational Intelligence and Software Engineering, 2009. CiSE 2009. International Conference on
DO  - 10.1109/CISE.2009.5366978
AB  - According to trusted computing standard, sealing provides assurance that sealed messages are bound to a set of platform metrics specified by the message sender, it ensures the security of data, but it can't handle asynchronous operation under multiple processes environment. An improved sealing method is given in this paper based on trusted storage, by introducing root of trust for reporting (RTR) and stored measurement log (SML) which provide real-time validation of trusting state, and it solves asynchronous operation problem for multiple tasks.
ER  - 

TY  - CONF
JO  - e-Business and Information System Security (EBISS), 2010 2nd International Conference on
TI  - Research and Design for Intrusion Detection System with Hybrid Detector and Apriori Algorithm
T2  - e-Business and Information System Security (EBISS), 2010 2nd International Conference on
IS  - 
SN  - 
VO  - 
SP  - 1
EP  - 4
AU  - Duanyang Zhao
AU  - Qingxiang Xu
AU  - Zhilin Feng
Y1  - 22-23 May 2010
PY  - 2010
KW  - data mining
KW  - design
KW  - security of data
KW  - anomaly detection mode
KW  - apriori algorithm
KW  - association rule
KW  - data mining programs
KW  - hybrid detector
KW  - intrusion detection system
KW  - misuse detection mode
KW  - security infrastructures
KW  - Algorithm design and analysis
KW  - Computer science
KW  - Content addressable storage
KW  - Data analysis
KW  - Data mining
KW  - Data security
KW  - Detectors
KW  - Educational institutions
KW  - Intrusion detection
KW  - Transaction databases
VL  - 
JA  - e-Business and Information System Security (EBISS), 2010 2nd International Conference on
DO  - 10.1109/EBISS.2010.5473646
AB  - Network and host Intrusion Detection Systems (IDS) have become a standard component in security infrastructures. As the action of intrusion represents variable, complicated, and uncertainty characteristic, they face so many problems to resolve for intrusion detection. Each approach has its strengths and weaknesses. We propose a hybrid IDS, which combines network and host IDS, with anomaly and misuse detection mode, utilizes auditing programs to extract an extensive set of features that describe each network connection or host session, and applies data mining programs to learn rules that accurately capture the behavior of intrusions and normal activities. We use an association rule to track all relevant data dependency rule sets for different access roles using a hierarchical structure. We identify malicious transactions from the transaction logs in the database using the data dependency rule sets. These rule sets are continuously updated and stored in a repository. The optimized algorithm actually improves the performance of IDS. Our approach is shown to reduce data access bottlenecks, and ensures minimal manual intervention for maintaining a secure database.
ER  - 

TY  - CONF
JO  - Computing, Communication and Networking, 2008. ICCCn 2008. International Conference on
TI  - Training MLP neural network to reduce false alerts in IDS
T2  - Computing, Communication and Networking, 2008. ICCCn 2008. International Conference on
IS  - 
SN  - 
VO  - 
SP  - 1
EP  - 7
AU  - Barapatre, P.
AU  - Tarapore, N.Z.
AU  - Pukale, S.G.
AU  - Dhore, M.L.
Y1  - 18-20 Dec. 2008
PY  - 2008
KW  - backpropagation
KW  - computer networks
KW  - multilayer perceptrons
KW  - security of data
KW  - IDS
KW  - MLP neural network
KW  - back-propagation algorithm
KW  - false alerts reduction
KW  - intrusion detection system
KW  - multilayer perceptron
KW  - network based computer attacks
KW  - Artificial neural networks
KW  - Computer networks
KW  - Data security
KW  - Databases
KW  - Information security
KW  - Intrusion detection
KW  - Multilayer perceptrons
KW  - Neural networks
KW  - Protection
KW  - Telecommunication traffic
VL  - 
JA  - Computing, Communication and Networking, 2008. ICCCn 2008. International Conference on
DO  - 10.1109/ICCCNET.2008.4787714
AB  - Due to the tremendous growth of the Internet and Network based services, the severity of network based computer attacks have significantly increased. Thus, IDS play a vital role in network security. Intrusion detection system tries to detect computer attacks by examining various data records, log audits etc. Many existing IDS such as Snort are signature based system. The problem with such a system is that it cannot detect novel attacks whose signature is not available and hence generates a high rate of alerts. In this paper Multilayer Perceptron (MLP) with Back-Propagation algorithm is used to classify attacks. We train and test MLP with KDD99 training dataset. We use KDD99 dataset which is a subset of the DARPA dataset. It is a preprocessed dataset and is most suitable for our system. We analyze the working of MLP by performing various experiments. We observed that MLP Neural network requires large training time. Once it trained, detects known as well as unknown attacks and also reduces false alerts.
ER  - 

TY  - JOUR
JO  - Dependable and Secure Computing, IEEE Transactions on
TI  - Detecting Anomalous Insiders in Collaborative Information Systems
T2  - Dependable and Secure Computing, IEEE Transactions on
IS  - 3
SN  - 1545-5971
VO  - 9
SP  - 332
EP  - 344
AU  - You Chen
AU  - Nyemba, S.
AU  - Malin, B.
Y1  - May-June 2012
PY  - 2012
KW  - groupware
KW  - medical information systems
KW  - security of data
KW  - statistical analysis
KW  - unsupervised learning
KW  - CIS
KW  - MetaCADS
KW  - access logs
KW  - anomalous insider detection
KW  - anomaly prediction
KW  - collaborative information systems
KW  - community anomaly detection system
KW  - community structures
KW  - electronic health record
KW  - insider threat detection
KW  - medical center
KW  - relational pattern extraction
KW  - security mechanisms
KW  - statistical model
KW  - unsupervised learning framework
KW  - Artificial intelligence
KW  - Collaboration
KW  - Communities
KW  - Design automation
KW  - Matrix decomposition
KW  - Medical services
KW  - Semantics
KW  - Privacy
KW  - data mining
KW  - insider threat detection.
KW  - social network analysis
VL  - 9
JA  - Dependable and Secure Computing, IEEE Transactions on
DO  - 10.1109/TDSC.2012.11
AB  - Collaborative information systems (CISs) are deployed within a diverse array of environments that manage sensitive information. Current security mechanisms detect insider threats, but they are ill-suited to monitor systems in which users function in dynamic teams. In this paper, we introduce the community anomaly detection system (CADS), an unsupervised learning framework to detect insider threats based on the access logs of collaborative environments. The framework is based on the observation that typical CIS users tend to form community structures based on the subjects accessed (e.g., patients' records viewed by healthcare providers). CADS consists of two components: 1) relational pattern extraction, which derives community structures and 2) anomaly prediction, which leverages a statistical model to determine when users have sufficiently deviated from communities. We further extend CADS into MetaCADS to account for the semantics of subjects (e.g., patients' diagnoses). To empirically evaluate the framework, we perform an assessment with three months of access logs from a real electronic health record (EHR) system in a large medical center. The results illustrate our models exhibit significant performance gains over state-of-the-art competitors. When the number of illicit users is low, MetaCADS is the best model, but as the number grows, commonly accessed semantics lead to hiding in a crowd, such that CADS is more prudent.
ER  - 

TY  - CONF
JO  - Dependable Systems and Networks Workshops (DSN-W), 2012 IEEE/IFIP 42nd International Conference on
TI  - Defending against VM rollback attack
T2  - Dependable Systems and Networks Workshops (DSN-W), 2012 IEEE/IFIP 42nd International Conference on
IS  - 
SN  - 
VO  - 
SP  - 1
EP  - 5
AU  - Yubin Xia
AU  - Yutao Liu
AU  - Haibo Chen
AU  - Binyu Zang
Y1  - 25-28 June 2012
PY  - 2012
KW  - cloud computing
KW  - security of data
KW  - trusted computing
KW  - virtual machines
KW  - IaaS platform
KW  - VM protection system
KW  - VM rollback attack
KW  - malicious hypervisor
KW  - malicious rollback
KW  - migration operations
KW  - suspend-resume operations
KW  - trusted computing
KW  - virtual machines
KW  - Booting
KW  - Cloning
KW  - Cryptography
KW  - Hardware
KW  - Processor scheduling
KW  - Virtual machine monitors
VL  - 
JA  - Dependable Systems and Networks Workshops (DSN-W), 2012 IEEE/IFIP 42nd International Conference on
DO  - 10.1109/DSNW.2012.6264690
AB  - Recently it became a hot topic to protect VMs from a compromised or even malicious hypervisor. However, most previous systems are vulnerable to rollback attack, since it is hard to distinguish from normal suspend/resume and migration operations that an IaaS platform usually offers. Some of the previous systems simply disable these features to defend rollback attack, while others heavily need user involvement. In this paper, we propose a new solution to make a balance between security and functionality. By securely logging all the suspend/resume and migration operation inside a small trusted computing base, a user can audit the log to check malicious rollback and constrain the operations on the VMs. The solution considers several practical issues including hardware limitations and minimizing user's interaction, and has been implemented on a recent VM protection system.
ER  - 

TY  - CONF
JO  - Military Communications Conference, 2009. MILCOM 2009. IEEE
TI  - Managing evolving security situations
T2  - Military Communications Conference, 2009. MILCOM 2009. IEEE
IS  - 
SN  - 
VO  - 
SP  - 1
EP  - 7
AU  - Kivimaa, J.
AU  - Ojamaa, A.
AU  - Tyugu, E.
Y1  - 18-21 Oct. 2009
PY  - 2009
KW  - knowledge acquisition
KW  - military computing
KW  - security of data
KW  - characteristic attack log analysis
KW  - data log analysis
KW  - graded security standards
KW  - integral security metrics
KW  - knowledge acquisition
KW  - military communications applications
KW  - security confidence
KW  - security measures
KW  - security reports
KW  - security situation management
KW  - statistical data model
KW  - Computer security
KW  - Cost function
KW  - Cybernetics
KW  - Data security
KW  - Information security
KW  - Investments
KW  - Optimization methods
KW  - Paper technology
KW  - Technology management
KW  - Technology planning
VL  - 
JA  - Military Communications Conference, 2009. MILCOM 2009. IEEE
DO  - 10.1109/MILCOM.2009.5380110
AB  - A method is described that takes into account the investments done in the security and/or achieved security confidence in planning new security measures. The method uses new integral security metrics and the well-known graded security model. A precondition for the application of this method is the availability of expert knowledge or statistical data for the model in use that describes a class of situations where the analyzed security situation belongs to. For a number of situations at present, this information has been extracted from standards of graded security. For specific military communications applications the data must be collected from a log analysis of characteristic attacks and security reports, as well as by the traditional knowledge acquisition means.
ER  - 

TY  - CONF
JO  - Multimedia Information Networking and Security (MINES), 2012 Fourth International Conference on
TI  - Research on System Logs Collection and Analysis Model of the Network and Information Security System by Using Multi-agent Technology
T2  - Multimedia Information Networking and Security (MINES), 2012 Fourth International Conference on
IS  - 
SN  - 
VO  - 
SP  - 23
EP  - 26
AU  - Shi Shengyan
AU  - Shen Xiaoliu
AU  - Zhao Jianbao
AU  - Ma Xinke
Y1  - 2-4 Nov. 2012
PY  - 2012
KW  - data analysis
KW  - data structures
KW  - multi-agent systems
KW  - security of data
KW  - system monitoring
KW  - event log data format
KW  - fixed-format data structure generation
KW  - information protection
KW  - information security system
KW  - log data analysis
KW  - log data collection
KW  - multiagent technology
KW  - network devices
KW  - network equipment
KW  - network security protection
KW  - network security system
KW  - safety equipment
KW  - security devices
KW  - security event log
KW  - system logs analysis model
KW  - system logs collection model
KW  - Analytical models
KW  - Databases
KW  - Educational institutions
KW  - Information security
KW  - Safety
KW  - Log collection agent
KW  - Multi-agent technology
KW  - log collection and analysis system
VL  - 
JA  - Multimedia Information Networking and Security (MINES), 2012 Fourth International Conference on
DO  - 10.1109/MINES.2012.181
AB  - In order to realize the full rang of information security, a variety of network equipment, safe equipment have been applied to deal with all aspects of information security and protection by many enterprise. These devices, systems produce a lot of security event log in the network security protection, and these event log data format are different, and different safety equipment may generate the same alerts logs, not only resulting in generating redundant events, but not conducive to the next work of network security situational awareness. Therefore, this paper proposed a method by using the multi-agent technology to collect and analysis the log data generated by network devices and security devices, and then generating a fixed-format data structure and building the log collection and analysis systems to facilitate the later maintenance and use of data.
ER  - 

TY  - CONF
JO  - Emerging Security Information, Systems and Technologies, 2008. SECURWARE '08. Second International Conference on
TI  - Collaborative Approach to Automatic Classification of Heterogeneous Information Security
T2  - Emerging Security Information, Systems and Technologies, 2008. SECURWARE '08. Second International Conference on
IS  - 
SN  - 
VO  - 
SP  - 294
EP  - 299
AU  - Benali, F.
AU  - Ubeda, S.
AU  - Legrand, V.
Y1  - 25-31 Aug. 2008
PY  - 2008
KW  - Bayes methods
KW  - classification
KW  - groupware
KW  - message passing
KW  - ontologies (artificial intelligence)
KW  - security of data
KW  - text analysis
KW  - vocabulary
KW  - automatic classification
KW  - collaborative approach
KW  - extraction module
KW  - heterogeneous information security
KW  - information system
KW  - k-nearest neighbour algorithm
KW  - malicious activity
KW  - naive Bayes
KW  - ontology
KW  - security devices
KW  - security log files messages
KW  - security log messages
KW  - security messages
KW  - text categorization technics
KW  - training algorithms
KW  - vocabulary size
KW  - Collaboration
KW  - Data security
KW  - Engines
KW  - Information analysis
KW  - Information security
KW  - Information systems
KW  - Monitoring
KW  - Ontologies
KW  - Probes
KW  - Vocabulary
KW  - Automatic Classification
KW  - Heterogeneous Probes
KW  - Intrusion Detection
KW  - Ontology
KW  - Security Messages
KW  - Text Categorization
VL  - 
JA  - Emerging Security Information, Systems and Technologies, 2008. SECURWARE '08. Second International Conference on
DO  - 10.1109/SECURWARE.2008.53
AB  - The messages generated by the security devices represent the necessary data for the detection of the malicious activities in an information system. The heterogeneity of the devices and the lack of a standard for the security messages make the automatic processing of the messages difficult. The messages are short, use a very wide vocabulary and have different formats. We propose in this article the collaboration between classifiers to increase the accuracy of the classification. We apply the text categorization technics for the automatic classification of security log files messages, in categories defined by an ontology. We develop an extraction module for the message attributes to reduce the vocabulary size. Then we apply two training algorithms: the k-nearest neighbour algorithm and the naive Bayes, on two corpus of security log messages. Finally we propose to collaborate the classifiers to produce a single classifier with better accuracy.
ER  - 

TY  - CONF
JO  - Information Assurance Workshop, 2005. IAW '05. Proceedings from the Sixth Annual IEEE SMC
TI  - Closing-the-loop: discovery and search in security visualizations
T2  - Information Assurance Workshop, 2005. IAW '05. Proceedings from the Sixth Annual IEEE SMC
IS  - 
SN  - 
VO  - 
SP  - 58
EP  - 63
AU  - Kiran Lakkaraju
AU  - Ratna Bearavolu
AU  - Slagell, A.
AU  - Yurcik, W.
Y1  - 15-17 June 2005
PY  - 2005
KW  - computer networks
KW  - data visualisation
KW  - pattern recognition
KW  - security of data
KW  - NVisionIP
KW  - attack behavior
KW  - closing-the-loop
KW  - data pattern discovery
KW  - network attack detection
KW  - network data visualization
KW  - network logs
KW  - security engineers
KW  - security visualizations
KW  - Creep
KW  - Data engineering
KW  - Data mining
KW  - Data security
KW  - Data visualization
KW  - Displays
KW  - Humans
KW  - Information security
KW  - Pattern matching
KW  - Turning
VL  - 
JA  - Information Assurance Workshop, 2005. IAW '05. Proceedings from the Sixth Annual IEEE SMC
DO  - 10.1109/IAW.2005.1495934
AB  - The tasks of security engineers include detecting attacks and responding to them. In order to accomplish this, a security engineer must be able to decide what behavior indicates an attack and then search for this behavior. Current security visualization tools provide rich and concise visualizations of network data that allow security engineers to determine the nature of attacks on the network. However, current security visualizations lack the ability for security engineers to search for these behaviors in the network logs. The process of finding interesting patterns in the data is called discovery, and finding instances of these patterns is called searching. Security engineers must do both discovery and search, but current security visualization tools only help in discovery. In this paper, we describe the modifications we have made to our security visualization tool, NVisionIP, that allow security engineers to not only discover patterns in the data, but also to search for those patterns in other data.
ER  - 

TY  - CONF
JO  - Web Services (ICWS), 2011 IEEE International Conference on
TI  - Automated Security Service Orchestration for the Identity Management in Web Service Based Systems
T2  - Web Services (ICWS), 2011 IEEE International Conference on
IS  - 
SN  - 
VO  - 
SP  - 596
EP  - 603
AU  - Warschofsky, R.
AU  - Menzel, M.
AU  - Meinel, C.
Y1  - 4-9 July 2011
PY  - 2011
KW  - Unified Modeling Language
KW  - Web services
KW  - formal specification
KW  - security of data
KW  - service-oriented architecture
KW  - UML use case model
KW  - Web service based system
KW  - authentication
KW  - authorization
KW  - expert knowledge
KW  - functional service
KW  - identity management service
KW  - message logging service
KW  - security requirement
KW  - security service orchestration
KW  - service security pattern
KW  - service-oriented architecture
KW  - Authentication
KW  - Context
KW  - Service oriented architecture
KW  - System analysis and design
KW  - Unified modeling language
KW  - Identity Management
KW  - Pattern-bases Security Engineering
KW  - Security Orchestration
KW  - Service-oriented Architectures
KW  - Web Services
VL  - 
JA  - Web Services (ICWS), 2011 IEEE International Conference on
DO  - 10.1109/ICWS.2011.41
AB  - Today, there is a huge amount of security services that can be used to implement different security requirements in Web Service based systems. For example, identity management services are required for authentication and authorization whereas message logging services are necessary to achieve non-repudiation. However, the deployment and configuration of these security services usually requires expert knowledge about the systems and expert knowledge about security requirements and implementations which a person can only learn by experience. Furthermore, today's Web Service based systems become increasingly complex. Thus, implementing security requirements is a complex and error prone task, even for experts. For this paper, we analysed several service-based implementations for identity management and their differences in the service orchestration. We present an approach to derive the needed security services, their configuration, and their connections to the functional services, based on defined security requirements for a Web Service based system. Therefore, we evaluate the UML use case model of the system and apply service security pattern derived during the analysis of the identity management implementations.
ER  - 

TY  - CONF
JO  - Conference For Homeland Security, 2009. CATCH '09. Cybersecurity Applications & Technology
TI  - Quickdraw: Generating Security Log Events for Legacy SCADA and Control System Devices
T2  - Conference For Homeland Security, 2009. CATCH '09. Cybersecurity Applications & Technology
IS  - 
SN  - 
VO  - 
SP  - 227
EP  - 229
AU  - Peterson, D.
Y1  - 3-4 March 2009
PY  - 2009
KW  - SCADA systems
KW  - control engineering computing
KW  - security of data
KW  - system monitoring
KW  - Quickdraw
KW  - control system devices
KW  - intrusion detection system
KW  - legacy SCADA
KW  - network IDS
KW  - security event logs
KW  - Communication system control
KW  - Communication system security
KW  - Computer security
KW  - Control systems
KW  - Data security
KW  - Distributed control
KW  - Event detection
KW  - Intrusion detection
KW  - National security
KW  - SCADA systems
KW  - Process Control System Security
KW  - Real-time security event assessment and mitigation
VL  - 
JA  - Conference For Homeland Security, 2009. CATCH '09. Cybersecurity Applications & Technology
DO  - 10.1109/CATCH.2009.33
AB  - Security event logs play a role in the early detection of attacks and in after incident investigations. Controllers used in SCADA, DCS and other control systems log almost no security events. This deficiency is addressed by the Quickdraw application, which is a passive security log generator for controllers. Quickdraw monitors communication like a network IDS, detects events that should be logged in a controller, creates the security events, and then sends the event to a historian, SEM or other log aggregator.
ER  - 

TY  - CONF
JO  - Securecomm and Workshops, 2006
TI  - Outsourcing Security Analysis with Anonymized Logs
T2  - Securecomm and Workshops, 2006
IS  - 
SN  - 
VO  - 
SP  - 1
EP  - 9
AU  - Jianqing Zhang
AU  - Borisov, N.
AU  - Yurcik, W.
Y1  - Aug. 28 2006-Sept. 1 2006
PY  - 2006
KW  - security of data
KW  - anonymized logs
KW  - managed security service providers
KW  - private security logs
KW  - security analysis outsourcing
KW  - security monitoring
KW  - Computer security
KW  - Data privacy
KW  - Data security
KW  - Information analysis
KW  - Information security
KW  - Monitoring
KW  - National security
KW  - Outsourcing
KW  - Performance analysis
KW  - Protection
VL  - 
JA  - Securecomm and Workshops, 2006
DO  - 10.1109/SECCOMW.2006.359577
AB  - As security monitoring grows both more complicated and more sophisticated, there is an increased demand for outsourcing these tasks to managed security service providers (MSSPs). However, the core problem of sharing private security logs creates a barrier to the widespread adoption of this business model. In this paper we analyze the logs used for security analysis with the concern of privacy and propose the constraints on anonymization of security monitor logs. We believe if the anonymization solution fulfills the constraints, MSSPs can detect the attacks efficiently and protect privacy simultaneously
ER  - 

TY  - CONF
JO  - Intelligent Control and Automation, 2008. WCICA 2008. 7th World Congress on
TI  - Network security data mining based on wavelet decomposition
T2  - Intelligent Control and Automation, 2008. WCICA 2008. 7th World Congress on
IS  - 
SN  - 
VO  - 
SP  - 6646
EP  - 6649
AU  - Ming Meng
Y1  - 25-27 June 2008
PY  - 2008
KW  - Internet
KW  - computer network management
KW  - data mining
KW  - database management systems
KW  - discrete wavelet transforms
KW  - security of data
KW  - telecommunication security
KW  - Internet
KW  - association rule
KW  - discrete wavelet transform method
KW  - multiresolution analysis
KW  - network security data mining
KW  - network security log file database
KW  - security management
KW  - wavelet decomposition
KW  - Clustering algorithms
KW  - Data analysis
KW  - Data mining
KW  - Data security
KW  - Databases
KW  - Discrete wavelet transforms
KW  - Fourier transforms
KW  - Frequency
KW  - Information security
KW  - Wavelet analysis
KW  - Data mining
KW  - Discrete wavelet transform
KW  - Network log file database
KW  - security management
VL  - 
JA  - Intelligent Control and Automation, 2008. WCICA 2008. 7th World Congress on
DO  - 10.1109/WCICA.2008.4593932
AB  - The network log file database is important to security management. But, with the development of Internet, it has become too bulkiness to be used directly. In this paper, the wavelet decomposition is adopted to mining the useful rules from the network security data. This is different from traditional data mining methods of association rules. Multi-resolution analysis of wavelet transform method is used to analyze and mine the data of network security log file database on different time scale. The period law of the attack number in different time domain is found by this method. These are important messages for taking relevant measures to insure network security.
ER  - 

TY  - CONF
JO  - Information Assurance and Security Workshop, 2007. IAW '07. IEEE SMC
TI  - Arachne: Integrated Enterprise Security Management
T2  - Information Assurance and Security Workshop, 2007. IAW '07. IEEE SMC
IS  - 
SN  - 
VO  - 
SP  - 214
EP  - 220
AU  - Burnside, M.
AU  - Keromytis, A.D.
Y1  - 20-22 June 2007
PY  - 2007
KW  - access control
KW  - security of data
KW  - telecommunication security
KW  - Arachne
KW  - admission-based access control
KW  - enterprise networks
KW  - enterprise-wide security
KW  - global knowledge
KW  - global security policies
KW  - integrated enterprise security management
KW  - policy decision
KW  - system-wide policy
KW  - Data security
KW  - Databases
KW  - IP networks
KW  - Information security
KW  - Intrusion detection
KW  - Modems
KW  - Prototypes
KW  - Space exploration
KW  - Tellurium
KW  - Web server
VL  - 
JA  - Information Assurance and Security Workshop, 2007. IAW '07. IEEE SMC
DO  - 10.1109/IAW.2007.381935
AB  - Security policies are a key component in protecting enterprise networks. There are many defensive options available to these policies, but current mechanically-enforced security policies are limited to traditional admission-based access control. There are defensive capabilities available that include logging, firewalls, honeypots, rollback/recovery, and intrusion detection systems, but policy enforcement is essentially limited to allow/deny semantics. Furthermore, access-control mechanisms operate independently on each service, which often leads to inconsistent or incorrect application of the intended system-wide policy. To begin to solve these problems, we propose a new system for defense-in-depth using global security policies. Under a global security policy, every policy decision is made with near-global knowledge, and re-evaluated as global knowledge changes, given an initial configuration provided by the administrator. Using a variety of actuators, we make the full array of defensive capabilities available to the global policy. We outline our proposal for enterprise-wide security policies, explore the design space, and discuss Arachne, our prototype implementation.
ER  - 

TY  - CONF
JO  - Information Systems and Technologies (CISTI), 2011 6th Iberian Conference on
TI  - Analysis of log files as a security aid
T2  - Information Systems and Technologies (CISTI), 2011 6th Iberian Conference on
IS  - 
SN  - 
VO  - 
SP  - 1
EP  - 6
AU  - Leite, J.P.
Y1  - 15-18 June 2011
PY  - 2011
KW  - security of data
KW  - academic institution
KW  - information retrieval open source tools
KW  - log file analysis
KW  - security aid
KW  - security related events
KW  - suspected behavior patterns
KW  - system intrusion
KW  - Colon
KW  - Documentation
KW  - Indexing
KW  - Information retrieval
KW  - Java
KW  - Security
KW  - data mining
KW  - information retrieval
KW  - log
KW  - security
VL  - 
JA  - Information Systems and Technologies (CISTI), 2011 6th Iberian Conference on
DO  - 
AB  - Log files are the history books of a computer system. In particular, they tell a good portion of the security related events and menaces that a system has to withstand and, sometimes, fails to resist to. Therefore, log files' analysis can be valuable to system and security administrators if the difficulty of extracting the relevant information of the different kinds of data and formats can be surmounted. And if we consider a huge system in terms of users, services and accesses, the difficulty of the analysis task rises enormously. In infra-structures where there is more than one server and communication link, it is possible for the administrator and security team to configure all systems to record their logs in a central huge repository, but the search for abnormalities is quite impossible without specialized tools. As we believe that the collected information on log files can be valuable, we use information retrieval open source tools to index the log files' fields and search for patterns of suspected behavior, which may indicate a system intrusion. Our development allows queries based on variables introduced by the analyst. The preliminary results obtained when using log files from an academic institution indicates that our approach is effective and can be used as a security aid.
ER  - 

TY  - CONF
JO  - Communications and Intelligence Information Security (ICCIIS), 2010 International Conference on
TI  - MS&#x0B2;IFS: A Multiple Source-Based Security Information Fusion System
T2  - Communications and Intelligence Information Security (ICCIIS), 2010 International Conference on
IS  - 
SN  - 
VO  - 
SP  - 215
EP  - 219
AU  - Jun Chang
AU  - Jiang Yu
AU  - Yijian Pei
Y1  - 13-14 Oct. 2010
PY  - 2010
KW  - security of data
KW  - telecommunication security
KW  - administration pressure
KW  - alert accuracy
KW  - alert fusion
KW  - alert redundancy
KW  - correlation analysis
KW  - information security
KW  - multiple source-based security information fusion system
KW  - network information systems
KW  - offline alert logs
KW  - online simulated attack data
KW  - security devices
KW  - system architecture
KW  - systems security performance
KW  - Algorithm design and analysis
KW  - Analytical models
KW  - Correlation
KW  - Redundancy
KW  - Security
KW  - Servers
KW  - alert correlation
KW  - information fusion
KW  - intrusion detection
KW  - risk evaluation
VL  - 
JA  - Communications and Intelligence Information Security (ICCIIS), 2010 International Conference on
DO  - 10.1109/ICCIIS.2010.32
AB  - Security Information Fusion System has recently become one of the major topics in the research area of information security. A great deal of security devices and components have been deployed in network information systems. While improving the systems security performance, they produced lots of redundant or unreliable information. Through the technologies of alert fusion and correlation analysis, alert redundancy can be decreased, administration pressure can be reduced and alert accuracy can be raised effectively. We propose the system architecture of multi-source security information fusion (MS<sup>2</sup>IFS), and discuss the design ideas and algorithm implementation of MS<sup>2</sup>IFS key modules. The results of testing on offline alert logs and online simulated attack data proved the feasibility and validity of MS<sup>2</sup>IFS system and satisfied the design requirement, presenting preferable.
ER  - 

TY  - JOUR
JO  - Security & Privacy, IEEE
TI  - Estimating Potential IT Security Losses: An Alternative Quantitative Approach
T2  - Security & Privacy, IEEE
IS  - 6
SN  - 1540-7993
VO  - 4
SP  - 44
EP  - 52
AU  - Lee, Vincent C S
AU  - Linyi Shao
Y1  - Nov.-Dec. 2006
PY  - 2006
KW  - cost-benefit analysis
KW  - data mining
KW  - information technology
KW  - security of data
KW  - stochastic processes
KW  - telecommunication security
KW  - IT security losses
KW  - IT security risk
KW  - Poisson arrival processing
KW  - data mining
KW  - enterprise information IT security
KW  - network-based information
KW  - network-centric open economy
KW  - port-scan record mining
KW  - stochastic process
KW  - stock exchange price dynamics
KW  - Computer security
KW  - Cost benefit analysis
KW  - Current measurement
KW  - Data security
KW  - Frequency estimation
KW  - Information analysis
KW  - Information security
KW  - Investments
KW  - Loss measurement
KW  - Risk management
KW  - jump diffusion process
KW  - port scan data
KW  - quantitative measuring of IT security loss
VL  - 4
JA  - Security & Privacy, IEEE
DO  - 10.1109/MSP.2006.151
AB  - In this article, we look at the potential relationship between the cost of IT security breaches and port-scan records - specifically, we postulate that the irregular movement of port-scan records is quite similar to stock exchange price dynamics, which have been partially satisfied with stochastic movement and Poisson arrival processing. By intuition, stochastic process methodology can emulate an enterprise's information IT security situation. Our proposed generic approach provides a reasonable procedure to estimate and evaluate IT security risk through data mining of port-scan logs
ER  - 

TY  - CONF
JO  - Information Assurance and Security, 2009. IAS '09. Fifth International Conference on
TI  - Information Security Monitoring System Based on Data Mining
T2  - Information Assurance and Security, 2009. IAS '09. Fifth International Conference on
IS  - 
SN  - 
VO  - 1
SP  - 472
EP  - 475
AU  - Lv Guangjuan
AU  - Xu Ruzhi
AU  - Zu Xiangrong
AU  - Deng Liwu
Y1  - 18-20 Aug. 2009
PY  - 2009
KW  - data mining
KW  - information systems
KW  - security of data
KW  - system monitoring
KW  - antivirus gateways
KW  - association rule extraction
KW  - correlation analysis engine
KW  - data mining techniques
KW  - firewalls
KW  - information security monitoring system
KW  - intrusion detection systems
KW  - log-based mining
KW  - multiprotocol supported framework
KW  - security event correlation
KW  - Data mining
KW  - Information security
KW  - Monitoring
KW  - data mining
KW  - security event
KW  - security monitoring
VL  - 1
JA  - Information Assurance and Security, 2009. IAS '09. Fifth International Conference on
DO  - 10.1109/IAS.2009.325
AB  - Some heterogeneous security equipments such as firewalls, intrusion detection systems, and anti-virus gateways, can produce massive security events which are difficult to manage efficiently. So a log-based mining, distributed, and multi-protocol supported framework of security monitoring system is proposed. This paper describes the architecture of the information security monitoring system, and focuses on the research of the correlation analysis engine, describes the process that the detection model is built using data mining techniques. Security event correlation based on data mining analysis can automatically extract association rules, analyze alarming and found new invasion model, so it is a highly intelligent solution.
ER  - 

TY  - CONF
JO  - Defense Science Research Conference and Expo (DSR), 2011
TI  - From system-centric to data-centric logging - Accountability, trust &amp; security in cloud computing
T2  - Defense Science Research Conference and Expo (DSR), 2011
IS  - 
SN  - 
VO  - 
SP  - 1
EP  - 4
AU  - Ko, R.K.L.
AU  - Kirchberg, M.
AU  - Bu Sung Lee
Y1  - 3-5 Aug. 2011
PY  - 2011
KW  - cloud computing
KW  - security of data
KW  - system monitoring
KW  - IT systems
KW  - TrustCloud
KW  - cloud computing security
KW  - cloud computing trust
KW  - cloud security
KW  - computing services
KW  - data access
KW  - data accountability
KW  - data ownership
KW  - data retention
KW  - data transfer
KW  - data-centric logging-accountability
KW  - data-centric thinking
KW  - security of data
KW  - system-centric logging-accountability
KW  - Business
KW  - Cloud computing
KW  - Google
KW  - History
KW  - Security
KW  - Servers
KW  - Virtual machining
KW  - TrustCloud framework
KW  - accountability
KW  - cloud computing
KW  - cloud computing security
KW  - data-centric logging
KW  - trust
VL  - 
JA  - Defense Science Research Conference and Expo (DSR), 2011
DO  - 10.1109/DSR.2011.6026885
AB  - Cloud computing signifies a paradigm shift from owning computing systems to buying computing services. As a result of this paradigm shift, many key concerns such as the transparency of data transfer and access within the cloud, and the lack of clarity in data ownership were surfaced. To address these concerns, we propose a new way of approaching traditional security and trust problems: To adopt a detective, data-centric thinking instead of the classical preventive, system-centric thinking. While classical preventive approaches are useful, they play a catch-up game; often do not address the problems (i.e. data accountability, data retention, etc) directly. In this paper, we propose a data-centric, detective approach to increase trust and security of data in the cloud. Our framework, known as TrustCloud, contains a suite of techniques that address cloud security, trust and accountability from a detective approach at all levels of granularity. TrustCloud also extends detective techniques to policies and regulations governing IT systems.
ER  - 

TY  - CONF
JO  - Visual Analytics Science and Technology (VAST), 2011 IEEE Conference on
TI  - Geovisual analytics for cyber security: Adopting the GeoViz Toolkit
T2  - Visual Analytics Science and Technology (VAST), 2011 IEEE Conference on
IS  - 
SN  - 
VO  - 
SP  - 315
EP  - 316
AU  - Giacobe, N.A.
AU  - Sen Xu
Y1  - 23-28 Oct. 2011
PY  - 2011
KW  - Perl
KW  - data analysis
KW  - data visualisation
KW  - geographic information systems
KW  - security of data
KW  - ArcGIS 10
KW  - GeoViz Toolkit
KW  - Perl scripts
KW  - VAST 2011 Network Security MiniChallenge
KW  - cyber security
KW  - data security
KW  - geography
KW  - geovisual analytics method
KW  - network security
KW  - Computer security
KW  - Data visualization
KW  - Geography
KW  - Histograms
KW  - IP networks
KW  - Shape
KW  - Coordinated and multiple views
KW  - GeoViz Toolkit
KW  - cyber security
KW  - geovisual analytics
KW  - situation awareness
VL  - 
JA  - Visual Analytics Science and Technology (VAST), 2011 IEEE Conference on
DO  - 10.1109/VAST.2011.6102491
AB  - For the VAST 2011 Network Security Mini-Challenge, we adopted geovisual analytic methods and applied them in the field of network security. We used the GeoViz Toolkit [1] to represent cyber security events, by fabricating a simple &#x201C;geography&#x201D; of several sets of blocks (one for the workstations, one for the servers, and one for the Internet) using ArcGIS 10 (by ESRI - Environmental System Research Institute). Security data was tabulated using Perl scripts to parse the logs in order to create representations of event frequency and where they occurred on the network. The tabulated security data was then added as attributes of the geography. Exploration of the data and subsequent analysis of the meaning and impact of the cyber security events was made possible using the GeoViz Toolkit.
ER  - 

TY  - CONF
JO  - ICT and Knowledge Engineering (ICT & Knowledge Engineering), 2012 10th International Conference on
TI  - A study of Security Issues Faced and Security Measures Practiced by Citizens of Pune City while working on Social Networking Websites
T2  - ICT and Knowledge Engineering (ICT & Knowledge Engineering), 2012 10th International Conference on
IS  - 
SN  - 2157-0981
VO  - 
SP  - 146
EP  - 150
AU  - Potdar, B.
AU  - Nandavadekar, V.
Y1  - 21-23 Nov. 2012
PY  - 2012
KW  - security of data
KW  - social networking (online)
KW  - Facebook
KW  - Flickr
KW  - Friendster
KW  - LinkdIn
KW  - MySpace
KW  - Orkut
KW  - Pune City
KW  - Twitter
KW  - games service
KW  - hi5
KW  - messaging service
KW  - scrapping service
KW  - security issue
KW  - security measure
KW  - social networking Web site
KW  - text-audio-video chatting service
KW  - Cities and towns
KW  - Data security
KW  - Facebook
KW  - Internet
KW  - MySpace
KW  - Identity Theft
KW  - Privacy Control
KW  - Security Breach
KW  - Security Measures
KW  - Social Networking
VL  - 
JA  - ICT and Knowledge Engineering (ICT & Knowledge Engineering), 2012 10th International Conference on
DO  - 10.1109/ICTKE.2012.6408545
AB  - Social Networking is widely accepted and used in and outside every organization. There are number of Social Networking Websites exist on server e.g. Orkut, Twitter, Facebook, LinkdIn, hi5, MySpace, Friendster, Flickr etc. and users log on to these websites on daily basis and avail various free services such as; Scrapping, Messaging, Text/Audio/Video Chatting, Games etc. However, it is observed that there are many users face different security problems while accessing these Social Networking Websites. These security problems are Identity Theft, Stealing of Personal, professional data and so on. Researchers have discussed various kinds of Security Issues Faced and Security Measures Practiced by Citizens of Pune City while working on Social Networking Websites. This paper is focused on the major issues of security and it also deals with possible solutions on these issues.
ER  - 

TY  - CONF
JO  - Parallel and Distributed Systems (ICPADS), 2011 IEEE 17th International Conference on
TI  - Visualization System for Log Analysis with Probabilities of Incorrect Operation
T2  - Parallel and Distributed Systems (ICPADS), 2011 IEEE 17th International Conference on
IS  - 
SN  - 1521-9097
VO  - 
SP  - 929
EP  - 934
AU  - Nishioka, C.
AU  - Kozaki, M.
AU  - Okada, K.
Y1  - 7-9 Dec. 2011
PY  - 2011
KW  - data visualisation
KW  - security of data
KW  - system monitoring
KW  - incorrect operation
KW  - information leakages
KW  - information society
KW  - log analysis
KW  - security managers
KW  - visualization system
KW  - Data visualization
KW  - Probability
KW  - Proposals
KW  - Security
KW  - Servers
KW  - Sorting
KW  - Visualization
KW  - Data Analysis
KW  - Data Visualization
KW  - Human Interface
KW  - Information Security
VL  - 
JA  - Parallel and Distributed Systems (ICPADS), 2011 IEEE 17th International Conference on
DO  - 10.1109/ICPADS.2011.147
AB  - As advancement of information society, information leakages grow into a serious problem. It is important for security managers to analysis log-files for finding out cause of leakages promptly. Existing methods of presenting log-files take the method of ordering them in time. It makes easy to understand a flow of operations. However, if a log recording an incorrect operation is included in the back of log-file, finding out it may drop back. To address this problem, this paper presents visualization system for log analysis with probabilities of incorrect operation. Incorrect operations are operations that may cause a security incident. Probabilities of incorrect operation are set up by rate of number of incorrect operations in past log-files. Security analysts set order of priority, and logs are sorted. Also, we introduce Visualize Part to help security analysts understand a flow of operations in spite of not ordering logs in time. We aim to contribute speedy security analyses by combine visualizing log-file with probabilities of incorrect operation. To evaluate our proposal, accuracy and efficiency are measured by user experiment. Our proposal tool was compared with the tool without probabilities of incorrect operation. As the result, in terms of accuracy, there are no significant difference between. However, our proposal demonstrate a 39.5% improved efficiency.
ER  - 

TY  - CONF
JO  - Globecom Workshops (GC Wkshps), 2013 IEEE
TI  - Massive distributed and parallel log analysis for organizational security
T2  - Globecom Workshops (GC Wkshps), 2013 IEEE
IS  - 
SN  - 
VO  - 
SP  - 194
EP  - 199
AU  - Xiaokui Shu
AU  - Smiy, J.
AU  - Danfeng Yao
AU  - Heshan Lin
Y1  - 9-13 Dec. 2013
PY  - 2013
KW  - cloud computing
KW  - distributed processing
KW  - security of data
KW  - system monitoring
KW  - Amazon cloud environments
KW  - EC2
KW  - MapReduce
KW  - S3
KW  - cloud-based distributed framework
KW  - dynamic task scheduling
KW  - log data demands
KW  - massive distributed frameworks
KW  - organizational security
KW  - parallel security log analysis framework
KW  - streaming logs
KW  - transaction logs
KW  - Cloud computing
KW  - Conferences
KW  - Data privacy
KW  - Organizations
KW  - Security
VL  - 
JA  - Globecom Workshops (GC Wkshps), 2013 IEEE
DO  - 10.1109/GLOCOMW.2013.6824985
AB  - Security log analysis is extremely useful for uncovering intrusions and anomalies. However, the sheer volume of log data demands new frameworks and techniques of computing and security. We present a lightweight distributed and parallel security log analysis framework that allows organizations to analyze a massive number of system, network, and transaction logs efficiently and scalably. Different from the general distributed frameworks, e.g., MapReduce, our framework is specifically designed for security log analysis. It features a minimum set of necessary properties, such as dynamic task scheduling for streaming logs. For prototyping, we implement our framework in Amazon cloud environments (EC2 and S3) with a basic analysis application. Our evaluation demonstrates the effectiveness of our design and shows the potential of our cloud-based distributed framework in large-scale log analysis scenarios.
ER  - 

TY  - CONF
JO  - IT Security Incident Management and IT Forensics, 2009. IMF '09. Fifth International Conference on
TI  - Safe-Keeping Digital Evidence with Secure Logging Protocols: State of the Art and Challenges
T2  - IT Security Incident Management and IT Forensics, 2009. IMF '09. Fifth International Conference on
IS  - 
SN  - 
VO  - 
SP  - 94
EP  - 110
AU  - Accorsi, R.
Y1  - 15-17 Sept. 2009
PY  - 2009
KW  - cryptographic protocols
KW  - legislation
KW  - system monitoring
KW  - legal requirement
KW  - logging protocol security
KW  - safe-keeping digital evidence
KW  - Computer security
KW  - Cryptographic protocols
KW  - Cryptography
KW  - Data security
KW  - Law
KW  - Legal factors
KW  - Legislation
KW  - Protection
KW  - Protective relaying
KW  - Systems engineering and theory
KW  - Digital evidence
KW  - Legal and security requirements
KW  - Preservation
KW  - Secure logging protocols
VL  - 
JA  - IT Security Incident Management and IT Forensics, 2009. IMF '09. Fifth International Conference on
DO  - 10.1109/IMF.2009.18
AB  - While log data are being increasingly used as digital evidence in court, the extent to which existing secure logging protocols used to collect log data fulfill the legal requirements for admissible evidence remain largely unclear. This paper elucidates a subset of the necessary secure requirements for digital evidence and extensively surveys the state of the art secure logging protocols, thereby demonstrating that none of the current protocols completely fulfills the elucidated requirements for admissible evidence. In analyzing the shortcoming of logging protocols, the paper also elaborates on the related research challenges.
ER  - 

TY  - CONF
JO  - New Trends in Information and Service Science, 2009. NISS '09. International Conference on
TI  - A Collaborative Network Security Platform in P2P Networks
T2  - New Trends in Information and Service Science, 2009. NISS '09. International Conference on
IS  - 
SN  - 
VO  - 
SP  - 1251
EP  - 1256
AU  - Chun-Hsin Wang
AU  - Chun-Wei Huang
Y1  - June 30 2009-July 2 2009
PY  - 2009
KW  - Internet
KW  - peer-to-peer computing
KW  - security of data
KW  - telecommunication traffic
KW  - Internet
KW  - P2P networks
KW  - TCP SYN flooding attack
KW  - collaborative network security platform
KW  - logging services
KW  - packets filtering
KW  - packets forwarding
KW  - packets sniffing
KW  - published services
KW  - Collaboration
KW  - Collaborative work
KW  - Computer crime
KW  - Floods
KW  - Information security
KW  - Internet
KW  - Intrusion detection
KW  - Monitoring
KW  - Protection
KW  - Telecommunication traffic
KW  - Collaborative Network Security
KW  - P2P
VL  - 
JA  - New Trends in Information and Service Science, 2009. NISS '09. International Conference on
DO  - 10.1109/NISS.2009.222
AB  - Network security problems emerge in an endless stream and cause the inestimable damage. To solve network security problems efficiently, it is not enough to make good protection at nodes or protect networks from outside attacks. Many network security problems should be solved efficiently in collaborative approaches which can integrate various resources over internet to defense network security. In this paper, we have designed and implemented a collaborative network security platform based on P2P system. The nodes participated in the P2P system can publish their designed defensible services against network security problems. Based on the published services, collaborative network applications can be developed easily to solve the network security problems on demand. An experiment against TCP SYN flooding attack is demonstrated by the designed defensible services including packets sniffing, forwarding, filtering, and logging services, which can trace the attack origins and filter malicious traffic efficiently.
ER  - 

TY  - CONF
JO  - Computer Software and Applications Conference, 2009. COMPSAC '09. 33rd Annual IEEE International
TI  - Log Data as Digital Evidence: What Secure Logging Protocols Have to Offer?
T2  - Computer Software and Applications Conference, 2009. COMPSAC '09. 33rd Annual IEEE International
IS  - 
SN  - 0730-3157
VO  - 2
SP  - 398
EP  - 403
AU  - Accorsi, R.
Y1  - 20-24 July 2009
PY  - 2009
KW  - security of data
KW  - system monitoring
KW  - digital evidence
KW  - log data
KW  - secure logging protocol
KW  - secure requirements
KW  - Computer applications
KW  - Computer security
KW  - Cryptographic protocols
KW  - Cryptography
KW  - Data security
KW  - Law
KW  - Legal factors
KW  - Proposals
KW  - Protection
KW  - Protective relaying
KW  - Digital Evidence
KW  - Forensics
KW  - Secure Logging
VL  - 2
JA  - Computer Software and Applications Conference, 2009. COMPSAC '09. 33rd Annual IEEE International
DO  - 10.1109/COMPSAC.2009.166
AB  - While log data are being increasingly used as digital evidence in judicial disputes, the extent to which existing secure logging protocols used to collect log data fulfill the legal requirements for admissible evidence remain largely unclear. We elucidate the necessary secure requirements for digital evidence and extensively survey the state of the art secure logging protocols,thereby demonstrating that none of the current proposals fulfills the necessary conditions for admissible evidence.
ER  - 

TY  - CONF
JO  - Information Technology: New Generations (ITNG), 2010 Seventh International Conference on
TI  - A Stepwise Approach Towards an Interoperable and Flexible Logging Principle for Audit Trails
T2  - Information Technology: New Generations (ITNG), 2010 Seventh International Conference on
IS  - 
SN  - 
VO  - 
SP  - 114
EP  - 119
AU  - Huemer, D.
AU  - A Min Tjoa
Y1  - 12-14 April 2010
PY  - 2010
KW  - Internet
KW  - XML
KW  - grid computing
KW  - security of data
KW  - system monitoring
KW  - XML technology
KW  - audit trails
KW  - cloud computing
KW  - computer system
KW  - distributed systems
KW  - flexible logging principle
KW  - grid computing
KW  - interoperable logging principle
KW  - security breaches
KW  - Authorization
KW  - Cloud computing
KW  - Computer security
KW  - Data security
KW  - Event detection
KW  - Grid computing
KW  - Information technology
KW  - Interactive systems
KW  - Power system security
KW  - XML
KW  - XML
KW  - audit trail
KW  - cloud computing
KW  - grid computing
KW  - logging
VL  - 
JA  - Information Technology: New Generations (ITNG), 2010 Seventh International Conference on
DO  - 10.1109/ITNG.2010.33
AB  - Although event recording on a computer system (also known as logging) is of utmost importance for reconstructing and detecting security relevant events, currently no adequate and sophisticated solution for complex environments, such as Grid and Cloud Computing, exist. Current LOG file formats lack of several important factors, hindering automatic evaluation needed for distributed systems to comply with laws and regulations or hindering detection of security breaches. In this paper we present a new concept utilizing XML technology to solve mentioned problems of current LOG file formats and describe the benefits of this new idea resulting in a flexible, interoperable LOG environment enabling automatic evaluation even in locally dispersed computing systems. As a result reliable, robust and consistent LOG files are produced allowing for automatic evaluation.
ER  - 

TY  - JOUR
JO  - Spectrum, IEEE
TI  - The threat from the net [Internet security]
T2  - Spectrum, IEEE
IS  - 8
SN  - 0018-9235
VO  - 34
SP  - 56
EP  - 63
AU  - Bishop, M.
AU  - Cheung, S.
AU  - Wee, C.
Y1  - Aug 1997
PY  - 1997
KW  - Internet
KW  - security of data
KW  - Internet intruder detection
KW  - Internet security
KW  - counterattack
KW  - damage assessment
KW  - event logs
KW  - intrusion analysis
KW  - intrusion detection
KW  - postattack analysis
KW  - security enforcement issues
KW  - security policy
KW  - Access control
KW  - Computer security
KW  - Credit cards
KW  - Cryptography
KW  - Information security
KW  - Mathematical model
KW  - Performance analysis
KW  - Permission
KW  - Protection
KW  - System performance
VL  - 34
JA  - Spectrum, IEEE
DO  - 10.1109/6.609475
AB  - As it stands today, the Internet is not secure, so the only option is to understand how attacks occur and how best to protect against them. Ways to detect an intrusion and assess what the intruder did must be well thought out. For the most part, they will rely upon the ability of each system on the Internet to keep a log of events. The logs are invaluable for intrusion detection and analysis, indeed, they are basic to all postattack analysis. Authors of the security policy must determine what to log (keeping in mind how the desired level of logging will affect system performance) and how the logs should be analyzed. The logs should note who has entered the system as well as what they have done. Before a detailed examination is made of security methods, the issues affecting security enforcement are reviewed. The detection of intrusion using manual and automatic methods are discussed as are counterattack and damage assessment
ER  - 

TY  - CONF
JO  - Cloud Computing and Big Data (CloudCom-Asia), 2013 International Conference on
TI  - Logging Solutions to Mitigate Risks Associated with Threats in Infrastructure as a Service Cloud
T2  - Cloud Computing and Big Data (CloudCom-Asia), 2013 International Conference on
IS  - 
SN  - 
VO  - 
SP  - 163
EP  - 170
AU  - Winai Wongthai
AU  - Rocha, F.
AU  - Van Moorsel, A.
Y1  - 16-19 Dec. 2013
PY  - 2013
KW  - cloud computing
KW  - operating systems (computers)
KW  - risk analysis
KW  - security of data
KW  - virtual machines
KW  - CSA
KW  - IaaS
KW  - cloud computing
KW  - cloud consumers
KW  - cloud providers
KW  - cloud security alliance
KW  - computational resources
KW  - consumers virtual machines-VM
KW  - infrastructure as a service cloud
KW  - logging solutions
KW  - malicious activities
KW  - risks associated
KW  - security concerns
KW  - Business
KW  - Cloud computing
KW  - Electronic mail
KW  - History
KW  - Postal services
KW  - Prototypes
KW  - Security
KW  - accountability
KW  - cloud monitoring
KW  - logging system
VL  - 
JA  - Cloud Computing and Big Data (CloudCom-Asia), 2013 International Conference on
DO  - 10.1109/CLOUDCOM-ASIA.2013.70
AB  - Cloud computing offers computational resources such as processing, networking, and storage to customers. However, the cloud also brings with it security concerns which affect both cloud consumers and providers. The Cloud Security Alliance (CSA) define the security concerns as the seven main threats. This paper investigates how threat number one (malicious activities performed in consumers' virtual machines/VMs) can affect the security of both consumers and providers. It proposes logging solutions to mitigate risks associated with this threat. We systematically design and implement a prototype of the proposed logging solutions in an IaaS to record the history of customer VM's files. The proposed system can be modified in order to record VMs' process behaviour log files. These log files can assist in identifying malicious activities (spamming) performed in the VMs as an example of how the proposed solutions benefits the provider side. The proposed system can record the log files while having a smaller trusted computing base compared to previous work. Thus, the logging solutions in this paper can assist in mitigating risks associated with the CSA threats to benefit consumers and providers.
ER  - 

TY  - CONF
JO  - Advances in Computing, Communications and Informatics (ICACCI, 2014 International Conference on
TI  - Sys-log classifier for Complex Event Processing system in network security
T2  - Advances in Computing, Communications and Informatics (ICACCI, 2014 International Conference on
IS  - 
SN  - 
VO  - 
SP  - 2031
EP  - 2035
AU  - Jayan, K.
AU  - Rajan, A.K.
Y1  - 24-27 Sept. 2014
PY  - 2014
KW  - Internet
KW  - pattern classification
KW  - security of data
KW  - support vector machines
KW  - CEP system
KW  - DNS exploit attack
KW  - DOS attack
KW  - Internet
KW  - SVM classifier
KW  - attack patterns
KW  - buffer overflow attack
KW  - complex event processing system
KW  - cross site attack
KW  - denial-of-service attack
KW  - domain name system attack
KW  - log information
KW  - network security
KW  - support vector machine
KW  - sys-log classifier
KW  - Engines
KW  - Kernel
KW  - Protocols
KW  - Complex Event Processing
KW  - Log data
KW  - Network security
KW  - SVM
KW  - Security attacks
KW  - Security devices
VL  - 
JA  - Advances in Computing, Communications and Informatics (ICACCI, 2014 International Conference on
DO  - 10.1109/ICACCI.2014.6968471
AB  - Internet is growing very rapidly; so is its security issues. There are a wide variety of attacks possible in networked machines. DOS attack, buffer overflow attack, cross site attack, DNS exploit attack are a few to name. Without security measures and controls in place, network and data might be subjected to attacks. The commonly deployed security devices are firewall, IDS, IPS, anti-virus etc. Potential number of threats is still pervading which are formulated as attacks by combining many unnoticed primitive events. The best solution is to install a Complex Event Processing (CEP) system which can analyze multiple devices to infer attack patterns. Log information of network devices is the best choice for analysis. In a large network, there will be millions of events logged. Correlated analysis of this huge volume of log is the main challenge in Complex Event Processing (CEP) system. We describe a method to reduce the input to the Complex Event Processing (CEP) system, using Support Vector Machine (SVM) classifier. Our experiment shows that the input size can be considerably reduce using the classifier. Hence improves the working of Complex Event Processing (CEP) system.
ER  - 

TY  - JOUR
JO  - Systems Journal, IEEE
TI  - Secure Logging as a Service&#x2014;Delegating Log Management to the Cloud
T2  - Systems Journal, IEEE
IS  - 2
SN  - 1932-8184
VO  - 7
SP  - 323
EP  - 334
AU  - Ray, I.
AU  - Belyaev, K.
AU  - Strizhov, M.
AU  - Mulamba, D.
AU  - Rajaram, M.
Y1  - June 2013
PY  - 2013
KW  - cloud computing
KW  - data privacy
KW  - recording
KW  - security of data
KW  - cloud
KW  - cloud-based log management service
KW  - cost saving measure
KW  - log records privacy
KW  - secure logging
KW  - Cryptography
KW  - Generators
KW  - Message authentication
KW  - Monitoring
KW  - Organizations
KW  - Protocols
KW  - Cloud computing
KW  - logging
KW  - privacy
KW  - security
VL  - 7
JA  - Systems Journal, IEEE
DO  - 10.1109/JSYST.2012.2221958
AB  - Securely maintaining log records over extended periods of time is very important to the proper functioning of any organization. Integrity of the log files and that of the logging process need to be ensured at all times. In addition, as log files often contain sensitive information, confidentiality and privacy of log records are equally important. However, deploying a secure logging infrastructure involves substantial capital expenses that many organizations may find overwhelming. Delegating log management to the cloud appears to be a viable cost saving measure. In this paper, we identify the challenges for a secure cloud-based log management service and propose a framework for doing the same.
ER  - 

TY  - CONF
JO  - Advanced Information Networking and Applications Workshops (WAINA), 2010 IEEE 24th International Conference on
TI  - Data Security and Information Privacy for PDA Accessible Clinical-Log for Medical Education in Problem-Based Learning (PBL) Approach
T2  - Advanced Information Networking and Applications Workshops (WAINA), 2010 IEEE 24th International Conference on
IS  - 
SN  - 
VO  - 
SP  - 979
EP  - 984
AU  - Luanrattana, R.
AU  - Khin Than Win
AU  - Fulcher, J.
Y1  - 20-23 April 2010
PY  - 2010
KW  - biomedical education
KW  - computer aided instruction
KW  - mobile computing
KW  - notebook computers
KW  - security of data
KW  - PDA accessible clinical-log
KW  - data security
KW  - information privacy
KW  - medical education
KW  - mobile technology
KW  - problem-based learning
KW  - Authentication
KW  - Cryptography
KW  - Data privacy
KW  - Data security
KW  - Digital signatures
KW  - Educational technology
KW  - Information security
KW  - Medical services
KW  - Personal digital assistants
KW  - Protection
VL  - 
JA  - Advanced Information Networking and Applications Workshops (WAINA), 2010 IEEE 24th International Conference on
DO  - 10.1109/WAINA.2010.42
AB  - Data security and information privacy are the important aspects to consider for the use of mobile technology for recording clinical experience and encounter in medical education. Objective: This study aims to address the qualitative findings of the appropriate data security and information privacy for PDA accessible clinical-log in problem-based learning (PBL) approach in medical education. Method: The semi-structured interviews were conducted with the medical faculty members, honorary clinical academics and medical education technology specialists. Results: Data security and information access plan were determined for managing clinical-log data. The results directed the guideline for the future development and implementation of clinical-log and other functionalities on PDAs. Conclusion: The findings provide the understanding of aspects, concerns and appropriate strategy to safeguard data security and information privacy of PDA accessible clinical-log.
ER  - 

TY  - CONF
JO  - IT Convergence and Security (ICITCS), 2013 International Conference on
TI  - Security Event Correlation Supported by Multi-Core Architecture
T2  - IT Convergence and Security (ICITCS), 2013 International Conference on
IS  - 
SN  - 
VO  - 
SP  - 1
EP  - 5
AU  - Feng Cheng
AU  - Azodi, A.
AU  - Jaeger, D.
AU  - Meinel, C.
Y1  - 16-18 Dec. 2013
PY  - 2013
KW  - multiprocessing systems
KW  - parallel architectures
KW  - pattern clustering
KW  - security of data
KW  - storage management
KW  - IT-Infrastructure
KW  - SAL platform
KW  - SIEM platform
KW  - application logs
KW  - central interface
KW  - central storage
KW  - computing resources
KW  - correlation algorithms
KW  - correlation tasks
KW  - event clustering
KW  - event correlation module
KW  - high performance security analytics
KW  - in-memory data management technique
KW  - k-means algorithm
KW  - management methods
KW  - multicore architecture
KW  - next generation security information and event management platform]
KW  - real-time event information
KW  - real-time events
KW  - security analytics lab
KW  - security event correlation
KW  - system logs
KW  - Algorithm design and analysis
KW  - Clustering algorithms
KW  - Correlation
KW  - Multicore processing
KW  - Parallel processing
KW  - Security
VL  - 
JA  - IT Convergence and Security (ICITCS), 2013 International Conference on
DO  - 10.1109/ICITCS.2013.6717881
AB  - A huge amount of information about real-time events are being generated in every second in a running IT-Infrastructure and recorded by the system logs, application logs, as well as the output from the deployed security or management methods, e.g., IDS alerts, firewall logs, scanning reports, etc. To rapidly gather, process, correlate, and analyze the massive event information is a challenging task. High performance security analytics is proposed to address this challenge by which the real-time event information can be normalized, centralized, and correlated to help identify the current running state of the target environment. As an example of next generation Security Information and Event Management (SIEM) platform, Security Analytics Lab (SAL) has been designed and implemented based on the newly emerged In-Memory data management technique, which makes it possible to efficiently organize, access, and process different types of event information through a consistent central storage and interface. In this paper, the multi-core architecture is introduced on the event correlation module of SAL platform by which the correlation tasks can be executed in parallel by different computing resources. The k-means algorithm is implemented as an example of possible event clustering and correlation algorithms. Several experiments are conducted and analyzed to show that the performance of analytics can be significantly improved by applying multi-core architecture in the event correlation procedure.
ER  - 

TY  - CONF
JO  - Empirical Software Engineering and Measurement, 2009. ESEM 2009. 3rd International Symposium on
TI  - An empirical study of security problem reports in Linux distributions
T2  - Empirical Software Engineering and Measurement, 2009. ESEM 2009. 3rd International Symposium on
IS  - 
SN  - 1938-6451
VO  - 
SP  - 481
EP  - 484
AU  - Anbalagan, P.
AU  - Vouk, M.
Y1  - 15-16 Oct. 2009
PY  - 2009
KW  - Linux
KW  - public domain software
KW  - security of data
KW  - statistical distributions
KW  - Fedora
KW  - RHEL
KW  - RedHat Enterprise Linux
KW  - Suse Linux distribution
KW  - Ubuntu
KW  - open source project
KW  - security problem report
KW  - Computer science
KW  - Computer security
KW  - Data security
KW  - Databases
KW  - Information security
KW  - Linux
KW  - National security
KW  - Software engineering
KW  - Software measurement
KW  - Statistics
VL  - 
JA  - Empirical Software Engineering and Measurement, 2009. ESEM 2009. 3rd International Symposium on
DO  - 10.1109/ESEM.2009.5315985
AB  - Existing studies on problem reports in open source projects focus primarily on the analysis of the general category of problem reports, or limit their attention to observations on the number of security problem reports. To evaluate the security of a project, it is necessary to know not only how many security problem reports are logged but also how many are reported and how promptly they are corrected etc. In this paper, we study publicly disclosed security problem reports from eight releases of Fedora, nine releases of Ubuntu, four releases of RedHat Enterprise Linux (RHEL) and two releases of Suse Linux distributions, analyse and discuss which type of problem reports and how frequently they are reported, and how promptly they are corrected. Overall, Fedora and Suse show good results with high and medium severity security problem reports resolved without a backlog. On the other hand, RHEL and Ubuntu show less positive results with presence of backlogs.
ER  - 

TY  - CONF
JO  - Advanced Information Networking and Applications Workshops (WAINA), 2013 27th International Conference on
TI  - A Generic Logging Template for Infrastructure as a Service Cloud
T2  - Advanced Information Networking and Applications Workshops (WAINA), 2013 27th International Conference on
IS  - 
SN  - 
VO  - 
SP  - 1153
EP  - 1160
AU  - Wongthai, W.
AU  - Rocha, F.L.
AU  - Van Moorsel, A.
Y1  - 25-28 March 2013
PY  - 2013
KW  - cloud computing
KW  - computerised monitoring
KW  - resource allocation
KW  - risk analysis
KW  - security of data
KW  - service-oriented architecture
KW  - CSA threat number
KW  - IaaS cloud
KW  - cloud security alliance
KW  - cloud-based infrastructure
KW  - generic architecture template
KW  - generic logging template
KW  - infrastructure as a service cloud
KW  - logging systems
KW  - pay-per-use cost model
KW  - production systems
KW  - raw computation resources
KW  - risk association
KW  - threat identification
KW  - threat mitigation
KW  - Cloud computing
KW  - Computer architecture
KW  - Electronic mail
KW  - Kernel
KW  - Monitoring
KW  - Security
KW  - Virtual machine monitors
KW  - IaaS
KW  - cloud monitoring
KW  - logging system
VL  - 
JA  - Advanced Information Networking and Applications Workshops (WAINA), 2013 27th International Conference on
DO  - 10.1109/WAINA.2013.108
AB  - Infrastructure as a Service (IaaS) consists of a cloud-based infrastructure to offer consumers raw computation resources such as storage and networking. These resources are billed using a pay-per-use cost model. However, this type of infrastructure is far from being a security haven as the seven main threats defined by the Cloud Security Alliance (CSA) indicate. Using logging systems can provide evidence to support accountability for an IaaS cloud, which helps us mitigating known threats. In this paper, we research to which extent such logging systems help mitigate risks associated with the threats identified by the CSA. A generic architecture 'template' for logging systems is proposed. This template encompasses all possible instantiations of logging solutions for IaaS cloud. We map existing logging systems to our generic template, and identify a logging solution to mitigate the risks associated with CSA threat number one (related to spam activities). We then argue that the template we suggest can be used to perform a systematic analysis of logging systems in terms of security before deploying them in production systems.
ER  - 

TY  - CONF
JO  - Intelligence and Security Informatics, 2009. ISI '09. IEEE International Conference on
TI  - New approaches for intrusion detection based on logs correlation
T2  - Intelligence and Security Informatics, 2009. ISI '09. IEEE International Conference on
IS  - 
SN  - 
VO  - 
SP  - 234
EP  - 234
AU  - Azarkasb, S.O.
AU  - Ghidary, S.S.
Y1  - 8-11 June 2009
PY  - 2009
KW  - security of data
KW  - denial-of-service attacks
KW  - intrusion detection
KW  - log correlation methods
KW  - log file entries
KW  - logs correlation
KW  - network administrators
KW  - probing
KW  - remote-to-local attacks
KW  - system security
KW  - user-to-root attacks
KW  - Artificial intelligence
KW  - Computer crime
KW  - Computer science
KW  - Computer security
KW  - Correlation
KW  - Information analysis
KW  - Information security
KW  - Intersymbol interference
KW  - Intrusion detection
KW  - USA Councils
KW  - Anomaly Detection
KW  - Computer Security
KW  - Intrusion Detection
KW  - Logs Correlation
KW  - Misuse Detection
VL  - 
JA  - Intelligence and Security Informatics, 2009. ISI '09. IEEE International Conference on
DO  - 10.1109/ISI.2009.5137316
AB  - Network administrators are able to correlate log file entries manually. Large volume and low quality of log files justify the need for further log processing. The manual log processing is lack of flexibility. It is time consuming, and one doesn't get the general view of the log files in the network. Without this general view it is hard to correlate information between the network components. Events seemingly unessential by themselves can in reality be a piece of a larger threat. In this regard, different log correlation methods are proposed to improve alert quality and to give a comprehensive view of system security. In this paper, we show how different attacks categorized in three categories with different behavior: Denial of service (DoS) attacks, user-to-root (U2R) &amp; remote-to-local (R2L) attacks and probing, are reflected in different logs and argue that some attacks are not evident when a single log is analyzed.
ER  - 

TY  - CONF
JO  - Software Testing, Verification, and Validation Workshops (ICSTW), 2010 Third International Conference on
TI  - Towards Security Vulnerability Detection by Source Code Model Checking
T2  - Software Testing, Verification, and Validation Workshops (ICSTW), 2010 Third International Conference on
IS  - 
SN  - 
VO  - 
SP  - 381
EP  - 387
AU  - Keqin Li
Y1  - 6-10 April 2010
PY  - 2010
KW  - formal verification
KW  - security of data
KW  - software quality
KW  - Bandera tool set
KW  - SAP security programming guidelines
KW  - cross-site scripting attack
KW  - security vulnerability detection
KW  - software code quality
KW  - source code model checking
KW  - Application software
KW  - Electronic mail
KW  - Guidelines
KW  - Information security
KW  - Java
KW  - Programming profession
KW  - Software quality
KW  - Software testing
KW  - Specification languages
KW  - Time factors
KW  - model checking
KW  - security
KW  - source code analysis
VL  - 
JA  - Software Testing, Verification, and Validation Workshops (ICSTW), 2010 Third International Conference on
DO  - 10.1109/ICSTW.2010.23
AB  - Security in code level is an important aspect to achieve high quality software. Various security programming guidelines are defined to improve the quality of software code. At the same time, enforcing mechanisms of these guidelines are needed. In this paper, we use source code model checking technique to check whether some security programming guidelines are followed, and correspondingly to detect related security vulnerabilities. Two SAP security programming guidelines related to logging sensitive information and Cross-Site Scripting attack are used as examples. In the case studies, Bandera Tool Set is used as source code model checker, and minimizing programmers' additional effort is set as one of the goals.
ER  - 

TY  - CONF
JO  - Privacy, Security and Trust, 2008. PST '08. Sixth Annual Conference on
TI  - LogView: Visualizing Event Log Clusters
T2  - Privacy, Security and Trust, 2008. PST '08. Sixth Annual Conference on
IS  - 
SN  - 
VO  - 
SP  - 99
EP  - 108
AU  - Makanju, A.
AU  - Brooks, S.
AU  - Zincir-Heywood, A.N.
AU  - Milios, E.E.
Y1  - 1-3 Oct. 2008
PY  - 2008
KW  - computer network management
KW  - data visualisation
KW  - file organisation
KW  - security of data
KW  - system monitoring
KW  - LogView
KW  - administration setup
KW  - event clustering algorithm
KW  - event log cluster visualization
KW  - hierarchical structure
KW  - log files
KW  - network administrator
KW  - network management
KW  - security issues
KW  - simple log file clustering tool
KW  - visualization tool
KW  - Clustering algorithms
KW  - Computer science
KW  - Computer security
KW  - Data analysis
KW  - Data security
KW  - Data visualization
KW  - Event detection
KW  - Fault detection
KW  - Monitoring
KW  - Privacy
KW  - Clustering
KW  - Event Logs
KW  - SLCT
KW  - Treemaps
KW  - Visualization
VL  - 
JA  - Privacy, Security and Trust, 2008. PST '08. Sixth Annual Conference on
DO  - 10.1109/PST.2008.17
AB  - Event logs or log files form an essential part of any network management and administration setup. While log files are invaluable to a network administrator, the vast amount of data they sometimes contain can be overwhelming and can sometimes hinder rather than facilitate the tasks of a network administrator. For this reason several event clustering algorithms for log files have been proposed, one of which is the event clustering algorithm proposed by Risto Vaarandi, on which his simple log file clustering tool (SLCT) is based. The aim of this work is to develop a visualization tool that can be used to view log files based on the clusters produced by SLCT. The proposed visualization tool, which is called LogView, utilizes treemaps to visualize the hierarchical structure of the clusters produced by SLCT. Our results based on different application log files show that LogView can ease the summarization of vast amount of data contained in the log files. This in turn can help to speed up the analysis of event data in order to detect any security issues on a given application.
ER  - 

TY  - CONF
JO  - Information Assurance and Security (IAS), 2013 9th International Conference on
TI  - Hierarchical object log format for normalisation of security events
T2  - Information Assurance and Security (IAS), 2013 9th International Conference on
IS  - 
SN  - 
VO  - 
SP  - 25
EP  - 30
AU  - Sapegin, A.
AU  - Jaeger, D.
AU  - Azodi, A.
AU  - Gawron, M.
AU  - Feng Cheng
AU  - Meinel, C.
Y1  - 4-6 Dec. 2013
PY  - 2013
KW  - security of data
KW  - system monitoring
KW  - hierarchical object log format
KW  - intrusion detection systems
KW  - security event normalisation
KW  - Bridges
KW  - Kernel
KW  - Receivers
KW  - Servers
KW  - common log format
KW  - intrusion detection
KW  - log normalisation
VL  - 
JA  - Information Assurance and Security (IAS), 2013 9th International Conference on
DO  - 10.1109/ISIAS.2013.6947748
AB  - The differences in log file formats employed in a variety of services and applications remain to be a problem for security analysts and developers of intrusion detection systems. The proposed solution, i.e. the usage of common log formats, has a limited utilization within existing solutions for security management. In our paper, we reveal the reasons for this limitation. We show disadvantages of existing common log formats for normalisation of security events. To deal with it we have created a new log format that fits for intrusion detection purposes and can be extended easily. Taking previous work into account, we would like to propose a new format as an extension to existing common log formats, rather than a standalone specification.
ER  - 

TY  - CONF
JO  - Digital EcoSystems and Technologies Conference, 2007. DEST '07. Inaugural IEEE-IES
TI  - A Robust Technique of Anti Key-Logging using Key-Logging Mechanism
T2  - Digital EcoSystems and Technologies Conference, 2007. DEST '07. Inaugural IEEE-IES
IS  - 
SN  - 
VO  - 
SP  - 314
EP  - 318
AU  - Baig, M.M.
AU  - Mahmood, W.
Y1  - 21-23 Feb. 2007
PY  - 2007
KW  - data privacy
KW  - security of data
KW  - antikeylogging
KW  - antivirus application
KW  - keyboard logging
KW  - keylogging mechanism
KW  - mouse logging
KW  - robust technique
KW  - screen logging
KW  - surveillance application
KW  - system privacy
KW  - system security
KW  - voice logging
KW  - Application software
KW  - Computer science
KW  - Ecosystems
KW  - Hardware
KW  - Internet
KW  - Keyboards
KW  - Operating systems
KW  - Privacy
KW  - Robustness
KW  - Surveillance
KW  - anti key-logging
KW  - application hooks
KW  - non-signature based scanning
KW  - software key-logger
KW  - system hooks
VL  - 
JA  - Digital EcoSystems and Technologies Conference, 2007. DEST '07. Inaugural IEEE-IES
DO  - 10.1109/DEST.2007.371990
AB  - System security and privacy always have to face new confronts. Continuous updates in the operating systems and anti-virus applications strive to amplify the system security level. In recent years 'key-loggers' have proved to be one of the prevalent intimidations to security and privacy. Key-logger is a surreptitious surveillance application, which is used to keep record of user's activities on the computer in various ways like keyboard logging, screen logging, mouse logging and voice logging, completely in imperceptible mode. Although key-loggers can also be used for prolific purposes but due to the tremendous increase in the Internet usage, the caustic use of key-loggers simply surmounts its advantages. Key-loggers have gained so much supremacy in their execution that they have become a serious intimidation to the privacy and security of a computer. The fact which makes the key-loggers more perilous is their undetectable nature against anti-virus and spy-where applications. This paper discusses some existing techniques of fortification against key-loggers and also exemplifies a new technique along with its proved advantages.
ER  - 

TY  - CONF
JO  - Security and Privacy, 1997. Proceedings., 1997 IEEE Symposium on
TI  - The design and implementation of a multilevel secure log manager
T2  - Security and Privacy, 1997. Proceedings., 1997 IEEE Symposium on
IS  - 
SN  - 1081-6011
VO  - 
SP  - 55
EP  - 64
AU  - Pesati, V.R.
AU  - Keefe, T.F.
AU  - Pal, S.
Y1  - 4-7 May 1997
PY  - 1997
KW  - data loggers
KW  - database management systems
KW  - security of data
KW  - software performance evaluation
KW  - bandwidth
KW  - flush latency
KW  - log management
KW  - multilevel secure database system
KW  - multilevel secure log manager
KW  - performance
KW  - security evaluation
KW  - system design
KW  - system implementation
KW  - Bandwidth
KW  - Data security
KW  - Database systems
KW  - Delay
KW  - Hardware
KW  - Information security
KW  - Multilevel systems
KW  - National security
KW  - Protocols
KW  - Prototypes
VL  - 
JA  - Security and Privacy, 1997. Proceedings., 1997 IEEE Symposium on
DO  - 10.1109/SECPRI.1997.601316
AB  - This paper discusses the security issues involved in log management for a multilevel secure database system and presents a design and implementation of a prototype multilevel secure log manager. The main goal of a log manager is to provide high bandwidth and low flush latency. We examine the performance of our design, by observing the flush latency and log bandwidth. We also informally evaluate the security of our approach
ER  - 

TY  - CONF
JO  - Internet of Things (iThings/CPSCom), 2011 International Conference on and 4th International Conference on Cyber, Physical and Social Computing
TI  - Empirical Analysis of Behavior on Information Security
T2  - Internet of Things (iThings/CPSCom), 2011 International Conference on and 4th International Conference on Cyber, Physical and Social Computing
IS  - 
SN  - 
VO  - 
SP  - 358
EP  - 363
AU  - Takemura, T.
Y1  - 19-22 Oct. 2011
PY  - 2011
KW  - decision making
KW  - ergonomics
KW  - organisational aspects
KW  - risk management
KW  - security of data
KW  - statistical analysis
KW  - decision making
KW  - empirical behavior analysis
KW  - incentive system
KW  - individual attributes
KW  - information security accidents
KW  - information security measures
KW  - organizational attributes
KW  - problematic behavior
KW  - risk aversion
KW  - risk perception
KW  - statistical tool
KW  - stepwise log it model
KW  - workplace satisfaction
KW  - Accidents
KW  - Companies
KW  - Electronic mail
KW  - Employment
KW  - Engineering profession
KW  - Information security
KW  - behavior
KW  - information security
KW  - logit model
KW  - risk aversion
KW  - risk perception
VL  - 
JA  - Internet of Things (iThings/CPSCom), 2011 International Conference on and 4th International Conference on Cyber, Physical and Social Computing
DO  - 10.1109/iThings/CPSCom.2011.8
AB  - In this article, we raise some prohibited matters within the organizations from the viewpoint of information security measures, and investigate factors such as individual attributes and organizational attributes which impact the decision making on not complying with the rule. By using the collected data from a survey that we conducted in March 2010 and employing a stepwise log it model as a statistical tool, we analyze the relationships between decision making on not complying with the rule and some attributes. As a result, we found the some features of respondents who took problematic behavior. First of all, the psychological factors such as risk perception and risk aversion influence the problematic behaviors. Next, the high individuals' morals are able to prevent their taking problematic behaviors. Third, the individuals who experienced some information security accidents, or the individuals whose degree of workplace satisfaction is low, tend to take problematic behavior. In addition, by introducing power shift from ruling body to lower organization as the incentive system for employee's working, individuals come to tend to take problematic behaviors. Though with regard to factors such as the individual's attitude toward the risk and/or moral as the rule within the organization, we propose to educate and train about the information security and the awareness. On the other hand, factors such as the experience of information security accidents, the degree of workplace satisfaction and the incentive system for employee's working may be controlled by the organization by designing the appropriate organizational environment.
ER  - 

TY  - CONF
JO  - Circuit, Power and Computing Technologies (ICCPCT), 2014 International Conference on
TI  - Anonymizing log management process for secure logging in the cloud
T2  - Circuit, Power and Computing Technologies (ICCPCT), 2014 International Conference on
IS  - 
SN  - 
VO  - 
SP  - 1559
EP  - 1564
AU  - Rajalakshmi, J.R.
AU  - Rathinraj, M.
AU  - Braveen, M.
Y1  - 20-21 March 2014
PY  - 2014
KW  - cloud computing
KW  - criminal law
KW  - cryptography
KW  - digital forensics
KW  - computer records
KW  - criminal investigations
KW  - cyber forensic process
KW  - electronic evidence
KW  - federal evidence rules
KW  - forensic computing
KW  - homomorphic encryption scheme
KW  - log files
KW  - log management process anonymization
KW  - secure cloud-based log management service
KW  - secure logging infrastructure
KW  - Ciphers
KW  - Cloud computing
KW  - Computers
KW  - Encryption
KW  - Public key
KW  - Fully Homomorphic Algorithm Tor Network
KW  - Homomorphic Encryption
KW  - Log Management
KW  - Logging
VL  - 
JA  - Circuit, Power and Computing Technologies (ICCPCT), 2014 International Conference on
DO  - 10.1109/ICCPCT.2014.7055014
AB  - Log files helps cyber forensic process in inquiring and seizing computer, obtaining electronic evidence for criminal investigations and maintaining computer records for the federal rules of evidence. So it can be said that logging is closely related to Forensic Computing. To make the logs permissible for the use in court, there is a stipulation to prove that the logs have not been modified after being generated. Moreover, since the logs contain private information, they must be protected strictly. Therefore a secure logging scheme that ensures the integrity and confidentiality of the logs is needed. And, deploying a secure logging infrastructure involves ample capital expenses that many organizations may find devastating. Delegating log management to the cloud appears to be a feasible cost saving measure. The challenges for a secure cloud-based log management service are identified and Homomorphic Encryption Scheme is proposed for doing the same.
ER  - 

TY  - CONF
JO  - Computer Security Foundations Symposium (CSF), 2014 IEEE 27th
TI  - How Task Familiarity and Cognitive Predispositions Impact Behavior in a Security Game of Timing
T2  - Computer Security Foundations Symposium (CSF), 2014 IEEE 27th
IS  - 
SN  - 
VO  - 
SP  - 111
EP  - 122
AU  - Grossklags, J.
AU  - Reitter, D.
Y1  - 19-22 July 2014
PY  - 2014
KW  - cognition
KW  - feedback
KW  - security of data
KW  - cognition
KW  - cognitive predispositions
KW  - critical decision-making factor
KW  - economic experiments
KW  - financial account monitor
KW  - impact behavior
KW  - log files
KW  - online security actions
KW  - psychometric scales
KW  - reflective disposition
KW  - risk propensity
KW  - security game
KW  - task familiarity
KW  - temporal-estimation task
KW  - timing-related security situations
KW  - visual feedback modality
KW  - Atmospheric measurements
KW  - Cognition
KW  - Decision making
KW  - Economics
KW  - Games
KW  - Security
KW  - Timing
KW  - behavior
KW  - cognitive biases
KW  - decision-making
KW  - games
KW  - individual differences
KW  - timing
VL  - 
JA  - Computer Security Foundations Symposium (CSF), 2014 IEEE 27th
DO  - 10.1109/CSF.2014.16
AB  - This paper addresses security and safety choices that involve a decision on the timing of an action. Examples of such decisions include when to check log files for intruders and when to monitor financial accounts for fraud or errors. To better understand how performance in timing-related security situations is shaped by individuals' cognitive predispositions, we effectively combine survey measures with economic experiments. Two behavioral experiments are presented in which the timing of online security actions is the critical decision-making factor. The feedback modality in the decision-environment is varied between visual feedback with history (Experiment 1), and temporal feedback without history (Experiment 2). Using psychometric scales, we study the role of individual difference variables, specifically risk propensity and need for cognition. The analysis is based on the data from over 450 participants. We find that risk propensity is not a hindrance in timing tasks. Participants of average risk propensity generally benefit from a reflective disposition (high need for cognition), particularly when visual feedback is given. Overall, participants benefit from need for cognition, however, in the more difficult, temporal-estimation task, this requires familiarity with the task.
ER  - 

TY  - CONF
JO  - Security and Privacy (SP), 2011 IEEE Symposium on
TI  - Verified Security for Browser Extensions
T2  - Security and Privacy (SP), 2011 IEEE Symposium on
IS  - 
SN  - 1081-6011
VO  - 
SP  - 115
EP  - 130
AU  - Guha, A.
AU  - Fredrikson, M.
AU  - Livshits, B.
AU  - Swamy, Nikhil
Y1  - 22-25 May 2011
PY  - 2011
KW  - application program interfaces
KW  - data visualisation
KW  - online front-ends
KW  - program compilers
KW  - security of data
KW  - API
KW  - access control
KW  - browser extensions
KW  - compilers
KW  - data log
KW  - high-level languages
KW  - policy analysis
KW  - program extensions
KW  - security-sensitive Web content
KW  - verified security
KW  - visualization tools
KW  - Browsers
KW  - Fires
KW  - History
KW  - Internet
KW  - Security
KW  - Web pages
KW  - extensions
KW  - policy languages
KW  - security
KW  - type system
KW  - verification
KW  - web browsers
VL  - 
JA  - Security and Privacy (SP), 2011 IEEE Symposium on
DO  - 10.1109/SP.2011.36
AB  - Popup blocking, form filling, and many other features of modern web browsers were first introduced as third-party extensions. New extensions continue to enrich browsers in unanticipated ways. However, powerful extensions require capabilities, such as cross-domain network access and local storage, which, if used improperly, pose a security risk. Several browsers try to limit extension capabilities, but an empirical survey we conducted shows that many extensions are over-privileged under existing mechanisms. This paper presents ibex, a new framework for authoring, analyzing, verifying, and deploying secure browser extensions. Our approach is based on using type-safe, high-level languages to program extensions against an API providing access to a variety of browser features. We propose using Data log to specify fine-grained access control and dataflow policies to limit the ways in which an extension can use this API, thus restricting its privilege over security-sensitive web content and browser resources. We formalize the semantics of policies in terms of a safety property on the execution of extensions and develop a verification methodology that allows us to statically check extensions for policy compliance. Additionally, we provide visualization tools to assist with policy analysis, and compilers to translate extension source code to either. NET byte code or JavaScript, facilitating cross-browser deployment of extensions. We evaluate our work by implementing and verifying~NumExt extensions with a diverse set of features and security policies. We deploy our extensions in Internet Explorer, Chrome, Fire fox, and a new experimental HTML5 platform called C3. In so doing, we demonstrate the versatility and effectiveness of our approach.
ER  - 

TY  - CONF
JO  - Information Technology: New Generations, 2006. ITNG 2006. Third International Conference on
TI  - Analysis of Log Files Intersections for Security Enhancement
T2  - Information Technology: New Generations, 2006. ITNG 2006. Third International Conference on
IS  - 
SN  - 
VO  - 
SP  - 452
EP  - 457
AU  - Kowalski, K.
AU  - Beheshti, M.
Y1  - 10-12 April 2006
PY  - 2006
KW  - data mining
KW  - recording
KW  - security of data
KW  - data mining
KW  - intrusion prevention
KW  - large-scale log processing
KW  - log files intersections
KW  - security enhancement
KW  - Companies
KW  - Computer crime
KW  - Computer networks
KW  - Computer security
KW  - Data mining
KW  - Data security
KW  - IP networks
KW  - Internet
KW  - Intrusion detection
KW  - Protection
KW  - Data Mining
KW  - Intrusion prevention
KW  - Log files
KW  - Security
KW  - architectures
VL  - 
JA  - Information Technology: New Generations, 2006. ITNG 2006. Third International Conference on
DO  - 10.1109/ITNG.2006.32
AB  - In this paper, we discuss our research in developing general and systematic methods for intrusion prevention. The key idea is to use data mining techniques to discover regular patterns of system features that describe program and user behavior. Server systems invariably write detailed activity logs whose value is useful in detecting intrusion. Unfortunately, production volumes overwhelm the capacity and manageability of traditional approach. This paper discusses the issues involving large-scale log processing that helps analyze log records. In this paper we propose to analyze intersections of log files that come from different applications and firewalls installed on one computer, and intersections resulting from log files coming from different computers. Intersections of log files are substantially smaller than full logs and consist of records that indicate abnormalities in accessing single computer or set of computers. The paper concludes with some lessons we learned in building the system
ER  - 

TY  - CONF
JO  - Information Visualisation (IV), 2013 17th International Conference
TI  - Pianola - Visualization of Multivariate Time-Series Security Event Data
T2  - Information Visualisation (IV), 2013 17th International Conference
IS  - 
SN  - 1550-6037
VO  - 
SP  - 123
EP  - 131
AU  - Thomson, A.
AU  - Graham, M.
AU  - Kennedy, J.
Y1  - 16-18 July 2013
PY  - 2013
KW  - data visualisation
KW  - security of data
KW  - small-to-medium enterprises
KW  - time series
KW  - CLI
KW  - IDS event logs
KW  - NASA task load index
KW  - Pianola visualization tool
KW  - SMBs
KW  - TLX
KW  - activity patterns
KW  - attack detection
KW  - attack patterns
KW  - command-line interface based tools
KW  - continuous timelines
KW  - information visualization techniques
KW  - intrusion detection system
KW  - log file monitoring
KW  - mental model
KW  - multivariate time-series security event data visualization
KW  - network security event analysis
KW  - network security visualizations
KW  - resource-stretched small and medium sized businesses
KW  - user subjective workload
KW  - visual patterns
KW  - information visualization
KW  - security visualization
VL  - 
JA  - Information Visualisation (IV), 2013 17th International Conference
DO  - 10.1109/IV.2013.15
AB  - Monitoring log files for network intrusions is unwieldy. To build a mental model of the log, an analyst is required to recognise continuous timelines and attack patterns from a dataset that is essentially limited to an ordered list of events. Information Visualization techniques arrange data into directly perceivable visual patterns that may alleviate some overheads associated with interpreting these datasets and improve the ability of users, especially those in resource-stretched Small and Medium sized Businesses (SMBs), to make sense of activity patterns in Intrusion Detection System (IDS) event logs. To this end, we discuss existing network security visualizations for IDS logs and after examining the strengths and drawbacks of those applications we have prototyped a visualization tool, Pianola, that arranges events on multiple timelines to reveal patterns both in time and across a network. The tool was evaluated against the traditional use of command-line interface (CLI)-based tools for analyzing network security events and displayed significant improvements in both recognition and detection of attacks and reduction in the users' subjective workload, measured using the NASA Task Load index (TLX).
ER  - 

TY  - CONF
JO  - Software Reliability Engineering Workshops (ISSREW), 2014 IEEE International Symposium on
TI  - An Experiment with Conceptual Clustering for the Analysis of Security Alerts
T2  - Software Reliability Engineering Workshops (ISSREW), 2014 IEEE International Symposium on
IS  - 
SN  - 
VO  - 
SP  - 335
EP  - 340
AU  - Paudice, A.
AU  - Sarkar, S.
AU  - Cotroneo, D.
Y1  - 3-6 Nov. 2014
PY  - 2014
KW  - cloud computing
KW  - security of data
KW  - anomalies
KW  - conceptual clustering
KW  - corporative networks
KW  - enterprise networks
KW  - intrusion detection systems
KW  - log systems
KW  - real large scale SaaS cloud system
KW  - security alerts
KW  - security incidents
KW  - vulnerability scans
KW  - Computer architecture
KW  - Data analysis
KW  - Manuals
KW  - Monitoring
KW  - Robustness
KW  - Security
KW  - Training
KW  - Cloud
KW  - SIEM
KW  - clustering
KW  - filtering
KW  - security
VL  - 
JA  - Software Reliability Engineering Workshops (ISSREW), 2014 IEEE International Symposium on
DO  - 10.1109/ISSREW.2014.82
AB  - In response to attack against corporative and enterprise networks, administrators deploy intrusion detection systems, monitors, vulnerability scans and log systems. These systems monitor and record host and network device activities searching for signs of anomalies and security incidents. Doing that, these systems generally produce a huge number of alerts that overwhelms security analysts. This paper proposes the application of a conceptual clustering technique for filtering alerts and shows the results obtained for seven months of security alerts generated in a real large scale SaaS Cloud system. The technique has been useful to support manual analysis activities conducted by the operations team of the reference Cloud system.
ER  - 

TY  - CONF
JO  - Intelligent Ubiquitous Computing and Education, 2009 International Symposium on
TI  - Design and Implement of Common Network Security Scanning System
T2  - Intelligent Ubiquitous Computing and Education, 2009 International Symposium on
IS  - 
SN  - 
VO  - 
SP  - 148
EP  - 151
AU  - Wentao Liu
Y1  - 15-16 May 2009
PY  - 2009
KW  - security of data
KW  - software architecture
KW  - Libnet
KW  - Libpcap
KW  - common network security scanning system
KW  - information log analysis module
KW  - packet analyzing module
KW  - packet capture
KW  - packet constructing
KW  - packet injection
KW  - packet sending
KW  - port scanning
KW  - remote operating system detection
KW  - system scalability
KW  - vulnerability scanning
KW  - Computer security
KW  - Data security
KW  - Databases
KW  - Information analysis
KW  - Information security
KW  - Network servers
KW  - Operating systems
KW  - Protection
KW  - Protocols
KW  - Web server
KW  - libnet
KW  - libpcap
KW  - network security
KW  - port scanning
KW  - remote operating system scanning
KW  - vulnerability scanning
VL  - 
JA  - Intelligent Ubiquitous Computing and Education, 2009 International Symposium on
DO  - 10.1109/IUCE.2009.24
AB  - This paper discusses the network security scanning and some scanning methods which contain port scanning, vulnerability scanning and remote operating system detection are studied. In order to reduce the complexity and get high performance, the architecture of a common network security scanning system based on Libnet and Libpcap is provided and the every module of system is designed and implemented. The key modules include packet constructing, packet sending, packet capture, packet analyzing module and information log analysis module. The methods of packet injection based on Libnet and the packet capture based on Libpcap are presented and it can improve scanning performance and enhance the scalability and expansibility of system.
ER  - 

TY  - CONF
JO  - Computer Security Applications Conference, 1999. (ACSAC '99) Proceedings. 15th Annual
TI  - Adding availability to log services of untrusted machines
T2  - Computer Security Applications Conference, 1999. (ACSAC '99) Proceedings. 15th Annual
IS  - 
SN  - 1063-9527
VO  - 
SP  - 199
EP  - 206
AU  - Arona, A.
AU  - Bruschi, D.
AU  - Rosti, E.
Y1  - 1999
PY  - 1999
KW  - auditing
KW  - security of data
KW  - software fault tolerance
KW  - system monitoring
KW  - accidental deletion
KW  - computer forensics
KW  - cryptographic functions
KW  - deliberate deletion
KW  - fault tolerant strategies
KW  - information availability
KW  - intrusion
KW  - log distribution
KW  - log file availability
KW  - log generation
KW  - log services
KW  - multiple independent machines
KW  - network bandwidth
KW  - performance
KW  - postprocessor
KW  - protection
KW  - real time system auditing
KW  - real time system monitoring
KW  - software deletion
KW  - storage space
KW  - uncorrupted log files
KW  - untrusted machines
KW  - Availability
KW  - Bandwidth
KW  - Computerized monitoring
KW  - Cryptography
KW  - Data security
KW  - File systems
KW  - Information processing
KW  - Information security
KW  - Protection
KW  - Read only memory
VL  - 
JA  - Computer Security Applications Conference, 1999. (ACSAC '99) Proceedings. 15th Annual
DO  - 10.1109/CSAC.1999.816028
AB  - Uncorrupted log files are the critical system component for computer forensics in case of intrusion and for real time system monitoring and auditing. Protection from tampering with information can be achieved using cryptographic functions that provide authenticity, integrity, and confidentiality. However, they cannot provide the prerequisite for any further information processing, i.e., information availability. In this case, fault tolerant strategies can be of great help improving information availability in case of accidental or deliberate deletion. In this paper we propose a system that increases log file availability in case of software deletion by reliably and efficiently distributing the logs on multiple independent machines. The proposed scheme is more efficient than simple replication, both from the storage space and the network bandwidth points of view. The proposed system has been implemented and its impact on performance has been measured. Since it operates as a postprocessor after log generation, the proposed system can be easily integrated with logging systems that provide various cryptographic functions for forensic purposes
ER  - 

TY  - CONF
JO  - Social Computing (SocialCom), 2010 IEEE Second International Conference on
TI  - Securing Shareable Life-logs
T2  - Social Computing (SocialCom), 2010 IEEE Second International Conference on
IS  - 
SN  - 
VO  - 
SP  - 1105
EP  - 1110
AU  - Rawassizadeh, R.
AU  - Tjoa, A.M.
Y1  - 20-22 Aug. 2010
PY  - 2010
KW  - Web sites
KW  - security of data
KW  - general security approaches
KW  - shareable life-logs
KW  - social community
KW  - Communities
KW  - Data models
KW  - Privacy
KW  - Security
KW  - Sensors
KW  - Social network services
KW  - Surveillance
KW  - life-log
KW  - personal information
KW  - privacy
KW  - security
KW  - social network
VL  - 
JA  - Social Computing (SocialCom), 2010 IEEE Second International Conference on
DO  - 10.1109/SocialCom.2010.164
AB  - Sharing life-log information in a social community has many advantages, both for the user and society. But sharing any type of personal information is a threat to privacy. In particular, life-log information requires higher security considerations since it may contains very sensitive information about the user such as biological information, location, communication logs, etc. In this paper, first we discuss the risks of sharing life-log information in a social community. Then we will introduce a sharing model, which can eliminate the sharing capability of a life-log information object, based on the expiration time. Furthermore, general security approaches which might decrease security related risks for life-log systems will be described.
ER  - 

TY  - CONF
JO  - Systematic Approaches to Digital Forensic Engineering, 2005. First International Workshop on
TI  - SecSyslog: an approach to secure logging based on covert channels
T2  - Systematic Approaches to Digital Forensic Engineering, 2005. First International Workshop on
IS  - 
SN  - 
VO  - 
SP  - 248
EP  - 263
AU  - Forte, D.V.
AU  - Maruti, C.
AU  - Vetturi, M.R.
AU  - Zambelli, M.
Y1  - 7-9 Nov. 2005
PY  - 2005
KW  - Linux
KW  - management information systems
KW  - protocols
KW  - security of data
KW  - telecommunication channels
KW  - LINUX
KW  - SecSyslog
KW  - corporate information systems
KW  - covert channels
KW  - digital forensic tool
KW  - level 3 ISO/OSI traffic
KW  - log traces
KW  - pcap-compatible output
KW  - secure logging
KW  - Communication channels
KW  - Computer hacking
KW  - Conferences
KW  - Digital forensics
KW  - ISO standards
KW  - Information security
KW  - Linux
KW  - Management information systems
KW  - Open systems
KW  - Protocols
KW  - Covert Channel
KW  - Forensic
KW  - Log Correlation
KW  - Log Integrity
KW  - Log analysis
KW  - Spyware.
VL  - 
JA  - Systematic Approaches to Digital Forensic Engineering, 2005. First International Workshop on
DO  - 10.1109/SADFE.2005.21
AB  - Today log traces are widely used to identify and prevent violations of corporate information systems. The most recent logging trend is to manage most level 3 ISO/OSI traffic via pcap-compatible output. But use of syslog is still very widespread, as are the security issues it entails, especially in its 'pure' version. This paper outlines the basic syslog problems as foreseen in the RFCs, examines the 'secure' alternatives to the protocol (and relative implementations) and proposes a transmission approach based on covert channels which, applied on the LINUX platform, might answer some of the intrinsic reliability problems which undermine its effectiveness as a digital forensic tool.
ER  - 

TY  - CONF
JO  - Machine Learning and Cybernetics, 2004. Proceedings of 2004 International Conference on
TI  - Security extension to grasshopper for network management based policy
T2  - Machine Learning and Cybernetics, 2004. Proceedings of 2004 International Conference on
IS  - 
SN  - 
VO  - 1
SP  - 10
EP  - 15 vol.1
AU  - Bo Yang
AU  - Da-you Liu
AU  - Kun Yang
AU  - Wan-Jun Yu
Y1  - 26-29 Aug. 2004
PY  - 2004
KW  - Internet
KW  - computer network management
KW  - mobile agents
KW  - security of data
KW  - Internet
KW  - encrypted communication
KW  - large-scaled network
KW  - mobile agent technology
KW  - network management
KW  - secured agent repository
KW  - security extension
KW  - Communication system security
KW  - Cryptography
KW  - Dispatching
KW  - Environmental management
KW  - IP networks
KW  - Mobile agents
KW  - Mobile communication
KW  - Permission
KW  - Resource management
KW  - Technology management
VL  - 1
JA  - Machine Learning and Cybernetics, 2004. Proceedings of 2004 International Conference on
DO  - 10.1109/ICMLC.2004.1380590
AB  - Mobile agent technology provides a promising means for the management of the large-scaled network, such as the Internet. But it also results in significant security threats to and from both malicious agents and hosts. Based on the analysis of the security threats that may occur during the mobile agent based network management applications, This work presents a security extension to a mobile agent system Grasshopper, and describes the main component of the extension: Mobile Agent Security Facility (MASF), MASF has several key features: 1) a secure mechanism for dispatching agents to given domains or network elements from secured agent repository; 2) provision of encrypted communication; 3) a safe mobile agents execution environment that gives mobile agents different resource access permissions according to the result of authentication and authorization; 4) logging services to record security relevant events. MASF architecture is further integrated and verified in a practical network management application: inter-domain IP VPN configuration. MASF is generic and can be used in other mobile agent applications.
ER  - 

TY  - CONF
JO  - Convergence and Hybrid Information Technology, 2008. ICCIT '08. Third International Conference on
TI  - Design and Evaluation of a Network Forensic Logging System
T2  - Convergence and Hybrid Information Technology, 2008. ICCIT '08. Third International Conference on
IS  - 
SN  - 
VO  - 2
SP  - 1125
EP  - 1130
AU  - Tae-Kyou Park
AU  - Ilkyeun Ra
Y1  - 11-13 Nov. 2008
PY  - 2008
KW  - computer networks
KW  - network servers
KW  - operating systems (computers)
KW  - security of data
KW  - telecommunication security
KW  - telecommunication traffic
KW  - TCSEC-B1 level secure operating system
KW  - malicious attack
KW  - network forensic logging system
KW  - network processor
KW  - network server
KW  - network traffic
KW  - Computer security
KW  - Costs
KW  - Data security
KW  - Databases
KW  - Forensics
KW  - Humans
KW  - Kernel
KW  - Network servers
KW  - Operating systems
KW  - Protection
KW  - Forensic
KW  - Logging
KW  - security
VL  - 2
JA  - Convergence and Hybrid Information Technology, 2008. ICCIT '08. Third International Conference on
DO  - 10.1109/ICCIT.2008.28
AB  - This paper describes a forensic logging system that collects fine-grained evidence from target servers and networks. For the logging system, we developed a TCSEC-B1 level secure operating system and a dedicated network processor that collects network traffic. The logging system is also capable of protecting servers from malicious attacks as well as allowing security managers to obtain forensic evidences when the target server is assaulted by violations. We describe the design and implementation of the system and discuss the benchmark result of the prototype system.
ER  - 

TY  - CONF
JO  - System Sciences (HICSS), 2011 44th Hawaii International Conference on
TI  - Log-Based Distributed Security Event Detection Using Simple Event Correlator
T2  - System Sciences (HICSS), 2011 44th Hawaii International Conference on
IS  - 
SN  - 1530-1605
VO  - 
SP  - 1
EP  - 7
AU  - Myers, J.
AU  - Grimaila, M.R.
AU  - Mills, R.F.
Y1  - 4-7 Jan. 2011
PY  - 2011
KW  - bandwidth allocation
KW  - distributed processing
KW  - fault diagnosis
KW  - security of data
KW  - database query efficiency
KW  - log event correlation
KW  - log-based distributed security event detection
KW  - network bandwidth utilization
KW  - network syslog traffic
KW  - security breaches
KW  - simple event correlator
KW  - system fault detection
KW  - Correlation
KW  - Databases
KW  - Measurement
KW  - Monitoring
KW  - Security
KW  - Web server
VL  - 
JA  - System Sciences (HICSS), 2011 44th Hawaii International Conference on
DO  - 10.1109/HICSS.2011.288
AB  - Log event correlation is an effective means of detecting system faults and security breaches encountered in information technology environments. Centralized, database-driven log event correlation is common, but suffers from flaws such as high network bandwidth utilization, significant requirements for system resources, and difficulty in detecting certain suspicious behaviors. Distributed event correlation is often assumed to be superior, but no research effort has been made which quantitatively evaluates its advantages and disadvantages. This research presents a distributed event correlation system which performs security event detection, and evaluates it experimentally, compared with a centralized alternative. The comparison measures the value in distributed event correlation by considering network bandwidth utilization, detection capability and database query efficiency. The implementation of these advantages allows a 99% reduction of network syslog traffic in the low accountability case. In addition, the system detects every implemented malicious use case, with a low false positive rate.
ER  - 

TY  - CONF
JO  - Applications of Digital Information and Web Technologies (ICADIWT), 2011 Fourth International Conference on the
TI  - Virtualized hosting: Devising a security policy through log analysis
T2  - Applications of Digital Information and Web Technologies (ICADIWT), 2011 Fourth International Conference on the
IS  - 
SN  - 
VO  - 
SP  - 25
EP  - 29
AU  - Guster, D.C.
AU  - Lee, O.F.
AU  - Rogers, D.C.
Y1  - 4-6 Aug. 2011
PY  - 2011
KW  - security of data
KW  - virtualisation
KW  - log analysis
KW  - security policy
KW  - security vulnerability
KW  - virtualization threat
KW  - virtualized hosting
KW  - Computers
KW  - Hardware
KW  - Monitoring
KW  - Random access memory
KW  - Security
KW  - Virtual machining
KW  - Wide area networks
KW  - Virtualization
KW  - log disaster recovery
KW  - security policy
VL  - 
JA  - Applications of Digital Information and Web Technologies (ICADIWT), 2011 Fourth International Conference on the
DO  - 10.1109/ICADIWT.2011.6041399
AB  - The paper provides an overview of three common virtualization threats that have been observed in log files in the authors' network and suggests solutions to mitigate those security vulnerabilities. The solutions offered have been implemented on a network with over 200 hosts 40 of which are virtualized.
ER  - 

TY  - CONF
JO  - Dependable Systems & Networks (DSN), 2011 IEEE/IFIP 41st International Conference on
TI  - Analysis of security data from a large computing organization
T2  - Dependable Systems & Networks (DSN), 2011 IEEE/IFIP 41st International Conference on
IS  - 
SN  - 1530-0889
VO  - 
SP  - 506
EP  - 517
AU  - Sharma, A.
AU  - Kalbarczyk, Z.
AU  - Barlow, J.
AU  - Iyer, R.
Y1  - 27-30 June 2011
PY  - 2011
KW  - security of data
KW  - National Center for Supercomputing Applications
KW  - University of Illinois
KW  - data security analysis
KW  - large computing organization
KW  - security monitoring
KW  - Analytical models
KW  - Computational modeling
KW  - Data models
KW  - IP networks
KW  - Monitoring
KW  - Security
KW  - Software
KW  - alerts
KW  - incident/attack data analysis
KW  - large scale computing systems
KW  - security monitoring
VL  - 
JA  - Dependable Systems & Networks (DSN), 2011 IEEE/IFIP 41st International Conference on
DO  - 10.1109/DSN.2011.5958263
AB  - This paper presents an in-depth study of the forensic data on security incidents that have occurred over a period of 5 years at the National Center for Supercomputing Applications at the University of Illinois. The proposed methodology combines automated analysis of data from security monitors and system logs with human expertise to extract and process relevant data in order to: (i) determine the progression of an attack, (ii) establish incident categories and characterize their severity, (iii) associate alerts with incidents, and (iv) identify incidents missed by the monitoring tools and examine the reasons for the escapes. The analysis conducted provides the basis for incident modeling and design of new techniques for security monitoring.
ER  - 

TY  - CONF
JO  - Business, Engineering and Industrial Applications (ISBEIA), 2011 IEEE Symposium on
TI  - Development of host based intrusion detection system for log files
T2  - Business, Engineering and Industrial Applications (ISBEIA), 2011 IEEE Symposium on
IS  - 
SN  - 
VO  - 
SP  - 281
EP  - 285
AU  - Bin Hamid Ali, F.A.
AU  - Yee Yong Len
Y1  - 25-28 Sept. 2011
PY  - 2011
KW  - operating systems (computers)
KW  - pattern matching
KW  - security of data
KW  - Microsoft Windows XP
KW  - computer security
KW  - computer system
KW  - host based intrusion detection system
KW  - intrusion detection pattern matching technique
KW  - security event log file
KW  - Computer science
KW  - Computers
KW  - Intrusion detection
KW  - Monitoring
KW  - Organizations
KW  - Pattern matching
KW  - Host based intrusion detection system
KW  - Security Event Lo
KW  - Windows XP
KW  - pattern matching technique
VL  - 
JA  - Business, Engineering and Industrial Applications (ISBEIA), 2011 IEEE Symposium on
DO  - 10.1109/ISBEIA.2011.6088821
AB  - Nowadays, computer security has become important issue in many organizations in this world. There are many ways to handle this issue including by using Intrusion Detection System on the computer system. It takes the role as a detector for any intrusion that is occurring from the computer system. The study is to develop host based intrusion detection system for Microsoft Windows XP environment. Method that had used in the study was applying intrusion detection pattern matching technique on the Security Event Log File for Microsoft Windows XP. The intrusion had identified when there was matching of intrusion pattern that is create with Security Event Log in Microsoft Windows XP. The system is hoping to evolve into IDS that include any kind of intrusion detection technique in future.
ER  - 

TY  - CONF
JO  - Computational Intelligence and Security, 2009. CIS '09. International Conference on
TI  - Intelligent Security Data Analysis
T2  - Computational Intelligence and Security, 2009. CIS '09. International Conference on
IS  - 
SN  - 
VO  - 1
SP  - 74
EP  - 78
AU  - Yun Shen
AU  - Martin, T.
AU  - Bramhall, P.
Y1  - 11-14 Dec. 2009
PY  - 2009
KW  - Internet
KW  - data analysis
KW  - data mining
KW  - security of data
KW  - terrorism
KW  - Web search logs
KW  - association rules
KW  - computational intelligence techniques
KW  - extended mass assignment framework
KW  - intelligent security data analysis
KW  - national security
KW  - terrorism incident databases
KW  - user behaviour profiling
KW  - Association rules
KW  - Competitive intelligence
KW  - Computational intelligence
KW  - Data analysis
KW  - Data mining
KW  - Data security
KW  - Databases
KW  - Information security
KW  - Terrorism
KW  - Web search
VL  - 1
JA  - Computational Intelligence and Security, 2009. CIS '09. International Conference on
DO  - 10.1109/CIS.2009.10
AB  - In this paper, we examine issues related to the research and applications of computational intelligence techniques in security data analysis. We focus on solve problems that involve incomplete, vague or uncertain information, which is difficult to come to a crisp solution. It is shown how an extended mass assignment framework can be used to extract relations between soft categories. These relations are association rules and are useful when integrating multiple information sources. Experimental results on terrorism incident databases and Web search logs, respectively relating to national security and user behaviour profiling, are demonstrated and discussed in this paper.
ER  - 

TY  - CONF
JO  - Network Operations and Management Symposium, 2008. NOMS 2008. IEEE
TI  - Mining event logs with SLCT and LogHound
T2  - Network Operations and Management Symposium, 2008. NOMS 2008. IEEE
IS  - 
SN  - 1542-1201
VO  - 
SP  - 1071
EP  - 1074
AU  - Vaarandi, R.
Y1  - 7-11 April 2008
PY  - 2008
KW  - data mining
KW  - security of data
KW  - telecommunication computing
KW  - LogHound
KW  - communication networks
KW  - event log analysis
KW  - event logs mining
KW  - log data
KW  - security events
KW  - system management personnel
KW  - Algorithm design and analysis
KW  - Clustering algorithms
KW  - Communication networks
KW  - Communication system security
KW  - Data analysis
KW  - Data mining
KW  - Data security
KW  - Event detection
KW  - Monitoring
KW  - Personnel
KW  - data mining
KW  - data security
KW  - event log analysis
VL  - 
JA  - Network Operations and Management Symposium, 2008. NOMS 2008. IEEE
DO  - 10.1109/NOMS.2008.4575281
AB  - With the growth of communication networks, event logs are increasing in size at a fast rate. Today, it is not uncommon to have systems that generate tens of gigabytes of log data per day. Log data are likely to contain information that deserves closer attention - such as security events - but the task of reviewing logs manually is beyond the capabilities of a human. This paper discusses data mining tools SLCT and log hound that were designed for assisting system management personnel in extracting knowledge from event logs.
ER  - 

TY  - CONF
JO  - Computer Security Applications Conference, 1994. Proceedings., 10th Annual
TI  - The design of an audit trail analysis tool
T2  - Computer Security Applications Conference, 1994. Proceedings., 10th Annual
IS  - 
SN  - 
VO  - 
SP  - 126
EP  - 132
AU  - Fisch, E.A.
AU  - White, G.B.
AU  - Pooch, U.W.
Y1  - 5-9 Dec 1994
PY  - 1994
KW  - auditing
KW  - data loggers
KW  - security of data
KW  - SunOS
KW  - audit trail analysis tool
KW  - compromised site
KW  - design issues
KW  - intruder activity log files
KW  - sanitization process
KW  - security-sensitive information
KW  - sensitive information
KW  - Automation
KW  - Communication channels
KW  - Computer hacking
KW  - Computer networks
KW  - Contracts
KW  - Cryptography
KW  - Data systems
KW  - Information analysis
KW  - Information security
KW  - Performance analysis
VL  - 
JA  - Computer Security Applications Conference, 1994. Proceedings., 10th Annual
DO  - 10.1109/CSAC.1994.367314
AB  - Discusses the design of a tool that automatically removes security-sensitive information from intruder activity log files collected at a compromised site. The sanitization of sensitive information enables researchers to study the log files without further compromising the security of the affected sites. This paper begins with a brief discussion of the importance of such a tool and a description of the complete sanitization process. This is followed by an examination of the important design issues of the sanitizer. The paper concludes with the final design of a sanitizer for SunOS-based intruder activity logs
ER  - 

TY  - CONF
JO  - Data and Software Engineering (ICODSE), 2014 International Conference on
TI  - Information system log visualization to monitor anomalous user activity based on time
T2  - Data and Software Engineering (ICODSE), 2014 International Conference on
IS  - 
SN  - 
VO  - 
SP  - 1
EP  - 6
AU  - Hanniel, J.J.
AU  - Widagdo, T.E.
AU  - Asnar, Y.D.W.
Y1  - 26-27 Nov. 2014
PY  - 2014
KW  - Internet
KW  - cognition
KW  - data analysis
KW  - data visualisation
KW  - information systems
KW  - security of data
KW  - Web-based data visualization
KW  - anomalous user activity detection
KW  - anomalous user activity monitoring
KW  - anomaly detection
KW  - cognition
KW  - data variables
KW  - design concept
KW  - dot plot
KW  - focused exploration
KW  - heatmap
KW  - information system log visualization
KW  - log data analysis
KW  - log files analysis
KW  - security
KW  - time-based data visualization method
KW  - Data visualization
KW  - Geology
KW  - Heating
KW  - IP networks
KW  - Information systems
KW  - Java
KW  - Monitoring
KW  - anomalous user activity
KW  - data visualization
KW  - log file
VL  - 
JA  - Data and Software Engineering (ICODSE), 2014 International Conference on
DO  - 10.1109/ICODSE.2014.7062673
AB  - As information systems start to manage the more crucial parts of human lives, their security cannot be neglected. One way to ensure the security is by analyzing their generated log files of anomalous user activity. Data visualization has become a common solution to help get around the problems in log analysis. In this paper, we tried to determine key characteristics of effective data visualization on detecting those anomalous user activity recorded in log files. First we analyzed the log data we have and derived 4 anomalies whose indicators are made into visualization topics. Hence we built 4 data visualizations to detect the 4 anomalies. Next, we transformed our data so that they can be visualized. After that, we analyzed the suitable time-based data visualization method to represent our data and decided on heatmap for its wide application on existing solutions and dot plot for it is able to accommodate all data variables needed on every visualization topic and has the suitable nuance for monitoring purposes. Next we decided on design concept of our data visualizations and implemented them as web-based data visualization. We conducted 2 tests in this paper to determine the key characteristics of effective data visualization. Even though the results are inconclusive, but they hinted that an effective data visualization on this matter should support large amount of perceived information through cognition and support focused exploration.
ER  - 

TY  - CONF
JO  - Internet Technology And Secured Transactions, 2012 International Conference for
TI  - Semantic hedgehog for log analysis
T2  - Internet Technology And Secured Transactions, 2012 International Conference for
IS  - 
SN  - 
VO  - 
SP  - 748
EP  - 752
AU  - Wiley, J.J.
AU  - Coyle, F.P.
Y1  - 10-12 Dec. 2012
PY  - 2012
KW  - business data processing
KW  - data analysis
KW  - security of data
KW  - Big Data
KW  - block access decision
KW  - close port decision
KW  - computer system log analysis
KW  - distributed enterprise
KW  - information security decision
KW  - patch system decision
KW  - semantic hedgehog
KW  - semantic technology
KW  - update configuration decision
KW  - Correlation
KW  - Engines
KW  - Security
KW  - proactive defense
KW  - semantic web
KW  - syslog
KW  - triples
VL  - 
JA  - Internet Technology And Secured Transactions, 2012 International Conference for
DO  - 
AB  - Computer system log analysis should proactively support information security decisions of all types. These security decisions will likely include whether to update configurations, close ports, block access, patch systems, maneuver the system elements, or to do nothing because the risk is acceptable. In a world with Big Data, and a heterogeneous, distributed enterprise, log analysis can be difficult at best. There is so much data from a multitude of logs (e.g. event, application, and security) within the enterprise. On top of that, enterprises have varying configurations based on hardware, software, current patch level, and operating systems. Logs must track all of this data on all of these devices. The authors suggest that semantic technologies hold one key to providing a capability for proactive, and more meaningful, log analysis.
ER  - 

TY  - CONF
JO  - Privacy, Security and Trust (PST), 2014 Twelfth Annual International Conference on
TI  - Semi-synthetic data set generation for security software evaluation
T2  - Privacy, Security and Trust (PST), 2014 Twelfth Annual International Conference on
IS  - 
SN  - 
VO  - 
SP  - 156
EP  - 163
AU  - Skopik, F.
AU  - Settanni, G.
AU  - Fiedler, R.
AU  - Friedberg, I.
Y1  - 23-24 July 2014
PY  - 2014
KW  - data handling
KW  - security of data
KW  - ICT security systems
KW  - ICT systems
KW  - heuristic detection engines
KW  - information and communication technology systems
KW  - intrusion detection and prevention systems
KW  - security software evaluation
KW  - self-learning intrusion detection systems
KW  - semisynthetic data set generation
KW  - virus infestation
KW  - Complexity theory
KW  - Data models
KW  - Databases
KW  - Intrusion detection
KW  - Testing
KW  - Virtual machining
KW  - anomaly detection evaluation
KW  - scalable system behavior model
KW  - synthetic data set generation
VL  - 
JA  - Privacy, Security and Trust (PST), 2014 Twelfth Annual International Conference on
DO  - 10.1109/PST.2014.6890935
AB  - Threats to modern ICT systems are rapidly changing these days. Organizations are not mainly concerned about virus infestation, but increasingly need to deal with targeted attacks. This kind of attacks are specifically designed to stay below the radar of standard ICT security systems. As a consequence, vendors have begun to ship self-learning intrusion detection systems with sophisticated heuristic detection engines. While these approaches are promising to relax the serious security situation, one of the main challenges is the proper evaluation of such systems under realistic conditions during development and before roll-out. Especially the wide variety of configuration settings makes it hard to find the optimal setup for a specific infrastructure. However, extensive testing in a live environment is not only cumbersome but usually also impacts daily business. In this paper, we therefore introduce an approach of an evaluation setup that consists of virtual components, which imitate real systems and human user interactions as close as possible to produce system events, network flows and logging data of complex ICT service environments. This data is a key prerequisite for the evaluation of modern intrusion detection and prevention systems. With these generated data sets, a system's detection performance can be accurately rated and tuned for very specific settings.
ER  - 

TY  - CONF
JO  - Industrial Informatics, 2003. INDIN 2003. Proceedings. IEEE International Conference on
TI  - Security log time synchronization for high-availability systems
T2  - Industrial Informatics, 2003. INDIN 2003. Proceedings. IEEE International Conference on
IS  - 
SN  - 
VO  - 
SP  - 199
EP  - 206
AU  - Naedele, M.
Y1  - 21-24 Aug. 2003
PY  - 2003
KW  - Internet
KW  - factory automation
KW  - protocols
KW  - security of data
KW  - synchronisation
KW  - Internet
KW  - denial-of-service attack
KW  - factory automation system
KW  - firewall
KW  - high-availability system
KW  - intrusion detection system log
KW  - protocol
KW  - security log time synchronization
KW  - time stamp synchronization
KW  - Best practices
KW  - Computer crime
KW  - Data security
KW  - Delay
KW  - IP networks
KW  - Intrusion detection
KW  - Manufacturing automation
KW  - Network topology
KW  - Protocols
KW  - Real time systems
VL  - 
JA  - Industrial Informatics, 2003. INDIN 2003. Proceedings. IEEE International Conference on
DO  - 10.1109/INDIN.2003.1300270
AB  - An increasing number of factory automation systems are connected to the Internet or other public networks, and secured by firewalls, intrusion detection systems (IDSs), etc. In order to detect attacks, correlation of firewalls, router, proxy, and IDS logs is necessary. Successful correlation requires, among other things, synchronized time stamps for all the log entries created by different sources. The automation system usually contains a rather accurate time source, which could be used to derive the time base for all system components, including the above-mentioned security mechanisms. A number of standard protocols exist for time synchronization. It will be shown that these protocols do not fulfill the necessary security requirements. In particular, they open up the automation system network to denial-of-service attacks from the outside. Various design alternatives and the requirements for an alternative time synchronization protocol are discussed.
ER  - 

TY  - CONF
JO  - Cluster Computing, 2006 IEEE International Conference on
TI  - NVision-PA: A Process Accounting Analysis Tool with a Security Focus on Masquerade Detection in HPC Clusters
T2  - Cluster Computing, 2006 IEEE International Conference on
IS  - 
SN  - 1552-5244
VO  - 
SP  - 1
EP  - 10
AU  - Ermopoulos, C.
AU  - Yurcik, W.
Y1  - 25-28 Sept. 2006
PY  - 2006
KW  - Linux
KW  - operating system kernels
KW  - security of data
KW  - software tools
KW  - statistical analysis
KW  - text analysis
KW  - workstation clusters
KW  - HPC clusters
KW  - Linux
KW  - NVision-PA
KW  - SSH identity theft
KW  - UNIX
KW  - cluster security
KW  - forensic investigation
KW  - graphic statistical summary
KW  - high performance computing clusters
KW  - process accounting analysis tool
KW  - process accounting logs
KW  - purloined authentication credentials
KW  - real-time masquerade detection sensor
KW  - security events
KW  - software tool
KW  - system administrators
KW  - text summary
KW  - Authentication
KW  - Forensics
KW  - Graphics
KW  - High performance computing
KW  - Kernel
KW  - Linux
KW  - Monitoring
KW  - Real time systems
KW  - Security
KW  - Software tools
KW  - SSH identity theft
KW  - cluster security
KW  - command behavior
KW  - high performance computing (HPC)
KW  - masquerade detection
KW  - process accounting
VL  - 
JA  - Cluster Computing, 2006 IEEE International Conference on
DO  - 10.1109/CLUSTR.2006.311856
AB  - In the UNIX/Linux environment the kernel can log every command process created by every user with process accounting. Thus process accounting logs have many potential uses, particularly the monitoring and forensic investigation of security events. Previous work successfully leveraged the use of process accounting logs to identify a difficult to detect and damaging intrusions within high performance computing (HPC) clusters, masquerade attacks, where intruders pose as legitimate users with purloined authentication credentials. This paper incrementally advances the goal of more accurately identifying masqueraders on HPC clusters by seeking to identify features within command sets that distinguish masqueraders. To accomplish this goal, we created NVision-PA, a software tool which produces text and graphic statistical summaries describing input processing accounting logs. This research is both a promising next step toward creating a real-time masquerade detection sensor for production HPC clusters as well as providing another tool for system administrators to use for statistically monitoring and managing legitimate workloads in HPC environments
ER  - 

TY  - CONF
JO  - GCC Conference and Exhibition (GCC), 2013 7th IEEE
TI  - Assessing the security of the cloud environment
T2  - GCC Conference and Exhibition (GCC), 2013 7th IEEE
IS  - 
SN  - 
VO  - 
SP  - 251
EP  - 256
AU  - Al Awadhi, E.
AU  - Salah, K.
AU  - Martin, T.
Y1  - 17-20 Nov. 2013
PY  - 2013
KW  - cloud computing
KW  - security of data
KW  - Amazon AWS Cloud
KW  - Dionaea honeypots
KW  - cloud security
KW  - cloud services
KW  - Cloud computing
KW  - Databases
KW  - IP networks
KW  - Malware
KW  - Ports (Computers)
KW  - Region 8
KW  - Amazon AWS
KW  - Cloud Computing
KW  - Cloud Security
KW  - Dionaea
KW  - EC2
KW  - Honeypots
VL  - 
JA  - GCC Conference and Exhibition (GCC), 2013 7th IEEE
DO  - 10.1109/IEEEGCC.2013.6705785
AB  - Nowadays, many organizations find it economically attractive to host their services in a cloud environment. Still, security remains the number one concern for many organizations in adopting the cloud services. In this paper, we assess and study the security of a typical cloud environment as that of the popular Amazon AWS Cloud. The cloud security is assessed by deploying and running Dionaea honeypots for a few months in the cloud provide networks. Collected data and logs from Dionaea honeypots show that the cloud environment is surprisingly insecure and is in fact a target of malicious activities and attacks from different countries.
ER  - 

TY  - CONF
JO  - Mechatronics and Automation, 2005 IEEE International Conference
TI  - A collaborative intrusion detection system using log server and neural networks
T2  - Mechatronics and Automation, 2005 IEEE International Conference
IS  - 
SN  - 
VO  - 2
SP  - 874
EP  - 877 Vol. 2
AU  - Donghai Guan
AU  - Kejun Wang
AU  - Xiufen Ye
AU  - Weixing Feng
Y1  - 29 July-1 Aug. 2005
PY  - 2005
KW  - Internet
KW  - Java
KW  - neural nets
KW  - security of data
KW  - system monitoring
KW  - Internet
KW  - Java
KW  - SSL
KW  - certificate authority
KW  - collaborative intrusion detection system
KW  - computer system security
KW  - key management
KW  - neural networks
KW  - remote logging server
KW  - Collaboration
KW  - Computer security
KW  - Computerized monitoring
KW  - Explosives
KW  - File servers
KW  - Intrusion detection
KW  - Network servers
KW  - Neural networks
KW  - Remote monitoring
KW  - Web server
VL  - 2
JA  - Mechatronics and Automation, 2005 IEEE International Conference
DO  - 10.1109/ICMA.2005.1626666
AB  - With the explosive rapid expansion of computer use during the past few years, security has become a crucial issue for modern computer systems. Today, there are so many intrusion detection systems (IDS) on the Internet. A variety of intrusion detection techniques and tools exist in the computer security community. We can easily download, install and configure them to our needs. But there is a potential problem involved with intrusion detection systems that are installed locally on the machines to be monitored. If the system being monitored is compromised, it is quite likely that the intruder will alter the system logs and the intrusion logs while the intrusion remains undetected. In this project KIT-I, we adopt remote logging server (RLS) mechanism, which is used to backup the log files to the server. Taking into account security, we make use of the function of SSL of Java and certificate authority (CA) based on key management. Furthermore, neural networks are applied in our project to detect the intrusion activities.
ER  - 

TY  - CONF
JO  - Cluster Computing and the Grid, 2005. CCGrid 2005. IEEE International Symposium on
TI  - Incorporating information from a cluster batch scheduler and center management software into automated log file analysis
T2  - Cluster Computing and the Grid, 2005. CCGrid 2005. IEEE International Symposium on
IS  - 
SN  - 
VO  - 1
SP  - 133
EP  - 139 Vol. 1
AU  - Prewett, J.E.
Y1  - 9-12 May 2005
PY  - 2005
KW  - batch processing (computers)
KW  - computer centres
KW  - processor scheduling
KW  - security of data
KW  - workstation clusters
KW  - LoGS log analysis tool
KW  - automated log analysis
KW  - automated log file analysis
KW  - center management software
KW  - cluster batch scheduler
KW  - cluster security
KW  - cluster style architectures
KW  - high performance computing
KW  - Computer architecture
KW  - Data security
KW  - Databases
KW  - Hardware
KW  - High performance computing
KW  - Information analysis
KW  - Laboratories
KW  - Performance analysis
KW  - Processor scheduling
KW  - Software performance
VL  - 1
JA  - Cluster Computing and the Grid, 2005. CCGrid 2005. IEEE International Symposium on
DO  - 10.1109/CCGRID.2005.1558544
AB  - Cluster style architectures are becoming increasingly important in the computing world. These machines have been the target of several recent attacks. In this paper, we examine how the unique characteristics of these machines, including how they are generally operated in the larger context of a computing center, can be leveraged to provide better security for these machines. We examine this by incorporating data from a batch scheduler and a user database to augment our automated log analysis using the freely available log analysis tool LoGS.
ER  - 

TY  - CONF
JO  - Collaborative Computing: Networking, Applications and Worksharing (Collaboratecom), 2013 9th International Conference Conference on
TI  - Finding anomalies in windows event logs using standard deviation
T2  - Collaborative Computing: Networking, Applications and Worksharing (Collaboratecom), 2013 9th International Conference Conference on
IS  - 
SN  - 
VO  - 
SP  - 563
EP  - 570
AU  - Dwyer, J.
AU  - Truta, T.M.
Y1  - 20-23 Oct. 2013
PY  - 2013
KW  - computational complexity
KW  - security of data
KW  - IT infrastructure
KW  - Windows event log anomalies
KW  - administrator attention
KW  - event log processing
KW  - intrusion detection systems
KW  - log data complexity
KW  - security information
KW  - standard deviation
KW  - Companies
KW  - Complexity theory
KW  - Cryptography
KW  - Databases
KW  - Servers
KW  - Standards
KW  - Anomaly Detection
KW  - Standard Deviation
KW  - Windows Event Logs
VL  - 
JA  - Collaborative Computing: Networking, Applications and Worksharing (Collaboratecom), 2013 9th International Conference Conference on
DO  - 
AB  - Security is one of the biggest concerns of any company that has an IT infrastructure. Windows event logs are a very useful source of data for security information, but sometimes can be nearly impossible to use due to the complexity of log data or the number of events generated per minute. For this reason, event log data must be automatically processed so that an administrator is given a list of events that actually need the administrator's attention. This has been standard in intrusion detection systems for many years to find anomalies in network traffic, but has not been common in event log processing. This paper will adapt these intrusion detection techniques for Windows event log data sets to find anomalies in these log data sets.
ER  - 

TY  - CONF
JO  - Application of Computer Vision, 2005. WACV/MOTIONS '05 Volume 1. Seventh IEEE Workshops on
TI  - Using Continuous Face Verification to Improve Desktop Security
T2  - Application of Computer Vision, 2005. WACV/MOTIONS '05 Volume 1. Seventh IEEE Workshops on
IS  - 
SN  - 
VO  - 1
SP  - 501
EP  - 507
AU  - Janakiraman, R.
AU  - Kumar, S.
AU  - Sheng Zhang
AU  - Sim, T.
Y1  - 5-7 Jan. 2005
PY  - 2005
KW  - face recognition
KW  - security of data
KW  - Bayesian framework
KW  - desktop security
KW  - face verification system
KW  - operating system process
KW  - processor overhead
KW  - system security
KW  - verification accuracy
KW  - Access control
KW  - Authentication
KW  - Computer science
KW  - Computer security
KW  - Delay systems
KW  - Drives
KW  - Face detection
KW  - Intrusion detection
KW  - National security
KW  - Operating systems
VL  - 1
JA  - Application of Computer Vision, 2005. WACV/MOTIONS '05 Volume 1. Seventh IEEE Workshops on
DO  - 10.1109/ACVMOT.2005.120
AB  - In this paper we describe the architecture, implementation, and performance of a face verification system that continually verifies the presence of a logged-in user at a computer console. It maintains a sliding window of about ten seconds of verification data points and uses them as input to a Bayesian framework to compute a probability that the logged-in user is still present at the console. If the probability falls below a threshold, the system can delay or freeze operating system processes belonging to the logged-in user. This helps prevent misuse of computer resources when an unauthorized user maliciously takes the place of an authorized user. Processes may be unconditionally frozen (they never return from a system call) or delayed (it takes longer to complete a system call, or appropriate action may be taken for certain classes of system calls, such as those that are considered security critical. We believe that the integrated system presented here is the first of its kind. Furthermore, we believe that the analysis of the tradeoffs between verification accuracy, processor overhead, and system security that we do in this paper has not been done elsewhere
ER  - 

TY  - CONF
JO  - e-Business and Information System Security (EBISS), 2010 2nd International Conference on
TI  - Notice of Retraction<BR>A Campus Network Security Emergency Response Technical System Based on Emergency Log
T2  - e-Business and Information System Security (EBISS), 2010 2nd International Conference on
IS  - 
SN  - 
VO  - 
SP  - 1
EP  - 3
AU  - Shengzhong Yuan
AU  - Wei Wang
Y1  - 22-23 May 2010
PY  - 2010
KW  - emergency services
KW  - security of data
KW  - software reliability
KW  - campus network security
KW  - emergency log
KW  - emergency response technical system
KW  - system reliability
KW  - Computer crime
KW  - Concrete
KW  - Disaster management
KW  - Information management
KW  - Information security
KW  - Maintenance engineering
KW  - Protection
KW  - Reliability engineering
KW  - Safety
KW  - Telecommunication traffic
VL  - 
JA  - e-Business and Information System Security (EBISS), 2010 2nd International Conference on
DO  - 10.1109/EBISS.2010.5473336
AB  - Notice of Retraction<BR><BR>After careful and considered review of the content of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.<BR><BR>We hereby retract the content of this paper. Reasonable effort should be made to remove all past references to this paper.<BR><BR>The presenting author of this paper has the option to appeal this decision by contacting TPII@ieee.org.<BR><BR>The campus network is the birthplace of a large number of attacks, and also the attacker's objective that is most easily broken. How to build up an effective technical system to support emergency response under the current state of lower invests in maintaining the campus network Yecurity is an issue worth studying. An emergency response log has been designed to record each emergency incident response process. The role of the log is more and more important with an increasing accumulation of information. With the log, engineers can determine the type of many non-deterministic emergency incidents grouped into deterministic events, in order to improve the system reliability of emergency response.
ER  - 

TY  - CONF
JO  - Informatics and Creative Multimedia (ICICM), 2013 International Conference on
TI  - Log Visualization of Intrusion and Prevention Reverse Proxy Server against Web Attacks
T2  - Informatics and Creative Multimedia (ICICM), 2013 International Conference on
IS  - 
SN  - 
VO  - 
SP  - 325
EP  - 329
AU  - Mantoro, T.
AU  - Binti Abdul Aziz, N.
AU  - Binti Meor Yusoff, N.D.
AU  - Binti Abu Talib, N.A.
Y1  - 4-6 Sept. 2013
PY  - 2013
KW  - Internet
KW  - SQL
KW  - data analysis
KW  - data visualisation
KW  - file servers
KW  - security of data
KW  - system monitoring
KW  - IP address
KW  - OWASP
KW  - SQL injection attack
KW  - SQLIA
KW  - Web Attacks
KW  - Web application security risks
KW  - data analysis
KW  - graph visualization
KW  - intrusion detection
KW  - intrusion log visualization
KW  - intrusion-prevention mechanism
KW  - log information
KW  - prevention reverse proxy server
KW  - server logging process
KW  - Databases
KW  - IP networks
KW  - Intrusion detection
KW  - Monitoring
KW  - Web servers
KW  - Firewall
KW  - ModSecurity
KW  - Network Intrusion Detection
KW  - Reverse Proxy.
KW  - SQL Injection Attack
VL  - 
JA  - Informatics and Creative Multimedia (ICICM), 2013 International Conference on
DO  - 10.1109/ICICM.2013.70
AB  - SQL Injection Attack (SQLIA) has made to the top of the OWASP, Top 10 Web Application Security Risks in 2013 and in 2010. The explosive use of web application with very little emphasis lay on securing it make this attack becoming more popular. Various methods have been discussed and proposed as countermeasure to the attack. Unfortunately, most of them are seen to be not comprehensive enough to address any kind of issues an organization might have when it comes to hardening the web security such as technical and financial matter for instance. This study presents a way to prevent and detect intrusion through the deployment of reverse proxy with an intrusion and prevention mechanism built in against web attacks especially SQLIA. With the flexibility offered in server logging process, we obtain and analyse preferred data to visualize the type of attack based on logs information. Our graph visualization development monitors three web security aspects, i.e. the top traffic blocked attempted by IP address, number of regular expression rules violated and detect the rules of intrusion detection.
ER  - 

TY  - CONF
JO  - Computer Application and System Modeling (ICCASM), 2010 International Conference on
TI  - Web log system of automatic backup and remote analysis
T2  - Computer Application and System Modeling (ICCASM), 2010 International Conference on
IS  - 
SN  - 
VO  - 2
SP  - V2-469
EP  - V2-472
AU  - Zhou Hangxia
AU  - Zheng Peng
AU  - Yan Yong
Y1  - 22-24 Oct. 2010
PY  - 2010
KW  - Web services
KW  - back-up procedures
KW  - security of data
KW  - system monitoring
KW  - Web log system
KW  - Web service environment
KW  - automatic network log backup system
KW  - network security coefficient
KW  - remote analysis
KW  - Artificial neural networks
KW  - Computers
KW  - log backup
KW  - remote analysis
KW  - server
KW  - shell
VL  - 2
JA  - Computer Application and System Modeling (ICCASM), 2010 International Conference on
DO  - 10.1109/ICCASM.2010.5620567
AB  - In this paper, we research and improve the automatic network log backup system to facilitate all types of log for further extraction and analysis, and at the same time the security coefficient of the network log backup will be greatly improved, anytime, a backup of an important running log can be automatically made. At last, we build a safe web service environment to realize the management and analysis of the log system which is based on the dynamic web form.
ER  - 

TY  - CONF
JO  - Emerging Intelligent Data and Web Technologies (EIDWT), 2012 Third International Conference on
TI  - Massive Processing of Activity Logs of a Virtual Campus
T2  - Emerging Intelligent Data and Web Technologies (EIDWT), 2012 Third International Conference on
IS  - 
SN  - 
VO  - 
SP  - 104
EP  - 110
AU  - Xhafa, F.
AU  - Ruiz, J.J.
AU  - Caballe, S.
AU  - Spaho, E.
AU  - Barolli, L.
AU  - Miho, R.
Y1  - 19-21 Sept. 2012
PY  - 2012
KW  - Internet
KW  - computer aided instruction
KW  - grid computing
KW  - security of data
KW  - user modelling
KW  - virtual reality
KW  - workstation clusters
KW  - activity logs
KW  - cleaned up log files
KW  - cluster computing
KW  - data analytics
KW  - data monitoring
KW  - data security
KW  - distributed infrastructures
KW  - futile information
KW  - information extraction
KW  - log data files
KW  - massive processing
KW  - online Web-based application
KW  - open grid engine
KW  - planet lab platform
KW  - preprocessed log files
KW  - real virtual campus
KW  - redundant information
KW  - user activity analysis
KW  - user interaction data
KW  - user modelling
KW  - Analytical models
KW  - Data mining
KW  - Distributed databases
KW  - Educational institutions
KW  - Electronic mail
KW  - Monitoring
KW  - Web servers
KW  - Cluster Computing
KW  - Log Files
KW  - Massive Processing
KW  - Planet Lab
KW  - Virtual Campus
VL  - 
JA  - Emerging Intelligent Data and Web Technologies (EIDWT), 2012 Third International Conference on
DO  - 10.1109/EIDWT.2012.64
AB  - Online web-based application that heavily require user interaction, either among users or among users and the application, generate huge amounts of data. Recording such user interaction data, usually in the form of log data files, could be very useful for different purposes such as user modelling, user activity analysis, data analytics, security, monitoring, etc. However, such data is not ready to be analysed due log files are to be pre-processed and cleaned up from redundant and futile information. Due to the large amounts of data generated daily, the massive processing is a foremost step in extracting useful information from log data files. In this work we study the viability of massive processing of log data files of a real Virtual Campus using different distributed infrastructures. More precisely, we study the time performance of processing daily log files of a Virtual Campus using cluster computing(under Open Grid Engine) and Planet Lab platform. The study reveals the complexity and challenges of massive processing in the big data era.
ER  - 

TY  - CONF
JO  - Computational Intelligence in Biometrics: Theory, Algorithms, and Applications, 2009. CIB 2009. IEEE Workshop on
TI  - A facial presence monitoring system for information security
T2  - Computational Intelligence in Biometrics: Theory, Algorithms, and Applications, 2009. CIB 2009. IEEE Workshop on
IS  - 
SN  - 
VO  - 
SP  - 69
EP  - 76
AU  - Qinghan Xiao
AU  - Xue Dong Yang
Y1  - March 30 2009-April 2 2009
PY  - 2009
KW  - biometrics (access control)
KW  - face recognition
KW  - graphical user interfaces
KW  - neural nets
KW  - security of data
KW  - eigenface method
KW  - face detection
KW  - facial presence monitoring system
KW  - facial recognition
KW  - facial recognition technology
KW  - graphical user interface
KW  - human physiological
KW  - information security
KW  - neural network-based algorithm
KW  - Anthropometry
KW  - Biometrics
KW  - Computer network reliability
KW  - Computerized monitoring
KW  - Face detection
KW  - Face recognition
KW  - Graphical user interfaces
KW  - Humans
KW  - Information security
KW  - Military computing
VL  - 
JA  - Computational Intelligence in Biometrics: Theory, Algorithms, and Applications, 2009. CIB 2009. IEEE Workshop on
DO  - 10.1109/CIB.2009.4925689
AB  - Information security requires a method to establish digital credentials that can reliably identify individual users. Since biometrics is concerned with the measurements of unique human physiological or behavioural characteristics, the technology has been used to verify the identity of computer or network users. Given today's heightened security requirements of military as well as other applications such as banking, health care, etc., it is becoming critical to be able to monitor the presence of the authenticated user throughout a session. This paper presents a prototype system that uses facial recognition technology to monitor the authenticated user. The objective is to ensure that the user who is using the computer is the same person that logged onto the system. A neural network-based algorithm is implemented to carry out face detection, and an eigenface method is employed to perform facial recognition. A graphical user interface (GUI) has been developed which allows the performance of face detection and facial recognition to be monitored at run time. The experimental results demonstrate the feasibility of near-real-time continuous user verification for high-level security information systems.
ER  - 

TY  - CONF
JO  - Communications and Network Security (CNS), 2013 IEEE Conference on
TI  - Discovering emergent norms in security logs
T2  - Communications and Network Security (CNS), 2013 IEEE Conference on
IS  - 
SN  - 
VO  - 
SP  - 438
EP  - 445
AU  - Pieczul, O.
AU  - Foley, S.N.
Y1  - 14-16 Oct. 2013
PY  - 2013
KW  - behavioural sciences
KW  - security of data
KW  - anomaly detection
KW  - behavioral norms
KW  - emergent norms
KW  - reflect patterns
KW  - security logs
KW  - system security evaluation
KW  - Approximation methods
KW  - Authentication
KW  - Automation
KW  - Computational modeling
KW  - Educational institutions
KW  - Kernel
VL  - 
JA  - Communications and Network Security (CNS), 2013 IEEE Conference on
DO  - 10.1109/CNS.2013.6682758
AB  - A model is presented that characterizes security logs as a collection of norms that reflect patterns of emergent behavior. An analysis technique for detecting behavioral norms based on these logs is described and evaluated. The application of behavioral norms is considered, including its use in system security evaluation and anomaly detection.
ER  - 

TY  - CONF
JO  - Visualization, 2007. APVIS '07. 2007 6th International Asia-Pacific Symposium on
TI  - Integrated visualization system for monitoring security in large-scale local area network
T2  - Visualization, 2007. APVIS '07. 2007 6th International Asia-Pacific Symposium on
IS  - 
SN  - 
VO  - 
SP  - 41
EP  - 44
AU  - Mukosaka, S.
AU  - Koike, H.
Y1  - 5-7 Feb. 2007
PY  - 2007
KW  - computerised monitoring
KW  - data visualisation
KW  - local area networks
KW  - security of data
KW  - telecommunication security
KW  - 3D visualization
KW  - IP address
KW  - filtering mechanism
KW  - geographical information
KW  - integrated visualization system
KW  - interaction capabilities
KW  - large-scale local area network
KW  - logical information
KW  - temporal information
KW  - visual security monitoring system
KW  - Computer interfaces
KW  - Computer networks
KW  - Computerized monitoring
KW  - Information security
KW  - Information systems
KW  - Intrusion detection
KW  - Large scale integration
KW  - Local area networks
KW  - Personal communication networks
KW  - Visualization
VL  - 
JA  - Visualization, 2007. APVIS '07. 2007 6th International Asia-Pacific Symposium on
DO  - 10.1109/APVIS.2007.329273
AB  - In monitoring security of enterprise or campus networks, detecting attacks from internal network to external network is becoming more and more important. After detecting such attacks, finding the location of the target PC is sometimes needed. This paper describes a visual security monitoring system for large-scale local area network. The system integrates three information, logical, temporal, and geographical information, in one 3D visualization. The system also provides effective interaction capabilities and filtering mechanism. IDS logs obtained at the computer center of our university were visualized, and typical examples such as botnet activities and SSH brute force attack were discussed.
ER  - 

TY  - CONF
JO  - DARPA Information Survivability Conference and Exposition, 2003. Proceedings
TI  - Fault-tolerant mesh of trust applied to DNS security
T2  - DARPA Information Survivability Conference and Exposition, 2003. Proceedings
IS  - 
SN  - 
VO  - 2
SP  - 84
EP  - 86 vol.2
AU  - Griffin, W.
AU  - Mundy, R.
AU  - Weiler, S.
AU  - Massey, D.
AU  - Vora, N.
Y1  - 22-24 April 2003
PY  - 2003
KW  - Internet
KW  - Web sites
KW  - computer operating procedures
KW  - electronic mail
KW  - fault tolerant computing
KW  - protocols
KW  - security of data
KW  - DNS security
KW  - Domain Name System
KW  - Internet
KW  - Web sites
KW  - e-mail messages
KW  - fault-tolerant mesh
KW  - log-in sessions
KW  - man-in-the-middle attacks
KW  - secure network protocols
KW  - trust
KW  - Authentication
KW  - Cryptography
KW  - Data security
KW  - Domain Name System
KW  - Fault tolerance
KW  - IP networks
KW  - Internet
KW  - Network servers
KW  - Protocols
KW  - Web server
VL  - 2
JA  - DARPA Information Survivability Conference and Exposition, 2003. Proceedings
DO  - 10.1109/DISCEX.2003.1194928
AB  - The Domain Name System is critical for the proper operation of applications on the Internet. Unfortunately, the DNS has a number of significant security weaknesses that can result in the compromise of Web sites, e-mail messages, and log-in sessions. Additionally, these weaknesses have been used as the basis for man-in-the-middle attacks on what are considered secure network protocols. This paper provides a short description of the weaknesses of the current DNS and a description of DNS security extensions that will solve the existing insecurities.
ER  - 

TY  - CONF
JO  - Security and Privacy for Emerging Areas in Communication Networks, 2005. Workshop of the 1st International Conference on
TI  - Sharing network logs for computer forensics: a new tool for the anonymization of netflow records
T2  - Security and Privacy for Emerging Areas in Communication Networks, 2005. Workshop of the 1st International Conference on
IS  - 
SN  - 
VO  - 
SP  - 37
EP  - 42
AU  - Slagell, A.J.
AU  - Yifan Li
AU  - Luo, K.
Y1  - 5-9 Sept. 2005
PY  - 2005
KW  - broadband networks
KW  - protocols
KW  - security of data
KW  - broadband networking applications
KW  - security protocols
KW  - trust management
KW  - wireless networking
KW  - Application software
KW  - Computer networks
KW  - Computer science education
KW  - Data security
KW  - Digital forensics
KW  - Government
KW  - Industrial training
KW  - Internet
KW  - Refining
KW  - Storage area networks
VL  - 
JA  - Security and Privacy for Emerging Areas in Communication Networks, 2005. Workshop of the 1st International Conference on
DO  - 10.1109/SECCMW.2005.1588293
AB  - The authors have begun to address the problem of anonymized data with the development of a new prototype tool CANINE: Converter and ANonymizer for Investigating Netflow Events. Originally, just a NetFlow converter, CANINE has been adapted to anonymize 8 of the most common fields found in all NetFlow formats. Most of these fields can be anonymized in multiple ways providing trade-offs between security and utility. This is the first tool the authors are aware of that supports many levels of anonymization and is the only NetFlow anonymizer of which we are aware ?? besides a previous, less advanced tool they developed. This article is organized as follows. part 2 discusses related work in log anonymization. Part 3 discusses CANINE??s anonymization algorithms and design decisions in depth. Finally, part 4 concludes and presents future work on CANINE and the anonymization of other log types.
ER  - 

TY  - CONF
JO  - Visual Analytics Science and Technology (VAST), 2011 IEEE Conference on
TI  - Guiding security analysis through visualization
T2  - Visual Analytics Science and Technology (VAST), 2011 IEEE Conference on
IS  - 
SN  - 
VO  - 
SP  - 317
EP  - 318
AU  - Harrison, L.
AU  - Wenwen Dou
AU  - Aidong Lu
AU  - Ribarsky, W.
AU  - Xiaoyu Wang
Y1  - 23-28 Oct. 2011
PY  - 2011
KW  - SQL
KW  - data visualisation
KW  - database management systems
KW  - security of data
KW  - SQL queries
KW  - VAST 2010 Mini Challenge 2
KW  - data security
KW  - database application
KW  - log event activity monitoring
KW  - multiple views visualization
KW  - network log data
KW  - security analysis
KW  - suspicious activity
KW  - Data visualization
KW  - Databases
KW  - Electronic mail
KW  - Image color analysis
KW  - Intrusion detection
KW  - Visualization
VL  - 
JA  - Visual Analytics Science and Technology (VAST), 2011 IEEE Conference on
DO  - 10.1109/VAST.2011.6102492
AB  - We present a multiple views visualization for the security data in the VAST 2010 Mini Challenge 2. The visualization is used to monitor log event activity on the network log data included in the challenge. Interactions are provided that allow analysts to investigate suspicious activity and escalate events as needed. Additionally, a database application is used to allow SQL queries for more detailed investigation.
ER  - 

TY  - CONF
JO  - Business Management and Electronic Information (BMEI), 2011 International Conference on
TI  - An improved algorithm with key attributes constraints for mining interesting association rules in network log
T2  - Business Management and Electronic Information (BMEI), 2011 International Conference on
IS  - 
SN  - 
VO  - 3
SP  - 104
EP  - 107
AU  - Jin Kezhong
AU  - Wu Chengwen
Y1  - 13-15 May 2011
PY  - 2011
KW  - computer forensics
KW  - data mining
KW  - pattern classification
KW  - security of data
KW  - association rule mining
KW  - computer forensic analysis
KW  - computer log data source
KW  - intrusion detection analysis
KW  - key attribute constraint
KW  - network access
KW  - network log data
KW  - outlier detection
KW  - user pattern mining
KW  - Algorithm design and analysis
KW  - Association rules
KW  - Computers
KW  - Databases
KW  - Performance evaluation
KW  - Protocols
KW  - association rule
KW  - data mining
KW  - key attribute
KW  - network log
VL  - 3
JA  - Business Management and Electronic Information (BMEI), 2011 International Conference on
DO  - 10.1109/ICBMEI.2011.5920405
AB  - Computer logs are generated by application activities, network accesses and system audit, which are important data sources for user pattern mining, computer forensic analysis, intrusion detection analysis and outlier detection. Algorithms for mining association rule are useful methods to find interesting rules implied in large computer log data. But existing algorithms which based on confidence and support are unfit for mining computer log data, many uninteresting rules will be generated and useful rules will be shadowed. To solve this problem, the concept of key attributes of network log data is introduced, and an algorithm with key attributes constraints for mining interesting association rules in network log data is designed. Experimental result shows that the number of uninteresting rules can be reduced effectively and the validity of rules which mined are improved.
ER  - 

TY  - CONF
JO  - Cyber Conflict (ICCC), 2011 3rd International Conference on
TI  - Preserving organizational privacy in intrusion detection log sharing
T2  - Cyber Conflict (ICCC), 2011 3rd International Conference on
IS  - 
SN  - 
VO  - 
SP  - 1
EP  - 14
AU  - Bahsi, H.
AU  - Levi, A.
Y1  - 7-10 June 2011
PY  - 2011
KW  - data privacy
KW  - outsourcing
KW  - security of data
KW  - statistical analysis
KW  - National Computer Emergency Response Team
KW  - centralized intrusion log management center
KW  - critical information systems
KW  - information security statistics
KW  - intrusion detection log sharing
KW  - intrusion detection management service
KW  - l-1 organizations
KW  - l-Diversity notion
KW  - organizational privacy preserving framework
KW  - outsourced company
KW  - Cryptography
KW  - Internet
KW  - Intrusion detection
KW  - Measurement
KW  - Organizations
KW  - Privacy
KW  - intrusion detection
KW  - log sharing
KW  - privacy preserving framework
VL  - 
JA  - Cyber Conflict (ICCC), 2011 3rd International Conference on
DO  - 
AB  - This paper presents a privacy-preserving framework for organizations that need to share their logs of intrusion detection systems with a centralized intrusion log management center. This centralized center may be an outsourced company that provides an intrusion detection management service to organizations or a system of the National Computer Emergency Response Team that probes the attacks targeting organizations that have critical information systems. For reasons of ensuring privacy, we adopt the notion of l-Diversity in the course of collecting intrusion logs from organizations. Within our framework, an organization ensures the people in the center cannot discern the exact origin of any intrusion log among the other l-1 organizations. Also, it is not possible to precisely identify the classification type of an intrusion log from among other l-1 types. Within this framework, the intrusion log management center can analyze the anonymous data, since the proposed privacy preserving solution creates little information loss. If required, it sends an alarm to the appropriate organization within a reasonable time. The center has the option of publishing useful information security statistics about specific organizations or about the whole ecosystem by using the privacy preserved intrusion logs.
ER  - 

TY  - CONF
JO  - Information Technology (ITSim), 2010 International Symposium in
TI  - Security challenges in designing an integrated web application for multiple online banking
T2  - Information Technology (ITSim), 2010 International Symposium in
IS  - 
SN  - 2155-897
VO  - 1
SP  - 1
EP  - 5
AU  - Ng, A.A.B.
AU  - Abdullah, N.L.
Y1  - 15-17 June 2010
PY  - 2010
KW  - Internet
KW  - banking
KW  - electronic commerce
KW  - security of data
KW  - I-PFO
KW  - independent personal financial organizer
KW  - integrated Web application
KW  - multiple online banking system
KW  - security
KW  - single Web site
KW  - user acceptance
KW  - Authentication
KW  - Banking
KW  - Fires
KW  - Internet
KW  - Online banking
KW  - Servers
KW  - integrated one-stop solution
KW  - integrated web application
KW  - multiple online banking
KW  - personalized financial organizer
KW  - security solution
VL  - 1
JA  - Information Technology (ITSim), 2010 International Symposium in
DO  - 10.1109/ITSIM.2010.5561291
AB  - Current online banking only allows payment to be made from a single bank account hence user needs to log in to several banking sites to settle the dues monthly. Paying bills/loans from multiple bank accounts in a single login would provide greater convenience. This paper reviewed the current on line banking system and discussed the challenges in designing an integrated web based application for independent personal financial organizer called I-PFO. I-PFO allows users to settle their financial commitments from multiple banks in a single website. It also provides ease of use, tracking due dates, organizing and personalization. The challenges of such system are security, value proposition to attract collaboration from banks and user acceptance. This paper focuses on security solutions for I-PFO.
ER  - 

TY  - CONF
JO  - Distributed Computing Systems, 2002. Proceedings. 22nd International Conference on
TI  - Key trees and the security of interval multicast
T2  - Distributed Computing Systems, 2002. Proceedings. 22nd International Conference on
IS  - 
SN  - 1063-6927
VO  - 
SP  - 467
EP  - 468
AU  - Gouda, M.G.
AU  - Chin-Tser Huang
AU  - Elnozahy, E.N.
Y1  - 2002
PY  - 2002
KW  - cryptography
KW  - multicast communication
KW  - security of data
KW  - tree data structures
KW  - arbitrary subgroup
KW  - distributed data structure
KW  - encryptions
KW  - interval multicast security
KW  - key trees
KW  - Broadcasting
KW  - Computer security
KW  - Costs
KW  - Cryptography
KW  - Data security
KW  - Distributed computing
KW  - Multicast protocols
KW  - System software
KW  - Tree data structures
VL  - 
JA  - Distributed Computing Systems, 2002. Proceedings. 22nd International Conference on
DO  - 10.1109/ICDCS.2002.1022293
AB  - A key tree is a distributed data structure of security keys that can be used by a group of users. In this paper we describe how any user in the group can use the different keys in the key tree to securely multicast data to different subgroups within the group. The cost of securely multicasting data to a subgroup whose users are "consecutive" is O(log n) encryptions, where n is the total number of users in the group. The cost of securely multicasting data to an arbitrary subgroup is O(n/2) encryptions. However this cost can be reduced to one encryption by introducing an additional key tree to the group.
ER  - 

TY  - CONF
JO  - Collaborative Computing: Networking, Applications and Worksharing, 2007. CollaborateCom 2007. International Conference on
TI  - Transparent security for collaborative environments
T2  - Collaborative Computing: Networking, Applications and Worksharing, 2007. CollaborateCom 2007. International Conference on
IS  - 
SN  - 
VO  - 
SP  - 79
EP  - 84
AU  - Hladka, E.
AU  - Kouril, D.
AU  - Prochazka, M.
AU  - Matyska, L.
AU  - Holub, P.
Y1  - 12-15 Nov. 2007
PY  - 2007
KW  - groupware
KW  - middleware
KW  - security of data
KW  - access control policies
KW  - authentication credentials
KW  - collaborative systems
KW  - collaborative tools
KW  - dynamic group management
KW  - middleware layer
KW  - transparent Security
KW  - user management
KW  - Application software
KW  - Authentication
KW  - Collaboration
KW  - Collaborative software
KW  - Collaborative tools
KW  - Collaborative work
KW  - Environmental management
KW  - Hardware
KW  - Middleware
KW  - Security
VL  - 
JA  - Collaborative Computing: Networking, Applications and Worksharing, 2007. CollaborateCom 2007. International Conference on
DO  - 10.1109/COLCOM.2007.4553814
AB  - Current collaborative tools are often not able to profit from existing systems for user management. It is therefore necessary for collaborative systems to administrate their users using their own solutions, which may not be adequate in terms of scalability or security. Many users may also experience problems working with authentication credentials (e.g. digital certificates) employed by collaborative systems. In this paper, we propose a general framework to provide easy-to-use yet secure access to collaborative systems, which offers a general middleware layer to accommodate various types of collaborative tools. The framework utilizes the emerging model of federations, which allows to provide a user-friendly means of logging in to a collaborative system as well as a solid basis for specifying access control policies. The framework handles all security aspects in a transparent way without requiring the users to perform complicated tasks. Using user attributes maintained in the federation, it is also possible to implement efficient and dynamic group management of the collaborating users.
ER  - 

TY  - CONF
JO  - Innovations in Information Technology (IIT), 2011 International Conference on
TI  - Evaluation of security methods for ensuring the integrity of digital evidence
T2  - Innovations in Information Technology (IIT), 2011 International Conference on
IS  - 
SN  - 
VO  - 
SP  - 220
EP  - 225
AU  - Saleem, S.
AU  - Popov, O.
AU  - Dahman, R.
Y1  - 25-27 April 2011
PY  - 2011
KW  - computational complexity
KW  - data acquisition
KW  - data integrity
KW  - security of data
KW  - ubiquitous computing
KW  - ISP record
KW  - Web page
KW  - computational complexity
KW  - digital evidence
KW  - e-service
KW  - information society
KW  - law enforcement
KW  - legal institution
KW  - malicious activity
KW  - mobile small scale digital device
KW  - network traffic
KW  - pervasive e-infrastructure
KW  - security method
KW  - social networks
KW  - statistical admissibility
KW  - Accuracy
KW  - Algorithm design and analysis
KW  - Authentication
KW  - Complexity theory
KW  - Digital signatures
KW  - Forensics
VL  - 
JA  - Innovations in Information Technology (IIT), 2011 International Conference on
DO  - 10.1109/INNOVATIONS.2011.5893821
AB  - The omnipresence of e-services running on various instances of pervasive e-infrastructures that are fundamental to the contemporary information society generates an abundance of digital evidence. The evidence in a digital form stems from a myriad of sources ranging from stand alone computers and their volatile and non-volatile storages, to mobile small scale digital devices, network traffic, ever-present applications comprising social networks, ISP records, logs, Web pages, databases and both global and local information systems. The acquisition and the analysis of this evidence is crucial to understanding and functioning of the digital world, regardless of the positive or negative implications of the actions and the activities that generated the evidence. In the case of the later, when the evidence comes from illegal, illicit and malicious activities, the protection of digital evidence is of major concern for the law enforcement and legal institutions, namely for investigators and prosecutors. To protect the integrity of the digital evidence, a number of security methods are used. These methods differ in terms of performance, accuracy, security levels, computational complexity, potential errors and the statistical admissibility of the produced results, as well as the vulnerabilities to accidental or malicious modifications. The work presented deals with the evaluation of these security methods in order to study and understand their &#x201C;goodness&#x201D; and suitability to protect the integrity of the digital evidence. The immediate outcome of the evaluation is a set of recommendations to be considered for selecting the right algorithm to protect integrity of the digital evidence in general.
ER  - 

TY  - CONF
JO  - Software Reliability Engineering Workshops (ISSREW), 2014 IEEE International Symposium on
TI  - Mining Security Vulnerabilities from Linux Distribution Metadata
T2  - Software Reliability Engineering Workshops (ISSREW), 2014 IEEE International Symposium on
IS  - 
SN  - 
VO  - 
SP  - 323
EP  - 328
AU  - Stuckman, J.
AU  - Purtilo, J.
Y1  - 3-6 Nov. 2014
PY  - 2014
KW  - Linux
KW  - data mining
KW  - security of data
KW  - Debian
KW  - Linux distribution metadata
KW  - Ubuntu
KW  - change logs
KW  - repository mining
KW  - security vulnerability
KW  - version branching structure
KW  - vulnerability patching history
KW  - Data mining
KW  - History
KW  - Indexes
KW  - Linux
KW  - Security
KW  - Software
KW  - Debian
KW  - Linux
KW  - repository mining
KW  - vulnerabilities
VL  - 
JA  - Software Reliability Engineering Workshops (ISSREW), 2014 IEEE International Symposium on
DO  - 10.1109/ISSREW.2014.101
AB  - Security vulnerability research has long been hindered by the difficulty in obtaining structured, detailed data on individual vulnerabilities in sufficient quantities for analysis. We mined vulnerabilities from historical change log data from Linux distribution packages, tapping a yet-unexplored source of security data. Change logs provide a unified view of a application's evolution, version branching structure, and vulnerability patching history, allowing for the large-scale compilation of data on susceptible (and, in some cases, non-susceptible) versions of the original application for each vulnerability. We then compiled vulnerability datasets for multiple releases of Debian and Ubuntu Linux, analyzing trends in vulnerability patching over time. Patching practices in Debian and Ubuntu were similar, and patch rates stayed constant throughout each distribution's lifetime.
ER  - 

TY  - CONF
JO  - Cloud Computing and Intelligence Systems (CCIS), 2011 IEEE International Conference on
TI  - ICAS: An inter-VM IDS Log Cloud Analysis System
T2  - Cloud Computing and Intelligence Systems (CCIS), 2011 IEEE International Conference on
IS  - 
SN  - 
VO  - 
SP  - 285
EP  - 289
AU  - Shun-Fa Yang
AU  - Wei-Yu Chen
AU  - Yao-Tsung Wang
Y1  - 15-17 Sept. 2011
PY  - 2011
KW  - cloud computing
KW  - security of data
KW  - virtual machines
KW  - Hadoop MapReduce algorithm analysis
KW  - ICAS
KW  - cloud computing
KW  - cloud hosting environment
KW  - conventional analysis system
KW  - information security
KW  - interVM IDS log cloud analysis system
KW  - intrusion detection system log files
KW  - mainframe management cost reduction
KW  - Cloud computing
KW  - Computer architecture
KW  - Correlation
KW  - File systems
KW  - Intrusion detection
KW  - Monitoring
KW  - Cloud Computing
KW  - Hadoop
KW  - IDS
KW  - MapReduce
VL  - 
JA  - Cloud Computing and Intelligence Systems (CCIS), 2011 IEEE International Conference on
DO  - 10.1109/CCIS.2011.6045076
AB  - Cloud computing can reduce mainframe management costs, so more and more users choose to build their own cloud hosting environment. In cloud computing, all the commands through the network connection, therefore, information security is particularly important. In this paper, we will explore the types of intrusion detection systems, and integration of these types, provided an effective and output reports, so system administrators can understand the attacks and damage quickly. With the popularity of cloud computing, intrusion detection system log files are also increasing rapidly, the effect is limited and inefficient by using the conventional analysis system. In this paper, we use Hadoop's MapReduce algorithm analysis of intrusion detection System log files, the experimental results also confirmed that the calculation speed can be increased by about 89%. For the system administrator, IDS Log Cloud Analysis System (called ICAS) can provide fast and high reliability of the system.
ER  - 

TY  - CONF
JO  - Computational Intelligence and Computing Research (ICCIC), 2013 IEEE International Conference on
TI  - Scalable intrusion detection systems log analysis using cloud computing infrastructure
T2  - Computational Intelligence and Computing Research (ICCIC), 2013 IEEE International Conference on
IS  - 
SN  - 
VO  - 
SP  - 1
EP  - 4
AU  - Kumar, M.
AU  - Hanumanthappa, M.
Y1  - 26-28 Dec. 2013
PY  - 2013
KW  - cloud computing
KW  - security of data
KW  - Hadoop
KW  - IDS
KW  - IDS monitors network traffic
KW  - cloud computing infrastructure
KW  - computer systems
KW  - data analytics
KW  - data processing
KW  - distributed computing system
KW  - distributed file system
KW  - malicious activities
KW  - malicious attacks
KW  - network administrator
KW  - network environment security
KW  - network security
KW  - open-source computing platform
KW  - scalable intrusion detection systems log analysis
KW  - storage services
KW  - suspicious activity
KW  - Cloud computing
KW  - Computer architecture
KW  - Computers
KW  - File systems
KW  - IP networks
KW  - Intrusion detection
KW  - Telecommunication traffic
KW  - Cloud Computing
KW  - Hadoop File System
KW  - Intrusion Detection System
KW  - MapReduce
VL  - 
JA  - Computational Intelligence and Computing Research (ICCIC), 2013 IEEE International Conference on
DO  - 10.1109/ICCIC.2013.6724158
AB  - An intrusion detection system (IDS) monitors network traffic and monitors for suspicious activity and alerts the system or network administrator. It identifies unauthorized use, misuse, and abuse of computer systems by both system insiders and external penetrators. Intrusion detection systems (IDS) are essential components in a secure network environment, allowing for early detection of malicious activities and attacks. By employing information provided by IDS, it is possible to apply appropriate countermeasures and mitigate attacks that would otherwise seriously undermine network security. However, current high volumes of network traffic overwhelm most IDS techniques requiring new approaches that are able to handle huge volume of log and packet analysis while still maintaining high throughput. Hadoop, an open-source computing platform of MapReduce and a distributed file system, has become a popular infrastructure for massive data analytics because it facilitates scalable data processing and storage services on a distributed computing system consisting of commodity hardware. The proposed architecture is able to efficiently handle large volumes of collected data and consequent high processing loads using Hadoop, MapReduce and cloud computing infrastructure. The main focus of the paper is to enhance the throughput and scalability of the IDS Log analysis. Once enough data is gathered, it is necessary to rapidly analyze it and determine whether any attacks or malicious activities are present, which is the main issue that impacts IDS performance.
ER  - 

TY  - CONF
JO  - Information and Communication Technology in Electrical Sciences (ICTES 2007), 2007. ICTES. IET-UK International Conference on
TI  - Enforcing security in .Net based web services
T2  - Information and Communication Technology in Electrical Sciences (ICTES 2007), 2007. ICTES. IET-UK International Conference on
IS  - 
SN  - 0537-9989
VO  - 
SP  - 996
EP  - 1001
AU  - Raj, E.G.D.
AU  - Sasikaladevi, N.
Y1  - 20-22 Dec. 2007
PY  - 2007
KW  - Web services
KW  - security of data
KW  - Web services
KW  - Windows platform
KW  - distributed architecture
KW  - event log
KW  - Event Log
KW  - Event Manager
KW  - Event Viewer
KW  - Web service
VL  - 
JA  - Information and Communication Technology in Electrical Sciences (ICTES 2007), 2007. ICTES. IET-UK International Conference on
DO  - 
AB  - Web services play a vital role in the design of enterprise application in distributed architecture. The dominant industry players offering standards and tools for the design of Web services are J2EE and .Net. J2EE allows the designer to create custom designed Web services and it allows in built support for security. JVM protects the system. CLR lacks in protection. In this paper, we have presented a simple method to enforce security in .Net based Web services in Windows platform. This paper discusses how to log exceptions in the event log and preventing unauthorized access.
ER  - 

TY  - CONF
JO  - Advanced Communication Technology (ICACT), 2011 13th International Conference on
TI  - Multi-level Intrusion Detection System and log management in Cloud Computing
T2  - Advanced Communication Technology (ICACT), 2011 13th International Conference on
IS  - 
SN  - 1738-9445
VO  - 
SP  - 552
EP  - 555
AU  - Jun-Ho Lee
AU  - Min-Woo Park
AU  - Jung-Ho Eom
AU  - Tai-Myoung Chung
Y1  - 13-16 Feb. 2011
PY  - 2011
KW  - cloud computing
KW  - security of data
KW  - virtual machines
KW  - cloud computing
KW  - cyber attacks
KW  - log management
KW  - multilevel intrusion detection system
KW  - virtual machine
KW  - Cloud Computing
KW  - Cooperative IDS
KW  - IDS
KW  - Intrusion Detection
KW  - Multilevel IDS
VL  - 
JA  - Advanced Communication Technology (ICACT), 2011 13th International Conference on
DO  - 
AB  - Cloud Computing is a new type of service which provides large scale computing resource to each customer. Cloud Computing systems can be easily threatened by various cyber attacks, because most of Cloud Computing systems provide services to so many people who are not proven to be trustworthy. Therefore, a Cloud Computing system needs to contain some Intrusion Detection Systems(IDSs) for protecting each Virtual Machine(VM) against threats. In this case, there exists a tradeoff between the security level of the IDS and the system performance. If the IDS provide stronger security service using more rules or patterns, then it needs much more computing resources in proportion to the strength of security. So the amount of resources allocating for customers decreases. Another problem in Cloud Computing is that, huge amount of logs makes system administrators hard to analyse them. In this paper, we propose a method that enables Cloud Computing system to achieve both effectiveness of using the system resource and strength of the security service without trade-off between them.
ER  - 

TY  - JOUR
JO  - Systems, Man, and Cybernetics, Part C: Applications and Reviews, IEEE Transactions on
TI  - Analyzing Log Files for Postmortem Intrusion Detection
T2  - Systems, Man, and Cybernetics, Part C: Applications and Reviews, IEEE Transactions on
IS  - 6
SN  - 1094-6977
VO  - 42
SP  - 1690
EP  - 1704
AU  - Garcia, K.A.
AU  - Monroy, R.
AU  - Trejo, L.A.
AU  - Mex-Perera, C.
AU  - Aguirre, E.
Y1  - Nov. 2012
PY  - 2012
KW  - authorisation
KW  - entropy
KW  - hidden Markov models
KW  - pattern classification
KW  - IT system
KW  - attack signature
KW  - computer security
KW  - cumulative detection rate
KW  - entropy-based approach
KW  - hidden Markov model
KW  - intrusion detection system maintenance
KW  - k-means classifier
KW  - log file analysis
KW  - postmortem intrusion detection
KW  - repetitive behavior
KW  - system vulnerability
KW  - Computational modeling
KW  - Computer crime
KW  - Hidden Markov models
KW  - Intrusion detection
KW  - Monitoring
KW  - Network security
KW  - Anomaly
KW  - hidden Markov model (HMM)
KW  - host-based intrusion detection
KW  - postmortem intrusion detection
KW  - sequitur
VL  - 42
JA  - Systems, Man, and Cybernetics, Part C: Applications and Reviews, IEEE Transactions on
DO  - 10.1109/TSMCC.2012.2217325
AB  - Upon an intrusion, security staff must analyze the IT system that has been compromised, in order to determine how the attacker gained access to it, and what he did afterward. Usually, this analysis reveals that the attacker has run an exploit that takes advantage of a system vulnerability. Pinpointing, in a given log file, the execution of one such an exploit, if any, is very valuable for computer security. This is both because it speeds up the process of gathering evidence of the intrusion, and because it helps taking measures to prevent a further intrusion, e.g., by building and applying an appropriate attack signature for intrusion detection system maintenance. This problem, which we call postmortem intrusion detection, is fairly complex, given both the overwhelming length of a standard log file, and the difficulty of identifying exactly where the intrusion has occurred. In this paper, we propose a novel approach for postmortem intrusion detection, which factors out repetitive behavior, thus, speeding up the process of locating the execution of an exploit, if any. Central to our intrusion detection mechanism is a classifier, which separates abnormal behavior from normal one. This classifier is built upon a method that combines a hidden Markov model with <i>k</i> -means. Our experimental results establish that our method is able to spot the execution of an exploit, with a cumulative detection rate of over 90%. In addition, we propose an entropy-based approach that speeds up the construction of a profile for ordinary system behavior.
ER  - 

TY  - CONF
JO  - Communications and Information Technology, 2005. ISCIT 2005. IEEE International Symposium on
TI  - A novel method for secure logging system call
T2  - Communications and Information Technology, 2005. ISCIT 2005. IEEE International Symposium on
IS  - 
SN  - 
VO  - 2
SP  - 955
EP  - 958
AU  - Jiang Tao Meng
AU  - Xianliang Lu
AU  - GuiShan Dong
Y1  - 12-14 Oct. 2005
PY  - 2005
KW  - data loggers
KW  - security of data
KW  - virtual machines
KW  - Xen-based secure logging system call
KW  - intrusion detection analysis techniques
KW  - system call
KW  - tamper-free logs
KW  - Application software
KW  - Control systems
KW  - Hardware
KW  - Intrusion detection
KW  - Linux
KW  - Operating systems
KW  - Prototypes
KW  - Virtual machine monitors
KW  - Virtual machining
KW  - Virtual manufacturing
VL  - 2
JA  - Communications and Information Technology, 2005. ISCIT 2005. IEEE International Symposium on
DO  - 10.1109/ISCIT.2005.1567025
AB  - Traditional methods of logging system activity are fundamentally insecure, which means that an attacker may compromise or tamper the logs. A novel method, Xen-based secure logging system call, is used to provide tamper-free logs of system call for some intrusion detection analysis techniques, which only require sequences of system call within a user process. Preliminary evaluation showed that the prototype is simple and efficient.
ER  - 

TY  - CONF
JO  - Autonomic Computing, 2005. ICAC 2005. Proceedings. Second International Conference on
TI  - An Intrusion-Tolerant and Self-Recoverable Network Service System Using A Security Enhanced Chip Multiprocessor
T2  - Autonomic Computing, 2005. ICAC 2005. Proceedings. Second International Conference on
IS  - 
SN  - 
VO  - 
SP  - 263
EP  - 273
AU  - Weidong Shi
AU  - Lee, H.-H.S.
AU  - Guofei Gu
AU  - Falk, Laura
AU  - Mudge, T.N.
AU  - Ghosh, M.
Y1  - 13-16 June 2005
PY  - 2005
KW  - buffer storage
KW  - checkpointing
KW  - distributed processing
KW  - fault tolerant computing
KW  - microprocessor chips
KW  - operating system kernels
KW  - security of data
KW  - buffer overflow
KW  - intrusion tolerance
KW  - intrusion-tolerant computing
KW  - kernel space rootkit attacks
KW  - network service system
KW  - rapid recovery system
KW  - reliable production services
KW  - security enhanced chip multiprocessor
KW  - self recovery
KW  - server applications
KW  - survivable service
KW  - Buffer overflow
KW  - Computer security
KW  - Costs
KW  - Design engineering
KW  - Educational institutions
KW  - Humans
KW  - Inspection
KW  - Protection
KW  - Virtual machining
KW  - Virtual manufacturing
KW  - Intrusion-tolerant computing
KW  - buffer overflow.
KW  - chip multi processor
KW  - rootkits
KW  - self-healing
KW  - survivable service
VL  - 
JA  - Autonomic Computing, 2005. ICAC 2005. Proceedings. Second International Conference on
DO  - 10.1109/ICAC.2005.8
AB  - This paper proposes a novel system design using a chip multiprocessor (CMP) to provide intrusion tolerance and self-recovery for server applications. Our platform provides three major advantages over previously proposed approaches, 1) security insulation from remote exploits and attacks; 2) close coupling between processor cores in a CMP to ensure immediate logging, fine-grained inspection and fast recovery; 3) concurrent and fine-grained inspection, logging and recovery techniques that are off of the critical path. We have designed a multi-point defense and recovery system to defeat remote exploits. We used a checkpoint based approach to recover server applications under attack. It takes a snapshot of the application's context and memory state before it handles the next request. If the request turns out to be malicious, the system can discard the malicious request and rollback the application's state to a known good one through checkpointing. We have also designed an rapid recovery system for kernel space rootkit attacks. Our intrusion survivable and self-recovery design provides reliable production services that System Administrators are seeking
ER  - 

TY  - CONF
JO  - Internet Technology And Secured Transactions, 2012 International Conference for
TI  - Two novel server-side attacks against log file in Shared Web Hosting servers
T2  - Internet Technology And Secured Transactions, 2012 International Conference for
IS  - 
SN  - 
VO  - 
SP  - 318
EP  - 323
AU  - Mirheidari, S.A.
AU  - Arshad, S.
AU  - Khoshkdahan, S.
AU  - Jalili, R.
Y1  - 10-12 Dec. 2012
PY  - 2012
KW  - Web sites
KW  - file organisation
KW  - security of data
KW  - Web site isolation
KW  - directory structure
KW  - file structure
KW  - log file separation
KW  - server administration
KW  - server maintenance
KW  - server-side attack
KW  - shared Web hosting server
KW  - HTML
KW  - Reactive power
KW  - World Wide Web
KW  - Log Poisoning
KW  - Log Snooping
KW  - Server-Side Attack
KW  - Shared Web Hosting
VL  - 
JA  - Internet Technology And Secured Transactions, 2012 International Conference for
DO  - 
AB  - Shared Web Hosting service enables hosting multitude of websites on a single powerful server. It is a well-known solution as many people share the overall cost of server maintenance and also, website owners do not need to deal with administration issues is not necessary for website owners. In this paper, we illustrate how shared web hosting service works and demonstrate the security weaknesses rise due to the lack of proper isolation between different websites, hosted on the same server. We exhibit two new server-side attacks against the log file whose objectives are revealing information of other hosted websites which are considered to be private and arranging other complex attacks. In the absence of isolated log files among websites, an attacker controlling a website can inspect and manipulate contents of the log file. These attacks enable an attacker to disclose file and directory structure of other websites and launch other sorts of attacks. Finally, we propose several countermeasures to secure shared web hosting servers against the two attacks subsequent to the separation of log files for each website.
ER  - 

TY  - CONF
JO  - Military Communications Conference, 1992. MILCOM '92, Conference Record. Communications - Fusing Command, Control and Intelligence., IEEE
TI  - Network security in the Unix environment
T2  - Military Communications Conference, 1992. MILCOM '92, Conference Record. Communications - Fusing Command, Control and Intelligence., IEEE
IS  - 
SN  - 
VO  - 
SP  - 1101
EP  - 1105 vol.3
AU  - Rich, L.D.
AU  - Chyan Yang
Y1  - 11-14 Oct 1992
PY  - 1992
KW  - Unix
KW  - computer networks
KW  - military computing
KW  - military systems
KW  - security of data
KW  - software packages
KW  - Network Information Service
KW  - Unix environment
KW  - anonymous FTP
KW  - dot files
KW  - file directory permissions
KW  - file exports
KW  - hidden files
KW  - login names
KW  - logs
KW  - network security
KW  - old accounts
KW  - passwords
KW  - sendmail configurations
KW  - software packages
KW  - Application software
KW  - Computer networks
KW  - Computer science
KW  - Computer security
KW  - File systems
KW  - Intelligent networks
KW  - Military computing
KW  - Personnel
KW  - Resource management
KW  - TCPIP
VL  - 
JA  - Military Communications Conference, 1992. MILCOM '92, Conference Record. Communications - Fusing Command, Control and Intelligence., IEEE
DO  - 10.1109/MILCOM.1992.244115
AB  - The authors enumerate some primary security holes that are the targets of intrusion, and they provide strategies for preventing them. Problems associated with passwords, file exports, Network Information Service, sendmail configurations, `dot' (hidden) files, file directory permissions, old accounts, login names, logs, and anonymous FTP. Software packages for addressing security problems are discussed
ER  - 

TY  - CONF
JO  - Information Assurance Workshop, 2004. Proceedings from the Fifth Annual IEEE SMC
TI  - A secure logging scheme for Forensic Computing
T2  - Information Assurance Workshop, 2004. Proceedings from the Fifth Annual IEEE SMC
IS  - 
SN  - 
VO  - 
SP  - 386
EP  - 393
AU  - Kawaguchi, N.
AU  - Ueda, S.
AU  - Obata, N.
AU  - Miyaji, R.
AU  - Kaneko, S.
AU  - Shigeno, H.
AU  - Okada, K.
Y1  - 10-11 June 2004
PY  - 2004
KW  - authorisation
KW  - data integrity
KW  - digital signatures
KW  - Forensic Computing
KW  - Forward Integrity
KW  - access control
KW  - confidential information
KW  - secure logging scheme
KW  - signature tree
KW  - Access control
KW  - Authentication
KW  - Computer crime
KW  - Computer hacking
KW  - Computer networks
KW  - Forensics
KW  - Government
KW  - Law
KW  - Legal factors
KW  - Paper technology
VL  - 
JA  - Information Assurance Workshop, 2004. Proceedings from the Fifth Annual IEEE SMC
DO  - 10.1109/IAW.2004.1437843
AB  - In this paper, we propose a secure logging scheme for Forensic Computing. Forensic Computing is the process conducted to identify the method of an attack and intruders in the case of system compromise. In Forensic Computing, trustworthy logs admissible for court are needed. Moreover, since the log contains various confidential information, the confidentiality of the log must be preserved. Our scheme achieves the integrity of logs and fine-grained access control for logs with small overhead size using the signature tree and Forward Integrity.
ER  - 

TY  - CONF
JO  - Computing, Control and Industrial Engineering (CCIE), 2011 IEEE 2nd International Conference on
TI  - IR4CF: A intrusion replay system for computer forensics
T2  - Computing, Control and Industrial Engineering (CCIE), 2011 IEEE 2nd International Conference on
IS  - 
SN  - 
VO  - 1
SP  - 66
EP  - 69
AU  - Lei Xu
AU  - Zhihong Tian
AU  - Jianwei Ye
AU  - Hongli Zhang
Y1  - 20-21 Aug. 2011
PY  - 2011
KW  - computer forensics
KW  - operating system kernels
KW  - system monitoring
KW  - IR4CF
KW  - append-only storage
KW  - computer forensics
KW  - intrusion replay system
KW  - kernel event information
KW  - logging machine
KW  - system-call hijacking technology
KW  - target system monitoring
KW  - Computers
KW  - File systems
KW  - Forensics
KW  - Kernel
KW  - Linux
KW  - Registers
KW  - Auditing
KW  - Forensics
KW  - Intrusion replay
VL  - 1
JA  - Computing, Control and Industrial Engineering (CCIE), 2011 IEEE 2nd International Conference on
DO  - 10.1109/CCIENG.2011.6007958
AB  - When computer intrusions occur, one of the most costly, time-consuming, and human-intensive tasks is to analysis and take the evidence of the compromised system. IR4CF: a system call based intrusion replay system for supporting the computer forensics. IR4CF uses three key mechanisms to improve the accuracy and reduce the human overhead of performing forensic analysis. First, it streams the kernel event information in real-time, to append-only storage on a separate, hardened, logging machine, making the system resilient to a wide variety of attacks. Second, it uses system-call hijacking technology to perform comprehensive monitoring of the execution of a target system at the kernel event level, giving a high-resolution, application-independent view of all activity. Third, it analyses and replays the intrusion actions dynamically, which can be used for evidence in a court of law.
ER  - 

TY  - CONF
JO  - Communications, 2008. ICC '08. IEEE International Conference on
TI  - Complexity Analysis of Retrieving Knowledge from Auditing Log Files for Computer and Network Forensics and Accountability
T2  - Communications, 2008. ICC '08. IEEE International Conference on
IS  - 
SN  - 
VO  - 
SP  - 1474
EP  - 1478
AU  - Takahashi, D.
AU  - Yang Xiao
Y1  - 19-23 May 2008
PY  - 2008
KW  - computational complexity
KW  - computer networks
KW  - security of data
KW  - FIFO
KW  - complexity analysis
KW  - computer network accountability
KW  - computer network forensics
KW  - knowledge retrieval
KW  - message-queue
KW  - network auditing log database
KW  - Analytical models
KW  - Computational modeling
KW  - Computer crime
KW  - Computer hacking
KW  - Computer networks
KW  - Databases
KW  - Forensics
KW  - Information retrieval
KW  - Layout
KW  - Personal digital assistants
VL  - 
JA  - Communications, 2008. ICC '08. IEEE International Conference on
DO  - 10.1109/ICC.2008.285
AB  - Behaviors of users in a computer or a computer network can be observed by system authorities via logs of all the actions. In a computer or network system, if at some point the fact that the content of a secret file is leaking has been already known, to figure out the reasons of the leaking, we can search partial or entire log files to find out direct or indirect accesses to the file; since a user who accessed the secret before may send messages containing the secret to other users (the secret is leaking due to indirect accesses) via packets in a computer network, or via pipe/FIFO/message-queue/etc. in a computer system, finding the reasons of the leaking is not a trivial task. In this paper, we analyze and simulate the complexity of retrieving knowledge from the computer and network auditing log database for forensics and accountability.
ER  - 

TY  - CONF
JO  - Computational Intelligence for Modelling, Control and Automation, 2006 and International Conference on Intelligent Agents, Web Technologies and Internet Commerce, International Conference on
TI  - AWide Area Log Analyzing System Based on Mobile Agents
T2  - Computational Intelligence for Modelling, Control and Automation, 2006 and International Conference on Intelligent Agents, Web Technologies and Internet Commerce, International Conference on
IS  - 
SN  - 
VO  - 
SP  - 26
EP  - 26
AU  - Katoh, T.
AU  - Kuzuno, H.
AU  - Kawahara, T.
AU  - Watanabe, A.
AU  - Nakai, Y.
AU  - Bista, B.B.
AU  - Takata, T.
Y1  - Nov. 28 2006-Dec. 1 2006
PY  - 2006
KW  - Internet
KW  - mobile agents
KW  - peer-to-peer computing
KW  - security of data
KW  - system monitoring
KW  - Internet
KW  - P2P network
KW  - hostile attacks detection
KW  - mobile agents
KW  - network administrators
KW  - network communication logs
KW  - network traffic logs
KW  - wide area log analyzing system
KW  - Cause effect analysis
KW  - Communication system security
KW  - Computer network management
KW  - Computer networks
KW  - Computer worms
KW  - Environmental management
KW  - IP networks
KW  - Mobile agents
KW  - Protection
KW  - Prototypes
VL  - 
JA  - Computational Intelligence for Modelling, Control and Automation, 2006 and International Conference on Intelligent Agents, Web Technologies and Internet Commerce, International Conference on
DO  - 10.1109/CIMCA.2006.56
AB  - The Internet is being widely used these days and many users are required to manage their network environments, because damages caused by worms, which spread using security holes of software, are also increasing rapidly. One of the effective means of detecting the damages caused by the worms in early stage is to analyze the network communication logs stored in computers that are spread over a wide area. However, almost all network administrators are not able to install many observation points, though a large number of observation points over a wide area of a network are needed to grasp symptoms of attacks precisely. In this paper, we propose an agent based log analyzing system by integrating the concepts of P2P network and mobile agents to realize detection and protection from the damages which may be caused by the worms in early stage. We also show results of experiments using our prototype system. The results show that our system can collect useful information from a wide area of a network, and provide means of flexible and on-demand analysis of network traffic logs to detect hostile attacks on the network.
ER  - 

TY  - CONF
JO  - Computers in Cardiology, 1999
TI  - Remote access to medical records via the Internet: feasibility, security and multilingual considerations
T2  - Computers in Cardiology, 1999
IS  - 
SN  - 0276-6547
VO  - 
SP  - 89
EP  - 92
AU  - Lees, P.J.
AU  - Chronaki, C.E.
AU  - Simantirakis, E.N.
AU  - Kostomanolakis, S.G.
AU  - Orphanoudakis, S.C.
AU  - Vardas, P.E.
Y1  - 1999
PY  - 1999
KW  - Internet
KW  - biomedical communication
KW  - medical information systems
KW  - security of data
KW  - English
KW  - Europe
KW  - Greek
KW  - central database
KW  - interlinked pages miniWebs
KW  - logged transactions
KW  - medical data security
KW  - multilingual considerations
KW  - password access requirement
KW  - patient's name
KW  - remote access to medical records
KW  - strict secure measures
KW  - usage profiles support
KW  - Biomedical informatics
KW  - Cardiology
KW  - Data mining
KW  - Demography
KW  - Europe
KW  - Hospitals
KW  - Internet
KW  - Medical treatment
KW  - Particle measurements
KW  - Telematics
VL  - 
JA  - Computers in Cardiology, 1999
DO  - 10.1109/CIC.1999.825913
AB  - This paper investigates the use of Internet technology to provide secure multilingual access to the medical records of a cardiology clinic. Information extracted from the medical record archive is used to create mini-Webs of interlinked pages that may be viewed using a standard browser. The main features of the reported project are its multilingual nature-of particular significance in Europe, the provision of strict secure measures, and the support of usage profiles. Medical data may be viewed either in Greek-the language in which the data are stored-or in English, through an automated translation process. Security measures are as follows: (1) a password is required for access; (2) all transactions are logged (3) the patient's name and medical data are never transmitted together; (4) mini-Webs are deleted when the transaction is complete, (5) the remote user never has direct access to the central database
ER  - 

TY  - CONF
JO  - Cluster, Cloud and Grid Computing (CCGrid), 2014 14th IEEE/ACM International Symposium on
TI  - Toward Detecting Compromised MapReduce Workers through Log Analysis
T2  - Cluster, Cloud and Grid Computing (CCGrid), 2014 14th IEEE/ACM International Symposium on
IS  - 
SN  - 
VO  - 
SP  - 41
EP  - 50
AU  - Eunjung Yoon
AU  - Squicciarini, A.
Y1  - 26-29 May 2014
PY  - 2014
KW  - data integrity
KW  - parallel processing
KW  - security of data
KW  - system monitoring
KW  - Hadoop logs
KW  - attack detection
KW  - compromised MapReduce worker detection
KW  - computation integrity checks
KW  - log analysis
KW  - malicious activity detection
KW  - malicious nodes
KW  - misconfigured nodes
KW  - Cloud computing
KW  - Cryptography
KW  - Data mining
KW  - Educational institutions
KW  - Instruments
KW  - Monitoring
KW  - Privacy
KW  - Cloud Computing
KW  - Computation Integrity
KW  - Hadoop
KW  - Log Analysis
KW  - MapReduce
KW  - System call log
VL  - 
JA  - Cluster, Cloud and Grid Computing (CCGrid), 2014 14th IEEE/ACM International Symposium on
DO  - 10.1109/CCGrid.2014.120
AB  - MapReduce is a framework for performing data intensive computations in parallel on commodity computers. When MapReduce is carried out in distributed settings, users maintain very little control over these computations, causing several security and privacy concerns. MapReduce activities may be subverted or compromised by malicious or cheating nodes. In this paper, we focus on the analysis and detection of attacks launched by malicious or mis configured nodes, which may tamper with the ordinary functions of the MapReduce framework. Our goal is to investigate the extent to which integrity and correctness of computation in a MapReduce environments can be verified while introducing no modifications on the original MapReduce operations or introductions of extra operations, neither computational nor cryptographic. We identify a number of data and computation integrity checks against aggregated low-level system traces and Hadoop logs, correlated with one another to obtain insights on the operations being performed by nodes. This information is then matched against system and program invariants to effectively detect malicious activities, from lazy nodes to nodes changing input/output or completing different computations.
ER  - 

TY  - CONF
JO  - Computer Applications and Information Systems (WCCAIS), 2014 World Congress on
TI  - SIEM implementation for global and distributed environments
T2  - Computer Applications and Information Systems (WCCAIS), 2014 World Congress on
IS  - 
SN  - 
VO  - 
SP  - 1
EP  - 6
AU  - Anastasov, I.
AU  - Davcev, D.
Y1  - 17-19 Jan. 2014
PY  - 2014
KW  - security of data
KW  - software agents
KW  - ArcSight ESM
KW  - SIEM
KW  - computer network
KW  - data protection
KW  - distributed environment
KW  - event management system
KW  - global environment
KW  - hierarchical managers model
KW  - log management
KW  - proactive threat detection
KW  - security event correlation
KW  - security information
KW  - security log data
KW  - Connectors
KW  - Databases
KW  - Electronic mail
KW  - Monitoring
KW  - Organizations
KW  - Security
KW  - Servers
KW  - ArcSight ESM [7]
KW  - Internet of Things
KW  - SIEM
KW  - computer security
KW  - event management
KW  - information security
KW  - log management
KW  - rules
KW  - use cases
VL  - 
JA  - Computer Applications and Information Systems (WCCAIS), 2014 World Congress on
DO  - 10.1109/WCCAIS.2014.6916651
AB  - Today's computer networks produce a huge amount of security log data. Handling this data is impossible without using Security Information and Event Management Systems (SIEM) to centralize the log management and increase the level of information security and data protection in the organization. SIEM collect and aggregate log data from various devices and applications through software called agents or connectors, filter uninteresting data and normalize to a proprietary format, analyses through correlation using contextual information and alert administrators in case of attack. SIEM provide proactive threat detection and real-time analysis of system activity. Handling these issues will be very hard without relying on consolidated, big data-powered SIEM. However, even having the most expensive SIEM solution, the organization should not expect the product to work great out of the box. The best SIEM solution does not guarantee success. The organization should focus on building various use cases to make their SIEM solution a success. In this paper, we propose a new model and architecture for SIEM implementation that is using multiple hierarchical SIEM Managers. The model is called &#x201C;Hierarchical Managers Model&#x201D;. We demonstrated how this model and architecture could be created and enabled in the leading SIEM system - ArcSight ESM [7]. We also provide examples of possible use cases that we have created and tested in our testing environment. These are meant to provide a good base starting point and should not be considered comprehensive for all situations. The use cases shown in this paper are created using the security event correlation framework from Hewlett-Packard - ArcSight ESM [7].
ER  - 

TY  - CONF
JO  - Communications and Networking in China (CHINACOM), 2014 9th International Conference on
TI  - SAFE-CROWD: Secure task allocation for collaborative mobile social network
T2  - Communications and Networking in China (CHINACOM), 2014 9th International Conference on
IS  - 
SN  - 
VO  - 
SP  - 347
EP  - 352
AU  - Xiaochen Fan
AU  - Panlong Yang
AU  - Qingyu Li
Y1  - 14-16 Aug. 2014
PY  - 2014
KW  - cloud computing
KW  - mobile computing
KW  - security of data
KW  - social networking (online)
KW  - SAFE-CROWD scheme
KW  - cloudlet
KW  - collaborative mobile social network
KW  - malicious user collusion method
KW  - mobile cloud
KW  - mobile user collaboration
KW  - network processing ability
KW  - queueing length
KW  - security issue
KW  - smart mobile devices
KW  - task allocation security
KW  - task reassignment policy
KW  - wireless networking technology
KW  - Collaboration
KW  - Mobile communication
KW  - Mobile computing
KW  - Resource management
KW  - Safety
KW  - Security
KW  - Social network services
KW  - Mobile Social Network
KW  - Security
KW  - Task Allocation
KW  - Traffic Balancing
VL  - 
JA  - Communications and Networking in China (CHINACOM), 2014 9th International Conference on
DO  - 10.1109/CHINACOM.2014.7054315
AB  - With the pervasive use of smart mobile devices and increasing wireless networking technologies, collaborations among mobile users are becoming deeper and ubiquitous. Appropriate task collaborations among mobile users could effectively improve the network processing ability with so called `mobile cloud' or `cloudlet'. However, task allocations confront with the security issues. The possible collusion or re-collaborations among the mobile users would possibly merge the allocated tasks of the specific users. Moreover, considering the delivery reliability and task execution efficiency, replications are applied for enhancement, which would also lead to more sever security threat for users. We investigate how to secure the security when task collaborations are allowed for mobile users. Our security scheme is built upon the load balancing scheme, and our intuitive solution is, if the tasks could be effectively balanced among users, the security issues could be guaranteed, because averaging the task assignment could effectively raise the threshold for collusion among potential malicious users. In this work, we propose `SAFE-CROWD': a secure task offloading and reassignment scheme among mobile users. The basic idea is simple, we leverage the `ball and bin' theory for task assignment, where d mobile users in contact range are investigated, and we select the least loaded ones among them. It has been proved that, such simple cases can effectively reduce the largest queueing length from &#x03B8;(log n/log log n) to &#x03B8;(log log n/log d). Inspired by this theoretical result, we develop a task reassignment policy for security issues. Simulation studies have shown that, our simple but effective scheme could enhance the security for mobile users, when the tasks are collaboratively executed among mobile devices.
ER  - 

TY  - CONF
JO  - Quality Software, 2009. QSIC '09. 9th International Conference on
TI  - ADAM: Web Anomaly Detection Assistant Based on Feature Matrix
T2  - Quality Software, 2009. QSIC '09. 9th International Conference on
IS  - 
SN  - 1550-6002
VO  - 
SP  - 123
EP  - 128
AU  - Sungdeok Cha
AU  - Junsup Lee
AU  - Sangrok Kim
AU  - Sanghyun Cho
Y1  - 24-25 Aug. 2009
PY  - 2009
KW  - Internet
KW  - data visualisation
KW  - security of data
KW  - ADAM
KW  - Anomaly Detection Assistant based on feature Matrix
KW  - Microsoft security
KW  - Web anomaly detection assistant
KW  - Web attacks
KW  - Web logs
KW  - Web security
KW  - Web-based economy
KW  - anomaly feature matrix
KW  - interactive analysis tool
KW  - mapping software
KW  - security analysis
KW  - signature-based misuse detection
KW  - visual analysis tool
KW  - Data engineering
KW  - Data security
KW  - Earth
KW  - Feedback
KW  - Network address translation
KW  - Open source software
KW  - Performance analysis
KW  - Privacy
KW  - Visualization
KW  - Web server
KW  - anomaly detection
KW  - network security
KW  - web data mining
KW  - web security
VL  - 
JA  - Quality Software, 2009. QSIC '09. 9th International Conference on
DO  - 10.1109/QSIC.2009.24
AB  - Importance of web security cannot be overemphasized in the era of web-based economy. Although anomaly detection has long been considered a promising alternative to signature-based misuse detection technique, most studies to date used either small scale or artificially generated attack data. In this paper, based on security analysis applied on anonymous www.microsoft.com log of about 250 GB, we propose Anomaly Feature Matrix (AFM) as an effective framework to characterize anomalies. Feature selection of AFM is based on the characteristics of well-known (e.g., DDoS) attacks as well as patterns of anomalous logs found in the Microsoft data. Independent security analysis performed on the same data by Microsoft security engineers concluded that 1) We did not miss any major attacks; and 2) AFM is a general enough framework to characterize likely web attacks. In order to assist AFM-based anomaly analysis in large organizations, we implemented an interactive and visual analysis tool named ADAM (Anomaly Detection Assistant based on feature Matrix). Integrated with mapping software such as Virtual Earth, ADAM enables efficient and focused security analysis on web logs.
ER  - 

TY  - CONF
JO  - Trust, Security and Privacy in Computing and Communications (TrustCom), 2013 12th IEEE International Conference on
TI  - S2Logger: End-to-End Data Tracking Mechanism for Cloud Data Provenance
T2  - Trust, Security and Privacy in Computing and Communications (TrustCom), 2013 12th IEEE International Conference on
IS  - 
SN  - 
VO  - 
SP  - 594
EP  - 602
AU  - Chun Hui Suen
AU  - Ko, R.K.L.
AU  - Yu Shyang Tan
AU  - Jagadpramana, P.
AU  - Bu Sung Lee
Y1  - 16-18 July 2013
PY  - 2013
KW  - cloud computing
KW  - data analysis
KW  - data loggers
KW  - data protection
KW  - security of data
KW  - system monitoring
KW  - S2Logger
KW  - atomic data events
KW  - cloud computing environments
KW  - cloud data provenance
KW  - cloud data provenance records
KW  - cloud servers
KW  - cloud stakeholders
KW  - critical data-related cloud security problems
KW  - data activities
KW  - data event analysis
KW  - data event capturing
KW  - data event logging mechanism
KW  - data event visualization
KW  - data leakages
KW  - data lifecycle
KW  - data movement accountability
KW  - data movement transparency
KW  - data policy violations
KW  - data tracking tools
KW  - data-centric logging techniques
KW  - end-to-end data tracking mechanism
KW  - file creation
KW  - file deletions
KW  - file duplication
KW  - file edition
KW  - file transfers
KW  - logging mechanisms
KW  - malicious actions
KW  - system-centric security tools
KW  - Cloud computing
KW  - Distributed databases
KW  - Kernel
KW  - Linux
KW  - Monitoring
KW  - Security
KW  - Virtual machine monitors
KW  - Cloud Computing
KW  - Cloud data provenance
KW  - S2Logger
KW  - accountability in cloud computing
KW  - cloud computing security
KW  - cloud computing transparency
KW  - data tracing
KW  - data tracking
KW  - file tracking
VL  - 
JA  - Trust, Security and Privacy in Computing and Communications (TrustCom), 2013 12th IEEE International Conference on
DO  - 10.1109/TrustCom.2013.73
AB  - The inability to effectively track data in cloud computing environments is becoming one of the top concerns for cloud stakeholders. This inability is due to two main reasons. Firstly, the lack of data tracking tools built for clouds. Secondly, current logging mechanisms are only designed from a system-centric perspective. There is a need for data-centric logging techniques which can trace data activities (e.g. file creation, edition, duplication, transfers, deletions, etc.) within and across all cloud servers. This will effectively enable full transparency and accountability for data movements in the cloud. In this paper, we introduce S2Logger, a data event logging mechanism which captures, analyses and visualizes data events in the cloud from the data point of view. By linking together atomic data events captured at both file and block level, the resulting sequence of data events depicts the cloud data provenance records throughout the data lifecycle. With this information, we can then detect critical data-related cloud security problems such as malicious actions, data leakages and data policy violations by analysing the data provenance. S2Logger also enables us to address the gaps and inadequacies of existing system-centric security tools.
ER  - 

TY  - CONF
JO  - Availability, Reliability and Security, 2009. ARES '09. International Conference on
TI  - A Practical Framework for the Dataflow Pointcut in AspectJ
T2  - Availability, Reliability and Security, 2009. ARES '09. International Conference on
IS  - 
SN  - 
VO  - 
SP  - 835
EP  - 840
AU  - Boukhtouta, A.
AU  - Alhadidi, D.
AU  - Debbabi, M.
Y1  - 16-19 March 2009
PY  - 2009
KW  - Java
KW  - data flow analysis
KW  - object-oriented programming
KW  - program compilers
KW  - security of data
KW  - AspectJ compiler
KW  - Java programming language
KW  - aspect-oriented programming language
KW  - dataflow pointcut
KW  - program execution
KW  - security aspect
KW  - Application software
KW  - Availability
KW  - Computer languages
KW  - Computer security
KW  - Data security
KW  - Design engineering
KW  - Information security
KW  - Java
KW  - Laboratories
KW  - Open source software
KW  - AspectJ
KW  - Dataflow
KW  - Security
VL  - 
JA  - Availability, Reliability and Security, 2009. ARES '09. International Conference on
DO  - 10.1109/ARES.2009.86
AB  - In this paper, we present the design and the implementation of the dataflow pointcut in AspectJ compiler ajc 1.5.0. Some security concerns are sensitive to flow of information in a program execution. The dataflow pointcut has been proposed by Masuhara and Kawauchi in order to easily implement such security concerns in aspect-oriented programming languages. The pointcut identifies join points based on the origins of values. The dataflow pointcut can detect and fix a lot of vulnerabilities that result from not validating input effectively, e.g., Web application vulnerabilities, process injection, log forging, and path injection. AspectJ extends the Java programming language to implement crosscutting concerns modularly in general. The implementation methodology of the dataflow pointcut which depends in define-use analysis is described in detail together with case studies that demonstrate how the implemented dataflow pointcut can detect a considerable number of vulnerabilities.
ER  - 

TY  - CONF
JO  - Networks, 2005. Jointly held with the 2005 IEEE 7th Malaysia International Conference on Communication., 2005 13th IEEE International Conference on
TI  - False positives reduction via intrusion alert quality framework
T2  - Networks, 2005. Jointly held with the 2005 IEEE 7th Malaysia International Conference on Communication., 2005 13th IEEE International Conference on
IS  - 
SN  - 1531-2216
VO  - 1
SP  - 6 pp.
EP  - 
AU  - Bakar, N.A.
AU  - Belaton, B.
AU  - Samsudin, A.
Y1  - 16-18 Nov. 2005
PY  - 2005
KW  - security of data
KW  - telecommunication security
KW  - DARPA 2000 network traffic
KW  - false positives reduction
KW  - filtering routers
KW  - firewalls
KW  - intrusion alert quality framework
KW  - security monitoring sensors
KW  - Computer science
KW  - Computer security
KW  - Computerized monitoring
KW  - Data security
KW  - Information filtering
KW  - Information filters
KW  - Information security
KW  - Intrusion detection
KW  - Organizing
KW  - Telecommunication traffic
KW  - Alert Correlation
KW  - Data Quality
KW  - Intrusion Detection
KW  - Security
VL  - 1
JA  - Networks, 2005. Jointly held with the 2005 IEEE 7th Malaysia International Conference on Communication., 2005 13th IEEE International Conference on
DO  - 10.1109/ICON.2005.1635545
AB  - Existing security monitoring sensors such as IDS/IPS, firewalls, filtering routers, and others often record logs and subsequently generate alerts to warn security analysts of what is perceived as posing security threat to the environment or organization they are monitoring. Unfortunately, these logs and alerts are not only huge in number but also poor in data quality i.e. containing false logs/alerts. This in turn poses two main challenges to higher-level operations; first computationally efficient algorithms are needed to process and shift through large unverified logs and alerts. Second is the need to develop algorithms that avoid making wrong conclusions due to poor quality logs and alerts. In this paper, we implement intrusion alert quality framework to reduce false positive alerts in IDS. Using this framework, we enrich each alert with quality parameters such as correctness, accuracy, reliability, and sensitivity. To compliment this effort, we normalize the enriched alerts in the IDMEF format. In this form (enriched and normalized), higher level operations are given the option to utilize the quality parameters values tagged in the alerts in their core operations in order to produce good conclusions. Finally, we demonstrate the efficacy of the framework in reducing false positive alerts using DARPA 2000 network traffic.
ER  - 

TY  - CONF
JO  - Visual Analytics Science and Technology (VAST), 2011 IEEE Conference on
TI  - An integrated visualization on network events VAST 2011 mini challenge #2 award: &#x201C;Outstanding integrated overview display&#x201D;
T2  - Visual Analytics Science and Technology (VAST), 2011 IEEE Conference on
IS  - 
SN  - 
VO  - 
SP  - 319
EP  - 321
AU  - Lamagna, W.M.
Y1  - 23-28 Oct. 2011
PY  - 2011
KW  - Internet
KW  - data visualisation
KW  - programming languages
KW  - public domain software
KW  - security of data
KW  - VAST 2011 mini challenge #2 award
KW  - Web programming languages
KW  - canvas
KW  - data set security trend visualization
KW  - heat map development
KW  - integrated visualization
KW  - log synchronization
KW  - network security trends
KW  - open source database
KW  - open source tools
KW  - outstanding integrated overview display
KW  - parallel coordinates charts
KW  - processing
KW  - Arrays
KW  - Data visualization
KW  - Fires
KW  - Heating
KW  - Image color analysis
KW  - Security
KW  - Synchronization
KW  - heat map
KW  - logs
KW  - security trends
KW  - vast challenge
KW  - visual analysis
VL  - 
JA  - Visual Analytics Science and Technology (VAST), 2011 IEEE Conference on
DO  - 10.1109/VAST.2011.6102493
AB  - To visualize security trends for the data set provided by the VAST 2011 Mini Challenge #2 a custom tool has been developed. Open source tools [1,2], web programming languages [4,7] and an open source database [3] has been used to work with the data and create a visualization for security log files containing network security trends. In this paper, the tools and methods used for the analysis are described. The methods include the log synchronization with different timezone and the development of heat maps and parallel coordinates charts. To develop the visualization, Processing and Canvas [4,7] was used.
ER  - 

TY  - CONF
JO  - Green Computing and Communications (GreenCom), 2010 IEEE/ACM Int'l Conference on & Int'l Conference on Cyber, Physical and Social Computing (CPSCom)
TI  - Effectiveness of Physical, Social and Digital Mechanisms against Laptop Theft in Open Organizations
T2  - Green Computing and Communications (GreenCom), 2010 IEEE/ACM Int'l Conference on & Int'l Conference on Cyber, Physical and Social Computing (CPSCom)
IS  - 
SN  - 
VO  - 
SP  - 727
EP  - 732
AU  - Dimkov, T.
AU  - Pieters, W.
AU  - Hartel, P.
Y1  - 18-20 Dec. 2010
PY  - 2010
KW  - security of data
KW  - social sciences
KW  - IT system security
KW  - data protection
KW  - digital security
KW  - laptop theft
KW  - open organization
KW  - physical protection
KW  - social protection
KW  - Access control
KW  - Buildings
KW  - Cameras
KW  - Organizations
KW  - Portable computers
KW  - Surveillance
KW  - case study
KW  - digital security
KW  - laptop theft
KW  - physical security
KW  - social engineering
VL  - 
JA  - Green Computing and Communications (GreenCom), 2010 IEEE/ACM Int'l Conference on & Int'l Conference on Cyber, Physical and Social Computing (CPSCom)
DO  - 10.1109/GreenCom-CPSCom.2010.165
AB  - Organizations rely on physical, digital and social mechanisms to protect their IT systems. Of all IT systems, laptops are probably the most troublesome to protect, since they are easy to remove and conceal. When the thief has physical possession of the laptop, it is also difficult to protect the data inside. In this study, we look at the effectiveness of the security mechanisms against laptop theft in two universities. The study considers the physical and social protection of the laptops. We analyze the logs from laptop thefts in both universities and complement the results with penetration tests. The results from the study show that the effectiveness of security mechanisms from the physical domain is limited, and it depends mostly from the social domain. The study serves as a motivation to further investigate the analysis of the alignment of the mechanisms across all three security domains to protect the IT assets in an organization.
ER  - 

TY  - CONF
JO  - Availability, Reliability and Security (ARES), 2013 Eighth International Conference on
TI  - Fraud Detection in Mobile Payments Utilizing Process Behavior Analysis
T2  - Availability, Reliability and Security (ARES), 2013 Eighth International Conference on
IS  - 
SN  - 
VO  - 
SP  - 662
EP  - 669
AU  - Rieke, R.
AU  - Zhdanova, M.
AU  - Repp, J.
AU  - Giot, R.
AU  - Gaber, C.
Y1  - 2-6 Sept. 2013
PY  - 2013
KW  - financial management
KW  - security of data
KW  - electronic money
KW  - fraud detection
KW  - mobile payments
KW  - money laundering activities
KW  - money transfer service
KW  - predictive security analyser
KW  - process behavior analysis
KW  - synthetic process behavior
KW  - Adaptation models
KW  - Computational modeling
KW  - Mobile communication
KW  - Monitoring
KW  - Predictive models
KW  - Runtime
KW  - Security
KW  - analysis of business process behavior
KW  - money laundering
KW  - predictive security analysis
KW  - security modeling and simulation
KW  - security monitoring
VL  - 
JA  - Availability, Reliability and Security (ARES), 2013 Eighth International Conference on
DO  - 10.1109/ARES.2013.87
AB  - Generally, fraud risk implies any intentional deception made for financial gain. In this paper, we consider this risk in the field of services which support transactions with electronic money. Specifically, we apply a tool for predictive security analysis at runtime which observes process behavior with respect to transactions within a money transfer service and tries to match it with expected behavior given by a process model. We analyze deviations from the given behavior specification for anomalies that indicate a possible misuse of the service related to money laundering activities. We evaluate the applicability of the proposed approach and provide measurements on computational and recognition performance of the tool - Predictive Security Analyser - produced using real operational and simulated logs. The goal of the experiments is to detect misuse patterns reflecting a given money laundering scheme in synthetic process behavior based on properties captured from real world transaction events.
ER  - 

TY  - CONF
JO  - Service Oriented System Engineering (SOSE), 2014 IEEE 8th International Symposium on
TI  - Multi-tenancy in Cloud Computing
T2  - Service Oriented System Engineering (SOSE), 2014 IEEE 8th International Symposium on
IS  - 
SN  - 
VO  - 
SP  - 344
EP  - 351
AU  - Aljahdali, H.
AU  - Albatli, A.
AU  - Garraghan, P.
AU  - Townend, P.
AU  - Lau, L.
AU  - Jie Xu
Y1  - 7-11 April 2014
PY  - 2014
KW  - cloud computing
KW  - resource allocation
KW  - security of data
KW  - Google trace logs
KW  - attack model
KW  - attack surfaces
KW  - attack vectors
KW  - cloud computing
KW  - cloud security
KW  - information technology computational model
KW  - multitenancy situation
KW  - public clouds
KW  - resource sharing
KW  - suspicious behavior
KW  - threat model
KW  - Cloud computing
KW  - Computational modeling
KW  - Databases
KW  - Resource management
KW  - Security
KW  - Servers
KW  - Virtualization
KW  - Attack Models
KW  - Cloud Computing
KW  - Cloud Data
KW  - Multi-Tenancy
KW  - Security
VL  - 
JA  - Service Oriented System Engineering (SOSE), 2014 IEEE 8th International Symposium on
DO  - 10.1109/SOSE.2014.50
AB  - As Cloud Computing becomes the trend of information technology computational model, the Cloud security is becoming a major issue in adopting the Cloud where security is considered one of the most critical concerns for the large customers of Cloud (i.e. governments and enterprises). Such valid concern is mainly driven by the Multi-Tenancy situation which refers to resource sharing in Cloud Computing and its associated risks where confidentiality and/or integrity could be violated. As a result, security concerns may harness the advancement of Cloud Computing in the market. So, in order to propose effective security solutions and strategies a good knowledge of the current Cloud implementations and practices, especially the public Clouds, must be understood by professionals. Such understanding is needed in order to recognize attack vectors and attack surfaces. In this paper we will propose an attack model based on a threat model designed to take advantage of Multi-Tenancy situation only. Before that, a clear understanding of Multi-Tenancy, its origin and its benefits will be demonstrated. Also, a novel way on how to approach Multi-Tenancy will be illustrated. Finally, we will try to sense any suspicious behavior that may indicate to a possible attack where we will try to recognize the proposed attack model empirically from Google trace logs. Google trace logs are a 29-day worth of data released by Google. The data set was utilized in reliability and power consumption studies, but not been utilized in any security study to the extent of our knowledge.
ER  - 

TY  - CONF
JO  - Information Management and Engineering (ICIME), 2010 The 2nd IEEE International Conference on
TI  - An intrusion detection model for satellite network
T2  - Information Management and Engineering (ICIME), 2010 The 2nd IEEE International Conference on
IS  - 
SN  - 
VO  - 
SP  - 167
EP  - 170
AU  - Zhang Wen-bo
AU  - Sun Peigen
AU  - Liu Zhi-guo
AU  - Xu Haifeng
Y1  - 16-18 April 2010
PY  - 2010
KW  - satellite communication
KW  - security of data
KW  - telecommunication computing
KW  - telecommunication security
KW  - cooperation mechanism
KW  - ground network
KW  - intrusion detecting agent
KW  - lamination distributed intrusion detection model
KW  - satellite network
KW  - satellite nodes
KW  - Authentication
KW  - Data security
KW  - Identity-based encryption
KW  - Information science
KW  - Information security
KW  - Intrusion detection
KW  - Lamination
KW  - Multiprocessor interconnection networks
KW  - Satellites
KW  - Sun
KW  - cooperation
KW  - intrusion detection model
KW  - satellite network
KW  - security field
VL  - 
JA  - Information Management and Engineering (ICIME), 2010 The 2nd IEEE International Conference on
DO  - 10.1109/ICIME.2010.5477489
AB  - The current intrusion detection technology based on the ground network is not adapted in the satellite network because of its motion. Analyzing the function characteristic of the satellite nodes, this paper builds a security domain based on the login and log-out mechanism, then a lamination distributed intrusion detection model of satellite network is proposed and a cooperation mechanism of intrusion detecting agent on satellite working inside and between the security domains is designed. The result of simulation proves that the intrusion detection model and cooperation mechanism are adapted in the satellites network.
ER  - 

TY  - CONF
JO  - Power and Energy (PECon), 2010 IEEE International Conference on
TI  - A retrofit network transaction data logger and intrusion detection system for transmission and distribution substations
T2  - Power and Energy (PECon), 2010 IEEE International Conference on
IS  - 
SN  - 
VO  - 
SP  - 958
EP  - 963
AU  - Morris, T.
AU  - Pavurapu, K.
Y1  - Nov. 29 2010-Dec. 1 2010
PY  - 2010
KW  - SCADA systems
KW  - data loggers
KW  - security of data
KW  - substations
KW  - SCADA
KW  - distribution substation
KW  - intrusion detection system
KW  - network traffic
KW  - phasor measurement unit
KW  - remote terminal unit
KW  - retrofit network transaction data logger
KW  - serial communication
KW  - substation
KW  - Data Logging
KW  - Intrusion Detection
KW  - Process Control System Cyber Security
KW  - SCADA Cyber Security
VL  - 
JA  - Power and Energy (PECon), 2010 IEEE International Conference on
DO  - 10.1109/PECON.2010.5697717
AB  - SCADA systems are widely used in electricity generation, distribution, and transmission control systems. NERC CIP 002-009 requires bulk electric providers to secure critical cyber assets electronically and physically. Transmission and distribution substations contain cyber critical assets including remote terminal units (RTU), intelligent electronic devices (IED) such as relays, phasor measurement units (PMU) and phasor data concentrators (PDC). Substation critical cyber assets are isolated in electronic security perimeters using firewalls. In this paper a retrofit data logger solution for serial communication based MODBUS and DNP3 network appliances is offered. The retrofit data logger allows existing control systems to be updated to log network transactions in support of substation based network intrusion detection. Substation based intrusion detection supports a defense in depth approach to cyber security in which multiple overlapping layers of security are used to protect critical cyber assets. The data logger is an embedded bump-in-the-wire retrofit device which captures, time stamps, cryptographically signs, encrypts, and store network traffic. Network traffic is forwarded to the existing network. Additionally, the data logger architecture supports use of signature based and statistics based intrusion detection algorithms at the network appliance edge.
ER  - 

TY  - CONF
JO  - Cognitive Methods in Situation Awareness and Decision Support (CogSIMA), 2012 IEEE International Multi-Disciplinary Conference on
TI  - Team-based cyber defense analysis
T2  - Cognitive Methods in Situation Awareness and Decision Support (CogSIMA), 2012 IEEE International Multi-Disciplinary Conference on
IS  - 
SN  - 
VO  - 
SP  - 218
EP  - 221
AU  - Champion, M.A.
AU  - Rajivan, P.
AU  - Cooke, N.J.
AU  - Jariwala, S.
Y1  - 6-8 March 2012
PY  - 2012
KW  - cognitive systems
KW  - security of data
KW  - task analysis
KW  - CTA
KW  - CyberCog
KW  - cognitive task analysis
KW  - cyber defense simulation environment
KW  - cyber security
KW  - intrusion alerts
KW  - network logs
KW  - situation awareness
KW  - team-based cyber defense analysis
KW  - Computer security
KW  - Computers
KW  - Educational institutions
KW  - Organizations
KW  - Software
KW  - Training
KW  - Cognitive Task Analysis
KW  - Cyber Security
KW  - Situation Awareness
KW  - Team Cyber Situation Awareness
KW  - Team Situation Awareness
VL  - 
JA  - Cognitive Methods in Situation Awareness and Decision Support (CogSIMA), 2012 IEEE International Multi-Disciplinary Conference on
DO  - 10.1109/CogSIMA.2012.6188386
AB  - Situation awareness (SA) in the cyber security domain is particularly relevant to teams of security analysts who are responsible for detecting cyber threats by perusing continual floods of data such as intrusion alerts and network logs. The challenges that analysts face are matched by those of researchers attempting to understand, measure, and impact SA in the cyber arena. The ground truth is not available except in simulated cyber situations. In this paper we outline a cognitive task analysis (CTA) focused on teams of analysts and the subsequent preliminary study conducted using a cyber defense simulation environment, CyberCog, built based on the CTA findings. Results from the CTA suggest three areas of fundamental challenge surrounding security analysts: team structure, communication, and information overload. These challenges could be associated to maladies such as cognitive tunneling and increased false alarms. These results are mirrored in the CyberCog pilot simulation study.
ER  - 

TY  - JOUR
JO  - Information Forensics and Security, IEEE Transactions on
TI  - Trail of Bytes: New Techniques for Supporting Data Provenance and Limiting Privacy Breaches
T2  - Information Forensics and Security, IEEE Transactions on
IS  - 6
SN  - 1556-6013
VO  - 7
SP  - 1876
EP  - 1889
AU  - Krishnan, S.
AU  - Snow, K.Z.
AU  - Monrose, F.
Y1  - Dec. 2012
PY  - 2012
KW  - computer forensics
KW  - data privacy
KW  - computer systems
KW  - data access
KW  - data exfiltration attempts
KW  - data provenance
KW  - forensic analysis
KW  - forensic layer records
KW  - forensic platform
KW  - hypervisor
KW  - multiple disks
KW  - privacy breaches
KW  - tracking mechanism
KW  - version-based audit log
KW  - virtualized environment
KW  - Couplings
KW  - Forensics
KW  - Monitoring
KW  - Semantics
KW  - Virtual machine monitors
KW  - Virtual machining
KW  - Computer security
KW  - checkpointing
KW  - information security
KW  - intrusion detection
KW  - operating systems
KW  - system recovery
KW  - virtual machine monitors
VL  - 7
JA  - Information Forensics and Security, IEEE Transactions on
DO  - 10.1109/TIFS.2012.2210217
AB  - Forensic analysis of computer systems requires that one first identify suspicious objects or events, and then examine them in enough detail to form a hypothesis as to their cause and effect. Sadly, while our ability to gather vast amounts of data has improved significantly over the past two decades, it is all too often the case that we lack detailed information just when we need it the most. In this paper, we attempt to improve on the state of the art by providing a forensic platform that transparently monitors and records data access events within a virtualized environment using only the abstractions exposed by the hypervisor. Our approach monitors accesses to objects on disk and follows the causal chain of these accesses across processes, even after the objects are copied into memory. Our forensic layer records these transactions in a tamper evident version-based audit log that allows for faithful, and efficient, reconstruction of the recorded events and the changes they induced. To demonstrate the utility of our approach, we provide an extensive empirical evaluation, including a real-world case study demonstrating how our platform can be used to reconstruct valuable information about the what, when, and how, after a compromise has been detected. We also extend our earlier work by providing a tracking mechanism that can monitor data exfiltration attempts across multiple disks and also block attempts to copy data over the network.
ER  - 

TY  - CONF
JO  - Open Systems (ICOS), 2012 IEEE Conference on
TI  - Real time implementation of intrusion detection system with reconfigurable architecture
T2  - Open Systems (ICOS), 2012 IEEE Conference on
IS  - 
SN  - 
VO  - 
SP  - 1
EP  - 5
AU  - Moghaddam, A.
Y1  - 21-24 Oct. 2012
PY  - 2012
KW  - computer network security
KW  - field programmable gate arrays
KW  - parallel processing
KW  - public domain software
KW  - reconfigurable architectures
KW  - security of data
KW  - system monitoring
KW  - KMP algorithm
KW  - Snort rules
KW  - VIRTEX4
KW  - computer network
KW  - computer security policies
KW  - computer system
KW  - events monitoring process
KW  - logs information
KW  - parallel architecture structure
KW  - real time intrusion detection system
KW  - reconfigurable architecture
KW  - reconfigurable hardware
KW  - standard security practices
KW  - Field programmable gate arrays
KW  - Hardware
KW  - Intrusion detection
KW  - Pattern matching
KW  - Read only memory
KW  - Real-time systems
KW  - Search engines
KW  - FPGA
KW  - Intrusion Detection
KW  - KMP
KW  - Network Security
KW  - Pattern Matching
VL  - 
JA  - Open Systems (ICOS), 2012 IEEE Conference on
DO  - 10.1109/ICOS.2012.6417634
AB  - Intrusion detection is the process of monitoring the events occurring in a computer system or network and analyzing them for signs of possible incidents, which are violations of computer security policies, or standard security practices. Intrusion detection system identifies possible incidents, logs information and provides report about them. In this article a real time intrusion detection system using SNORT rules and KMP algorithm is implemented in reconfigurable hardware. The parallel structure of this architecture let us to achieve a high real time performance at rate 100Mbps as it is shown by simulation and synthesis results on VIRTEX4.
ER  - 

TY  - JOUR
JO  - Dependable and Secure Computing, IEEE Transactions on
TI  - Empirical Analysis of System-Level Vulnerability Metrics through Actual Attacks
T2  - Dependable and Secure Computing, IEEE Transactions on
IS  - 6
SN  - 1545-5971
VO  - 9
SP  - 825
EP  - 837
AU  - Holm, H.
AU  - Ekstedt, M.
AU  - Andersson, D.
Y1  - Nov.-Dec. 2012
PY  - 2012
KW  - security of data
KW  - statistical analysis
KW  - CVSS data
KW  - NVD
KW  - US National Vulnerability Database
KW  - attacker logs
KW  - common vulnerability scoring system
KW  - computer systems
KW  - empirical analysis
KW  - international cyber defense exercise
KW  - network traffic logs
KW  - network vulnerabilities
KW  - observer logs
KW  - security vulnerabilities
KW  - statistical analysis
KW  - system-level vulnerability metrics
KW  - time-to-compromise
KW  - Authorization
KW  - Computational modeling
KW  - Computer crime
KW  - Mathematical model
KW  - Network security
KW  - Risk management
KW  - Telecommunication network management
KW  - Network-level security and protection
KW  - network management
KW  - phreaking)
KW  - risk management
KW  - unauthorized access (hacking
VL  - 9
JA  - Dependable and Secure Computing, IEEE Transactions on
DO  - 10.1109/TDSC.2012.66
AB  - The Common Vulnerability Scoring System (CVSS) is a widely used and well-established standard for classifying the severity of security vulnerabilities. For instance, all vulnerabilities in the US National Vulnerability Database (NVD) are scored according to this method. As computer systems typically have multiple vulnerabilities, it is often desirable to aggregate the score of individual vulnerabilities to a system level. Several such metrics have been proposed, but their quality has not been studied. This paper presents a statistical analysis of how 18 security estimation metrics based on CVSS data correlate with the time-to-compromise of 34 successful attacks. The empirical data originates from an international cyber defense exercise involving over 100 participants and were collected by studying network traffic logs, attacker logs, observer logs, and network vulnerabilities. The results suggest that security modeling with CVSS data alone does not accurately portray the time-to-compromise of a system. However, results also show that metrics employing more CVSS data are more correlated with time-to-compromise. As a consequence, models that only use the weakest link (most severe vulnerability) to compose a metric are less promising than those that consider all vulnerabilities.
ER  - 

TY  - CONF
JO  - Innovative Mobile and Internet Services in Ubiquitous Computing (IMIS), 2012 Sixth International Conference on
TI  - Lazy Monitoring for Distributed Computing Environments
T2  - Innovative Mobile and Internet Services in Ubiquitous Computing (IMIS), 2012 Sixth International Conference on
IS  - 
SN  - 
VO  - 
SP  - 259
EP  - 265
AU  - Caravagna, G.
AU  - Costa, G.
AU  - Wiegand, L.
AU  - Pardini, G.
Y1  - 4-6 July 2012
PY  - 2012
KW  - Web services
KW  - application program interfaces
KW  - security of data
KW  - system monitoring
KW  - OSGi bundles
KW  - Web services
KW  - commons logging package
KW  - distributed computing
KW  - lazy controllers
KW  - lazy monitoring
KW  - logging API
KW  - security state
KW  - target system
KW  - Automata
KW  - Delay
KW  - Medical services
KW  - Monitoring
KW  - Prototypes
KW  - Security
KW  - Standards
KW  - Security monitoring
KW  - distributed computing
KW  - mobile code
VL  - 
JA  - Innovative Mobile and Internet Services in Ubiquitous Computing (IMIS), 2012 Sixth International Conference on
DO  - 10.1109/IMIS.2012.186
AB  - Lazy controllers are a class of execution monitors that do not continuously observe the behaviour of their target. Monitors are activated and deactivated according to a scheduling strategy. When a lazy controller is activated, it checks the current security state and, in case of a violation, terminates the execution. Instead, if the current execution trace is safe, the monitor is suspended and its activation is scheduled again. The inactivity period is computed by considering the risk that, from the current state, the target can reach a faulty configuration. This behaviour is particularly interesting for systems which is difficult to monitor with standard solutions, e.g., web services. In this paper we present a prototype using existing logging API, i.e., the Commons Logging Package, for remotely watching the execution of OSGi bundles. We claim that our solution can efficiently follow the target system keeping under control the delay in detecting violations. Also, as we use standard OSGi platform and facilities, we show that our monitors can run under very realistic assumptions in the context of web services.
ER  - 

TY  - JOUR
JO  - Software, IEEE
TI  - What Microsoft's identity metasystem means to developers
T2  - Software, IEEE
IS  - 1
SN  - 0740-7459
VO  - 23
SP  - 108
EP  - 111
AU  - McLaughlin, L.
Y1  - Jan.-Feb. 2006
PY  - 2006
KW  - open systems
KW  - risk analysis
KW  - security of data
KW  - software houses
KW  - software maintenance
KW  - Microsoft
KW  - identity metasystem
KW  - identity technology
KW  - interoperability
KW  - ongoing security risk analysis
KW  - software maintenance
KW  - GOSCON
KW  - IT
KW  - InfoCard
KW  - Internet
KW  - Kerberos
KW  - Liberty Alliance
KW  - SAML
KW  - Security Assertion Markup Language
KW  - Sxip Identity
KW  - X.509
KW  - government
KW  - identity metasystem
KW  - open source
KW  - security
VL  - 23
JA  - Software, IEEE
DO  - 10.1109/MS.2006.18
AB  - Every day we use a bevy of identity technologies to prove who we are to various systems, whether we are logging on to a virtual private network or buying from an online store. For users, this means an increasing number of passwords and routines, leading to security fatigue and occasional confusion. For developers, it means working with a hodgepodge of different identity technologies - a situation that doesn't help with maintenance, ongoing security risk analysis, or innovation. Microsoft's proposed answer to this problem is an identity metasystem - a framework that lets varying identity technologies communicate and interoperate using common standards
ER  - 

TY  - CONF
JO  - Reliable Distributed Systems (SRDS), 2011 30th IEEE Symposium on
TI  - Identifying Compromised Users in Shared Computing Infrastructures: A Data-Driven Bayesian Network Approach
T2  - Reliable Distributed Systems (SRDS), 2011 30th IEEE Symposium on
IS  - 
SN  - 1060-9857
VO  - 
SP  - 127
EP  - 136
AU  - Pecchia, A.
AU  - Sharma, A.
AU  - Kalbarczyk, Z.
AU  - Cotroneo, D.
AU  - Iyer, R.K.
Y1  - 4-7 Oct. 2011
PY  - 2011
KW  - belief networks
KW  - security of data
KW  - NCSA
KW  - authentication protocols
KW  - data driven Bayesian network approach
KW  - national center for supercomputing applications
KW  - security incidents
KW  - shared computing infrastructures
KW  - stolen credentials
KW  - storage capabilities
KW  - Authentication
KW  - Bayesian methods
KW  - IP networks
KW  - Monitoring
KW  - Protocols
KW  - Vectors
KW  - Bayesian network
KW  - correlation
KW  - credential stealing
KW  - intrusion detection
KW  - security
VL  - 
JA  - Reliable Distributed Systems (SRDS), 2011 30th IEEE Symposium on
DO  - 10.1109/SRDS.2011.24
AB  - The growing demand for processing and storage capabilities has led to the deployment of high-performance computing infrastructures. Users log into the computing infrastructure remotely, by providing their credentials (e.g., username and password), through the public network and using well-established authentication protocols, e.g., SSH. However, user credentials can be stolen and an attacker (using a stolen credential) can masquerade as the legitimate user and penetrate the system as an insider. This paper deals with security incidents initiated by using stolen credentials and occurred during the last three years at the National Center for Supercomputing Applications (NCSA) at the University of Illinois. We analyze the key characteristics of the security data produced by the monitoring tools during the incidents and use a Bayesian network approach to correlate (i) data provided by different security tools (e.g., IDS and Net Flows) and (ii) information related to the users' profiles to identify compromised users, i.e., the users whose credentials have been stolen. The technique is validated with the real incident data. The experimental results demonstrate that the proposed approach is effective in detecting compromised users, while allows eliminating around 80% of false positives (i.e., not compromised user being declared compromised).
ER  - 

TY  - CONF
JO  - Cluster Computing and the Grid, 2008. CCGRID '08. 8th IEEE International Symposium on
TI  - Publication and Protection of Sensitive Site Information in a Grid Infrastructure
T2  - Cluster Computing and the Grid, 2008. CCGRID '08. 8th IEEE International Symposium on
IS  - 
SN  - 
VO  - 
SP  - 639
EP  - 644
AU  - Cholia, S.
AU  - Porter, R.J.
Y1  - 19-22 May 2008
PY  - 2008
KW  - grid computing
KW  - groupware
KW  - resource allocation
KW  - scheduling
KW  - scientific information systems
KW  - security of data
KW  - grid infrastructure
KW  - information protection
KW  - open science grid
KW  - private user information
KW  - resource scheduling
KW  - resource selection
KW  - resource-providing site
KW  - sensitive site information publication
KW  - site verification data
KW  - virtual organization
KW  - Collaboration
KW  - Data analysis
KW  - Data security
KW  - Grid computing
KW  - Information analysis
KW  - Information security
KW  - Laboratories
KW  - Monitoring
KW  - Protection
KW  - Scheduling
KW  - Computer Security
KW  - Grid Computing
KW  - Privacy
KW  - Site Security Monitoring
VL  - 
JA  - Cluster Computing and the Grid, 2008. CCGRID '08. 8th IEEE International Symposium on
DO  - 10.1109/CCGRID.2008.86
AB  - In order to create a successful grid infrastructure, sites and resource providers must be able to publish information about their underlying resources and services. This information makes it easier for users and virtual organizations to make intelligent decisions about resource selection and scheduling, and can be used by the grid infrastructure for accounting and troubleshooting services. However, such an outbound stream may include data deemed sensitive by a resource-providing site, exposing potential security vulnerabilities or private user information to the world at large, including malicious entities. This study analyzes the various vectors of information being published from sites to grid infrastructures. In particular, it examines the data being published to, and collected by the Open Science Grid, including resource selection, monitoring, accounting, troubleshooting, logging and site verification data. We analyze the risks and potential threat models posed by the publication and collection of such data. We also offer some recommendations and best practices for sites and grid infrastructures to manage and protect sensitive data.
ER  - 

TY  - CONF
JO  - Advanced Informatics: Concept, Theory and Application (ICAICTA), 2014 International Conference of
TI  - Anomaly-based intrusion detection and prevention system on website usage using rule-growth sequential pattern analysis: Case study: Statistics of Indonesia (BPS) website
T2  - Advanced Informatics: Concept, Theory and Application (ICAICTA), 2014 International Conference of
IS  - 
SN  - 
VO  - 
SP  - 203
EP  - 208
AU  - Trio Pramono, Y.W.
AU  - Suhardi
Y1  - 20-21 Aug. 2014
PY  - 2014
KW  - Web sites
KW  - data mining
KW  - security of data
KW  - Indonesia Website
KW  - Website security protection mechanism
KW  - Website usage
KW  - anomaly-based intrusion detection system
KW  - intrusion prevention system
KW  - log data mining
KW  - rule-growth sequential pattern analysis
KW  - Data mining
KW  - Databases
KW  - IP networks
KW  - Informatics
KW  - Intrusion detection
KW  - Uniform resource locators
KW  - intrusion detection
KW  - sequential pattern
KW  - user behavior
KW  - website security
VL  - 
JA  - Advanced Informatics: Concept, Theory and Application (ICAICTA), 2014 International Conference of
DO  - 10.1109/ICAICTA.2014.7005941
AB  - Websites are now widely used by many public and private sectors known to be the most popular system that is able to reach many users in many platforms through internet. Since websites are generally placed as a public domain, there is a large chance of intruders to compromise the system at any time. C-panel raw access log data nowadays widely used by many security tools to analysis the user behaviors regarding to the activities on the web, however they commonly provide the descriptive analysis, rather than prescriptive analysis for prediction and estimation. In this paper, we introduce the solution about the website security protection mechanism by investigating the user behaviors using sequential rule-pattern analysis. Rule-patterns of user behaviors are created by mining the log data with sequential pattern analysis in near real time. Using anomaly-based intrusion detection and prevention system approach, our proposed system then can predict the user behaviors activities whether identified as baseline patterns or malicious patterns.
ER  - 

TY  - JOUR
JO  - Information Forensics and Security, IEEE Transactions on
TI  - Database Access Pattern Protection Without Full-Shuffles
T2  - Information Forensics and Security, IEEE Transactions on
IS  - 1
SN  - 1556-6013
VO  - 6
SP  - 189
EP  - 201
AU  - Xuhua Ding
AU  - Yanjiang Yang
AU  - Deng, R.
Y1  - March 2011
PY  - 2011
KW  - computational complexity
KW  - data privacy
KW  - database management systems
KW  - query processing
KW  - security of data
KW  - PIR scheme
KW  - computational complexity
KW  - database access pattern generation
KW  - database access pattern protection
KW  - database outsourcing
KW  - online computation cost
KW  - privacy protection
KW  - private information retrieval
KW  - query execution
KW  - secure storage
KW  - Database
KW  - data privacy
KW  - information security
VL  - 6
JA  - Information Forensics and Security, IEEE Transactions on
DO  - 10.1109/TIFS.2010.2101062
AB  - Privacy protection is one of the fundamental security requirements for database outsourcing. A major threat is information leakage from database access patterns generated by query executions. The standard private information retrieval (PIR) schemes, which are widely regarded as theoretical solutions, entail <i>O</i>(<i>n</i>) computational overhead per query for a database with <i>n</i> items. Recent works propose to protect access patterns by introducing a trusted component with constant storage size. The resulting privacy assurance is as strong as PIR, though with <i>O</i>(1) online computation cost, they still have <i>O</i>(<i>n</i>) amortized cost per query due to periodically full database shuffles. In this paper, we design a novel scheme in the same model with provable security, which only shuffles a portion of the database. The amortized server computational complexity is reduced to <i>O</i>(&#x221A;{<i>n</i>log<i>n</i>/<i>k</i>}). With a secure storage storing thousands of items, our scheme can protect the access pattern privacy of databases of billions of entries, at a lower cost than those using ORAM-based poly-logarithm algorithms.
ER  - 

TY  - CONF
JO  - Communication Systems and Network Technologies (CSNT), 2013 International Conference on
TI  - Ontology for Detection of Web Attacks
T2  - Communication Systems and Network Technologies (CSNT), 2013 International Conference on
IS  - 
SN  - 
VO  - 
SP  - 612
EP  - 615
AU  - Khairkar, A.D.
AU  - Kshirsagar, D.D.
AU  - Kumar, S.
Y1  - 6-8 April 2013
PY  - 2013
KW  - Internet
KW  - ontologies (artificial intelligence)
KW  - security of data
KW  - semantic Web
KW  - Web application
KW  - Web attacks
KW  - application layer
KW  - computer attacks
KW  - domain knowledge
KW  - intrusion detection system
KW  - legacy data
KW  - malicious activity detection
KW  - ontology concepts
KW  - security issues
KW  - semantic Web
KW  - Conferences
KW  - Data mining
KW  - Feature extraction
KW  - Genetic algorithms
KW  - Intrusion detection
KW  - Ontologies
KW  - Attacks
KW  - Intrusion Detection System
KW  - Ontology security
KW  - Semantic security
KW  - Web attacks
VL  - 
JA  - Communication Systems and Network Technologies (CSNT), 2013 International Conference on
DO  - 10.1109/CSNT.2013.131
AB  - Intrusion Detection System (IDS) must reliably detect malicious activity. The expansion of web application also exponentially increases cyber threats. Current survey shows that application layer is more vulnerable to web attacks. There are more than 75% of attacks are deployed at application layer and out of that 90% are vulnerable to attacks. In this paper, we address issues of existing IDS i.e. low false positive rate, low false negative rate and data overload. We discuss about use of semantic web in the Intrusion Detection Systems. This article presents a proposition of using Semantic Web and Ontology concepts to define an approach to analyze Security logs with the goal to identify possible security issues. It extracts semantic relations between computer attacks and intrusions in an Intrusion Detection System. Ontology provides to enable, reuse of domain knowledge and it is also easier to understand and update legacy data.
ER  - 

TY  - CONF
JO  - Availability, Reliability and Security (ARES), 2014 Ninth International Conference on
TI  - No Smurfs: Revealing Fraud Chains in Mobile Money Transfers
T2  - Availability, Reliability and Security (ARES), 2014 Ninth International Conference on
IS  - 
SN  - 
VO  - 
SP  - 11
EP  - 20
AU  - Zhdanova, M.
AU  - Repp, J.
AU  - Rieke, R.
AU  - Gaber, C.
AU  - Hemery, B.
Y1  - 8-12 Sept. 2014
PY  - 2014
KW  - banking
KW  - electronic money
KW  - fraud
KW  - learning (artificial intelligence)
KW  - mobile computing
KW  - security of data
KW  - ML technique
KW  - MMT services
KW  - PSA@R
KW  - banking relationships
KW  - electronic money
KW  - event-driven process analysis
KW  - financial inclusion
KW  - fraud chain detection
KW  - fraudster identification
KW  - fund microstructuring
KW  - fund transfers
KW  - mobile devices
KW  - mobile money transfer services
KW  - model-based approach
KW  - money laundering detection
KW  - predictive security analysis at runtime
KW  - smurfing
KW  - Automata
KW  - Computational modeling
KW  - Databases
KW  - Mobile communication
KW  - Monitoring
KW  - Runtime
KW  - Security
KW  - fraud detection
KW  - machine learning
KW  - mobile money transfer systems
KW  - money laundering
KW  - predictive security analysis
KW  - process behavior analysis
VL  - 
JA  - Availability, Reliability and Security (ARES), 2014 Ninth International Conference on
DO  - 10.1109/ARES.2014.10
AB  - Mobile Money Transfer (MMT) services provided by mobile network operators enable funds transfers made on mobile devices of end-users, using digital equivalent of cash (electronic money) without any bank accounts involved. MMT simplifies banking relationships and facilitates financial inclusion, and, therefore, is rapidly expanding all around the world, especially in developing countries. MMT systems are subject to the same controls as those required for financial institutions, including the detection of Money Laundering (ML) - a source of concern for MMT service providers. In this paper we focus on an often practiced ML technique known as micro-structuring of funds or smurfing and introduce a new method for detection of fraud chains in MMT systems. Whereas classical detection methods are based on machine learning and data mining, this work builds on Predictive Security Analysis at Runtime (PSA@R), a model-based approach for event-driven process analysis. We provide an extension to PSA@R which allows us to identify fraudsters in an MMT service monitoring network behavior of its end-users. We evaluate our method on simulated transaction logs, containing approximately 460,000 transactions for 10,000 end-users, and compare it with classical fraud detection approaches. With 99.81% precision and 90.18% recall, we achieve better recognition performance in comparison with the state of the art.
ER  - 

TY  - CONF
JO  - Pervasive Computing (ICPC), 2015 International Conference on
TI  - Design consideration of Network Intrusion detection system using Hadoop and GPGPU
T2  - Pervasive Computing (ICPC), 2015 International Conference on
IS  - 
SN  - 
VO  - 
SP  - 1
EP  - 6
AU  - Bandre, S.R.
AU  - Nandimath, J.N.
Y1  - 8-10 Jan. 2015
PY  - 2015
KW  - data analysis
KW  - graphics processing units
KW  - parallel architectures
KW  - parallel programming
KW  - security of data
KW  - CUDA
KW  - Compute Unified Device Architecture
KW  - Flume
KW  - GPGPU
KW  - HBase
KW  - Hadoop framework
KW  - Hive
KW  - NIDS
KW  - NVidia
KW  - PF-ICF
KW  - Pig
KW  - general purpose graphical processing unit
KW  - intrusion pattern analytics
KW  - log files
KW  - network intrusion detection system
KW  - network security policy
KW  - network traffic
KW  - parallel programming model
KW  - pattern frequency inverse cluster frequency
KW  - score-weight approach
KW  - streaming data handling
KW  - Algorithm design and analysis
KW  - Ecosystems
KW  - Graphics processing units
KW  - Intrusion detection
KW  - Servers
KW  - Telecommunication traffic
KW  - CUDA
KW  - GPGPU
KW  - Hadoop
KW  - NIDS
KW  - Network Security
VL  - 
JA  - Pervasive Computing (ICPC), 2015 International Conference on
DO  - 10.1109/PERVASIVE.2015.7087201
AB  - Modern computing has primarily shifted towards the distributed environment using commodity resources which results in increase in data and its security concern. This paper deals with design consideration of Network Intrusion Detection System (NIDS) based on the Hadoop framework and acceleration of its performance by using General Purpose Graphical Processing Unit (GPGPU). The large volume of data from an entire infrastructure is assigned to Hadoop framework and intrusion detections are carried out on GPGPU. This approach improves NIDS performance and it enables to provide quick response to various attacks on the network. In order to perform the general purposed computation on the GPU, NVidia provides the Compute Unified Device Architecture (CUDA) which is a parallel programming model which performs high-end complex operations using GPU. In order to process large volumes of data in distributed networks, Hadoop framework has to configure with various supporting ecosystems like Flume, Pig, Hive and HBase. These ecosystems enable the Hadoop framework to handle streaming data on the network and large log files on servers. The proposed system is capable of performing analytics over intrusion pattern and their behavior on the network, which helps a network administrator to configure network security policy and settings. Analytics over intrusion is done by using a Score-Weight approach called as Pattern Frequency Inverse Cluster Frequency (PF-ICF). The design consideration of accelerated NIDS is a solution towards the performance issues of various NIDS that faces due to the large volumes of the network traffic.
ER  - 

TY  - CONF
JO  - Natural Computation (ICNC), 2010 Sixth International Conference on
TI  - The out-of-band virtualization model of network storage based on trusted computing
T2  - Natural Computation (ICNC), 2010 Sixth International Conference on
IS  - 
SN  - 
VO  - 8
SP  - 4354
EP  - 4357
AU  - Zhang Qiang
AU  - Cui Dong
AU  - Wu Yunlong
AU  - Dang Zhuang
Y1  - 10-12 Aug. 2010
PY  - 2010
KW  - security of data
KW  - storage management
KW  - network storage
KW  - out-of-band virtualization
KW  - secure system
KW  - security problems
KW  - storage systems
KW  - storage virtualization
KW  - trusted computing
KW  - Computational modeling
KW  - Computers
KW  - Information security
KW  - Risk management
KW  - Servers
KW  - Software
KW  - security
KW  - storage
KW  - trusted computing
KW  - virtualization
VL  - 8
JA  - Natural Computation (ICNC), 2010 Sixth International Conference on
DO  - 10.1109/ICNC.2010.5583590
AB  - The technology of storage virtualization is widely used all around the world. The out-of-band network-based virtualization is an essential tendency of storage systems. According to the analysis, we find that there are some security problems in out-of-band network-based virtualization, for example the attacks to one server will do harm to the whole storage network. So we propose a secure system of storage virtualization based on trusted computing which ensures the whole storage network trusted with TPM. It introduces the risk evaluation mechanism to measure the integrity of servers. And then all the behaviors which operate storage network can be trusted. Meanwhile, it builds the trusted booting process in the virtualization controller and uses the log mechanism in servers. So the system can measure the integrity of servers through log reports.
ER  - 

TY  - CONF
JO  - Computing, Communications and Networking Technologies (ICCCNT),2013 Fourth International Conference on
TI  - An approach to detect malicious activities in SCADA systems
T2  - Computing, Communications and Networking Technologies (ICCCNT),2013 Fourth International Conference on
IS  - 
SN  - 
VO  - 
SP  - 1
EP  - 7
AU  - Pramod, T.C.
AU  - Sunitha, N.R.
Y1  - 4-6 July 2013
PY  - 2013
KW  - SCADA systems
KW  - control engineering computing
KW  - industrial control
KW  - industrial plants
KW  - production engineering computing
KW  - security of data
KW  - system monitoring
KW  - Log analysis approach
KW  - SCADA systems
KW  - cluster based architecture
KW  - cooperative monitoring
KW  - critical infrastructure
KW  - cyber attacks
KW  - human machine interface
KW  - industrial automation
KW  - industrial plant
KW  - malicious activity detection
KW  - malicious activity identification
KW  - node level
KW  - supervisory control and data acquisition system
KW  - Computer architecture
KW  - Jamming
KW  - Magnetic heads
KW  - Monitoring
KW  - SCADA systems
KW  - Security
KW  - Sensors
KW  - Aggregation
KW  - ICS-Industrial control system
KW  - Log Management
KW  - SCADA
KW  - attack
VL  - 
JA  - Computing, Communications and Networking Technologies (ICCCNT),2013 Fourth International Conference on
DO  - 10.1109/ICCCNT.2013.6726619
AB  - Supervisory Control and Data Acquisition System (SCADA) is an emerging application for industrial automation. It is being widely used in critical infrastructure for monitoring and controlling the activities. The collaborative environment and interconnectivity of SCADA system needs communications and transmission of sensed real time data like status of machines, breaks and leakages in the system across various devices in the industrial plant. Such real time data provoke security breaches to SCADA systems and results in compromise of availability, integrity, confidentiality and trust relationship between the devices of SCADA systems. As the numbers of deliberate cyber attacks on these systems are increasing, providing a scheme to identify malicious activities and defend the attacks; thereby create secure environment for SCADA systems is an essential task. By considering constraints and efficiency requirements for such networks, we are proposing a scheme that uses Log to identify some malicious activities through continuous monitoring. In Log, we have only prioritized some parameters that help us to detect some vulnerable activities and at node level by using cooperative monitoring the nodes itself takes care of some attacks. In this new approach Log analysis for the identification of malicious activities is made using cluster based architecture. This work also considers the constraints of the SCADA system thereby providing an elegant identification of malicious activities for the current SCADA system.
ER  - 

TY  - CONF
JO  - Intelligence and Security Informatics (ISI), 2013 IEEE International Conference on
TI  - Modeling and detecting anomalous topic access
T2  - Intelligence and Security Informatics (ISI), 2013 IEEE International Conference on
IS  - 
SN  - 
VO  - 
SP  - 100
EP  - 105
AU  - Gupta, S.
AU  - Hanson, C.
AU  - Gunter, C.A.
AU  - Frank, M.
AU  - Liebovitz, D.
AU  - Malin, B.
Y1  - 4-7 June 2013
PY  - 2013
KW  - data mining
KW  - feature extraction
KW  - health care
KW  - learning (artificial intelligence)
KW  - medical information systems
KW  - random processes
KW  - security of data
KW  - LDA
KW  - ROA model
KW  - RTA
KW  - RTAD
KW  - anomalous topic access detection
KW  - anomalous topic access modeling
KW  - anomalous user detection
KW  - anomaly detection framework
KW  - attacker characteristics
KW  - feature extraction
KW  - healthcare
KW  - hospital audit logs
KW  - information systems
KW  - insider threat detection
KW  - k-NN algorithm
KW  - k-nearest neighbor algorithm
KW  - latent Dirichlet allocation
KW  - machine learning
KW  - model formalization
KW  - outlier detection
KW  - random object access model
KW  - random record access
KW  - random topic access detection
KW  - random topic access model
KW  - random users
KW  - statistics
KW  - strategy development
KW  - topic summarization
KW  - user roles
KW  - workflow knowledge
KW  - Context
KW  - Hospitals
KW  - Information systems
KW  - Medical diagnostic imaging
KW  - Sociology
KW  - Statistics
KW  - Access Logs
KW  - Anomaly Detection
KW  - Data Mining
KW  - Electronic Health Records
KW  - Healthcare Security
KW  - Insider threats
VL  - 
JA  - Intelligence and Security Informatics (ISI), 2013 IEEE International Conference on
DO  - 10.1109/ISI.2013.6578795
AB  - There has been considerable success in developing strategies to detect insider threats in information systems based on what one might call the random object access model or ROA. This approach models illegitimate users as ones who randomly access records. The goal is to use statistics, machine learning, knowledge of workflows and other techniques to support an anomaly detection framework that finds such users. In this paper we introduce and study a random topic access model or RTA aimed at users whose access may be illegitimate but is not fully random because it is focused on common semantic themes. We argue that this model is appropriate for a meaningful range of attacks and develop a system based on topic summarization that is able to formalize the model and provide anomalous user detection effectively for it. To this end, we use healthcare as an example and propose a framework for evaluating the ability to recognize various types of random users called random topic access detection or RTAD. Specifically, we utilize a combination of Latent Dirichlet Allocation (LDA), for feature extraction, a k-nearest neighbor (k-NN) algorithm for outlier detection and evaluate the ability to identify different adversarial types. We validate the technique in the context of hospital audit logs where we show varying degrees of success based on user roles and the anticipated characteristics of attackers. In particular, it was found that RTAD exhibits strong performance for roles are described by a few topics, but weaker performance when users are more topic-agnostic.
ER  - 

TY  - CONF
JO  - Technologies and Applications of Artificial Intelligence (TAAI), 2013 Conference on
TI  - Frequent Pattern Based User Behavior Anomaly Detection for Cloud System
T2  - Technologies and Applications of Artificial Intelligence (TAAI), 2013 Conference on
IS  - 
SN  - 
VO  - 
SP  - 61
EP  - 66
AU  - Chien-Yi Chiu
AU  - Chi-Tien Yeh
AU  - Yuh-Jye Lee
Y1  - 6-8 Dec. 2013
PY  - 2013
KW  - cloud computing
KW  - data mining
KW  - sampling methods
KW  - security of data
KW  - virtual machines
KW  - activated system processes frequent patterns
KW  - cloud computing
KW  - cloud system
KW  - false-positive rate
KW  - frequent pattern mining techniques
KW  - malicious activities detection
KW  - malicious tools process log collection
KW  - normal users process log collection
KW  - random re-sampling techniques
KW  - user behavior anomaly detection
KW  - user behavior profiling
KW  - virtual machine
KW  - Cloud computing
KW  - Databases
KW  - Security
KW  - Testing
KW  - Training
KW  - Training data
KW  - Virtual machining
KW  - Data Mining
KW  - Information Security
KW  - Machine learning
VL  - 
JA  - Technologies and Applications of Artificial Intelligence (TAAI), 2013 Conference on
DO  - 10.1109/TAAI.2013.25
AB  - Cloud Computing is a hot topic in the global IT industry, which is considered as the main part of the network and computing service provider in recent years. Some security issues will be more threatening in cloud computing, such as account theft and insider threat. We propose a framework to utilize anomaly detection and random re-sampling techniques for profiling user's behaviors via the frequent patterns of activated system processes. By utilizing the user profiles learned from normal data, our method can detect malicious activities and discriminate suspicious activities from different users. We use virtual machine (VM) to collect process log of normal users and malicious tools. The collected data is used on verifying if our method can detect the malicious activities on the system. The results show that all the malicious activities are detected with less than 4.6% false-positive rate. We also collect real-world data for testing the ability of discriminating activities collected from different users. The results showed that the user profiles can averagely detect 86% suspicious behaviors from different users with less than 1% false positive rate.
ER  - 

TY  - CONF
JO  - Computational Science and Engineering, 2009. CSE '09. International Conference on
TI  - A Lightweight Architecture for Secure Two-Party Mobile Payment
T2  - Computational Science and Engineering, 2009. CSE '09. International Conference on
IS  - 
SN  - 
VO  - 2
SP  - 326
EP  - 333
AU  - Zhu, Y.
AU  - Rice, J.E.
Y1  - 29-31 Aug. 2009
PY  - 2009
KW  - Java
KW  - banking
KW  - mobile computing
KW  - public key cryptography
KW  - security of data
KW  - software architecture
KW  - Java ME
KW  - SA2pMP
KW  - banking transaction
KW  - lightweight architecture
KW  - mobile devices
KW  - mobile payment
KW  - money transfer
KW  - public key cryptography systems
KW  - secured architecture
KW  - symmetric key cryptography systems
KW  - wireless networks
KW  - Authentication
KW  - Banking
KW  - Communication system security
KW  - Computer architecture
KW  - Information security
KW  - Java
KW  - Mobile computing
KW  - Public key cryptography
KW  - Sun
KW  - Wireless networks
KW  - lightweight architecture
KW  - mobile payment
KW  - security
VL  - 2
JA  - Computational Science and Engineering, 2009. CSE '09. International Conference on
DO  - 10.1109/CSE.2009.364
AB  - The evolution of wireless networks and mobile devices has resulted in increased concerns about performance and security of mobile payment systems. In this paper we propose SA2pMP, a lightweight secured architecture for two-party mobile payments. SA2pMP employs a lightweight cryptography scheme that combines public key and symmetric key cryptography systems (ECDSA and AES), as well as a multi-factor authentication mechanism. These are coupled with a transaction log strategy to satisfy the properties of confidentiality, authentication, integrity and non-repudiation. We simulate SA2pMP in a context of money transfer banking transaction, on three different emulators:Sun Java Wireless Toolkit 2.5.2 for CLDC emulator, Sony Ericsson SDK 2.5.0.3 Z800 emulator, and Nokia S60 3rd Edition emulator. We also compare SA2pMP to some existing mobile payment platforms. The result of simulation and comparison proves that SA2pMP is a lightweight secured mechanism that is feasible and suitable for two-party mobile payment transactions, e.g. mobile banking, over Java ME enabled, resource-limited mobile devices.
ER  - 

TY  - CONF
JO  - Dependable Systems and Networks (DSN), 2012 42nd Annual IEEE/IFIP International Conference on
TI  - Confidentiality of event data in policy-based monitoring
T2  - Dependable Systems and Networks (DSN), 2012 42nd Annual IEEE/IFIP International Conference on
IS  - 
SN  - 1530-0889
VO  - 
SP  - 1
EP  - 12
AU  - Montanari, M.
AU  - Campbell, R.H.
Y1  - 25-28 June 2012
PY  - 2012
KW  - distributed processing
KW  - security of data
KW  - application logs
KW  - configuration data
KW  - distributed systems
KW  - event data confidentiality
KW  - policy-based security monitoring
KW  - potential attack detection
KW  - single zero-day vulnerability
KW  - topology information
KW  - Computers
KW  - Correlation
KW  - Monitoring
KW  - Organizations
KW  - Security
KW  - Servers
KW  - Software
KW  - confidentiality
KW  - distributed systems
KW  - monitoring
KW  - policy compliance
KW  - security
VL  - 
JA  - Dependable Systems and Networks (DSN), 2012 42nd Annual IEEE/IFIP International Conference on
DO  - 10.1109/DSN.2012.6263954
AB  - Monitoring systems observe important information that could be a valuable resource to malicious users: attackers can use the knowledge of topology information, application logs, or configuration data to target attacks and make them hard to detect. The increasing need for correlating information across distributed systems to better detect potential attacks and to meet regulatory requirements can potentially exacerbate the problem if the monitoring is centralized. A single zero-day vulnerability would permit an attacker to access all information. This paper introduces a novel algorithm for performing policy-based security monitoring. We use policies to distribute information across several hosts, so that any host compromise has limited impact on the confidentiality of the data about the overall system. Experiments show that our solution spreads information uniformly across distributed monitoring hosts and forces attackers to perform multiple actions to acquire important data.
ER  - 

TY  - CONF
JO  - Information Science and Engineering (ISISE), 2012 International Symposium on
TI  - 3D-IDS: IaaS User-oriented Intrusion Detection System
T2  - Information Science and Engineering (ISISE), 2012 International Symposium on
IS  - 
SN  - 2160-1283
VO  - 
SP  - 12
EP  - 15
AU  - Jie He
AU  - Chuan Tang
AU  - Yuexiang Yang
AU  - Yong Qiao
AU  - Chaobin Liu
Y1  - 14-16 Dec. 2012
PY  - 2012
KW  - cloud computing
KW  - security of data
KW  - virtual machines
KW  - virtualisation
KW  - 3D-IDS system
KW  - IaaS user-oriented intrusion detection system
KW  - cloud computing data center
KW  - computing resource utilization
KW  - host behavior
KW  - information collection
KW  - infrastructure-as-service
KW  - network behavior
KW  - security detection
KW  - server virtualization
KW  - virtual machines
KW  - Cloud Computing
KW  - IaaS
KW  - Intrusion Detection
KW  - Virtualization Security
VL  - 
JA  - Information Science and Engineering (ISISE), 2012 International Symposium on
DO  - 10.1109/ISISE.2012.12
AB  - Being the core technology of IaaS (Infrastructure as a Service) cloud computing data center, server virtualization can virtualize servers with surplus resources to multiple virtual machines with diverse configurations according to users' demands. This can significantly improve the utilization of computing resources. Traditional intrusion detection techniques are no longer suitable for security detection of IaaS cloud computing data centers any more due to the specificity of IaaS architecture. Furthermore, most current intrusion detection systems (IDS) for cloud environment are provider-oriented. It's hard for IaaS users to obtain the statuses of virtual machines rented by them. In this paper, we designed a three-dimensional intrusion detection system (3D-IDS). Through a distributed collection of information on virtual machines, such as system logs, host behaviors and network behaviors, our system can synthesize related information to supply detailed security statuses of each virtual machine for IaaS users.
ER  - 

TY  - CONF
JO  - Self-Adaptive and Self-Organizing Systems Workshops (SASOW), 2012 IEEE Sixth International Conference on
TI  - Hardware Support for Safety Interlocks and Introspection
T2  - Self-Adaptive and Self-Organizing Systems Workshops (SASOW), 2012 IEEE Sixth International Conference on
IS  - 
SN  - 
VO  - 
SP  - 1
EP  - 8
AU  - Dhawan, U.
AU  - Kwon, A.
AU  - Kadric, E.
AU  - Hritcu, C.
AU  - Pierce, B.C.
AU  - Smith, J.M.
AU  - DeHon, A.
AU  - Malecha, G.
AU  - Morrisett, G.
AU  - Knight, T.F.
AU  - Sutherland, A.
AU  - Hawkins, T.
AU  - Zyxnfryx, A.
AU  - Wittenberg, D.
AU  - Trei, P.
AU  - Ray, S.
AU  - Sullivan, G.
Y1  - 10-14 Sept. 2012
PY  - 2012
KW  - field programmable gate arrays
KW  - meta data
KW  - security of data
KW  - FPGA
KW  - authority-changing procedure call mechanism
KW  - concurrent check
KW  - field programmable gate array
KW  - fine-grained privilege separation
KW  - hardware introspection
KW  - hardware support
KW  - hardware type mechanism
KW  - latency requirement
KW  - logic requirement
KW  - memory requirement
KW  - pointer mechanism
KW  - processor cycle time
KW  - processor-supported authority mechanism
KW  - programmable metadata validation-and-propagation mechanism
KW  - safety hardware interlock
KW  - security check
KW  - semantic invariant
KW  - semiconductor technology
KW  - software handler
KW  - tag cache
KW  - Processor
KW  - complete mediation
KW  - hardware interlocks
KW  - least privilege
KW  - security
KW  - separation of privilege
VL  - 
JA  - Self-Adaptive and Self-Organizing Systems Workshops (SASOW), 2012 IEEE Sixth International Conference on
DO  - 10.1109/SASOW.2012.11
AB  - Hardware interlocks that enforce semantic invariants and allow fine-grained privilege separation can be built with reasonable costs given modern semiconductor technology. In the common error-free case, these mechanisms operate largely in parallel with the intended computation, monitoring the semantic intent of the computation on an operation-by-operation basis without sacrificing cycles to perform security checks. We specifically explore five mechanisms: (1) pointers with manifest bounds (fat pointers), (2) hardware types (atomic groups), (3) processor-supported authority, (4)authority-changing procedure calls (gates), and (5) programmable metadata validation and propagation (tags and dynamic tag management). These mechanisms allow the processor to continuously introspect on its operation, efficiently triggering software handlers on events that require logging, merit sophisticated inspection, or prompt adaptation. We present results from our prototype FPGA implementation of a processor that incorporates these mechanisms, quantifying the logic, memory, and latency requirements. We show that the dominant cost is the wider memory necessary to hold our metadata (the atomic groups and programmable tags), that the added logic resources make up less than 20% of the area of the processor, that the concurrent checks do not degrade processor cycle time, and that the tag cache is comparable to a small L1 data cache.
ER  - 

TY  - CONF
JO  - Object/Component/Service-Oriented Real-Time Distributed Computing, 2009. ISORC '09. IEEE International Symposium on
TI  - Online Self-Healing Support for Embedded Systems
T2  - Object/Component/Service-Oriented Real-Time Distributed Computing, 2009. ISORC '09. IEEE International Symposium on
IS  - 
SN  - 1555-0885
VO  - 
SP  - 283
EP  - 287
AU  - Lei Sun
AU  - Nilsson, D.K.
AU  - Katori, T.
AU  - Nakajima, T.
Y1  - 17-20 March 2009
PY  - 2009
KW  - Linux
KW  - data structures
KW  - embedded systems
KW  - operating system kernels
KW  - security of data
KW  - software fault tolerance
KW  - system recovery
KW  - virtual machines
KW  - Linux kernel
KW  - embedded system
KW  - online system-level self-healing support
KW  - recovery service
KW  - runtime detection service
KW  - runtime kernel data structure
KW  - security attack
KW  - self-diagnosis
KW  - system inconsistency
KW  - system performance
KW  - system resources
KW  - virtual machine monitor
KW  - Data analysis
KW  - Data structures
KW  - Embedded system
KW  - Intrusion detection
KW  - Kernel
KW  - Linux
KW  - Performance analysis
KW  - Runtime
KW  - Software prototyping
KW  - Virtual machine monitors
KW  - monitor
KW  - recovery
KW  - reliability
KW  - security
VL  - 
JA  - Object/Component/Service-Oriented Real-Time Distributed Computing, 2009. ISORC '09. IEEE International Symposium on
DO  - 10.1109/ISORC.2009.31
AB  - In this paper, online system-level self-healing support is presented for embedded systems. Different from off-line log analysis methods used by conventional intrusion detection systems, our research focuses on analyzing runtime kernel data structures hence perform self-diagnosis and self-healing. Inside the infrastructure, self-diagnosis and self-healing solutions have been implemented based on several selected critical kernel data structures. They can fully represent current system status and are also closely related with system resources. At runtime once any system inconsistency has been detected, predefined recovery functions are invoked. Our prototype is developed based on a lightweight virtual machine monitor, above on which the monitored Linux kernel, runtime detection and recovery services run simultaneously. The proposed infrastructure requires few modifications to current Linux kernel source code, thus it can be easily adopted into existing embedded systems. It is also fully software-based without introducing any specific hardware, therefore it is cost-efficient. The evaluation experiment results indicate that our prototype system can correctly detect inconsistent kernel data structures caused by security attacks with acceptable penalty to system performance.
ER  - 

TY  - CONF
JO  - Computer Modeling and Simulation (EMS), 2011 Fifth UKSim European Symposium on
TI  - Non-Statistical metrics for estimating redundancies in forensic investigations of network intrusions
T2  - Computer Modeling and Simulation (EMS), 2011 Fifth UKSim European Symposium on
IS  - 
SN  - 
VO  - 
SP  - 36
EP  - 41
AU  - Nehinbe, J.O.
Y1  - 16-18 Nov. 2011
PY  - 2011
KW  - Internet
KW  - security of data
KW  - Internet facilitiy
KW  - computer security
KW  - cyber crime
KW  - forensic investigation
KW  - network intrusion
KW  - nonstatistical metrics
KW  - serialization modelling method
KW  - Computational modeling
KW  - Computer crime
KW  - Correlation
KW  - Forensics
KW  - Measurement
KW  - Redundancy
KW  - Statistical analysis
KW  - Forensics networking
KW  - Intrusion Detection System
KW  - alerts aggregation
KW  - redundant alerts
VL  - 
JA  - Computer Modeling and Simulation (EMS), 2011 Fifth UKSim European Symposium on
DO  - 10.1109/EMS.2011.93
AB  - Most statistical methods do not perfectly conform to real cases of cyber crimes. Consequently, using statistical methods to analyze intrusion logs in order to present evidentiary values in courts of law are often refuted as baseless and inadmissible evidences regardless of the input spent to generate the reports and whether the reports are well-grounded evidences or not. Sometimes, complainants are often bewildered and confused because it is almost certain that the prime suspects will be absolved in courts of law. These are tragic developments to computer security experts, corporate and private organizations that leverage on the usage of the Internet facilities to boost service delivery, business activities and profitability. Thus, this paper presents non-statistical metrics that adopt Serialization Modelling Method (S2M) to improve interpretations of intrusion logs. The approach instantiates tokens and serializes alerts triggered by Snort using well-defined values. Experiments illustrate that duplicate tokens or patterns of alerts that exhibit increased propensity are indicative of redundant alerts to a certain degree.
ER  - 

TY  - CONF
JO  - Communications (ICC), 2014 IEEE International Conference on
TI  - Honeypots deployment for the analysis and visualization of malware activity and malicious connections
T2  - Communications (ICC), 2014 IEEE International Conference on
IS  - 
SN  - 
VO  - 
SP  - 1819
EP  - 1824
AU  - Koniaris, I.
AU  - Papadimitriou, G.
AU  - Nicopolitidis, P.
AU  - Obaidat, M.
Y1  - 10-14 June 2014
PY  - 2014
KW  - data visualisation
KW  - invasive software
KW  - public domain software
KW  - cyber attackers
KW  - financial motivations
KW  - honeypots deployment
KW  - malicious connections
KW  - malicious software
KW  - malware activity
KW  - open source visualization tool
KW  - threat agents
KW  - Data visualization
KW  - Grippers
KW  - IP networks
KW  - Malware
KW  - Ports (Computers)
KW  - Servers
KW  - Software
KW  - data visualization
KW  - honeypot
KW  - intrusion detection
KW  - log file analysis
KW  - malware
VL  - 
JA  - Communications (ICC), 2014 IEEE International Conference on
DO  - 10.1109/ICC.2014.6883587
AB  - Honeypots are systems aimed at deceiving threat agents. In most of the cases the latter are cyber attackers with financial motivations, and malicious software with the ability to launch automated attacks. Honeypots are usually deployed as either production systems or as research units to study the methods employed by attackers. In this paper we present the results of two distinct research honeypots. The first acted as a malware collector, a device usually deployed in order to capture self-propagating malware and monitor their activity. The second acted as a decoy server, dropping but logging every malicious connection attempt. Both of these systems have remained online for a lengthy period of time to study the aforementioned malicious activity. During this assessment it was shown that human attackers and malicious software are constantly attacking servers, trying to break into systems or spread across networks. It was also shown that the usage of honeypots for malware monitoring and attack logging can be very effective and provide valuable data. Lastly, we present an open source visualization tool which was developed to help security professionals and researchers during the analysis and conclusion drawing phases, for use with one of the systems fielded in our study.
ER  - 

TY  - CONF
JO  - Security Technology (ICCST), 2013 47th International Carnahan Conference on
TI  - Conformance checking of electronic business processes to secure distributed transactions
T2  - Security Technology (ICCST), 2013 47th International Carnahan Conference on
IS  - 
SN  - 
VO  - 
SP  - 1
EP  - 6
AU  - Talamo, M.
AU  - Arcieri, F.
AU  - Schunck, C.H.
AU  - D'Iddio, A.C.
Y1  - 8-11 Oct. 2013
PY  - 2013
KW  - business data processing
KW  - data mining
KW  - distributed processing
KW  - security of data
KW  - Italian Electronic Identity card
KW  - computer technology
KW  - conformance checking
KW  - distributed transaction security
KW  - electronic business process
KW  - event logs precision
KW  - event logs reliability
KW  - process mining
KW  - security purpose
KW  - validation authority
KW  - Automata
KW  - Business
KW  - Computational modeling
KW  - Data mining
KW  - Petri nets
KW  - Security
KW  - Software agents
VL  - 
JA  - Security Technology (ICCST), 2013 47th International Carnahan Conference on
DO  - 10.1109/CCST.2013.6922056
AB  - Advances in computer technologies facilitate the implementation of inter-organizational business processes. At the same time, managing the security of these processes is increasingly difficult. Compliance with high level specifcations, like normatives and pre-agreed protocols, rules and requirements, is difficult to validate. Here we discuss how Conformance Checking, a specific area of Process Mining, can be adapted for this purpose. Its role is to verify if an execution of a business process satisfies specifications represented by formal models (e.g. Petri Nets, Transition Systems, structures based on partial orders, etc). In the process mining literature, few efforts have been dedicated to online checking of business processes and choreographies for security purposes. The main requirement is high precision and reliability of event logs. They should record, precisely and unambiguously, all security-relevant activities of the analyzed process. Mantaining high-level logs becomes difficult with choreographies: log data are distributed, and must be related to events. Important metadata of event logs, like timestamps, can be ambiguous. Moreover, some data cannot be distributed due to security or privacy issues. These problems result in security-relevant ambiguities in event logs. Here we define a framework to create high-level event logs for online inter-organizational compliance checking using a Validation Authority. The system described here has been implemented in the issuing infrastructure for the Italian Electronic Identity card.
ER  - 

TY  - CONF
JO  - Reliable Distributed Systems, 1988. Proceedings., Seventh Symposium on
TI  - Quorum consensus algorithms for secure and reliable data
T2  - Reliable Distributed Systems, 1988. Proceedings., Seventh Symposium on
IS  - 
SN  - 
VO  - 
SP  - 44
EP  - 53
AU  - Agrawal, D.
AU  - El Abbadi, A.
Y1  - 10-12 Oct 1988
PY  - 1988
KW  - database management systems
KW  - fault tolerant computing
KW  - protocols
KW  - security of data
KW  - data-management protocol
KW  - fault-tolerant replicated database
KW  - information-dispersal algorithm
KW  - log-based propagation mechanism
KW  - quorum consensus algorithms
KW  - security
KW  - Availability
KW  - Communication system security
KW  - Computer science
KW  - Data security
KW  - Fault tolerance
KW  - Gratings
KW  - Information security
KW  - Maintenance
KW  - Protocols
KW  - Transaction databases
VL  - 
JA  - Reliable Distributed Systems, 1988. Proceedings., Seventh Symposium on
DO  - 10.1109/RELDIS.1988.25779
AB  - The authors address the issue of maintaining security in a fault-tolerant replicated database. They present a data-management protocol that integrates the information-dispersal algorithm (for security) and the quorum-consensus algorithm (for reliability). Although this protocol provides the desired level of security, it does not achieve the same level of availability for both read and write operations as the quorum-consensus algorithm. By integrating a log-based propagation mechanism with their protocol, the authors are able to achieve the same level of availability for both read and write operations as other quorum-consensus protocols, while maintaining the desired level of security
ER  - 

TY  - CONF
JO  - Cloud Computing Technology and Science (CloudCom), 2014 IEEE 6th International Conference on
TI  - A Multi-layer and MultiTenant Cloud Assurance Evaluation Methodology
T2  - Cloud Computing Technology and Science (CloudCom), 2014 IEEE 6th International Conference on
IS  - 
SN  - 
VO  - 
SP  - 386
EP  - 393
AU  - Hudic, A.
AU  - Tauber, M.
AU  - Lorunser, T.
AU  - Krotsiani, M.
AU  - Spanoudakis, G.
AU  - Mauthe, A.
AU  - Weippl, E.R.
Y1  - 15-18 Dec. 2014
PY  - 2014
KW  - cloud computing
KW  - security of data
KW  - cloud applications
KW  - multilayer multitenant cloud assurance evaluation methodology
KW  - overall application security
KW  - Cloud computing
KW  - Clouds
KW  - Equations
KW  - Mathematical model
KW  - Monitoring
KW  - Security
KW  - Vectors
KW  - assurance
KW  - cloud
KW  - critical infrastructures
VL  - 
JA  - Cloud Computing Technology and Science (CloudCom), 2014 IEEE 6th International Conference on
DO  - 10.1109/CloudCom.2014.85
AB  - Data with high security requirements is being processed and stored with increasing frequency in the Cloud. To guarantee that the data is being dealt in a secure manner we investigate the applicability of Assurance methodologies. In a typical Cloud environment the setup of multiple layers and different stakeholders determines security properties of individual components that are used to compose Cloud applications. We present a methodology adapted from Common Criteria for aggregating information reflecting the security properties of individual constituent components of Cloud applications. This aggregated information is used to categorise overall application security in terms of Assurance Levels and to provide a continuous assurance level evaluation. It gives the service owner an overview of the security of his service, without requiring detailed manual analyses of log files.
ER  - 

TY  - CONF
JO  - Global Telecommunications Conference (GLOBECOM 2010), 2010 IEEE
TI  - A Distributed Multi-Target Software Vulnerability Discovery and Analysis Infrastructure for Smart Phones
T2  - Global Telecommunications Conference (GLOBECOM 2010), 2010 IEEE
IS  - 
SN  - 1930-529X
VO  - 
SP  - 1
EP  - 5
AU  - Krishnan, S.P.T.
AU  - Lee Wang Hao
AU  - Sathya, S.A.
AU  - Devi, L.
Y1  - 6-10 Dec. 2010
PY  - 2010
KW  - mobile computing
KW  - mobile handsets
KW  - operating systems (computers)
KW  - security of data
KW  - source coding
KW  - telecommunication computing
KW  - MAFIA
KW  - OS retail phones models
KW  - analysis infrastructure
KW  - distributed multitarget software vulnerability discovery
KW  - fuzz testing
KW  - image-handling libraries
KW  - mobile devices
KW  - mobile platforms
KW  - multitarget automated fuzzing infrastructure and arsenal
KW  - operating systems
KW  - security bugs
KW  - smart phones
KW  - source code
KW  - system security
KW  - Computer crashes
KW  - Libraries
KW  - Mobile communication
KW  - Mobile handsets
KW  - Security
KW  - Servers
KW  - Software
VL  - 
JA  - Global Telecommunications Conference (GLOBECOM 2010), 2010 IEEE
DO  - 10.1109/GLOCOM.2010.5684011
AB  - Smart phones of today have increasingly sophisticated software. As the feature set grows further, the probability of system security related defects is likely to increase as well. Today, the security of mobile platforms and applications comes under great scrutiny as they are getting widely adopted. It is therefore crucial that code for mobile devices gets well tested and security bugs eliminated where possible. A popular and effective testing technique to identify severe security bugs in source code is fuzz testing. However, it is extremely time consuming to generate randomized input and test them on each version of the mobile phone and its software. This paper presents, MAFIA - Multi-target Automated Fuzzing Infrastructure and Arsenal, a composite, distributed client-server fuzz testing infrastructure for software applications and libraries in virtually any smartphone platform. The set of tools in MAFIA is file-format agnostic and can be used across various applications &amp; libraries. With MAFIA, we conducted a large number of tests against image-handling libraries and logged more than 13,000 mutated inputs that successfully crash several Symbian OS retail phones models. The system is scalable and can be easily extended to be used on new devices and operating systems.
ER  - 

TY  - CONF
JO  - Information & Communication Technologies (ICICT), 2013 5th International Conference on
TI  - Design and development of wireless RTU and cybersecurity framework for SCADA system
T2  - Information & Communication Technologies (ICICT), 2013 5th International Conference on
IS  - 
SN  - 
VO  - 
SP  - 1
EP  - 6
AU  - Durrani, S.
AU  - Jattala, I.
AU  - Farooqi, J.
AU  - Shakeel, N.
AU  - Murad, M.
Y1  - 14-15 Dec. 2013
PY  - 2013
KW  - SCADA systems
KW  - cellular radio
KW  - control engineering computing
KW  - microprocessor chips
KW  - modems
KW  - security of data
KW  - wireless LAN
KW  - ARM Coter-M4 processor
KW  - CMC
KW  - COTS modules
KW  - Ethernet
KW  - GIS portal
KW  - GPS
KW  - GSM modem
KW  - SCADA system
KW  - WRTU
KW  - WiFi
KW  - commercial-of-the-shelf modules
KW  - communication infrastructure
KW  - control and monitoring centre
KW  - critical infrastructure
KW  - data acquisition monitoring and control capability
KW  - forecasting
KW  - historical analysis
KW  - multitiered cybersecurity framework
KW  - multitiered secure hardened wireless remote terminal unit
KW  - national security
KW  - national security agency
KW  - oil &amp; gas industry
KW  - operational centric data
KW  - report generation
KW  - satellite modem
KW  - standard industrial sensors
KW  - wireless RTU
KW  - wireless over wire-line
KW  - Communication system security
KW  - Computer security
KW  - GSM
KW  - Monitoring
KW  - Sensors
KW  - Cybersecurity
KW  - HART Protocol
KW  - SCADA
KW  - Satellite Modem
KW  - Wireless RTU
VL  - 
JA  - Information & Communication Technologies (ICICT), 2013 5th International Conference on
DO  - 10.1109/ICICT.2013.6732781
AB  - The wave adoption of wireless over wire-line is a global phenomenon and the Oil &amp; Gas industry is no exception. This paper presents a multi-tiered secure hardened Wireless Remote Terminal Unit (WRTU) for SCADA systems. The WRTU is based on ARM Coter-M4 Processor supported by GSM &amp; Satellite modems, GPS, WiFi and Ethernet. WRTU can be interfaced to an array of standard industrial sensors, and actuators. WRTU has data acquisition, monitoring, and control capabilities. WRTU is an inexpensive device, using Commercial-of-The-Shelf (COTS) modules. WRTU is an easy to deploy (using existing public available infrastructure) device and `single device that does it all'. WRTU connects to the Control and Monitoring Centre (CMC) via GSM or Satellite modem. The CMC monitors and controls all peripherals of the WRTU, maintains a log of alerts, sensors information, set critical limits and provides geo-location of all WRTU s via its GIS portal. The CMC has a database to maintain all operational centric data, for report generation, historical analysis and forecasting. A multi-tiered cybersecurity framework has been designed to protect entire communication infrastructure. Cybersecurity is critically important for national security in todays connected world. Cybersecurity is of critical importance as most national security agencies want the introduction of cybersecurity into all new SCADA systems used in critical infrastructure.
ER  - 

TY  - CONF
JO  - Computational Intelligence and Security Workshops, 2007. CISW 2007. International Conference on
TI  - Trusted Computing Enabled Access Control for Virtual Organizations
T2  - Computational Intelligence and Security Workshops, 2007. CISW 2007. International Conference on
IS  - 
SN  - 
VO  - 
SP  - 490
EP  - 493
AU  - Jing Zhan
AU  - Huanguo Zhang
Y1  - 15-19 Dec. 2007
PY  - 2007
KW  - Linux
KW  - grid computing
KW  - resource allocation
KW  - security of data
KW  - virtual reality
KW  - Linux
KW  - grid computing
KW  - grid security infrastructure
KW  - hardware module
KW  - resource collaboration
KW  - resource exchange
KW  - tamper-proof attestative behavior
KW  - trusted computing enabled access control
KW  - trusted platform module
KW  - virtual organizations
KW  - Access control
KW  - Collaborative work
KW  - Computer security
KW  - Grid computing
KW  - Hardware
KW  - Information security
KW  - Linux
KW  - Protection
KW  - Resource management
KW  - Space technology
VL  - 
JA  - Computational Intelligence and Security Workshops, 2007. CISW 2007. International Conference on
DO  - 10.1109/CISW.2007.4425540
AB  - Grid computing is becoming the prominent paradigm for resource exchange and collaboration, which are supported by dynamic, multi-institutional virtual organizations (VOs) overcoming limitations of time or space. As the de-facto standard for grid security, grid security infrastructure provides no effective mechanism to protect both resource providers and users from access of malicious entities. The trusted computing (TC) technology adopted by the trusted computing group (TCG) defines that an entity can be trusted as long as it always behaves in the expected manner. Specifically, the TC uses a hardware module called trusted platform module (TPM) to ensure the tamper-proof attestative behavior by integrity measurement, logging and report. In this paper, we propose and design a TC enabled system based on Linux and TPM hardware to enforce access control policy for the VO.
ER  - 

TY  - CONF
JO  - Machine Learning and Cybernetics, 2008 International Conference on
TI  - Using Fuzzy Neural Networks and rule heuristics for anomaly intrusion detection on database connection
T2  - Machine Learning and Cybernetics, 2008 International Conference on
IS  - 
SN  - 
VO  - 6
SP  - 3607
EP  - 3612
AU  - Rung-Ching Chen
AU  - Kai-Fang Cheng
AU  - Cheng-Chia Hsieh
Y1  - 12-15 July 2008
PY  - 2008
KW  - expert systems
KW  - fuzzy neural nets
KW  - security of data
KW  - anomaly intrusion detection
KW  - database connection
KW  - fuzzy adaptive resonance theory
KW  - fuzzy neural networks
KW  - Data security
KW  - File servers
KW  - Fuzzy neural networks
KW  - Information security
KW  - Intrusion detection
KW  - Network servers
KW  - Neural networks
KW  - Protocols
KW  - Resonance
KW  - Spatial databases
KW  - Fuzzy ART
KW  - expert system
KW  - intrusion detection system (IDS)
KW  - misuse intrusion detection
VL  - 6
JA  - Machine Learning and Cybernetics, 2008 International Conference on
DO  - 10.1109/ICMLC.2008.4621030
AB  - This paper addresses the issue of intrusion detection in database security management. A fuzzy adaptive resonance theory neural network and rule heuristics are used to build a model of company security judgment. The model is based on analysis of the log file of connections from the client side to the database of server side. The log file information includes user name, network address of client, the time of connection, the database name, the program used, and the protocol. Those features are inputted to a fuzzy adaptive resonance theory neural network for security judgment. An experiment using records from a local government office database indicates that our system has good results in detecting anomalous intrusions.
ER  - 

TY  - CONF
JO  - Information Assurance Workshop, 2005. IAW '05. Proceedings from the Sixth Annual IEEE SMC
TI  - Virtual honeynets revisited
T2  - Information Assurance Workshop, 2005. IAW '05. Proceedings from the Sixth Annual IEEE SMC
IS  - 
SN  - 
VO  - 
SP  - 232
EP  - 239
AU  - Lok Kwong Yan
Y1  - 15-17 June 2005
PY  - 2005
KW  - Linux
KW  - computer networks
KW  - data handling
KW  - network operating systems
KW  - security of data
KW  - virtual machines
KW  - virtual private networks
KW  - FSLog
KW  - GenII honeynet
KW  - Linux
KW  - SELinux
KW  - UML
KW  - data control
KW  - disk imager
KW  - file system logging
KW  - forensic images
KW  - honeypot controller
KW  - system call redirection
KW  - trusted honeywall
KW  - virtual honeynet architecture
KW  - virtual honeynet security
KW  - virtual machine
KW  - Computer architecture
KW  - Control systems
KW  - Costs
KW  - File systems
KW  - Forensics
KW  - Image analysis
KW  - Linux
KW  - Security
KW  - Unified modeling language
KW  - Virtual machining
VL  - 
JA  - Information Assurance Workshop, 2005. IAW '05. Proceedings from the Sixth Annual IEEE SMC
DO  - 10.1109/IAW.2005.1495957
AB  - A new user-mode Linux based virtual honeynet architecture is presented in this paper. The new architecture has improved functionality that is difficult to realize in the GenII honeynet. Two new honeynet capabilities in particular are introduced. Honeypot controller is a new virtual honeynet component that assists in data control. The honeywall promises to have finer control over the honeypots through signal and system call redirections. The second new capability is the disk imager. The disk imager is capable of making forensic images of the virtual machine's file systems for further analysis. Since security for virtual honeynets is a big concern, the new virtual honeynet architecture utilizes security enhanced Linux to isolate the untrusted honeypots from the completely trusted honeywall. SELinux and other research work done in this field made the new honeynet architecture a viable alternative to GenII honeynets. A file system logging mechanism, FSLog, has been developed for the UML based virtual honeynet. In conjunction with the built-in tty logger, UML based honeynets have logging capabilities that are equivalent to their GenII honeynet counterparts. The current version of FSLog successfully logs eighteen virtual file systems system calls including the common, read(), write(), open() and close() functions. Its current functionality and how it pieces into the new architecture is also discussed. This work provides researchers with an alternative honeynet platform. The new virtual honeynet architecture is more portable, easier to setup, more cost effective and as secure as the GenII honeynet. The addition of the honeypot controller and disk imager components also makes the new virtual honeynet architecture more capable.
ER  - 

TY  - CONF
JO  - Computer Engineering and Technology, 2009. ICCET '09. International Conference on
TI  - Design of Intrusion Detection System Based on a New Pattern Matching Algorithm
T2  - Computer Engineering and Technology, 2009. ICCET '09. International Conference on
IS  - 
SN  - 
VO  - 1
SP  - 545
EP  - 548
AU  - Zhang Hu
Y1  - 22-24 Jan. 2009
PY  - 2009
KW  - computer networks
KW  - data acquisition
KW  - pattern matching
KW  - security of data
KW  - data acquisition module
KW  - feature matching module
KW  - information security foundation structure
KW  - intrusion detection system
KW  - intrusion response module
KW  - log record module
KW  - network attacks
KW  - pattern matching algorithm
KW  - processing module
KW  - security management
KW  - Algorithm design and analysis
KW  - Concrete
KW  - Data acquisition
KW  - Hardware
KW  - Information security
KW  - Intrusion detection
KW  - Monitoring
KW  - Packaging
KW  - Pattern matching
KW  - Protocols
KW  - data acquisition module
KW  - intrusion detection system
KW  - pattern matching algorithm
VL  - 1
JA  - Computer Engineering and Technology, 2009. ICCET '09. International Conference on
DO  - 10.1109/ICCET.2009.244
AB  - Intrusion detection technology can help the system to deal with network attacks, extend the security management ability of the system manager and increase the integrality of information security foundation structure. Pattern matching algorithm is the core algorithm of intrusion detection system based on feature matching as well as an algorithm which is universally used in current intrusion detection equipment. A design scheme of intrusion detection system based on pattern matching algorithm is proposed in this paper. Meanwhile, aiming at several key modules of intrusion detection system, a detailed analysis of data acquisition module, protocol processing module, feature matching module, log record module and intrusion response module is also given in this paper.
ER  - 

TY  - CONF
JO  - Computer Communications and Networks, 2009. ICCCN 2009. Proceedings of 18th Internatonal Conference on
TI  - Why Anti-Virus Products Slow Down Your Machine?
T2  - Computer Communications and Networks, 2009. ICCCN 2009. Proceedings of 18th Internatonal Conference on
IS  - 
SN  - 1095-2055
VO  - 
SP  - 1
EP  - 6
AU  - Wei Yan
AU  - Ansari, N.
Y1  - 3-6 Aug. 2009
PY  - 2009
KW  - Internet
KW  - file organisation
KW  - operating systems (computers)
KW  - security of data
KW  - storage management
KW  - Internet
KW  - antivirus software
KW  - cloud-based security infrastructure
KW  - data block access
KW  - deobfuscating binary payload
KW  - emulation technology
KW  - file index search
KW  - log-normal distribution
KW  - new technology file system
KW  - operation system
KW  - security company
KW  - software-protected computer
KW  - virus scan
KW  - virus signature
KW  - Application software
KW  - Computer viruses
KW  - Data security
KW  - Databases
KW  - Fingerprint recognition
KW  - Floods
KW  - Internet
KW  - Machinery production industries
KW  - Operating systems
KW  - Search engines
VL  - 
JA  - Computer Communications and Networks, 2009. ICCCN 2009. Proceedings of 18th Internatonal Conference on
DO  - 10.1109/ICCCN.2009.5235239
AB  - Customers always complain that anti-virus softwares bog down their computers by consuming much of PC memories and resources. With the popularity and variety of zero- day threats over the Internet, security companies have to keep on inserting new virus signatures into their databases. However, is the increasing size of the signature file the sole reason to drag computers to a crawl during the virus scan? This paper outlines other three reasons for slowing down software-protected computers, which actually are not directly related to the signature file. First, the rising time consumption of de-obfuscating binary payloads by using the emulation technology requires anti-virus softwares take more time to scan a packed file than an unpacked file. Second, new technology file system causes self-similarity in file index searching and data block accessing. Even if file sizes fit the log-normal distribution, there are still many "spikes" of high virus-scanning latency which cannot be ignored. Last but not least, temporal changes in file size, file type, and storage capacity in modern operation systems are slowing down virus scan. The paper also discusses the cloud-based security infrastructure for deploying a light-weight and fast anti-virus products.
ER  - 

TY  - CONF
JO  - Computer and Information Technology (CIT), 2011 IEEE 11th International Conference on
TI  - Effective SQL Injection Attack Reconstruction Using Network Recording
T2  - Computer and Information Technology (CIT), 2011 IEEE 11th International Conference on
IS  - 
SN  - 
VO  - 
SP  - 552
EP  - 556
AU  - Pomeroy, A.
AU  - Qing Tan
Y1  - Aug. 31 2011-Sept. 2 2011
PY  - 2011
KW  - Internet
KW  - SQL
KW  - banking
KW  - financial management
KW  - organisational aspects
KW  - security of data
KW  - SQL injection attack reconstruction
KW  - Web application
KW  - attack reconstruction
KW  - cyber criminals
KW  - data security
KW  - network recording
KW  - online banking
KW  - security logs
KW  - web applications
KW  - Business
KW  - Databases
KW  - Forensics
KW  - Payloads
KW  - Security
KW  - USA Councils
KW  - Web servers
KW  - Bro-IDS
KW  - SQL injection attacks
KW  - digital evidence
KW  - intrusion detection
KW  - network recording
KW  - time machine
VL  - 
JA  - Computer and Information Technology (CIT), 2011 IEEE 11th International Conference on
DO  - 10.1109/CIT.2011.103
AB  - Web applications offer business and convenience services that society has become dependent on, such as online banking. Success of these applications is dependent on end user trust, although these services have serious weaknesses that can be exploited by attackers. Application owners must take additional steps to ensure the security of customer data and integrity of the applications, since web applications are under siege from cyber criminals seeking to steal confidential information and disable or damage the services offered by these applications. Successful attacks have lead to some organizations experiencing financial difficulties or even being forced out of business. Organizations have insufficient tools to detect and respond to attacks on web applications, since traditional security logs have gaps that make attack reconstruction nearly impossible. This paper explores network recording challenges, benefits and possible future use. A network recording solution is proposed to detect and capture SQL injection attacks, resulting in the ability to successfully reconstruct SQL injection attacks in order to maintain application integrity.
ER  - 

TY  - CONF
JO  - Wireless Communications, Networking and Mobile Computing, 2009. WiCom '09. 5th International Conference on
TI  - The Intrusion Detection Model Based on the Fuzzy Judgment in Ad Hoc Network
T2  - Wireless Communications, Networking and Mobile Computing, 2009. WiCom '09. 5th International Conference on
IS  - 
SN  - 
VO  - 
SP  - 1
EP  - 4
AU  - Liu Shi-rui
AU  - Zhe Li
Y1  - 24-26 Sept. 2009
PY  - 2009
KW  - ad hoc networks
KW  - fuzzy set theory
KW  - optimisation
KW  - routing protocols
KW  - security of data
KW  - telecommunication security
KW  - transport protocols
KW  - vectors
KW  - AODV routing protocol
KW  - IP packet
KW  - ad hoc network
KW  - ad hoc on-demand distance vector routing
KW  - aggressive behavior judging rule
KW  - communication network security mechanism
KW  - distributed intrusion detection model
KW  - dynamic classification
KW  - fuzzy evaluation
KW  - fuzzy judgment model
KW  - multipoint cooperation
KW  - network resource distribution
KW  - network simulation platform
KW  - resource optimization
KW  - software NS2
KW  - Ad hoc networks
KW  - Bandwidth
KW  - Collaboration
KW  - Distributed computing
KW  - Eigenvalues and eigenfunctions
KW  - Information science
KW  - Information security
KW  - Intrusion detection
KW  - Routing protocols
KW  - Safety
VL  - 
JA  - Wireless Communications, Networking and Mobile Computing, 2009. WiCom '09. 5th International Conference on
DO  - 10.1109/WICOM.2009.5301829
AB  - The problem of security in communication has been researched to ad hoc network in this paper, and it also has been proposed a distributed intrusion detection model which is based on the fuzzy judgment. The judging rules of the aggressive behavior are summarized in this model according to the different offensive features in routing layer. Moreover, the log is established according to IP packets in routing layer, and the fuzzy evaluation to the data is carried on. On the base of this, a way to enhance the correct rate of the detection by joint detection mechanism of multi-point cooperation has been introduced in this paper. To distribute the network resource dynamically and reasonably, all nodes are treated dynamically with safety classification. Taking AODV routing protocol as an example, the working mechanism of the model is introduced, and the software NS2 is used to put up network simulation platform. The experimental results show that the detection model can guarantee the network security better.
ER  - 

TY  - CONF
JO  - Internet Monitoring and Protection, 2008. ICIMP '08. The Third International Conference on
TI  - Dynamic Verification and Control of Mobile Peer-to-Peer Systems
T2  - Internet Monitoring and Protection, 2008. ICIMP '08. The Third International Conference on
IS  - 
SN  - 
VO  - 
SP  - 1
EP  - 10
AU  - Spanoudakis, G.
AU  - Kloukinas, C.
AU  - Androutsopoulos, K.
Y1  - June 29 2008-July 5 2008
PY  - 2008
KW  - formal verification
KW  - mobile computing
KW  - peer-to-peer computing
KW  - security of data
KW  - software fault tolerance
KW  - system monitoring
KW  - application-level properties
KW  - attack prevention
KW  - centralised control
KW  - dependability checking
KW  - dynamic verification
KW  - fault-tolerance
KW  - mobile peer-to-peer system control
KW  - runtime monitoring
KW  - security enhancement
KW  - Centralized control
KW  - Collaboration
KW  - Condition monitoring
KW  - Control systems
KW  - Fault tolerant systems
KW  - Mobile computing
KW  - Peer to peer computing
KW  - Power system security
KW  - Runtime
KW  - System testing
KW  - Peer-to-Peer systems
KW  - control
KW  - dynamic verification
VL  - 
JA  - Internet Monitoring and Protection, 2008. ICIMP '08. The Third International Conference on
DO  - 10.1109/ICIMP.2008.11
AB  - The development of dependable mobile P2P systems is an inherently challenging task since such systems may operate in largely uncontrolled environments and may engage new peers or lose existing ones without any form of centralised control. In these circumstances, dependability and security can be enhanced through the runtime monitoring (a.k.a. dynamic verification) of the compliance of the system behaviour against specific dependability and security properties and the execution of control in cases where properties are violated. In this paper we present a framework for the dynamic verification and control of mobile P2P systems, which uses peer-specific monitoring policies to specify application-level properties. The deployment of this framework for monitoring system behaviour adds an extra layer of security and dependability checking, which is independent from checks performed directly by the P2P system that is being monitored. Thus, it makes the system more fault-tolerant and enables event logging that could be used for further analysis and prevention of attacks.
ER  - 

TY  - CONF
JO  - Broadband Network & Multimedia Technology, 2009. IC-BNMT '09. 2nd IEEE International Conference on
TI  - Analysis of anomaly packet's feature based on honeypot
T2  - Broadband Network & Multimedia Technology, 2009. IC-BNMT '09. 2nd IEEE International Conference on
IS  - 
SN  - 
VO  - 
SP  - 271
EP  - 275
AU  - Wang Xinliang
AU  - Liu Fang
AU  - Chen Luying
AU  - Lei Zhenming
Y1  - 18-20 Oct. 2009
PY  - 2009
KW  - Internet
KW  - computer centres
KW  - computer network management
KW  - log normal distribution
KW  - normal distribution
KW  - security of data
KW  - statistical analysis
KW  - Internet data center
KW  - anomaly traffic sampling
KW  - log-normal distribution
KW  - security research community
KW  - statistical model
KW  - Gaussian distribution
KW  - Internet
KW  - Log-normal distribution
KW  - Monitoring
KW  - Robustness
KW  - Sampling methods
KW  - Security
KW  - Spine
KW  - Traffic control
KW  - Web server
KW  - Anomaly detection
KW  - Anomaly feature
KW  - Heavy-tail
KW  - Honeypot
VL  - 
JA  - Broadband Network & Multimedia Technology, 2009. IC-BNMT '09. 2nd IEEE International Conference on
DO  - 10.1109/ICBNMT.2009.5348493
AB  - The deep study of anomaly feature based on the particular server was made in this paper. By continuously monitoring on the honeypot deployed in Internet Data Center for more than two months, the experimental results were summarized and some initial exploratory models were built. The models show that the number of attackers for the main attack types and ports can be described by normal distribution; meanwhile, the average packet number that each attacker generates per day can be described by log-normal distribution. This research aims to contribute to endeavor in the wider security research community to build methods and obtain some statistical models, grounded on strong empirical work, for assessment of the robustness of systems in hostile environments, and the anomaly traffic sampling, detection and classification on the backbone.
ER  - 

TY  - CONF
JO  - Computational and Information Sciences (ICCIS), 2013 Fifth International Conference on
TI  - A Sufficient Way of Mass Data Storage for Cloud Computing Based on Hashing Strategy
T2  - Computational and Information Sciences (ICCIS), 2013 Fifth International Conference on
IS  - 
SN  - 
VO  - 
SP  - 1493
EP  - 1496
AU  - Xie Yong
AU  - Zhang Yilai
Y1  - 21-23 June 2013
PY  - 2013
KW  - cloud computing
KW  - data integrity
KW  - meta data
KW  - naming services
KW  - pattern clustering
KW  - security of data
KW  - storage management
KW  - BLT
KW  - HDFS
KW  - Internet mass data storage
KW  - NameNode clusters
KW  - NameNode mapping management
KW  - cloud computing
KW  - data consistency
KW  - fault tolerance
KW  - hashing strategy
KW  - improved data storage method
KW  - information technology
KW  - log copy
KW  - metadata allocation
KW  - metadata security
KW  - read and write performance
KW  - Cloud computing
KW  - Computer architecture
KW  - Data security
KW  - Databases
KW  - File systems
KW  - Memory
KW  - Cloud Computing
KW  - Cloud Storage
KW  - HDFS
KW  - NameNode
VL  - 
JA  - Computational and Information Sciences (ICCIS), 2013 Fifth International Conference on
DO  - 10.1109/ICCIS.2013.393
AB  - With the rapid development of information technology and Internet, the mass data storage of Internet is an urgent problem to be solved. Due to the insufficiency of single NameNode, a new improved data storage method based on HDFS was proposed. In this method, consistent hashing strategy was adopted for metadata allocation; NameNode clusters were built and BLT was used for management of NameNode mapping; log copy was used to achieve data security and consistency. Simulation results showed that the method can effectively improve read and write performance under ensuring metadata's security, consistency and fault tolerance.
ER  - 

TY  - CONF
JO  - High Assurance Systems Engineering Symposium, 2007. HASE '07. 10th IEEE
TI  - A Secure and Scalable Update Protocol for P2P Data Grids
T2  - High Assurance Systems Engineering Symposium, 2007. HASE '07. 10th IEEE
IS  - 
SN  - 1530-2059
VO  - 
SP  - 423
EP  - 424
AU  - Manghui Tu
AU  - Tadayon, N.
AU  - Zhonghang Xia
AU  - Enyue Lu
Y1  - 14-16 Nov. 2007
PY  - 2007
KW  - concurrency control
KW  - data integrity
KW  - grid computing
KW  - groupware
KW  - peer-to-peer computing
KW  - replicated databases
KW  - security of data
KW  - transaction processing
KW  - P2P data grid
KW  - access concurrency
KW  - data consistency
KW  - data replication
KW  - data storage
KW  - epidemic algorithms
KW  - query latency
KW  - read transaction
KW  - scalable update protocol
KW  - security protocol
KW  - system delusion
KW  - transaction causal order
KW  - transaction logs
KW  - two-level vector clock
KW  - update transaction
KW  - user collaboration
KW  - Access protocols
KW  - Clocks
KW  - Computer science
KW  - Concurrent computing
KW  - Data engineering
KW  - Data security
KW  - Delay
KW  - Information systems
KW  - Power system modeling
KW  - Systems engineering and theory
VL  - 
JA  - High Assurance Systems Engineering Symposium, 2007. HASE '07. 10th IEEE
DO  - 10.1109/HASE.2007.40
AB  - Data stored in the P2P data grids are replicated at peers to achieve good query latency, low communication cost, and high availability. However, replication also raises consistency issues, which should be well defined and carefully maintained to facilitate better user collaboration. Epidemic algorithms were designed to satisfy a level of consistency weaker than serializability but preserve the causal order of update operations or transactions. However, with large system scale and with high access concurrency, epidemic lazy update algorithms can also introduce system delusion (e.g., with too many conflicting update or read transactions, system cannot reach consistency that there is no way to repair by reconciling conflicting transactions), which is due to their lazy update nature. Also, in P2P systems, some peers may be malicious because of being compromised, thus, security is a critical issue that should be addressed. In this paper, we propose an update mechanism to limit system delusion while providing high access concurrency. A two-level vector clock is developed to maintain the causal order of transactions in the data grid and maintain the consistency of peers with transaction logs. Security protocol is designed and integrated into the update mechanism to address the security issues.
ER  - 

TY  - CONF
JO  - Computer and Communication Engineering, 2008. ICCCE 2008. International Conference on
TI  - Access control scheme for Web services ( ACSWS )
T2  - Computer and Communication Engineering, 2008. ICCCE 2008. International Conference on
IS  - 
SN  - 
VO  - 
SP  - 854
EP  - 858
AU  - Elsheikh, S.
Y1  - 13-15 May 2008
PY  - 2008
KW  - Web services
KW  - access control
KW  - data mining
KW  - security of data
KW  - PrefixSpan algorithms
KW  - Web pages
KW  - Web server log
KW  - Web services security
KW  - access control
KW  - association rules mining
KW  - data mining
KW  - electronic data storage
KW  - service-oriented computing
KW  - user access data
KW  - user password
KW  - Access control
KW  - Authorization
KW  - Control systems
KW  - Data mining
KW  - Data security
KW  - History
KW  - Memory
KW  - Web pages
KW  - Web server
KW  - Web services
KW  - Access Control
KW  - PrefixSpan
KW  - Web Mining
KW  - Web Sevices
KW  - Web Usage Mining
VL  - 
JA  - Computer and Communication Engineering, 2008. ICCCE 2008. International Conference on
DO  - 10.1109/ICCCE.2008.4580726
AB  - The development and the wide spread use of web services allow for convenient electronic data storage and distribution all over the world. As this web services is a new service-oriented computing paradigm which poses the unique security challenges due to its inherent heterogeneity, multi-domain characteristic and highly dynamic nature. A key challenge in Web services security is to design effective access control schemes. However, most current access control systems base authorization decisions on subjectpsilas identity. In this paper, we suggest web access control scheme which incorporating user password and web server log. The major objective of the proposed model is to provide mechanisms to allow control of web user access based on the user access behavior by tracking the web access history. The system controls access to web pages depending on user password, date of last request, page visited (URL) and status action. The active userpsilas access pattern is matched with user access data discovered from user access history, based on mining web usage data using association rules mining and PrefixSpan algorithms, then analyzed to make the access control decision (web access is permitted or denied).
ER  - 

TY  - JOUR
JO  - Computer Graphics and Applications, IEEE
TI  - Visual correlation of network alerts
T2  - Computer Graphics and Applications, IEEE
IS  - 2
SN  - 0272-1716
VO  - 26
SP  - 48
EP  - 59
AU  - Foresti, S.
AU  - Agutter, J.
AU  - Livnat, Y.
AU  - Shaun Moon
AU  - Erbacher, R.
Y1  - March-April 2006
PY  - 2006
KW  - data visualisation
KW  - security of data
KW  - telecommunication security
KW  - VisAlert visual correlation tool
KW  - alert files
KW  - complex network environment
KW  - information visualization techniques
KW  - log files
KW  - network security
KW  - situational awareness
KW  - Computer crime
KW  - Computer worms
KW  - Computerized monitoring
KW  - Data security
KW  - Data visualization
KW  - Decision making
KW  - Humans
KW  - Intrusion detection
KW  - Large-scale systems
KW  - Scalability
KW  - Cybersecurity
KW  - Data Correlation
KW  - Network Intrusion
KW  - Network Monitoring
KW  - Situational Awareness
KW  - User Centered Design
KW  - Visualization
KW  - Computer Communication Networks
KW  - Computer Graphics
KW  - Information Storage and Retrieval
KW  - Signal Processing, Computer-Assisted
KW  - Software
KW  - User-Computer Interface
VL  - 26
JA  - Computer Graphics and Applications, IEEE
DO  - 10.1109/MCG.2006.49
AB  - The VisAlert visual correlation tool facilitates situational awareness in complex network environments by providing a holistic view of network security to help detect malicious activities. Information visualization techniques and methods in many applications have effectively increased operators' situational awareness, letting them more effectively detect, diagnose, and treat anomalous conditions. Visualization elevates information comprehension by fostering rapid correlation and perceived associations. Our visualization technique integrates the information in log and alert files into an intuitive, flexible, extensible, and scalable visualization tool - VisAlert - that presents critical information concerning network activity in an integrated manner, increasing the user's situational awareness.
ER  - 

TY  - CONF
JO  - Communications, 2005. ICC 2005. 2005 IEEE International Conference on
TI  - Extracting and querying network attack scenarios knowledge in IDS using PCTCG and alert semantic networks
T2  - Communications, 2005. ICC 2005. 2005 IEEE International Conference on
IS  - 
SN  - 
VO  - 3
SP  - 1512
EP  - 1517 Vol. 3
AU  - Wei Yan
AU  - Hou, E.
AU  - Ansari, N.
Y1  - 16-20 May 2005
PY  - 2005
KW  - grammars
KW  - knowledge acquisition
KW  - learning (artificial intelligence)
KW  - query languages
KW  - security of data
KW  - semantic Web
KW  - semantic networks
KW  - telecommunication security
KW  - 2-AASN
KW  - 2-atom alert semantic network
KW  - IDS
KW  - PCTCG
KW  - hidden attack scenario
KW  - intrusion detection system
KW  - knowledge extraction
KW  - machine-understandable uniform alert stream
KW  - querying network
KW  - security administrator
KW  - spreading activation technique
KW  - Computer networks
KW  - Data mining
KW  - Information security
KW  - Intelligent agent
KW  - Intelligent networks
KW  - Intrusion detection
KW  - Keyword search
KW  - Monitoring
KW  - Semantic Web
KW  - XML
VL  - 3
JA  - Communications, 2005. ICC 2005. 2005 IEEE International Conference on
DO  - 10.1109/ICC.2005.1494597
AB  - The increasing use of intrusion detection system gives rise to a huge volume of alert logs, making it hard for security administrators to uncover hidden attack scenarios. In this paper, we propose a four-layer semantic scheme designed to allow inferring attack scenarios and enabling attack semantic queries. The modified case grammar, PCTCG, is used to convert the raw alerts into machine-understandable uniform alert streams. The 2-atom alert semantic network, 2-AASN are used to generate attack scenario classes. Afterwards, based on the alert context, attack scenario instances are extracted and attack semantic query results on attack scenario instances using spreading activation technique are forwarded to the security administrator.
ER  - 

TY  - CONF
JO  - System Sciences, 2001. Proceedings of the 34th Annual Hawaii International Conference on
TI  - webXice: an infrastructure for information commerce on the WWW
T2  - System Sciences, 2001. Proceedings of the 34th Annual Hawaii International Conference on
IS  - 
SN  - 
VO  - 
SP  - 10 pp.
EP  - 
AU  - Wombacher, A.
AU  - Kostaki, P.
AU  - Aberer, K.
Y1  - 6-6 Jan. 2001
PY  - 2001
KW  - electronic commerce
KW  - electronic data interchange
KW  - information industry
KW  - information resources
KW  - protocols
KW  - security of data
KW  - Information and Content Exchange
KW  - Web browsers
KW  - Web-enabled Extended ICE protocol
KW  - World Wide Web
KW  - flexible business models
KW  - functional requirements
KW  - implementation strategy
KW  - information businesses
KW  - information commerce infrastructure
KW  - information consumers
KW  - information providers
KW  - lightweight clients
KW  - minimal system platforms
KW  - minimal technological requisites
KW  - nonreputable messages
KW  - persistent message logging
KW  - proofs
KW  - security requirements
KW  - system architecture
KW  - system implementation
KW  - webXice
KW  - Application software
KW  - Business
KW  - Consumer electronics
KW  - Electronics industry
KW  - Ice
KW  - Information security
KW  - Information systems
KW  - Information technology
KW  - Protocols
KW  - World Wide Web
VL  - 
JA  - System Sciences, 2001. Proceedings of the 34th Annual Hawaii International Conference on
DO  - 10.1109/HICSS.2001.927275
AB  - Systems for information commerce on the WWW have to support flexible business models if they are to cover the wide range of requirements that are imposed by the different types of information businesses. This leads to non-trivial functional and security requirements both on the provider side and on the consumer side, for which we introduce an architecture and a system implementation called webXice (Web-enabled eXtended Information and Content Exchange protocol). We focus on the question of how participants with minimal technological requisites, i.e. with just standard Web browsers available, can be technologically enabled to participate in the information commerce at a system level, while not sacrificing the functionality and security required by an autonomous participant in an information commerce scenario. In particular, we propose an implementation strategy to efficiently support persistent message logging for lightweight clients, which enables the clients to collect and manage non-reputable messages as proofs. We believe that the capability to support minimal system platforms is a necessary pre-condition for the widespread use of any information commerce infrastructure.
ER  - 

TY  - CONF
JO  - Network and Parallel Computing, 2009. NPC '09. Sixth IFIP International Conference on
TI  - A Virtual Machine Replay System Based on Para-virtualized Xen
T2  - Network and Parallel Computing, 2009. NPC '09. Sixth IFIP International Conference on
IS  - 
SN  - 
VO  - 
SP  - 44
EP  - 50
AU  - Tao Cui
AU  - Hai Jin
AU  - Xiaofei Liao
AU  - Haikun Liu
Y1  - 19-21 Oct. 2009
PY  - 2009
KW  - monitoring
KW  - operating systems (computers)
KW  - program debugging
KW  - security of data
KW  - shared memory systems
KW  - virtual machines
KW  - intrusion detection method
KW  - operating system debugging
KW  - operating system security
KW  - paravirtualized Xen
KW  - real-time monitoring module
KW  - virtual machine replay system
KW  - Computer science
KW  - Computer security
KW  - Grid computing
KW  - Hardware
KW  - Intrusion detection
KW  - Operating systems
KW  - Parallel processing
KW  - Software debugging
KW  - Virtual machine monitors
KW  - Virtual machining
KW  - Advanced Programmable Interrupt Controller
KW  - Non-masked Interrupt
KW  - Para-virtualized Device Model
KW  - Shared Memory
VL  - 
JA  - Network and Parallel Computing, 2009. NPC '09. Sixth IFIP International Conference on
DO  - 10.1109/NPC.2009.29
AB  - Operating system debugging and system security are two important issues which are attracted more and more attentions. However, traditional solutions of software debugging can not make an integrated replay towards the status of operating system. Moreover, most intrusion detection methods all depend on the operating system excessively, but operating system ex-poses so many interfaces to outside that it can not ensure its own security and is attacked vulnerably. Therefore, a para-virtualized model based full system replay is developed in this paper. According to the para-virtualized device model pro-vided by Xen, this system captures the non-deterministic events in the target operating system through a real-time monitoring module, and saves the data and time point of these events occurrence as log file elaborately. Using these data, we can make a full-system replay to the target operating system accurately. Compared with the previous replay systems, this system is more efficient and has two prominent advantages: no need to modify the target operating system, and no need to reboot the target operating system before executing replay.
ER  - 

TY  - CONF
JO  - Cybernetics and Intelligent Systems, 2008 IEEE Conference on
TI  - An integrated intrusion detection system by using multiple neural networks
T2  - Cybernetics and Intelligent Systems, 2008 IEEE Conference on
IS  - 
SN  - 
VO  - 
SP  - 22
EP  - 27
AU  - Guisong Liu
AU  - Xiaobin Wang
Y1  - 21-24 Sept. 2008
PY  - 2008
KW  - computer networks
KW  - principal component analysis
KW  - security of data
KW  - self-organising feature maps
KW  - telecommunication security
KW  - Internet
KW  - KDD CUP 1999 Intrusion Detection Evaluation dataset
KW  - LAN
KW  - address resolution protocol
KW  - clustering analysis
KW  - integrated intrusion detection system
KW  - local area network
KW  - multiple neural networks
KW  - network security
KW  - neural gas networks
KW  - principal component neural networks
KW  - principal component self- organizing map networks
KW  - Computer networks
KW  - Computer security
KW  - Intrusion detection
KW  - Laboratories
KW  - Local area networks
KW  - Monitoring
KW  - Neural networks
KW  - Protection
KW  - Protocols
KW  - Waste materials
KW  - Address Resolution Protocol
KW  - Intrusion Detection System
KW  - Neural Gas Networks
KW  - Principal Component Neural Networks
KW  - Self-Organizing Map
VL  - 
JA  - Cybernetics and Intelligent Systems, 2008 IEEE Conference on
DO  - 10.1109/ICCIS.2008.4670871
AB  - Neural networks approach is one of the most promising methodologies for intrusion detection in network security. An integrated intrusion detection system (IIDS) scheme based on multiple neural networks is proposed. The approaches used in IIDS include principal component neural networks, growing neural gas networks and principal component self-organizing map networks. By the abilities of classification and clustering analysis of the above methods, IIDS can be adapted to both anomaly and misuse detections for intrusive outsiders. The training stage is a mixture of supervised manner and unsupervised one. Furthermore, IIDS uses the buffering and spoofing principles of address resolution protocol (ARP) to capture and refuse the insider intruders trying to log on a local area network (LAN). Therefore, IIDS is able to detect the intrusions/attacks both from the outer Internet and an inner LAN. Experiments are carried out to illustrate the performance of the proposed intrusion detection system by using the KDD CUP 1999 Intrusion Detection Evaluation dataset.
ER  - 

TY  - CONF
JO  - E-Business and Information System Security, 2009. EBISS '09. International Conference on
TI  - Behavior Trust Prediction and Control Based on Electronic Commerce System
T2  - E-Business and Information System Security, 2009. EBISS '09. International Conference on
IS  - 
SN  - 
VO  - 
SP  - 1
EP  - 4
AU  - Rao Li-Ping
AU  - He Ming
AU  - Huang Song
AU  - Bao Rui
Y1  - 23-24 May 2009
PY  - 2009
KW  - Java
KW  - computer viruses
KW  - electronic commerce
KW  - network operating systems
KW  - security of data
KW  - software architecture
KW  - .NET framework
KW  - Ajax
KW  - behavior trust forecast
KW  - behavior trust prediction
KW  - behavior trustworthiness
KW  - controllability
KW  - electronic commerce system security
KW  - intrusion
KW  - multitiered distributed architecture
KW  - multitrust-attribute conditions
KW  - trustworthy computing circumstance
KW  - trustworthy model
KW  - trustworthy validation
KW  - unaware viruses
KW  - user behaviors
KW  - Automatic control
KW  - Automation
KW  - Control systems
KW  - Controllability
KW  - Electronic commerce
KW  - Helium
KW  - Information security
KW  - Meteorology
KW  - Predictive models
KW  - Programmable logic arrays
VL  - 
JA  - E-Business and Information System Security, 2009. EBISS '09. International Conference on
DO  - 10.1109/EBISS.2009.5137946
AB  - With the increasing development of electronic commerce application, electronic commerce system security is facing the heavy challenge. To arrive at the goal of intensifying the behavior trustworthiness and controllability of electronic commerce systems, we focus on building the trustworthy model between electronic commerce system and user behaviors and strengthening the manageability of electronic commerce systems. By setting up the trustworthy computing circumstance and supplying the trustworthy validation and the active protection based on identity and behavior for trustworthy electronic commerce system, we may defend the unaware viruses and intrusion. Furthermore, the controllability of behavior trust is studied. It can be predicted trust level under the multi-trust- attribute conditions. The experiment results show the .NET Framework of creating user behavior log to provide evidences for behavior trust forecast and control. The Multi-tiered Distributed architecture based on Ajax and .Net is built to realize the content mentioned above and applied to the practical project.
ER  - 

TY  - CONF
JO  - Computer Security Applications Conference, 2009. ACSAC '09. Annual
TI  - An Empirical Approach to Modeling Uncertainty in Intrusion Analysis
T2  - Computer Security Applications Conference, 2009. ACSAC '09. Annual
IS  - 
SN  - 1063-9527
VO  - 
SP  - 494
EP  - 503
AU  - Xinming Ou
AU  - Rajagopalan, S.R.
AU  - Sakthivelmurugan, S.
Y1  - 7-11 Dec. 2009
PY  - 2009
KW  - inference mechanisms
KW  - security of data
KW  - Datalog-like language
KW  - Prolog system
KW  - Snort add-on
KW  - Snort alerts
KW  - Snort rule repository
KW  - ad hoc human reasoning
KW  - automated intrusion analysis
KW  - automated reasoning process
KW  - intrusion detection system
KW  - logical language
KW  - malicious activities
KW  - natural language description
KW  - reasoning model
KW  - reasoning system
KW  - system monitoring data
KW  - system monitoring tools
KW  - uncertain events
KW  - uncertainty
KW  - Application software
KW  - Computer security
KW  - Computerized monitoring
KW  - Data mining
KW  - Data security
KW  - Forensics
KW  - Humans
KW  - Intrusion detection
KW  - Production systems
KW  - Uncertainty
KW  - intrusion detection
KW  - logic
KW  - uncertainty
VL  - 
JA  - Computer Security Applications Conference, 2009. ACSAC '09. Annual
DO  - 10.1109/ACSAC.2009.53
AB  - Uncertainty is an innate feature of intrusion analysis due to the limited views provided by system monitoring tools, intrusion detection systems (IDS), and various types of logs. Attackers are essentially invisible in cyber space and monitoring tools can only observe the symptoms or effects of malicious activities. When mingled with similar effects from normal or non-malicious activities they lead intrusion analysis to conclusions of varying confidence and high false positive/negative rates. This paper presents an empirical approach to the problem of uncertainty where the inferred security implications of low-level observations are captured in a simple logical language augmented with certainty tags. We have designed an automated reasoning process that enables us to combine multiple sources of system monitoring data and extract highly-confident attack traces from the numerous possible interpretations of low-level observations. We have developed our model empirically: the starting point was a true intrusion that happened on a campus network that we studied to capture the essence of the human reasoning process that led to conclusions about the attack. We then used a Datalog-like language to encode the model and a Prolog system to carry out the reasoning process. Our model and reasoning system reached the same conclusions as the human administrator on the question of which machines were certainly compromised. We then automatically generated the reasoning model needed for handling Snort alerts from the natural-language descriptions in the Snort rule repository, and developed a Snort add-on to analyze Snort alerts. Keeping the reasoning model unchanged, we applied our reasoning system to two third-party data sets and one production network. Our results showed that the reasoning model is effective on these data sets as well. We believe such an empirical approach has the potential of codifying the seemingly ad-hoc human reasoning of uncertain events, and can yield useful to- ols for automated intrusion analysis.
ER  - 

TY  - CONF
JO  - Emerging Security Technologies (EST), 2012 Third International Conference on
TI  - On the Helmholtz Principle for Data Mining
T2  - Emerging Security Technologies (EST), 2012 Third International Conference on
IS  - 
SN  - 
VO  - 
SP  - 99
EP  - 102
AU  - Dadachev, B.
AU  - Balinsky, A.
AU  - Balinsky, H.
AU  - Simske, S.
Y1  - 5-7 Sept. 2012
PY  - 2012
KW  - data mining
KW  - document handling
KW  - security of data
KW  - social networking (online)
KW  - Gestalt human perception theory
KW  - Helmholtz principle
KW  - change detection
KW  - data mining
KW  - files stream
KW  - image processing
KW  - information extraction
KW  - large document automatic summarization
KW  - security applications
KW  - short document stream
KW  - social networks theory
KW  - unusual behaviour detection
KW  - Containers
KW  - Data mining
KW  - Electronic mail
KW  - Humans
KW  - Information retrieval
KW  - Measurement
KW  - Security
KW  - Helmholtz Principle
KW  - Keywords Extraction
KW  - Small-World Networks
KW  - Summarization
KW  - Unusual Behaviour Detection
VL  - 
JA  - Emerging Security Technologies (EST), 2012 Third International Conference on
DO  - 10.1109/EST.2012.11
AB  - Unusual behaviour detection and information extraction in streams of short documents and files (emails, news, tweets, log files, messages, etc.) are important problems in security applications. In [1], [2], a new approach to rapid change detection and automatic summarization of large documents was introduced. This approach is based on a theory of social networks and ideas from image processing and especially on the Helmholtz Principle from the Gestalt Theory of human perception. In this article we modify, optimize and verify the approach from [1], [2] to unusual behaviour detection and information extraction from small documents.
ER  - 

TY  - CONF
JO  - Availability, Reliability, and Security, 2010. ARES '10 International Conference on
TI  - 2-clickAuth Optical Challenge-Response Authentication
T2  - Availability, Reliability, and Security, 2010. ARES '10 International Conference on
IS  - 
SN  - 
VO  - 
SP  - 79
EP  - 86
AU  - Vapen, A.
AU  - Byers, D.
AU  - Shahmehri, N.
Y1  - 15-18 Feb. 2010
PY  - 2010
KW  - Internet
KW  - identification
KW  - security of data
KW  - user interfaces
KW  - 2-clickAuth solution
KW  - 2D barcodes
KW  - Internet users
KW  - OpenID system
KW  - federated identity management
KW  - optical challenge-response authentication
KW  - passwords
KW  - usernames
KW  - Authentication
KW  - Availability
KW  - Cameras
KW  - Computer security
KW  - Data security
KW  - Facebook
KW  - Identity management systems
KW  - Internet
KW  - Mobile handsets
KW  - MySpace
KW  - OpenID
KW  - QR code
KW  - authentication
KW  - federated identity management
KW  - trusted device
VL  - 
JA  - Availability, Reliability, and Security, 2010. ARES '10 International Conference on
DO  - 10.1109/ARES.2010.85
AB  - Internet users today often have usernames and passwords at multiple web sites. To simplify things, many sites support some form of federated identity management, such as OpenID, that enables users to have a single account that allows them to log on to many different sites by authenticating to a single identity provider. Most identity providers perform authentication using a username and password. Should these credentials be compromised, e.g. captured by a key logger or malware on an untrusted computer, all the user's accounts become compromised. Therefore a more secure authentication method is desirable. We have implemented 2-clickAuth, an optical challenge-response solution where a web camera and a camera phone are used for authentication. Two-dimensional barcodes are used for the communication between phone and computer, which allows 2-clickAuth to transfer relatively large amounts of data in a short period of time. 2-clickAuth is considerably more secure than passwords while still being easy to use and easy to distribute to users. This makes 2-clickAuth a viable alternative to passwords in systems where enhanced security is desired, but availability, ease-of-use, and cost cannot be compromised. We have implemented an identity provider in the OpenID federated identity management system that uses 2-clickAuth for authentication, making 2-clickAuth available to all users of sites that support OpenID, including Facebook, Sourceforge and MySpace.
ER  - 

TY  - CONF
JO  - Networks Security, Wireless Communications and Trusted Computing, 2009. NSWCTC '09. International Conference on
TI  - Research on Behavior Trust Based on Trustworthy Distributed System
T2  - Networks Security, Wireless Communications and Trusted Computing, 2009. NSWCTC '09. International Conference on
IS  - 
SN  - 
VO  - 1
SP  - 396
EP  - 399
AU  - Ming He
AU  - Aiqun Hu
AU  - Hangping Qiu
Y1  - 25-26 April 2009
PY  - 2009
KW  - behavioural sciences
KW  - distributed processing
KW  - security of data
KW  - .NET framework
KW  - behavior trustworthiness
KW  - distributed systems controllability
KW  - trustworthy distributed system
KW  - trustworthy validation
KW  - Automation
KW  - Buildings
KW  - Communication system security
KW  - Computer networks
KW  - Controllability
KW  - Helium
KW  - Information science
KW  - Information security
KW  - Protection
KW  - Wireless communication
KW  - behavior trust
KW  - trustworthiness
KW  - trustworthy distributed system
VL  - 1
JA  - Networks Security, Wireless Communications and Trusted Computing, 2009. NSWCTC '09. International Conference on
DO  - 10.1109/NSWCTC.2009.401
AB  - With the increasing development of distributed system application, distributed system security is facing the heavy challenge. To arrive at the goal of intensifying the behavior trustworthiness and controllability of distributed systems, we focus on building the trustworthy model between distributed system and user behaviors and strengthening the manageability of distributed systems. By setting up the trustworthy computing circumstance and supplying the trustworthy validation and the active protection based on identity and behavior for trustworthy distributed system, we may defend the unaware viruses and intrusion. Furthermore, the controllability of behavior trust is studied. It can be predicted trust level under the multi-trust-attribute conditions. The experiment results show the .NET framework of creating user behavior log to provide evidences for behavior trust forecast and control.
ER  - 

TY  - CONF
JO  - Research in Security and Privacy, 1990. Proceedings., 1990 IEEE Computer Society Symposium on
TI  - A little knowledge goes a long way: faster detection of compromised data in 2-D tables
T2  - Research in Security and Privacy, 1990. Proceedings., 1990 IEEE Computer Society Symposium on
IS  - 
SN  - 
VO  - 
SP  - 86
EP  - 94
AU  - Gusfield, D.
Y1  - 7-9 May 1990
PY  - 1990
KW  - data analysis
KW  - data privacy
KW  - security of data
KW  - statistics
KW  - column sums
KW  - compromised data detection
KW  - initial needed bounds
KW  - integer statistics
KW  - interval deduction
KW  - law violation
KW  - nonsensitive data
KW  - public data disclosure
KW  - row sums
KW  - security
KW  - sensitive data protection
KW  - tightest lower bounds
KW  - tightest upper bounds
KW  - two dimensional data
KW  - undisclosed cells
KW  - Computer networks
KW  - Computer science
KW  - Data security
KW  - Government
KW  - Privacy
KW  - Protection
KW  - Statistics
KW  - Storage area networks
KW  - Testing
KW  - Upper bound
VL  - 
JA  - Research in Security and Privacy, 1990. Proceedings., 1990 IEEE Computer Society Symposium on
DO  - 10.1109/RISP.1990.63841
AB  - A reexamination is made of the problem of protecting sensitive data in an <e1>n</e1> by <e1>n</e1> table of integer statistics, when the nonsensitive data are made public along with the row and column sums for the table. Consideration is given to the problem of computing the tightest upper bounds on the values of sensitive (undisclosed) cells. These bounds, together with tightest lower bounds (which can be efficiently computed), define precisely the smallest intervals that an adversary can deduce for the missing sensitive cell values. Small intervals compromise the security of the undisclosed data, and in some cases violate laws on public data disclosure. It is observed that the number of initial needed bounds can be reduced from <e1>O</e1>(<e1>n</e1> log <e1>n</e1>) to 2<e1>n</e1>-1 by exploiting a recent result of C.K. Cheung and T.C. Hu (1988)
ER  - 

TY  - CONF
JO  - Data Engineering Workshop, 2008. ICDEW 2008. IEEE 24th International Conference on
TI  - Provenance-aware secure networks
T2  - Data Engineering Workshop, 2008. ICDEW 2008. IEEE 24th International Conference on
IS  - 
SN  - 
VO  - 
SP  - 188
EP  - 193
AU  - Wenchao Zhou
AU  - Cronin, E.
AU  - Boon Thau Loo
Y1  - 7-12 April 2008
PY  - 2008
KW  - DATALOG
KW  - Internet
KW  - computer network management
KW  - distributed databases
KW  - message authentication
KW  - query processing
KW  - telecommunication security
KW  - Internet
KW  - P2 declarative networking system
KW  - authenticated communication
KW  - data provenance computation
KW  - distributed data stream query
KW  - forensic analysis
KW  - malicious node identification
KW  - network Datalog language
KW  - network accountability
KW  - network diagnostics
KW  - provenance-aware secure network
KW  - trust management policy
KW  - Computer networks
KW  - Data flow computing
KW  - Data security
KW  - Database languages
KW  - Distributed computing
KW  - Forensics
KW  - IP networks
KW  - Information security
KW  - Performance analysis
KW  - Taxonomy
VL  - 
JA  - Data Engineering Workshop, 2008. ICDEW 2008. IEEE 24th International Conference on
DO  - 10.1109/ICDEW.2008.4498315
AB  - Network accountability and forensic analysis have become increasingly important, as a means of performing network diagnostics, identifying malicious nodes, enforcing trust management policies, and imposing diverse billing over the Internet. This has led to a series of work to provide better network support for accountability, and efficient mechanisms to trace packets and information flows through the Internet. In this paper, we make the following contributions. First, we show that network accountability and forensic analysis can be posed generally as data provenance computations and queries over distributed streams. In particular, one can utilize declarative networks with appropriate security and provenance extensions to provide a unified declarative framework for specifying, analyzing and auditing networks. Second, we propose a taxonomy of data provenance along multiple axes, and show that they map naturally to different use cases in networks. Third, we suggest techniques to efficiently compute and store network provenance, and provide an initial performance evaluation on the P2 declarative networking system with modifications to support authenticated communication and provenance.
ER  - 

TY  - CONF
JO  - Information and Communication Technologies and Development, 2007. ICTD 2007. International Conference on
TI  - Design and implementation of the KioskNet system
T2  - Information and Communication Technologies and Development, 2007. ICTD 2007. International Conference on
IS  - 
SN  - 
VO  - 
SP  - 1
EP  - 10
AU  - Guo, S.
AU  - Falaki, M.H.
AU  - Oliver, E.A.
AU  - Ur Rahman, S.
AU  - Seth, A.
AU  - Zaharia, M.A.
AU  - Ismail, U.
AU  - Keshav, S.
Y1  - 15-16 Dec. 2007
PY  - 2007
KW  - Internet
KW  - government data processing
KW  - security of data
KW  - KioskNet system
KW  - e-governance services
KW  - kiosk deployments
KW  - log collection
KW  - mechanical backhaul
KW  - rural Internet kiosks
KW  - user management
KW  - Communication system control
KW  - Computer architecture
KW  - Control systems
KW  - Costs
KW  - Hard disks
KW  - Linux
KW  - Personal communication networks
KW  - Security
KW  - Vehicles
KW  - Web and internet services
VL  - 
JA  - Information and Communication Technologies and Development, 2007. ICTD 2007. International Conference on
DO  - 10.1109/ICTD.2007.4937417
AB  - Rural Internet kiosks in developing countries can cost-effectively provide communication and e-governance services to the poorest sections of society. Unfortunately, a variety of technical and non-technical issues have caused most kiosk deployments to be unsustainable. KioskNet addresses the key technical problems underlying kiosk failure by using robust dasiamechanical backhaulpsila for connectivity, and by using low-cost and reliable kiosk controllers to support services delivered from one or more recycled PCs. KioskNet also addresses related issues such as security, user management, and log collection. In this paper, we describe the KioskNet system and outline its hardware, software, and security architectures. We describe a pilot deployment, and how we used lessons from this deployment to re-design our initial prototype.
ER  - 

TY  - CONF
JO  - Network and Distributed System Security, 1995., Proceedings of the Symposium on
TI  - NERD: Network Event Recording Device: an automated system for network anomaly detection and notification
T2  - Network and Distributed System Security, 1995., Proceedings of the Symposium on
IS  - 
SN  - 
VO  - 
SP  - 87
EP  - 93
AU  - Simmons, D.G.
AU  - Wilkins, R.
Y1  - 16-17 Feb 1995
PY  - 1995
KW  - computer network management
KW  - computer network reliability
KW  - data handling
KW  - real-time systems
KW  - security of data
KW  - NERD
KW  - Network Event Recording Device
KW  - automated real-time system
KW  - central secure data collection point
KW  - configurable notification options
KW  - continuous monitoring
KW  - distributed system logs
KW  - network anomaly detection
KW  - network anomaly notification
KW  - network managers
KW  - security logs
KW  - significant network events
KW  - system logs
KW  - timely notification
KW  - Computer networks
KW  - Cranes
KW  - Displays
KW  - Event detection
KW  - Hardware
KW  - Laboratories
KW  - Monitoring
KW  - National security
KW  - Production systems
KW  - Quality management
VL  - 
JA  - Network and Distributed System Security, 1995., Proceedings of the Symposium on
DO  - 10.1109/NDSS.1995.390643
AB  - The Network Event Recording Device is an automated, real-time system for monitoring and detecting network anomalies, as well as providing timely notification to network managers of significant network events. The NERD system allows for continuous monitoring of system and security logs, easily configurable notification options and a central, secure data collection point for distributed system logs
ER  - 

TY  - CONF
JO  - Knowledge and Systems Engineering, 2009. KSE '09. International Conference on
TI  - A New CPT Extension Scheme for High Data Embedding Ratio in Binary Images
T2  - Knowledge and Systems Engineering, 2009. KSE '09. International Conference on
IS  - 
SN  - 
VO  - 
SP  - 61
EP  - 66
AU  - Phan Trung Huy
AU  - Vu Phuong Bac
AU  - Nguyen Manh Thang
AU  - Truong Duc Manh
AU  - Vu Tien Duc
AU  - Nguyen Tuan Nam
Y1  - 13-17 Oct. 2009
PY  - 2009
KW  - image coding
KW  - security of data
KW  - watermarking
KW  - binary images
KW  - block-based approach
KW  - high data embedding ratio
KW  - modified CPT extension scheme
KW  - security
KW  - watermarking
KW  - Concrete
KW  - Data engineering
KW  - Data security
KW  - Informatics
KW  - Information technology
KW  - Knowledge engineering
KW  - Mathematics
KW  - Systems engineering and theory
KW  - Watermarking
KW  - CPT scheme
KW  - CPTE scheme
KW  - MCPTE scheme
KW  - data embedding ratio
KW  - watermarking
VL  - 
JA  - Knowledge and Systems Engineering, 2009. KSE '09. International Conference on
DO  - 10.1109/KSE.2009.29
AB  - In this paper, an extension of CPT scheme (CPTE for short) is introduced. In watermarking area, this is a block-based approach. Given a binary image B, in each block F of a size mn of B, by CPT scheme, ones can embed r = Round(log<sub>2</sub>(m.n+1)) bits by changing at most two entries of F. In our CPTE scheme, r = Round(log<sub>2</sub>(m.n)) + 1 bits can be embedded by changing at most two entries of F, too. That means, in most cases, the number of embedded bits given by our CPTE scheme is larger than that by CPT scheme one bit. As a consequence, we establish a new modified scheme-MCPTE based on CPTE scheme to control the quality for security of the embedded blocks, in the same way due to [Tseng-Pan, 2001]. Experimental results show that in general, MCPTE scheme gives more embedded bits than those by Tseng-Pan' scheme, while the quality is the same.
ER  - 

TY  - CONF
JO  - Information Technology, 2008. ICIT '08. International Conference on
TI  - Secure and Efficient Authentication Scheme for Remote Systems
T2  - Information Technology, 2008. ICIT '08. International Conference on
IS  - 
SN  - 
VO  - 
SP  - 226
EP  - 231
AU  - Das, M.L.
Y1  - 17-20 Dec. 2008
PY  - 2008
KW  - security of data
KW  - authentication scheme
KW  - login identity
KW  - remote server
KW  - remote systems
KW  - two-factor user authentication
KW  - Authentication
KW  - Communication system security
KW  - Communications technology
KW  - Consumer electronics
KW  - Elliptic curves
KW  - Equations
KW  - Information security
KW  - Information technology
KW  - Public key
KW  - Smart cards
KW  - Authentication
KW  - Hash function
KW  - Map-to-Point.
KW  - Password
KW  - Smart card
VL  - 
JA  - Information Technology, 2008. ICIT '08. International Conference on
DO  - 10.1109/ICIT.2008.21
AB  - Authentication is one of the important security properties in applications ranging from border security to consumer electronics. This paper presents an efficient mutual authentication scheme for remote systems, which provides two-factor user authentication. The scheme attains two interesting features such as avoids the possibility of many logged in users with the same login identity and eliminates the overhead of maintaining verifier table at remote server.
ER  - 

TY  - CONF
JO  - Image Processing (ICIP), 2011 18th IEEE International Conference on
TI  - Distributed source coding for securing a hand-based biometric recognition system
T2  - Image Processing (ICIP), 2011 18th IEEE International Conference on
IS  - 
SN  - 1522-4880
VO  - 
SP  - 1825
EP  - 1828
AU  - Ramalho, M.
AU  - Correia, P.
AU  - Soares, L.D.
Y1  - 11-14 Sept. 2011
PY  - 2011
KW  - image coding
KW  - palmprint recognition
KW  - security of data
KW  - source coding
KW  - UST hand image database
KW  - binarization technique
KW  - biometric data security
KW  - distributed source coding
KW  - finger surface
KW  - hand geometry
KW  - hand-based biometric recognition system
KW  - information compression
KW  - large databases
KW  - log-likelihood ratio initialization
KW  - multimodal hand-based recognition system
KW  - palmprint
KW  - soft biometric
KW  - Databases
KW  - Decoding
KW  - Feature extraction
KW  - Parity check codes
KW  - Security
KW  - Thumb
KW  - LDPC
KW  - biometric template protection
KW  - distributed source coding
KW  - multimodal biometric system
KW  - secure biometrics
VL  - 
JA  - Image Processing (ICIP), 2011 18th IEEE International Conference on
DO  - 10.1109/ICIP.2011.6115820
AB  - Distributed source coding (DSC) is typically used to compress information from multiple correlated sources that do not communicate with each other. In this paper, DSC principles are used to secure biometric data in a multimodal hand-based recognition system, introducing a novel approach for Log-Likelihood Ratio initialization and a new binarization technique. The proposed biometric recognition system relies on a new architecture using three hand-based biometric traits: palmprint, finger surface and hand geometry, being the latter used as a soft biometric to accelerate the identification process. Promising results are achieved in terms of recognition accuracy, speed and security when performing identification on large databases, like the UST Hand Image Database.
ER  - 

TY  - CONF
JO  - Communications, Computers and Signal Processing, 2007. PacRim 2007. IEEE Pacific Rim Conference on
TI  - A Survey of Connection-Chains Detection Techniques
T2  - Communications, Computers and Signal Processing, 2007. PacRim 2007. IEEE Pacific Rim Conference on
IS  - 
SN  - 
VO  - 
SP  - 219
EP  - 222
AU  - Almulhem, A.
AU  - Traore, I.
Y1  - 22-24 Aug. 2007
PY  - 2007
KW  - Internet
KW  - security of data
KW  - computer security
KW  - connection-chains detection techniques
KW  - sequentially logging
KW  - stepping-stones
KW  - taxonomy
KW  - Computer crime
KW  - Computer hacking
KW  - Computer security
KW  - Cryptography
KW  - Fluid flow measurement
KW  - Frequency measurement
KW  - Payloads
KW  - Performance evaluation
KW  - Taxonomy
KW  - Timing
VL  - 
JA  - Communications, Computers and Signal Processing, 2007. PacRim 2007. IEEE Pacific Rim Conference on
DO  - 10.1109/PACRIM.2007.4313215
AB  - A connection-chain is a set of connections created by sequentially logging into a series of hosts, known as stepping-stones. It provides an effective scheme for attackers to manually interact with a victim machine without disclosing their true origin. The victim will only identify the last host in the chain, while the true origin is hidden behind a series of stepping-stones. Addressing connection-chains poses challenges for researchers in the field of computer security. Accordingly, several approaches have been proposed in the literature. In this paper, we review those approaches and classify them according to a proposed taxonomy.
ER  - 

TY  - CONF
JO  - Software Engineering (ICSE), 2013 35th International Conference on
TI  - Billions and billions of constraints: Whitebox fuzz testing in production
T2  - Software Engineering (ICSE), 2013 35th International Conference on
IS  - 
SN  - 
VO  - 
SP  - 122
EP  - 131
AU  - Bounimova, E.
AU  - Godefroid, P.
AU  - Molnar, D.
Y1  - 18-26 May 2013
PY  - 2013
KW  - computability
KW  - data analysis
KW  - program diagnostics
KW  - program testing
KW  - security of data
KW  - virtual machines
KW  - JobCenter
KW  - SAGAN system
KW  - SMT solver
KW  - Windows 7
KW  - binary traces
KW  - commodity virtual machines
KW  - constraint solving performance
KW  - constraint-based whitebox fuzz testing
KW  - data analysis
KW  - dynamic test generation
KW  - program testing
KW  - satisfiability modulo theories
KW  - security vulnerability
KW  - symbolic execution
KW  - Computer bugs
KW  - Monitoring
KW  - Production
KW  - Security
KW  - Servers
KW  - Testing
VL  - 
JA  - Software Engineering (ICSE), 2013 35th International Conference on
DO  - 10.1109/ICSE.2013.6606558
AB  - We report experiences with constraint-based whitebox fuzz testing in production across hundreds of large Windows applications and over 500 machine years of computation from 2007 to 2013. Whitebox fuzzing leverages symbolic execution on binary traces and constraint solving to construct new inputs to a program. These inputs execute previously uncovered paths or trigger security vulnerabilities. Whitebox fuzzing has found one-third of all file fuzzing bugs during the development of Windows 7, saving millions of dollars in potential security vulnerabilities. The technique is in use today across multiple products at Microsoft. We describe key challenges with running whitebox fuzzing in production. We give principles for addressing these challenges and describe two new systems built from these principles: SAGAN, which collects data from every fuzzing run for further analysis, and JobCenter, which controls deployment of our whitebox fuzzing infrastructure across commodity virtual machines. Since June 2010, SAGAN has logged over 3.4 billion constraints solved, millions of symbolic executions, and tens of millions of test cases generated. Our work represents the largest scale deployment of whitebox fuzzing to date, including the largest usage ever for a Satisfiability Modulo Theories (SMT) solver. We present specific data analyses that improved our production use of whitebox fuzzing. Finally we report data on the performance of constraint solving and dynamic test generation that points toward future research problems.
ER  - 

TY  - CONF
JO  - Network and System Security (NSS), 2011 5th International Conference on
TI  - Trustable outsourcing of business processes to cloud computing environments
T2  - Network and System Security (NSS), 2011 5th International Conference on
IS  - 
SN  - 
VO  - 
SP  - 280
EP  - 284
AU  - Alsouri, S.
AU  - Katzenbeisser, S.
AU  - Biedermann, S.
Y1  - 6-8 Sept. 2011
PY  - 2011
KW  - business data processing
KW  - cloud computing
KW  - security of data
KW  - software architecture
KW  - virtualisation
KW  - Internet-based service
KW  - ODE process engine
KW  - Xen
KW  - business process
KW  - cloud computing
KW  - cost-effective outsourcing
KW  - trustable outsourcing
KW  - virtualization technology
KW  - Computational modeling
KW  - Computer architecture
KW  - Hardware
KW  - Outsourcing
KW  - Security
KW  - Web services
VL  - 
JA  - Network and System Security (NSS), 2011 5th International Conference on
DO  - 10.1109/ICNSS.2011.6060015
AB  - Cloud Computing, the next generation of Internet-based services, will allow cost-effective outsourcing of applications and business processes. However, outsourcing business processes to potentially untrusted servers poses significant security and privacy problems. Despite having no direct control over the hardware platform on which the business processes run, clients still need to obtain assurance of correct execution. In this paper, we propose an architecture based on Trusted Computing technologies that allows fine-granular and policy-based remote attestation of outsourced business processes running on remote hosts. In particular, we let the provider generate, during execution of the business process, secure execution logs that allow to verify correct execution of the process at a later time by the client. Our architecture allows a cloud provider to host business processes for multiple tenants, considering at the same time multi-instance processes. We show how such an architecture can be implemented using Trusted Computing technologies, traditional virtualization technologies like Xen and the ODE process engine.
ER  - 

TY  - CONF
JO  - Information Security and Cryptology (ISCISC), 2013 10th International ISC Conference on
TI  - A new method for occupational fraud detection in process aware information systems
T2  - Information Security and Cryptology (ISCISC), 2013 10th International ISC Conference on
IS  - 
SN  - 
VO  - 
SP  - 1
EP  - 5
AU  - Mardani, S.
AU  - Shahriari, H.R.
Y1  - 29-30 Aug. 2013
PY  - 2013
KW  - security of data
KW  - statistical analysis
KW  - occupational fraud detection
KW  - ordinary fraud detection method
KW  - process aware information system
KW  - statistical information
KW  - Classification algorithms
KW  - Companies
KW  - Information systems
KW  - Process control
KW  - Security
KW  - Vectors
KW  - anomaly detection
KW  - fraud detection
KW  - outlier detection
KW  - process aware information systems
KW  - process mining
VL  - 
JA  - Information Security and Cryptology (ISCISC), 2013 10th International ISC Conference on
DO  - 10.1109/ISCISC.2013.6767348
AB  - Today, companies have turned to use fraud detection methods to reduce their financial losses that have been arisen in this way. Thus, Process aware information systems are vulnerable to insider frauds. Flexibility in these systems gives the opportunity for fraudsters to commit illegal activities. Strict security controls on these systems at runtime reduces their flexibility. Moreover, the frequent changes in these systems make inefficient the ordinary fraud detection methods and it remains as a challenge for organizations. In this paper, we propose a new fraud detection method that uses both statistical information about system's log and process model mined from it to detect fraudulent instances. Our method reduces false positive rate and supports loop, parallel and selection structures in processes. The experimental results show effectiveness of the approach as it represents value of more than 0.8 for F-measure.
ER  - 

TY  - CONF
JO  - Southeastcon '89. Proceedings. Energy and Information Technologies in the Southeast., IEEE
TI  - An implementation of a zero-knowledge protocol for a secure network login procedure
T2  - Southeastcon '89. Proceedings. Energy and Information Technologies in the Southeast., IEEE
IS  - 
SN  - 
VO  - 
SP  - 197
EP  - 201 vol.1
AU  - Jaramillo, C.I.
AU  - Richard, G.G.
AU  - Sperry, S.O.
AU  - Patterson, W.
Y1  - 9-12 Apr 1989
PY  - 1989
KW  - operating systems (computers)
KW  - protocols
KW  - security of data
KW  - Sun-3
KW  - implementation
KW  - model 110
KW  - operating system security
KW  - secure network login procedure
KW  - zero-knowledge protocol
KW  - Access protocols
KW  - Arithmetic
KW  - Computer networks
KW  - Information security
KW  - Keyboards
KW  - Local area networks
KW  - Operating systems
KW  - Sun
KW  - TCPIP
KW  - Workstations
VL  - 
JA  - Southeastcon '89. Proceedings. Energy and Information Technologies in the Southeast., IEEE
DO  - 10.1109/SECON.1989.132358
AB  - The concept of a zero-knowledge protocol has led to the consideration of the feasibility of implementing a secure password protocol, one that does not involve the transmission of any information concerning the password during the logon process. The authors have constructed a demonstration of the feasibility of the use of such a model, and describe a number of instances in which the zero-knowledge protocol model could practically be used for greater operating system security. The protocol was executed on a Sun-3, model 110, using a number of different values for the size of the secret and also for the number of iterations. It is shown that values of the pair (log <e1>p</e1>,<e1>n</e1>) approximating (60,35) can run in approximately five minutes on currently available workstations
ER  - 

TY  - JOUR
JO  - Network and Service Management, IEEE Transactions on
TI  - Agent-Based Self-Adaptable Context-Aware Network Vulnerability Assessment
T2  - Network and Service Management, IEEE Transactions on
IS  - 3
SN  - 1932-4537
VO  - 10
SP  - 255
EP  - 270
AU  - Jiang, F.
AU  - Daoyi Dong
AU  - Longbing Cao
AU  - Frater, M.R.
Y1  - September 2013
PY  - 2013
KW  - file organisation
KW  - multi-agent systems
KW  - security of data
KW  - service-oriented architecture
KW  - ubiquitous computing
KW  - agent based self-adaptable context aware network vulnerability assessment
KW  - computer security
KW  - dynamic reconfiguration
KW  - file access
KW  - file rewritability
KW  - human immune system
KW  - information theoretic analysis
KW  - next generation service oriented network operation system
KW  - Computer security
KW  - Intrusion detection
KW  - Management information systems
KW  - Measurement uncertainty
KW  - Multi-agent systems
KW  - Next generation networking
KW  - Service-oriented architecture
KW  - Vulnerability assessment
KW  - agent-based system
KW  - intrusion detection system (IDS)
KW  - management information base (MIB)
KW  - threats awareness analysis
VL  - 10
JA  - Network and Service Management, IEEE Transactions on
DO  - 10.1109/TNSM.2013.090313.120388
AB  - Immunology inspired computer security has attracted enormous attention as its potential impacts on the next generation service-oriented network operation system. In this paper, we propose a new agent-based threat awareness assessment strategy inspired by the human immune system to dynamically adapt against attacks. Specifically, this approach is based on the dynamic reconfiguration of the file access right for system calls or logs (e.g., file rewritability) with balanced adaptability and vulnerability. Based on an information-theoretic analysis on the coherently associations of adaptability, autonomy as well as vulnerability, a generic solution is suggested to break down their coherent links. The principle is to maximize context-situation awared systems' adaptability and reduce systems' vulnerability simultaneously. Experimental results show the efficiency of the proposed biological behaviour-inspired vulnerability awareness system.
ER  - 

TY  - JOUR
JO  - Network, IEEE
TI  - Flow-net methodology for accountability in wireless networks
T2  - Network, IEEE
IS  - 5
SN  - 0890-8044
VO  - 23
SP  - 30
EP  - 37
AU  - Yang Xiao
Y1  - September 2009
PY  - 2009
KW  - access protocols
KW  - computer network management
KW  - mobile computing
KW  - security of data
KW  - telecommunication network routing
KW  - computing infrastructure
KW  - flow net methodology
KW  - forensic purposes
KW  - intrusion detection purposes
KW  - media access control
KW  - network infrastructure
KW  - routing layers
KW  - traffic data collection
KW  - wireless network accountability
KW  - Communication system traffic control
KW  - Computer crime
KW  - Computer networks
KW  - Forensics
KW  - Information security
KW  - Intrusion detection
KW  - Media Access Protocol
KW  - Reconnaissance
KW  - Routing
KW  - Wireless networks
VL  - 23
JA  - Network, IEEE
DO  - 10.1109/MNET.2009.5274919
AB  - Accountability implies that any entity should be held responsible for its own specific action or behavior so that the entity is part of larger chains of accountability. One of the goals of accountability is that once an event has transpired, the events that took place are traceable so that the causes can be determined afterward. The poor accountability provided by today's computers and networks wastes a great deal of money and effort; examples include activities to identify whether a system is under reconnaissance or attack, and the difficulties of distinguishing legitimate emails from phishing attacks. This is due to the simple fact that today's computing and network infrastructure was not built with accountability in mind. In this article we propose a novel methodology called flow-net for accountability. We apply this methodology to media access control and routing layers in wireless networks. We then compare the performance of flow-net with audit log files. This article presents a novel approach for traffic data collection that can also be used for forensics and intrusion detection purposes.
ER  - 

TY  - CONF
JO  - Hot Topics in Operating Systems, 2001. Proceedings of the Eighth Workshop on
TI  - Don't trust your file server
T2  - Hot Topics in Operating Systems, 2001. Proceedings of the Eighth Workshop on
IS  - 
SN  - 
VO  - 
SP  - 113
EP  - 118
AU  - Mazires, D.
AU  - Shasha, D.
Y1  - 20-22 May 2001
PY  - 2001
KW  - file servers
KW  - security of data
KW  - SUNDR network file system
KW  - collocation servers
KW  - data management
KW  - data storage
KW  - network connectivity
KW  - outsourcing
KW  - untrusted servers
KW  - Computer science
KW  - Computer security
KW  - Data security
KW  - Data warehouses
KW  - Engineering profession
KW  - File servers
KW  - File systems
KW  - Memory
KW  - Network servers
KW  - Protection
VL  - 
JA  - Hot Topics in Operating Systems, 2001. Proceedings of the Eighth Workshop on
DO  - 10.1109/HOTOS.2001.990070
AB  - All too often, decisions about whom to trust in computer systems are driven by the needs of system management rather than data security. In particular data storage is often entrusted to people who have no role in creating or using the data-through outsourcing of data management, hiring of outside consultants to administer servers, or even collocation servers in physically insecure machine rooms to gain better network, connectivity. This paper outlines the design of SUNDR, a network file system designed to run on untrusted servers. SUNDR servers can safely be managed by people who have no permission to read or write data stored in the file system. Thus, people can base their trust decisions on who needs to use data and their administrative decisions on how best to manage the data. Moreover, with SUNDR, attackers will no longer be able to wreak havoc by compromising servers and tampering with data. They will need to compromise clients while legitimate users are logged on. Since clients do not need to accept incoming network connections, they can more easily be firewalled and protected from compromise than servers.
ER  - 

TY  - CONF
JO  - Network Operations and Management Symposium (NOMS), 2012 IEEE
TI  - Privileged identity management in enterprise service-hosting environments
T2  - Network Operations and Management Symposium (NOMS), 2012 IEEE
IS  - 
SN  - 1542-1201
VO  - 
SP  - 736
EP  - 749
AU  - Bhaskaran, K.
AU  - Hernandez, M.
AU  - Laredo, J.
AU  - Luan, L.
AU  - Yaoping Ruan
AU  - Vukovic, M.
AU  - Driscoll, P.
AU  - Miller, D.
AU  - Skinner, A.
AU  - Verma, G.
AU  - Vivekanandan, P.
AU  - Chen, L.
AU  - Gaskill, G.
Y1  - 16-20 April 2012
PY  - 2012
KW  - business data processing
KW  - cloud computing
KW  - security of data
KW  - IAM
KW  - ID scheme
KW  - cloud
KW  - compliance policies
KW  - cost savings
KW  - enterprise service-hosting environments
KW  - identity and access managment
KW  - lifecycle management activities
KW  - privileged identity management
KW  - security compliance readiness platform
KW  - Business
KW  - Cloud computing
KW  - Computer architecture
KW  - Radio access networks
KW  - Security
KW  - Servers
VL  - 
JA  - Network Operations and Management Symposium (NOMS), 2012 IEEE
DO  - 10.1109/NOMS.2012.6211991
AB  - IAM needs will only grow as devices, servers, and end points continue to increase . Current schemes are not sustainable as the number of IDs will explode. Environment is heterogeneous, and constantly adding new systems including Cloud. Our solution offers a platform where a user gets an individual user ID on a system - but only if they need it, when they need it, for only as long as they need it . Reusable ID scheme reduces the number of IDs in the system yielding cost savings on lifecycle management activities, improved security compliance . A compliance readiness platform can be enabled to prevent, flag, or monitor questionable access in or near real-time . Provide easily accessible logs to prove compliance policies.
ER  - 

TY  - CONF
JO  - Pervasive Health Conference and Workshops, 2006
TI  - Privacy Preserving Trust Negotiation for Pervasive Healthcare
T2  - Pervasive Health Conference and Workshops, 2006
IS  - 
SN  - 
VO  - 
SP  - 1
EP  - 9
AU  - Changyu Dong
AU  - Dulay, N.
Y1  - Nov. 29 2006-Dec. 1 2006
PY  - 2006
KW  - Java
KW  - computerised monitoring
KW  - health care
KW  - mobile computing
KW  - patient monitoring
KW  - protocols
KW  - security of data
KW  - ETTG
KW  - J2ME platforms
KW  - Java
KW  - access-control policies
KW  - disclosure policies
KW  - extended trust target graph protocol
KW  - patient monitoring
KW  - pervasive healthcare systems
KW  - privacy preserving trust negotiation
KW  - Access protocols
KW  - Biomedical monitoring
KW  - Java
KW  - Medical services
KW  - Mobile handsets
KW  - Patient monitoring
KW  - Privacy
KW  - Remote monitoring
KW  - Security
KW  - Sensor systems
VL  - 
JA  - Pervasive Health Conference and Workshops, 2006
DO  - 10.1109/PCTHEALTH.2006.361626
AB  - Healthcare systems are being extended to monitor patients with body sensors wirelessly linked to a mobile phone that interacts with remote healthcare services and staff. As such systems become more widespread, with multiple healthcare providers and security domains, the establishment of trust between users, providers and medical staff will become important. In this paper we implement the ETTG privacy-preserving trust negotiation protocol and show how it can be used to automatically establish mutual trust between interacting parties in compliance with access-control and disclosure policies. The protocol is implemented in Java and can be run on J2ME platforms. The trust negotiation steps are logged and the resulting trust graphs can be visualised to show how policy compliance was achieved. We also develop a new easier-to-understand syntax for ETTG and use it to define access-control and disclosure policies for a small pervasive healthcare scenario
ER  - 

TY  - CONF
JO  - Dependable Systems and Networks Workshops (DSN-W), 2012 IEEE/IFIP 42nd International Conference on
TI  - An independent verification of errors and vulnerabilities in SaaS cloud
T2  - Dependable Systems and Networks Workshops (DSN-W), 2012 IEEE/IFIP 42nd International Conference on
IS  - 
SN  - 
VO  - 
SP  - 1
EP  - 6
AU  - Ganesan, R.
AU  - Sarkar, S.
AU  - Tewari, N.
Y1  - 25-28 June 2012
PY  - 2012
KW  - cloud computing
KW  - formal verification
KW  - security of data
KW  - virtualisation
KW  - SaaS public cloud
KW  - SaaS subscriber
KW  - URL
KW  - iCirrus-Val tool
KW  - independent analysis
KW  - independent verification
KW  - pay-per-use model
KW  - physical infrastructure
KW  - software-as-a-service
KW  - virtualization technology
KW  - Cloud computing
KW  - Organizations
KW  - Security
KW  - Standards organizations
KW  - Transient analysis
KW  - Web servers
KW  - SaaS
KW  - permanent and transient error
KW  - vulnerability
VL  - 
JA  - Dependable Systems and Networks Workshops (DSN-W), 2012 IEEE/IFIP 42nd International Conference on
DO  - 10.1109/DSNW.2012.6264695
AB  - Software-as-a-Service (SaaS) offers immense advantages to a subscriber, as the SaaS subscriber pays for the amount of service he has consumed. This pay-per-use model offered by the SaaS provider is supported by the underlying virtualization technology, which allows sharing of the physical infrastructure among several clients who subscribe to the SaaS cloud to optimize the cost of usage. However, with this cost-benefit come several risks related to reliability, security and availability (RAS). Consequently, a potential subscriber of a SaaS offering wants to perform several reviews and validations related to RAS before the subscription. In fact, the subscriber often prefers an independent validation of these QoS aspects, as an on-going basis. In this work, we propose a validation methodology and a tool iCirrus-Val for a SaaS subscriber, to perform an independent analysis and verification of functional errors and vulnerabilities of a SaaS cloud from its weblogs. iCirrus-Val groups the logged URLs into whitelist and suspect categories. A whitelisted URL belonging to a business process, is analyzed for permanent and transient faults. A suspect URL is further analyzed to check if it falls into an &#x201C;attempt to an attack category&#x201D;. Our approach is lightweight and does not require data from other parts of the system that is typically unavailable to a SaaS subscriber. It is restricted to a study of RAS from the subscribers entry point. However, we believe that our approach has a potential to identify a large number of the vulnerabilities. Our belief, though not empirically validated here, rests upon recent research findings which indicate that vulnerabilities are increasingly targeted at the entry point such as the web server, as attackers find it difficult to hack the core server.We illustrate our approach using a real-life data of an organization that has adopted a SaaS public cloud.
ER  - 

TY  - CONF
JO  - Cyber-Physical Systems, Networks, and Applications (CPSNA), 2013 IEEE 1st International Conference on
TI  - Defending malicious attacks in Cyber Physical Systems
T2  - Cyber-Physical Systems, Networks, and Applications (CPSNA), 2013 IEEE 1st International Conference on
IS  - 
SN  - 
VO  - 
SP  - 13
EP  - 18
AU  - Chia-Mei Chen
AU  - Han-Wei Hsiao
AU  - Peng-Yu Yang
AU  - Ya-Hui Ou
Y1  - 19-20 Aug. 2013
PY  - 2013
KW  - Internet
KW  - hidden Markov models
KW  - security of data
KW  - Internet attacks
KW  - access information
KW  - appropriate defense mechanism
KW  - communication channel
KW  - control system
KW  - cyber physical systems
KW  - defending malicious attacks
KW  - detection rules
KW  - hidden Markov chain
KW  - industrial control systems
KW  - multiple attack stages
KW  - multistage attacks
KW  - target system
KW  - unauthorized access
KW  - Control systems
KW  - Correlation
KW  - Hidden Markov models
KW  - Joints
KW  - Monitoring
KW  - Security
KW  - Training
KW  - Hidden Markov Model
KW  - cloud computing
KW  - intrusion detection
VL  - 
JA  - Cyber-Physical Systems, Networks, and Applications (CPSNA), 2013 IEEE 1st International Conference on
DO  - 10.1109/CPSNA.2013.6614240
AB  - An increasing number of security incidents on industrial control systems drew a lot of concerns lately. Many attacks involved multiple attack vectors similar to internet attacks. However, CPS are more vulnerable to attacks. To evade detection, a hacker may apply multiple attack stages to gain the access to a control system. For example, he first employs a group of zombies (compromised machines) to identify the vulnerabilities of the target system and the findings would send back to the hacker through a communication channel. Once the correct access information is found by the zombies, the hacker could gain unauthorized access without violating any detection rules. The control system may be compromised by such multi-stage attacks and an appropriate defense mechanism is desired. In order to detect the sequence of such attack, this study correlates network information and system logs to find the stages of the attack. Finite state model, hidden Markov chain, is adopted to identify the multi-stage attacks and to prevent real damage. The results show that the proposed system can identify the multi-stage attacks in the early stage efficiently to prevent further damage in the networks.
ER  - 

TY  - CONF
JO  - Networked Computing and Advanced Information Management (NCM), 2010 Sixth International Conference on
TI  - [Back cover]
T2  - Networked Computing and Advanced Information Management (NCM), 2010 Sixth International Conference on
IS  - 
SN  - 
VO  - 
SP  - c4
EP  - c4
Y1  - 16-18 Aug. 2010
PY  - 2010
KW  - Kalman filters
KW  - Web sites
KW  - business data processing
KW  - computer aided instruction
KW  - data mining
KW  - data warehouses
KW  - discrete event simulation
KW  - distance learning
KW  - distributed processing
KW  - image processing
KW  - ontologies (artificial intelligence)
KW  - security of data
KW  - software architecture
KW  - sorting
KW  - telecommunication network routing
KW  - wireless sensor networks
KW  - Bayesian network
KW  - Hilbert transform
KW  - IP transition phase
KW  - Internet
KW  - OpenMP
KW  - VoIP network forensic analysis
KW  - WLAN
KW  - Wordnet ontology
KW  - ad hoc network
KW  - artificial neural network
KW  - automatic tongue diagnosis
KW  - backup sip server system
KW  - billing system
KW  - business process management
KW  - computer assisted audit technique
KW  - context sensitiveness
KW  - customer relationship management
KW  - data mining
KW  - data sharing
KW  - decentralized e-business
KW  - distance education
KW  - distributed data warehouse
KW  - distributed scheduling
KW  - extended Kalman filter
KW  - fault detection
KW  - fingerprint identification
KW  - game based digital learning
KW  - geospatial image
KW  - healthcare information system
KW  - image processing
KW  - induction motor
KW  - information security
KW  - ipv6 network
KW  - kernel support vector machine
KW  - large scale enterprise
KW  - logistics
KW  - mail sorting machine
KW  - mobile IP tunneling mechanism
KW  - mobility management
KW  - neighbor aware dynamic backoff algorithm
KW  - network layer topology
KW  - optimization
KW  - parallel discrete event simulation
KW  - peer-to-peer streaming
KW  - portal based Web site
KW  - principal component analysis
KW  - raid system
KW  - risk assessment method
KW  - rough set model
KW  - service oriented architecture
KW  - smart home
KW  - software reverse engineering
KW  - stock market
KW  - vehicle routing
KW  - video conference transmission security
KW  - wireless sensor network
VL  - 
JA  - Networked Computing and Advanced Information Management (NCM), 2010 Sixth International Conference on
DO  - 
AB  - The following topics are dealt with: distributed scheduling approach; backup sip server system; raid system; fault detection; neighbor-aware dynamic backoff algorithm; ad hoc networks; IP transition phase; data sharing; large scale enterprise; data mining; distributed data warehouses; parallel discrete event simulation; peer-to-peer streaming; OpenMP; service oriented architecture; business process management; portal-based Web sites; decentralized e-business; logistics; software reverse engineering; smart home; induction motors; extended Kalman filter; network layer topology; optimization problem; wireless sensor networks; distance education; mobile IP tunneling mechanism; semantic Web services; ipv6 network; computer-assisted audit techniques; rough set model; accounting information systems; VoIP network forensic analysis; tunneling protocol; mobile network; mobility management; Internet; kernel support vector machines; interoperability; Bayesian networks; information security; risk assessment method; billing system; vehicle routing; context sensitiveness; Hilbert transform; artificial neural networks; healthcare information system; video conference transmission security; WLAN; customer relationship management; stock markets; geospatial image; mail sorting machine; principal component analysis; fingerprint identification; automatic tongue diagnosis; Wordnet ontology; game-based digital learning; image processing.
ER  - 

TY  - CONF
JO  - Geoinformatics, 2010 18th International Conference on
TI  - Research of multi-level data exchange mechanism based on Land and Resource E-government
T2  - Geoinformatics, 2010 18th International Conference on
IS  - 
SN  - 
VO  - 
SP  - 1
EP  - 5
AU  - Shiwu Xu
AU  - Yilang Zhao
AU  - Feng Zhu
Y1  - 18-20 June 2010
PY  - 2010
KW  - XML
KW  - electronic data interchange
KW  - extranets
KW  - intranets
KW  - public administration
KW  - research and development
KW  - security of data
KW  - ConfigurationXML document
KW  - DataXML document
KW  - Hubei Province Land
KW  - XML technology
KW  - data security
KW  - data storage module
KW  - extranet
KW  - government affairs information
KW  - intranet
KW  - land e-government
KW  - multilevel data exchange mechanism
KW  - network service
KW  - resource e-government
KW  - spatial data
KW  - Databases
KW  - Electronic government
KW  - Extranets
KW  - Memory
KW  - Middleware
KW  - XML
KW  - XML
KW  - data exchange mechanism
KW  - intranet and extranet
KW  - middleware indication
KW  - representation of data
VL  - 
JA  - Geoinformatics, 2010 18th International Conference on
DO  - 10.1109/GEOINFORMATICS.2010.5568021
AB  - Multi-level data on land and resource plays an increasingly important role in the development of Land and Resource E-government. In view of data security, there must be physical isolation between intranet and extranet of Land and Resource E-government of governments at all levels, therefore, it reduces officials' efficiency to a certain extent, especially for those on a business trip. Multi-level data exchange mechanism aiming at sharing of data between intranet and extranet is strongly demanded as the rapid development of network service. This paper divides multi-level data on land and resource into two categories: spatial data and government affairs information. We concern with data exchange mechanism mainly in the respect of government affairs information which is considered as an absolute solution to improvement of officials' efficiency. This paper expounds that the design philosophy of data exchange mechanism based on XML technology is representation of data into XML document. The purpose of the research is to establish data exchange mechanism not only sharing of government affairs information between intranet and extranet but also ensuring data security of Land and Resource E-government. After deeply studying data exchange theories, we propose a development plan based on middleware including three functional modules: data extraction module, middleware indication module indicated by ConfigurationXML document and DataXML document and data storage module. In this paper, data exchange mechanism has been applied in Hubei Province Land and Resource E-government System which is proved in practice as a kind of effective assistance and polish to the system mainly in terms of processing business. In addition, data exchange mechanism used digital certificates technology and the method of tracking security log to assure data security.
ER  - 

TY  - CONF
JO  - Applications and the Internet (SAINT) Workshops, 2002. Proceedings. 2002 Symposium on
TI  - XML digital signature system independent of existing applications
T2  - Applications and the Internet (SAINT) Workshops, 2002. Proceedings. 2002 Symposium on
IS  - 
SN  - 
VO  - 
SP  - 150
EP  - 157
AU  - Takase, T.
AU  - Uramoto, N.
AU  - Baba, K.
Y1  - 2002
PY  - 2002
KW  - Internet
KW  - hypermedia markup languages
KW  - message authentication
KW  - security of data
KW  - Web service
KW  - XML Security Services Suite
KW  - XML digital signature system
KW  - content-based key selection
KW  - inbound messages
KW  - outbound messages
KW  - security-related Web services
KW  - signature proxy
KW  - Access control
KW  - Application software
KW  - Digital signatures
KW  - Laboratories
KW  - Law
KW  - Network servers
KW  - Watches
KW  - Web server
KW  - Web services
KW  - XML
VL  - 
JA  - Applications and the Internet (SAINT) Workshops, 2002. Proceedings. 2002 Symposium on
DO  - 10.1109/SAINTW.2002.994565
AB  - This paper describes a turnkey solution to add a XML digital signature capability without modifying existing XML-based B2B systems. The signature proxy between applications watches for XML messages exchanged on the network. Outbound messages are received by the proxy and automatically signed and by a signature server implemented as a Web service. Inbound messages are also verified by using the proxy and the signature server The existing applications do not care about handling of digital signatures. The signature server can also provide (1) content-based key selection and (2) logging of signed documents with fine-grain access control. The system introduced in this paper is called the XML Security Services Suite (XS-Cube), a set of security-related Web services including digital signatures
ER  - 

TY  - CONF
JO  - Advanced Cloud and Big Data (CBD), 2013 International Conference on
TI  - Pushing the Limits in Event Normalisation to Improve Attack Detection in IDS/SIEM Systems
T2  - Advanced Cloud and Big Data (CBD), 2013 International Conference on
IS  - 
SN  - 
VO  - 
SP  - 69
EP  - 76
AU  - Azodi, A.
AU  - Jaeger, D.
AU  - Feng Cheng
AU  - Meinel, C.
Y1  - 13-15 Dec. 2013
PY  - 2013
KW  - security of data
KW  - CEE
KW  - IDS-SIEM systems
KW  - IT systems
KW  - attack detection
KW  - common event expression standard
KW  - correlation based intrusion detection systems
KW  - event normalisation
KW  - log formats
KW  - query
KW  - security information and event management systems
KW  - Data mining
KW  - Databases
KW  - Intrusion detection
KW  - Servers
KW  - Software
KW  - Standards
KW  - Event Management
KW  - Event Normalisation
KW  - Intrusion Detection
KW  - Knowledge base
VL  - 
JA  - Advanced Cloud and Big Data (CBD), 2013 International Conference on
DO  - 10.1109/CBD.2013.27
AB  - The current state of affairs regarding the way events are logged by IT systems is the source of many problems for the developers of Intrusion Detection Systems (IDS) and Security Information and Event Management (SIEM) systems. These problems stand in the way of the development of more accurate security solutions that draw their results from the data included within the logs they process. This is mainly caused by a lack of standards that can encapsulate all events in a coherent way. As a result, correlating between logs produced by different systems that use different log formats has been difficult and infeasible in many cases. In order to solve the challenges faced by Correlation Based Intrusion Detection Systems, we provide a platform for normalising events1 into a unified super event loosely based on the Common Event Expression standard (CEE) developed by the Mitre corporation. We show how our solution is able to normalise seemingly unrelated events into a unified format. Additionally, we demonstrate queries that can detect attacks on collections of normalised logs from different sources.
ER  - 

TY  - CONF
JO  - Computational Aspects of Social Networks (CASoN), 2011 International Conference on
TI  - A complete operational architecture of alert correlation
T2  - Computational Aspects of Social Networks (CASoN), 2011 International Conference on
IS  - 
SN  - 
VO  - 
SP  - 243
EP  - 248
AU  - Amiri, F.
AU  - Gharaee, H.
AU  - Enayati, A.R.
Y1  - 19-21 Oct. 2011
PY  - 2011
KW  - security of data
KW  - alert correlation architecture
KW  - alert correlation engine
KW  - anomaly-based analysis
KW  - incident response
KW  - intrusion detection systems
KW  - knowledge base system
KW  - log management
KW  - operational architecture
KW  - security threats
KW  - Computer architecture
KW  - Correlation
KW  - Intrusion detection
KW  - Knowledge based systems
KW  - Sensor phenomena and characterization
KW  - alert correlation
KW  - anomaly-base analysis
KW  - correlation techniques
KW  - knowledge base
VL  - 
JA  - Computational Aspects of Social Networks (CASoN), 2011 International Conference on
DO  - 10.1109/CASON.2011.6085952
AB  - To defend against various attacks, many security systems such as intrusion detection systems are deployed into hosts and networks to better protect digital assets. However, there are well-known problems related to the current intrusion detection systems. To better understand security threats from various sources and take appropriate response, it is necessary to perform alert correlation. This paper proposes a general alert correlation architecture, including four important components: log management, alert correlation, incident response and knowledge base system. The focus is to describe most important operations in alert correlation component. The proposed architecture includes anomaly-based analysis in alert correlation component. Different techniques for alert correlation are reviewed and compared. This study proposes that a hybrid model of multiple techniques leads to better performance of alert correlation engine.
ER  - 

TY  - CONF
JO  - Mobile Data Management (MDM), 2013 IEEE 14th International Conference on
TI  - A Novel Mobile Framework for Anonymity Techniques and Services Research
T2  - Mobile Data Management (MDM), 2013 IEEE 14th International Conference on
IS  - 
SN  - 
VO  - 1
SP  - 353
EP  - 355
AU  - Sakkopoulos, E.
AU  - Mersini, P.
AU  - Tsakalidis, A.
AU  - Sioutas, S.
AU  - Verykios, V.
Y1  - 3-6 June 2013
PY  - 2013
KW  - data privacy
KW  - mobile computing
KW  - security of data
KW  - smart phones
KW  - K-anonymity
KW  - anonymity techniques
KW  - location-based services
KW  - mobile devices
KW  - mobile framework
KW  - positioning capabilities
KW  - privacy
KW  - real-life environment
KW  - real-life logging dataset
KW  - security
KW  - services research
KW  - smartphones
KW  - Algorithm design and analysis
KW  - Conferences
KW  - Internet
KW  - Mobile communication
KW  - Mobile handsets
KW  - Privacy
KW  - Servers
KW  - Big-data management for mobile applications
KW  - anonymity
KW  - experimentation
KW  - mobile applications
KW  - privacy
VL  - 1
JA  - Mobile Data Management (MDM), 2013 IEEE 14th International Conference on
DO  - 10.1109/MDM.2013.55
AB  - Positioning capabilities offered in modern mobile devices enable usage of location-based services. Privacy and security is of great importance for related applications. We present a framework that allows conducting research on anonymity techniques in a real-life environment using smartphones. The proposed solution also includes logging mechanisms that facilitate positioning research dataset development in open format. To present the capabilities of the solution, we deliver the concept of K-anonymity to protect mobile users that issue queries to location-based services. Experimental evaluation of the solution includes development of real-life logging dataset using smartphones by volunteers. Different flavours of anonymity algorithms are easy to be included and tested. The solution has received encouraging feedback and successfully assists the researchers of location based services to experiment, validate and develop their techniques in real life environment.
ER  - 

TY  - CONF
JO  - Applications and the Internet (SAINT), 2012 IEEE/IPSJ 12th International Symposium on
TI  - SSH Dictionary Attack Detection Based on Flow Analysis
T2  - Applications and the Internet (SAINT), 2012 IEEE/IPSJ 12th International Symposium on
IS  - 
SN  - 
VO  - 
SP  - 51
EP  - 59
AU  - Satoh, A.
AU  - Nakamura, Y.
AU  - Ikenaga, T.
Y1  - 16-20 July 2012
PY  - 2012
KW  - learning (artificial intelligence)
KW  - security of data
KW  - SANS
KW  - SSH dictionary attack detection
KW  - campus networks
KW  - connection protocol
KW  - flow analysis
KW  - log files
KW  - machine learning algorithms
KW  - network traffic
KW  - security threat
KW  - stealthy attacks
KW  - Authentication
KW  - Dictionaries
KW  - IP networks
KW  - Payloads
KW  - Protocols
KW  - Servers
KW  - flow analysis
KW  - machine learning algorithm
KW  - network operation
KW  - ssh dictionary attack
VL  - 
JA  - Applications and the Internet (SAINT), 2012 IEEE/IPSJ 12th International Symposium on
DO  - 10.1109/SAINT.2012.16
AB  - SSH services are run on many hosts with various scopes other than just operation, so dictionary attack against the service is a common security threat. SANS has reported the emergence of distributed SSH dictionary attacks, which are very stealthy in comparison with a simple one. Since even one success of such an attack causes serious problems, administrators should implement countermeasures. SSH dictionary attacks have been detected in two basic ways that rely on either log files or network traffic. Both approaches, however, have limitations. The first approach imposes upon administrators heavy maintenance costs, which grow linearly with the number of hosts in networks. The second approach cannot distinguish between successful and unsuccessful attacks. Of more immediate concern, neither approach is effective against stealthy attacks because the login attempts of these attacks have little impact on log files or network traffic. An ideal method would be able to detect individual attacks and distinguish between an attack's success or failure, using information derived from only network traffic. In this paper, we describe such a method, which was developed by combining two novel elements. First, on the basis of our assumptions, we use two criteria: "existence of a connection protocol" and "difference in the inter-arrival time of an auth-packet". These criteria are not available, though, owing to the confidentiality and flexibility of the SSH protocol. Second, we resolve this problem by identifying transition points of a sub-protocol through flow features and machine learning algorithms. We evaluate the effectiveness of the proposed method through experiments on real traffic traces collected at the edge in our campus networks. The experimental results are encouraging for this research direction, though they are derived from reduced datasets of SSH dictionary attacks and under simplifying assumptions. The significant contribution is the demonstration that an ideal metho- for detecting SSH dictionary attacks seems feasible.
ER  - 

TY  - CONF
JO  - Computational Intelligence in Cyber Security, 2009. CICS '09. IEEE Symposium on
TI  - A self-organizing map and its modeling for discovering malignant network traffic
T2  - Computational Intelligence in Cyber Security, 2009. CICS '09. IEEE Symposium on
IS  - 
SN  - 
VO  - 
SP  - 122
EP  - 129
AU  - Langin, C.
AU  - Hongbo Zhou
AU  - Rahimi, S.
AU  - Gupta, B.
AU  - Zargham, M.
AU  - Sayeh, M.R.
Y1  - March 30 2009-April 2 2009
PY  - 2009
KW  - Internet
KW  - data mining
KW  - peer-to-peer computing
KW  - security of data
KW  - telecommunication traffic
KW  - P2P botnet traffic
KW  - denied Internet firewall log entries
KW  - knowledge discovery
KW  - malignant network traffic
KW  - model-based intrusion detection
KW  - self-organizing map
KW  - Cancer
KW  - Cryptography
KW  - Internet
KW  - Intrusion detection
KW  - Military computing
KW  - Peer to peer computing
KW  - Protocols
KW  - Relays
KW  - Telecommunication traffic
KW  - Traffic control
VL  - 
JA  - Computational Intelligence in Cyber Security, 2009. CICS '09. IEEE Symposium on
DO  - 10.1109/CICYBS.2009.4925099
AB  - Model-based intrusion detection and knowledge discovery are combined to cluster and classify P2P botnet traffic and other malignant network activity by using a self-organizing map (som) self-trained on denied Internet firewall log entries. The SOM analyzed new firewall log entries in a case study to classify similar network activity, and discovered previously unknown local P2P bot traffic and other security issues.
ER  - 

TY  - CONF
JO  - Computer-Based Medical Systems (CBMS), 2013 IEEE 26th International Symposium on
TI  - A process mining analysis on a Virtual Electronic Patient Record system
T2  - Computer-Based Medical Systems (CBMS), 2013 IEEE 26th International Symposium on
IS  - 
SN  - 
VO  - 
SP  - 554
EP  - 555
AU  - Rebuge, A.
AU  - Velez Lapao, L.
AU  - Freitas, A.
AU  - Cruz-Correia, R.
Y1  - 20-22 June 2013
PY  - 2013
KW  - data mining
KW  - health care
KW  - medical information systems
KW  - security of data
KW  - time series
KW  - virtual reality
KW  - Central Hospital
KW  - VEPR
KW  - Virtual Electronic Patient Record system
KW  - event logs
KW  - global behavior characterization
KW  - global behavior discovery
KW  - healthcare processes
KW  - information extraction
KW  - log out
KW  - process mining analysis
KW  - security problem
KW  - task profile execution
KW  - technical analyses
KW  - time series analysis
KW  - user activity
KW  - Business
KW  - Data mining
KW  - Hospitals
KW  - Information systems
KW  - Inspection
KW  - Time series analysis
VL  - 
JA  - Computer-Based Medical Systems (CBMS), 2013 IEEE 26th International Symposium on
DO  - 10.1109/CBMS.2013.6627874
AB  - Process mining can be used to extract healthcare processes related information from event logs by performing analysis exploiting the information recorded in it. We report a process mining analysis made to an event log containing traces on user activity recorded by a Virtual Electronic Patient Record (VEPR) system of a Central Hospital. A set of technical analyses were performed. Results from the discovery and characterization of global behavior and from a time series analysis on observed user tasks are reported. Process mining was applied successfully to discover, characterize and analyze user behavior recorded from VEPR. Worth noting the execution of tasks profile observed after log out, revealing significant security problems.
ER  - 

TY  - CONF
JO  - Future Networks, 2010. ICFN '10. Second International Conference on
TI  - Behavioural Correlation for Detecting P2P Bots
T2  - Future Networks, 2010. ICFN '10. Second International Conference on
IS  - 
SN  - 
VO  - 
SP  - 323
EP  - 327
AU  - Al-Hammadi, Y.
AU  - Aickelin, U.
Y1  - 22-24 Jan. 2010
PY  - 2010
KW  - peer-to-peer computing
KW  - protocols
KW  - security of data
KW  - IRC bots
KW  - Internet security
KW  - P2P Bots detection
KW  - behavioural correlation
KW  - distributed denial of services attacks
KW  - keystrokes logging
KW  - malicious programs
KW  - peer to peer protocols
KW  - spamming
KW  - traffic sniffing
KW  - Command and control systems
KW  - Computer science
KW  - Information analysis
KW  - Information technology
KW  - Internet
KW  - Network servers
KW  - Peer to peer computing
KW  - Protocols
KW  - Viruses (medical)
KW  - Web server
KW  - P2P
KW  - Peacomm
KW  - bot
KW  - botnet
KW  - correlation
VL  - 
JA  - Future Networks, 2010. ICFN '10. Second International Conference on
DO  - 10.1109/ICFN.2010.72
AB  - In the past few years, IRC bots, malicious programs which are remotely controlled by attackers through IRC servers, have become a major threat to the Internet and for users. These bots can be used in different malicious ways such as issuing distributed denial of services attacks to shut down other networks and services, keystrokes logging, spamming, traffic sniffing cause serious disruption on networks and users. New bots use peer to peer (P2P) protocols start to appear as the upcoming threat to Internet security due to the fact that P2P bots do not have a centralized point to shutdown or trace back, thus making the detection of P2P bots is a real challenge. In response to these threats, we present an algorithm to detect an individual P2P bot running on a system by correlating its activities. Our evaluation shows that correlating different activities generated by P2P bots within a specified time period can detect these kind of bots.
ER  - 

TY  - CONF
JO  - Security Technology, 2007 41st Annual IEEE International Carnahan Conference on
TI  - Indirect Human Computer Interaction-Based Biometrics for Intrusion Detection Systems
T2  - Security Technology, 2007 41st Annual IEEE International Carnahan Conference on
IS  - 
SN  - 
VO  - 
SP  - 138
EP  - 145
AU  - Yampolskiy, R.V.
Y1  - 8-11 Oct. 2007
PY  - 2007
KW  - biometrics (access control)
KW  - human computer interaction
KW  - security of data
KW  - GUI interaction events
KW  - audit logs
KW  - biometrics
KW  - call-stack data
KW  - computer software
KW  - indirect human computer interaction
KW  - intrusion detection systems
KW  - network traffic
KW  - registry access data
KW  - storage activity
KW  - system calls
KW  - user monitoring
KW  - Application software
KW  - Biometrics
KW  - Computer networks
KW  - Computerized monitoring
KW  - Graphical user interfaces
KW  - Human computer interaction
KW  - Intrusion detection
KW  - Mice
KW  - Muscles
KW  - Telecommunication traffic
KW  - ANN
KW  - HCI. system calls
KW  - IDS
KW  - Indirect biometrics
KW  - network traffic
VL  - 
JA  - Security Technology, 2007 41st Annual IEEE International Carnahan Conference on
DO  - 10.1109/CCST.2007.4373481
AB  - The indirect HCI-based biometrics are events that can be obtained by monitoring users' HCI behavior indirectly via observable low-level actions of computer software, those include records in audit logs, call-stack data, GUI interaction events, network traffic, registry access data, storage activity, and system calls. These low-level events are produced unintentionally by the user during interaction with different software applications during pursuit of some, potentially mischievous, high level goals. This paper concentrates on the review and analysis of indirect human computer interaction-based biometrics frequently used in intrusion detection systems. We conclude with an experimental demonstration of an intrusion detection system based on network traffic analysis as an example of application of indirect human computer interaction-based behavioral biometrics.
ER  - 

TY  - CONF
JO  - Smart Grid Communications (SmartGridComm), 2012 IEEE Third International Conference on
TI  - AMIDS: A multi-sensor energy theft detection framework for advanced metering infrastructures
T2  - Smart Grid Communications (SmartGridComm), 2012 IEEE Third International Conference on
IS  - 
SN  - 
VO  - 
SP  - 354
EP  - 359
AU  - McLaughlin, S.
AU  - Holbert, B.
AU  - Zonouz, S.
AU  - Berthier, R.
Y1  - 5-8 Nov. 2012
PY  - 2012
KW  - metering
KW  - power system security
KW  - security of data
KW  - sensor fusion
KW  - smart power grids
KW  - AMI intrusion detection system
KW  - AMIDS
KW  - advanced metering infrastructures
KW  - analog devices
KW  - computerized smart meters
KW  - cyber events
KW  - data sources
KW  - information fusion
KW  - meter audit logs
KW  - multisensor energy theft detection framework
KW  - smart grid component
KW  - theft-related behavior detection
KW  - Hidden Markov models
KW  - Home appliances
KW  - Intrusion detection
KW  - Load modeling
KW  - Monitoring
KW  - Power measurement
KW  - Sensors
VL  - 
JA  - Smart Grid Communications (SmartGridComm), 2012 IEEE Third International Conference on
DO  - 10.1109/SmartGridComm.2012.6486009
AB  - The advanced metering infrastructure (AMI) is a crucial component of the smart grid, replacing traditional analog devices with computerized smart meters. Smart meters have not only allowed for efficient management of many end-users, but also have made AMI an attractive target for remote exploits and local physical tampering with the end goal of stealing energy. While smart meters posses multiple sensors and data sources that can indicate energy theft, in practice, the individual methods exhibit many false positives. In this paper, we present AMIDS, an AMI intrusion detection system that uses information fusion to combine the sensors and consumption data from a smart meter to more accurately detect energy theft. AMIDS combines meter audit logs of physical and cyber events with consumption data to more accurately model and detect theft-related behavior. Our experimental results on normal and anomalous load profiles show that AMIDS can identify energy theft efforts with high accuracy. Furthermore, AMIDS correctly identified legitimate load profile changes that more elementary analyses classified as malicious.
ER  - 

TY  - CONF
JO  - Future Dependable Distributed Systems, 2009 Software Technologies for
TI  - An Alarm Annunciatior for Domain Administrators
T2  - Future Dependable Distributed Systems, 2009 Software Technologies for
IS  - 
SN  - 
VO  - 
SP  - 54
EP  - 58
AU  - Ikeda, J.
AU  - Akioka, S.
AU  - Muraoka, Y.
Y1  - 17-17 March 2009
PY  - 2009
KW  - information management
KW  - security of data
KW  - systems analysis
KW  - alarm annunciatior
KW  - domain administrator
KW  - large-scale brute-force attack
KW  - personal information management
KW  - security market
VL  - 
JA  - Future Dependable Distributed Systems, 2009 Software Technologies for
DO  - 10.1109/STFSSD.2009.31
AB  - Security market has been expanding steadily as people become more conscious about personal information management. On the other hand, administrators are competing with the flood of logs every day, and chasing for the updates on a new kind of attacks. Given the overwhelming tasks of administrators, it is extremely hard to keep the peace without the help of professionals such as hired system administrators and security analysts. In this paper, we propose an alarm annunciator to analyze system. We constructed the system that detects a large-scale brute-force attack and warns administrating PCs, as a prototype of the system that collects and analyzes logs, and alerts system administrators automatically. The alarm annunciator enables variety of domain administrators to manage their domains and servers with the minimum effort and expense.
ER  - 

TY  - CONF
JO  - Computing and Communication Systems (NCCCS), 2012 National Conference on
TI  - Fingerprint pores extractor
T2  - Computing and Communication Systems (NCCCS), 2012 National Conference on
IS  - 
SN  - 
VO  - 
SP  - 1
EP  - 5
AU  - Mngenge, N.A.
AU  - Nelufule, N.N.
AU  - Nelwamondo, F.V.
AU  - Msimang, N.
Y1  - 21-22 Nov. 2012
PY  - 2012
KW  - Gaussian processes
KW  - feature extraction
KW  - fingerprint identification
KW  - image matching
KW  - image resolution
KW  - security of data
KW  - AFRS pores
KW  - FDR
KW  - Fourier domain
KW  - Laplacian of Gaussian
KW  - LoG
KW  - TDR ranges
KW  - automatic fingerprint recognition systems
KW  - computationally expensive
KW  - distinct databases
KW  - false detection rate
KW  - feature level 3 extraction
KW  - fingerprint image matching
KW  - fingerprint image quality research
KW  - fingerprint pores extractor
KW  - fingerprint resolution standards
KW  - fingerprint scanning devices
KW  - hash working conditions
KW  - minutiae orientation
KW  - minutiae position
KW  - pore extraction techniques
KW  - resolution changes
KW  - security independent
KW  - sweat pores
KW  - true detection rate
KW  - Databases
KW  - Educational institutions
KW  - Equations
KW  - Feature extraction
KW  - Fingerprint recognition
KW  - Laplace equations
KW  - Standards
VL  - 
JA  - Computing and Communication Systems (NCCCS), 2012 National Conference on
DO  - 10.1109/NCCCS.2012.6412980
AB  - Automatic Fingerprint Recognition Systems (AFRSs) rely on minutiae position and orientation within the fingerprint image for matching. Minutiae information is highly accurate provided that the fingerprint image matched is of high quality. However, this is not always the case because of diseases and hash working conditions that affect fingerprints. In order to maintain high level of security independent of varying fingerprint image quality research suggests the use of other fingerprint features to compliment minutiae. These are things like ridge contours, sweat pores, dots, and incipient ridges. Sweat pores have been proven as one of the most distinctive among these feature. Thus in order to improve accuracy of AFRSs pores can be fused with minutiae or used alone. Sweat pores have been less utilized in the past due to constraints imposed by fingerprint scanning devices and resolution standards. Recently, progress has been made on both scanning devices and resolution standards to support the use of pores in AFRSs. However, very few techniques exist for extracting, matching and fusing them with minutiae. Matching and fusion can only be possible if pores are available. Some techniques have been proposed to reliable extract pores. However, existing techniques can only work on one resolution i.e. an algorithm proposed and tested on 500dpi cannot work on 1000dpi without minor modifications because pores size change if resolution changes. In addition, existing pore extraction techniques are computationally expensive. In this paper an algorithm to extract feature level 3 (pores) is proposed. The algorithm uses Laplacian of Gaussian (LoG) in Fourier domain in order to reduce computation. The performance of the proposed algorithm is tested on two distinct databases with different resolutions in order to validate its accuracy. The accuracy of the proposed algorithm is further measured using false detection rate (FDR) and true detection rate (TDR). Results show that FDR ranges - rom 10-35% while TDR ranges from 65-90%.
ER  - 

TY  - CONF
JO  - Reliable Distributed Systems, 1998. Proceedings. Seventeenth IEEE Symposium on
TI  - Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems (Cat. No.98CB36281)
T2  - Reliable Distributed Systems, 1998. Proceedings. Seventeenth IEEE Symposium on
IS  - 
SN  - 1060-9857
VO  - 
SP  - i
EP  - 
Y1  - 20-23 Oct. 1998
PY  - 1998
KW  - Internet
KW  - back-up procedures
KW  - distributed processing
KW  - fault tolerant computing
KW  - multimedia systems
KW  - security of data
KW  - system recovery
KW  - wireless LAN
KW  - Internet
KW  - case studies
KW  - checkpointing
KW  - consensus
KW  - data security
KW  - databases
KW  - distributed software environments
KW  - message logging
KW  - multimedia
KW  - parallel systems
KW  - primary backup
KW  - process groups
KW  - reliable distributed systems
KW  - replicated objects
KW  - wireless systems
VL  - 
JA  - Reliable Distributed Systems, 1998. Proceedings. Seventeenth IEEE Symposium on
DO  - 10.1109/RELDIS.1998.740457
AB  - The following topics were dealt with: message logging; replicated objects; checkpointing; wireless systems; distributed software environments; primary backup; Internet; process groups; case studies; consensus; databases; parallel systems; data security; and multimedia
ER  - 

TY  - CONF
JO  - Signal Processing and Information Technology, 2005. Proceedings of the Fifth IEEE International Symposium on
TI  - Detecting denial of service attacks using emergent self-organizing maps
T2  - Signal Processing and Information Technology, 2005. Proceedings of the Fifth IEEE International Symposium on
IS  - 
SN  - 
VO  - 
SP  - 375
EP  - 380
AU  - Mitrokotsa, A.
AU  - Douligeris, C.
Y1  - 21-21 Dec. 2005
PY  - 2005
KW  - computer networks
KW  - monitoring
KW  - security of data
KW  - self-organising feature maps
KW  - telecommunication security
KW  - telecommunication traffic
KW  - denial of service attacks
KW  - emergent self-organizing maps
KW  - network security
KW  - traffic monitoring
KW  - Computer crime
KW  - Computer networks
KW  - Informatics
KW  - Intrusion detection
KW  - Monitoring
KW  - Neural networks
KW  - Self organizing feature maps
KW  - Telecommunication traffic
KW  - Traffic control
KW  - Visualization
VL  - 
JA  - Signal Processing and Information Technology, 2005. Proceedings of the Fifth IEEE International Symposium on
DO  - 10.1109/ISSPIT.2005.1577126
AB  - Denial of service attacks constitute one of the greatest problem in network security. Monitoring traffic is one of the main techniques used in order to find out the existence of possible outliers in the traffic patterns. In this paper, we propose an approach that detects denial of service attacks using emergent self-organizing maps. The approach is based on classifying "normal" traffic against "abnormal" traffic in the sense of denial of service attacks. The approach permits the automatic classification of events that are contained in logs and visualization of network traffic. Extensive simulations show the effectiveness of this approach compared to previously proposed approaches regarding false alarms and detection probabilities
ER  - 

TY  - CONF
JO  - Computer Systems and Applications (AICCSA), 2010 IEEE/ACS International Conference on
TI  - Data analyzer based on data mining for Honeypot Router
T2  - Computer Systems and Applications (AICCSA), 2010 IEEE/ACS International Conference on
IS  - 
SN  - 
VO  - 
SP  - 1
EP  - 6
AU  - Ghourabi, A.
AU  - Abbes, T.
AU  - Bouhoula, A.
Y1  - 16-19 May 2010
PY  - 2010
KW  - data analysis
KW  - data mining
KW  - security of data
KW  - Honeypot router
KW  - data analyzer
KW  - data mining
KW  - security tool
KW  - Classification algorithms
KW  - Clustering algorithms
KW  - Data analysis
KW  - Data mining
KW  - Feature extraction
KW  - Routing
KW  - Routing protocols
KW  - Clustering
KW  - Honeypot Router
KW  - data analysis
KW  - data mining
KW  - routing attack detection
VL  - 
JA  - Computer Systems and Applications (AICCSA), 2010 IEEE/ACS International Conference on
DO  - 10.1109/AICCSA.2010.5587041
AB  - Honeypot is an effective security tool, which is intended to be attacked and compromised to gain more information about the attacker and his attack techniques. To study these attacks, the honeypot must capture and log large amounts of data which are very difficult to process manually. So, the analysis of these logs has become a very difficult and time consuming task. To resolve this problem, several researchers have proposed the use of data mining techniques in order to classify logged traffic and extract useful information. In this paper, we present a data analysis tool for our Honeypot Router. This tool is based on data mining clustering. The main idea is to extract useful features from data captured by the Honeypot Router. These data will be then clustered by using the DBSCAN clustering algorithm in order to classify the captured packets and extract those that are suspicious. Suspicious packets will be then verified by a human expert. This solution is very useful to detect novel routing attacks.
ER  - 

TY  - JOUR
JO  - Parallel and Distributed Systems, IEEE Transactions on
TI  - A protocol to achieve independence in constant rounds
T2  - Parallel and Distributed Systems, IEEE Transactions on
IS  - 7
SN  - 1045-9219
VO  - 11
SP  - 636
EP  - 647
AU  - Gennaro, R.
Y1  - Jul 2000
PY  - 2000
KW  - fault tolerant computing
KW  - security of data
KW  - transport protocols
KW  - communication protocol
KW  - constant round protocol
KW  - constant rounds
KW  - fault-tolerant distributed computing
KW  - independence
KW  - security
KW  - simultaneous broadcast
KW  - Broadcasting
KW  - Communication networks
KW  - Computational modeling
KW  - Computer networks
KW  - Cryptographic protocols
KW  - Cryptography
KW  - Distributed computing
KW  - Fault tolerance
KW  - Intelligent networks
KW  - Variable structure systems
VL  - 11
JA  - Parallel and Distributed Systems, IEEE Transactions on
DO  - 10.1109/71.877748
AB  - Independence is a fundamental property needed to achieve security in fault-tolerant distributed computing. In practice, distributed communication networks are neither fully synchronous or fully asynchronous, but rather loosely synchronized. By this, we mean that in a communication protocol, messages at a given round may depend on messages from other players at the same round. These possible dependencies among messages create problems if we need n players to announce independently chosen values. This task is called simultaneous broadcast. In this paper, we present the first constant round protocol for simultaneous broadcast in a reasonable computation model (which includes a common shared random string among the players). The protocol is provably secure under general cryptographic assumptions. In the process, we develop a new and stronger formal definition for this problem. Previously known protocols for this task required either O(log n) or expected constant rounds to complete (depending on the computation model considered)
ER  - 

TY  - CONF
JO  - Security and Privacy (SP), 2013 IEEE Symposium on
TI  - Anon-Pass: Practical Anonymous Subscriptions
T2  - Security and Privacy (SP), 2013 IEEE Symposium on
IS  - 
SN  - 1081-6011
VO  - 
SP  - 319
EP  - 333
AU  - Lee, M.Z.
AU  - Dunn, A.M.
AU  - Waters, B.
AU  - Witchel, E.
AU  - Katz, J.
Y1  - 19-22 May 2013
PY  - 2013
KW  - mobile computing
KW  - music
KW  - operating systems (computers)
KW  - security of data
KW  - Android-based subway-pass application
KW  - Anon-Pass application
KW  - Web proxy
KW  - anonymous subscription service
KW  - authentication window
KW  - client latency
KW  - music service
KW  - user login
KW  - Couplings
KW  - Protocols
KW  - Public key
KW  - Servers
KW  - Streaming media
KW  - Subscriptions
KW  - Anonymous Subscriptions
VL  - 
JA  - Security and Privacy (SP), 2013 IEEE Symposium on
DO  - 10.1109/SP.2013.29
AB  - We present the design, security proof, and implementation of an anonymous subscription service. Users register for the service by providing some form of identity, which might or might not be linked to a real-world identity such as a credit card, a web login, or a public key. A user logs on to the system by presenting a credential derived from information received at registration. Each credential allows only a single login in any authentication window, or epoch. Logins are anonymous in the sense that the service cannot distinguish which user is logging in any better than random guessing. This implies unlinkability of a user across different logins. We find that a central tension in an anonymous subscription service is the service provider's desire for a long epoch (to reduce server-side computation) versus users' desire for a short epoch (so they can repeatedly "re-anonymize" their sessions). We balance this tension by having short epochs, but adding an efficient operation for clients who do not need unlinkability to cheaply re-authenticate themselves for the next time period. We measure performance of a research prototype of our protocol that allows an independent service to offer anonymous access to existing services. We implement a music service, an Android-based subway-pass application, and a web proxy, and show that adding anonymity adds minimal client latency and only requires 33 KB of server memory per active user.
ER  - 

TY  - CONF
JO  - Complex Systems (WCCS), 2014 Second World Conference on
TI  - Vertical query-join benchmark in a cloud database environment
T2  - Complex Systems (WCCS), 2014 Second World Conference on
IS  - 
SN  - 
VO  - 
SP  - 581
EP  - 586
AU  - Kohler, J.
AU  - Specht, T.
Y1  - 10-12 Nov. 2014
PY  - 2014
KW  - Big Data
KW  - benchmark testing
KW  - cloud computing
KW  - competitive intelligence
KW  - data protection
KW  - database management systems
KW  - query processing
KW  - security of data
KW  - social networking (online)
KW  - trusted computing
KW  - RAM memory
KW  - SAP HANA
KW  - big data
KW  - business intelligence
KW  - cloud computing
KW  - cloud database environment
KW  - cloud providers
KW  - compliance rules
KW  - data analysis
KW  - data protection
KW  - data security
KW  - data storage
KW  - hardware infrastructures
KW  - heterogeneous data
KW  - in-memory databases
KW  - internal enterprise network
KW  - public cloud
KW  - query times
KW  - sensor data
KW  - social networks
KW  - vertical query-join benchmark
KW  - vertically distributed database tables
KW  - Cloud computing
KW  - Databases
KW  - Instruction sets
KW  - Distributed Cloud Database Performance
KW  - Vertical Join
KW  - Vertical Partitioning
VL  - 
JA  - Complex Systems (WCCS), 2014 Second World Conference on
DO  - 10.1109/ICoCS.2014.7060950
AB  - Nowadays, enterprises across all branches and sectors face a new hype regarding &#x201C;Big Data&#x201D;. Thus, new requirements in the context of Business Intelligence emerge. Big Data demands to process vast amounts of unstructured data from social networks, sensor data, etc. in near real-time. In order to tackle these challenges, current research works aim to develop new ways of data storage and analysis from a database point of view. This is the advent of so-called &#x201C;In-Memory&#x201D; databases (e.g. SAP HANA) that hold entire data volumes in their fast RAM memory and use hard disks only for logging or archiving purposes. Another promising technology with respect to this topic is "Cloud Computing". Storing and analyzing vast amounts of heterogeneous data require appropriate underlying hardware infrastructures. Obtaining such hardware capabilities form external cloud providers is an auspicious way to avoid expensive investments in new hardware. However, using external hardware resources from the public cloud always means that crucial data has to leave the internal enterprise network and enterprises have to trust external providers. Bringing "Big Data" into the cloud, our approach follows the principle of vertically distributed database tables. The main idea is to divide crucial database data and distribute it across different (public and private) cloud providers. Thus, every provider only gets a small part of the data. These individual small parts are worthless without the other parts and enable enterprises to meet their compliance rules concerning data security and protection. So Cloud Computing becomes an interesting alternative to store vast amounts of data. This work evaluates our approach from a performance point of view and presents the corresponding query times with and without vertically partitioned data.
ER  - 

TY  - CONF
JO  - Networking and Distributed Computing (ICNDC), 2011 Second International Conference on
TI  - Semantic P2P Networks: Future Architecture of Cloud Computing
T2  - Networking and Distributed Computing (ICNDC), 2011 Second International Conference on
IS  - 
SN  - 
VO  - 
SP  - 336
EP  - 339
AU  - Huang, Lican
Y1  - 21-24 Sept. 2011
PY  - 2011
KW  - cloud computing
KW  - data privacy
KW  - peer-to-peer computing
KW  - security of data
KW  - trees (mathematics)
KW  - DHT-based structured P2P networks
KW  - VIRGO
KW  - centralized computing modes
KW  - cloud computing
KW  - data privacies
KW  - scalability
KW  - security
KW  - semantic P2P networks
KW  - transparency
KW  - virtual hierarchical tree gird organizations
KW  - Cloud computing
KW  - Computers
KW  - Organizations
KW  - Semantics
KW  - Servers
KW  - Topology
KW  - Virtual groups
KW  - Cloud computing
KW  - VIRGO
KW  - infrastructure
KW  - semantic P2P network
VL  - 
JA  - Networking and Distributed Computing (ICNDC), 2011 Second International Conference on
DO  - 10.1109/ICNDC.2011.72
AB  - When more and more nodes and users are connected in the Internet in the cases of Cloud computing, centralized computing modes will be failed with the limit band width and privacies. P2P technologies may be the future architecture of Cloud computing. This paper introduces a different P2P technology: semantic P2P Network -- Virtual Hierarchical Tree Gird Organizations (VIRGO), which can solve the Cloud computing's problems of transparency, scalability, and security. Other than unstructured and DHT-based structured P2P networks, the nodes of VIRGO are identified as domain names classified by the semantic meaning of roles in the organizations. VIRGO is easy to implement transparency as it have a simple global name schema. It also is scalable with log(N). Due to the data are stored in the users' own computers, VIRGO is easy to keep data privacies.
ER  - 

TY  - CONF
JO  - Technology Management and Emerging Technologies (ISTMET), 2014 International Symposium on
TI  - SQL injection detection and prevention system with raspberry Pi honeypot cluster for trapping attacker
T2  - Technology Management and Emerging Technologies (ISTMET), 2014 International Symposium on
IS  - 
SN  - 
VO  - 
SP  - 163
EP  - 166
AU  - Djanali, S.
AU  - Arunanto, F.X.
AU  - Pratomo, B.A.
AU  - Studiawan, H.
AU  - Nugraha, S.G.
Y1  - 27-29 May 2014
PY  - 2014
KW  - Internet
KW  - SQL
KW  - query processing
KW  - security of data
KW  - Raspberry Pi honeypot cluster
KW  - SQL injection attacks
KW  - SQL injection detection
KW  - SQL injection prevention system
KW  - SQL query attribute values
KW  - Web application
KW  - Web server
KW  - malicious query attributes
KW  - online banking
KW  - security attack
KW  - Business
KW  - Computers
KW  - Databases
KW  - Ports (Computers)
KW  - Time factors
KW  - Web servers
KW  - Cluster
KW  - Honeypot
KW  - SQL Injection
VL  - 
JA  - Technology Management and Emerging Technologies (ISTMET), 2014 International Symposium on
DO  - 10.1109/ISTMET.2014.6936499
AB  - One of the most common security attack for web application is SQL injection. It is an attack to acquire access to application's database through injection of script or malicious query attributes. This attack can be executed in any page of web application which interacts with database. SQL injection could be more dangerous if the victim was an enterprise system such as online banking. Many methods have been researched and developed to prevent SQL injection attacks. One of them is the use of a honeypot. This paper proposed a method for increasing system's capability to detect and prevent SQL injection attacks based on removal of SQL query attribute values and honeypot for trapping attackers. A honeypot is placed as decoy system to hide actual web server from attacker. Malicious queries from attackers will be sent to honeypot while normal queries will be sent directly to the real web server. Honeypot is also used to provide activity logging of each attack which can be used for further analysis. We play with Raspberry Pi because it is cheap and effective to be used as a honeypot. Due to its limited computational ability, we make cluster to improve its power. Based on conducted experiments, we could achieve up to 64% accuracy of SQL injection attack. Moreover, with the redirection, our honeypot could get more attack data to be analyzed.
ER  - 

TY  - CONF
JO  - Computational Science and Engineering Workshops, 2008. CSEWORKSHOPS '08. 11th IEEE International Conference on
TI  - Applying Computational Grids for Enhancing Intrusion Detection Systems
T2  - Computational Science and Engineering Workshops, 2008. CSEWORKSHOPS '08. 11th IEEE International Conference on
IS  - 
SN  - 
VO  - 
SP  - 149
EP  - 156
AU  - Senger, H.
AU  - Nakahara, J.
Y1  - 16-18 July 2008
PY  - 2008
KW  - grid computing
KW  - groupware
KW  - security of data
KW  - digital library
KW  - grid computing platforms
KW  - grid-based collaborative environment
KW  - intrusion detection systems
KW  - network attacks
KW  - response systems
KW  - security resources
KW  - Application software
KW  - Collaboration
KW  - Collaborative software
KW  - Computer networks
KW  - Distributed computing
KW  - Grid computing
KW  - Intrusion detection
KW  - Manufacturing
KW  - Pervasive computing
KW  - Protection
KW  - grid computing
KW  - intrusion detection
VL  - 
JA  - Computational Science and Engineering Workshops, 2008. CSEWORKSHOPS '08. 11th IEEE International Conference on
DO  - 10.1109/CSEW.2008.70
AB  - This paper proposes the use of grid computing platforms as an enabling technology for the implementation of a pervasive infrastructure which aims at improving the efficiency and effectiveness in the containment of network attacks. First, we identify a set requirements and design principles for the construction of new intrusion detection systems. Then, we present a set of grid capabilities and features which are fundamental for the implementation of new intrusion detection and response systems. As a contribution, we show how a grid-based collaborative environment can be employed in the production, delivery, and use of knowledge and mechanisms for intrusion detection and containment. Such environment provides a laboratory for the development of security resources (signatures, software patches, informative texts, logs, and others), augmented by a digital library capable of supporting the efficient storage, manipulation, and deployment of such resources. As the main result of this work, the efficiency and effectiveness of mechanisms and strategies can be enhanced.
ER  - 

TY  - JOUR
JO  - Selected Areas in Communications, IEEE Journal on
TI  - A Multi-Sensor Energy Theft Detection Framework for Advanced Metering Infrastructures
T2  - Selected Areas in Communications, IEEE Journal on
IS  - 7
SN  - 0733-8716
VO  - 31
SP  - 1319
EP  - 1330
AU  - McLaughlin, S.
AU  - Holbert, B.
AU  - Fawaz, A.
AU  - Berthier, R.
AU  - Zonouz, S.
Y1  - July 2013
PY  - 2013
KW  - computerised instrumentation
KW  - power engineering computing
KW  - power system protection
KW  - power system security
KW  - security of data
KW  - smart meters
KW  - smart power grids
KW  - AMI intrusion detection system
KW  - AMIDS
KW  - advanced metering infrastructures
KW  - analog devices
KW  - computerized smart meters
KW  - data sources
KW  - elementary analyses
KW  - end-users management
KW  - information fusion
KW  - multisensor energy theft detection framework
KW  - smart grid
KW  - Power grid critical infrastructures
KW  - advanced metering infrastructures
KW  - intrusion alert correlation
KW  - intrusion and energy theft detection
KW  - multi-sensor inference and information fusion
VL  - 31
JA  - Selected Areas in Communications, IEEE Journal on
DO  - 10.1109/JSAC.2013.130714
AB  - The advanced metering infrastructure (AMI) is a crucial component of the smart grid, replacing traditional analog devices with computerized smart meters. Smart meters have not only allowed for efficient management of many end-users, but also have made AMI an attractive target for remote exploits and local physical tampering with the end goal of stealing energy. While smart meters posses multiple sensors and data sources that can indicate energy theft, in practice, the individual methods exhibit many false positives. In this paper, we present AMIDS, an AMI intrusion detection system that uses information fusion to combine the sensors and consumption data from a smart meter to more accurately detect energy theft. AMIDS combines meter audit logs of physical and cyber events with consumption data to more accurately model and detect theft-related behavior. Our experimental results on normal and anomalous load profiles show that AMIDS can identify energy theft efforts with high accuracy. Furthermore, AMIDS correctly identified legitimate load profile changes that more elementary analyses classified as malicious.
ER  - 

TY  - CONF
JO  - Innovations for Community Services (I4CS), 2014 14th International Conference on
TI  - A matrix-based damage assessment and recovery algorithm
T2  - Innovations for Community Services (I4CS), 2014 14th International Conference on
IS  - 
SN  - 
VO  - 
SP  - 22
EP  - 27
AU  - Haraty, R.A.
AU  - Zbib, M.
Y1  - 4-6 June 2014
PY  - 2014
KW  - Internet
KW  - matrix algebra
KW  - security of data
KW  - Internet technology
KW  - electronic attacks
KW  - information systems
KW  - malicious transactions
KW  - malicious users
KW  - matrix-based damage assessment
KW  - Algorithm design and analysis
KW  - Bismuth
KW  - Clustering algorithms
KW  - Computer crime
KW  - Databases
KW  - Intrusion detection
KW  - TV
KW  - damage assessment
KW  - data dependency
KW  - malicious transactions
KW  - recovery
KW  - transaction dependency
VL  - 
JA  - Innovations for Community Services (I4CS), 2014 14th International Conference on
DO  - 10.1109/I4CS.2014.6860548
AB  - With the advancement of Internet technology, securing information systems from electronic attacks has become a significant concern. With all the preventive methods, malicious users still find new ways to overcome the system security and access and modify sensitive information. To make the process of damage assessment and recovery fast and effective (not scanning the entire log), researchers have proposed different methods for segmenting the log file, and accordingly presented different damage assessment and recovery algorithms. In this work we present efficient damage assessment and recovery algorithms to recover from malicious transactions in a database based on the concept of the matrix. We also compare the various approaches and present the performance results.
ER  - 

TY  - CONF
JO  - Communication Software and Networks (ICCSN), 2011 IEEE 3rd International Conference on
TI  - Behavior based network traffic analysis tool
T2  - Communication Software and Networks (ICCSN), 2011 IEEE 3rd International Conference on
IS  - 
SN  - 
VO  - 
SP  - 649
EP  - 652
AU  - Kakuru, S.
Y1  - 27-29 May 2011
PY  - 2011
KW  - pattern matching
KW  - security of data
KW  - Wireshark
KW  - behavior analysis tool
KW  - behavior based network traffic analysis tool
KW  - behavioral analysis
KW  - firewalls
KW  - intrusion detection system
KW  - log traffic
KW  - network administrator
KW  - network model
KW  - operating system
KW  - packet sniffer
KW  - pattern matching system
KW  - user statistics
KW  - Electronic publishing
KW  - Filtering
KW  - Information services
KW  - Internet
KW  - Network behavior analysis (NBA)
KW  - Packet capture
KW  - network administrator (NA)
VL  - 
JA  - Communication Software and Networks (ICCSN), 2011 IEEE 3rd International Conference on
DO  - 10.1109/ICCSN.2011.6014810
AB  - Pattern matching systems are mainly based on network models, which are formed from detailed analysis of user statistics and network traffic. These models are used in developing traffic analysis tools. This paper focuses on development of a behavior analysis tool on any operating system and its use on detecting internal active/passive attacks. Many kinds of tools and firewalls are in market to help network administrator to prevent intrusion from outside network, but very few tools to stop attacks from internal part of the network. This tool provides a way to detect any unusual behavior by a legitimate user in a network. It uses packet sniffer like Wireshark to record log traffic over a network. Furthermore, behavioral analysis is carried in two phases. In the first phase, Wireshark records the user's interaction with the network for a period of time and is stored in database. In second phase, current activity is compared to the past activity and notifies any new behavior to network administrator. This tool adds an additional layer of security along with the intrusion detection systems available from any network attacks. Many additional features can be incorporated in this tool for future enhancement.
ER  - 

TY  - CONF
JO  - Dependable, Autonomic and Secure Computing (DASC), 2014 IEEE 12th International Conference on
TI  - Analysis of HTTP Requests for Anomaly Detection of Web Attacks
T2  - Dependable, Autonomic and Secure Computing (DASC), 2014 IEEE 12th International Conference on
IS  - 
SN  - 
VO  - 
SP  - 406
EP  - 411
AU  - Zolotukhin, M.
AU  - Hamalainen, T.
AU  - Kokkonen, T.
AU  - Siltanen, J.
Y1  - 24-27 Aug. 2014
PY  - 2014
KW  - Web services
KW  - hypermedia
KW  - security of data
KW  - HTTP logs
KW  - HTTP request
KW  - Web attack
KW  - Web server
KW  - Web services
KW  - Web-based application
KW  - anomaly detection
KW  - clustering algorithm
KW  - global network security threat
KW  - network intrusion detection
KW  - Accuracy
KW  - Data mining
KW  - Entropy
KW  - Feature extraction
KW  - Training
KW  - Vectors
KW  - Web servers
KW  - anomaly detection
KW  - data mining
KW  - entropy
KW  - intrusion detection
KW  - n-gram
VL  - 
JA  - Dependable, Autonomic and Secure Computing (DASC), 2014 IEEE 12th International Conference on
DO  - 10.1109/DASC.2014.79
AB  - Attacks against web servers and web-based applications remain a serious global network security threat. Attackers are able to compromise web services, collect confidential information from web data bases, interrupt or completely paralyze web servers. In this study, we consider the analysis of HTTP logs for the detection of network intrusions. First, a training set of HTTP requests which does not contain any attacks is analyzed. When all relevant information has been extracted from the logs, several clustering and anomaly detection algorithms are employed to describe the model of normal users behavior. This model is then used to detect network attacks as deviations from the norms in an online mode. The simulation results presented show that, compared to other data mining algorithms, the method results in a higher accuracy rate.
ER  - 

TY  - CONF
JO  - Internet (AH-ICI), 2011 Second Asian Himalayas International Conference on
TI  - SSENet-2011: A Network Intrusion Detection System dataset and its comparison with KDD CUP 99 dataset
T2  - Internet (AH-ICI), 2011 Second Asian Himalayas International Conference on
IS  - 
SN  - 2157-0647
VO  - 
SP  - 1
EP  - 5
AU  - Vasudevan, A.R.
AU  - Harshini, E.
AU  - Selvakumar, S.
Y1  - 4-6 Nov. 2011
PY  - 2011
KW  - Internet
KW  - security of data
KW  - Internet
KW  - KDD CUP 99 dataset
KW  - NIDS
KW  - SSENet-2011
KW  - data mining
KW  - evolutionary algorithms
KW  - machine learning
KW  - network intrusion detection system
KW  - security experts
KW  - Business
KW  - Complexity theory
KW  - Fires
KW  - Fluid flow measurement
KW  - Internet
KW  - Silicon
KW  - Vectors
KW  - Feature Construction
KW  - NIDS
KW  - SSENet-2011
KW  - Tstat
VL  - 
JA  - Internet (AH-ICI), 2011 Second Asian Himalayas International Conference on
DO  - 10.1109/AHICI.2011.6113948
AB  - In recent years the attack vectors in the network world have increased many fold with the increased usage of Internet and with the exponential growth of various applications. Network Intrusion Detection System (NIDS) is one of the most sought after system by security experts in safeguarding the network from both external and internal attacks. NIDS works mainly in two modes: Online and Offline. Online or real-time NIDS, such as Snort, Bro, etc., examines the packet structure to find intrusions, if any, and alerts the administrator. On the other hand, offline NIDS logs the packets flowing to and from the network, constructs features based on connections, and creates a dataset. Such NIDS datasets are used in research purposes for applying data mining, machine learning, evolutionary algorithms, etc., to detect attacks. KDD CUP 99 is one such widely used popular IDS dataset. KDD CUP 99 dataset is obsolete because many of the attacks performed to create the dataset do not exist now. Moreover, the features constructed do not pertain to network activities. It is a mixture of host based as well as network based features. So, the need for a new dataset, conforming to the present network activities and attack vectors, is inevitable. This motivated us to come out with a NIDS dataset, SSENet-2011 dataset, in this paper. SSENet-2011 dataset was constructed using Tstat tool. A real time experiment was performed, the network packets were captured, features were constructed, and the dataset was created. The created SSENet-2011 dataset was compared with the KDD CUP 99 dataset. From the experiments it is evident that a closed and secluded network such as SSENet and Tstat tool help researchers in developing and analyzing a new dataset which reflects the changing scenario of network activities.
ER  - 

TY  - CONF
JO  - Intelligence and Security Informatics (ISI), 2013 IEEE International Conference on
TI  - Layered behavioral trace modeling for threat detection
T2  - Intelligence and Security Informatics (ISI), 2013 IEEE International Conference on
IS  - 
SN  - 
VO  - 
SP  - 173
EP  - 175
AU  - Mappus, R.L.
AU  - Briscoe, E.
Y1  - 4-7 June 2013
PY  - 2013
KW  - security of data
KW  - anomalous behavior
KW  - computer usage monitoring
KW  - layered behavioral trace modeling
KW  - malicious behavior
KW  - multiple temporal scales
KW  - threat detection
KW  - Tracking
VL  - 
JA  - Intelligence and Security Informatics (ISI), 2013 IEEE International Conference on
DO  - 10.1109/ISI.2013.6578813
AB  - A fundamental problem in detecting threats to security by monitoring computer usage is the high number of false positives that are created when analyzing a large data set for anomalous behavior. We address the problem by modeling user behavior at multiple scales so as to allow for the identification potential insider threats from users' logged activity by tracking users' activity over time. In this work, we apply a novel method for representing user activity at multiple temporal scales to a dataset that contains malicious behavior. We report our detection results and discuss how a layered detection method may be advantageous for the discovery of specific types of malicious behavior.
ER  - 

TY  - CONF
JO  - Web Systems Evolution (WSE), 2011 13th IEEE International Symposium on
TI  - Cracking the Smart ClickBot
T2  - Web Systems Evolution (WSE), 2011 13th IEEE International Symposium on
IS  - 
SN  - 2160-6153
VO  - 
SP  - 125
EP  - 134
AU  - Walgampaya, C.
AU  - Kantardzic, M.
Y1  - 30-30 Sept. 2011
PY  - 2011
KW  - Internet
KW  - learning (artificial intelligence)
KW  - pattern classification
KW  - security of data
KW  - Bayesian classifier
KW  - Web hypertext structure
KW  - Web information retrieval
KW  - Web robot
KW  - Web traversing
KW  - click fraud
KW  - competitive intelligence tool
KW  - investment portal
KW  - machine learning algorithm
KW  - search engines
KW  - smart clickbot
KW  - software bot
KW  - software programs
KW  - Browsers
KW  - Humans
KW  - IP networks
KW  - Internet
KW  - Robots
KW  - Servers
KW  - Software
KW  - Bayesian Classifier
KW  - Click Fraud
KW  - Smart ClickBot
VL  - 
JA  - Web Systems Evolution (WSE), 2011 13th IEEE International Symposium on
DO  - 10.1109/WSE.2011.6081830
AB  - Nowadays, almost every task involving Web traversing and information retrieval recurs to Web robots. Web robots are software programs that automatically traverse the Web's hypertext structure. They proliferate rapidly aside with the growth of the Web and are extremely valuable and important means not only for the large search engines, but also for many specialized services such as investment portals, competitive intelligence tools, etc. While many web robots serve useful purposes, recently, there have been cases linked to fraudulent activities committed by these Web robots. Click fraud, which is the act of generating illegitimate clicks, is one of them. This paper details the architecture and functionality of the Smart ClickBot, a sophisticated software bot that is designed to commit click fraud. It was first detected and reported by NetMosaics Inc. in March, 2010, a real time click fraud detection and prevention solution provider. We discuss the machine learning algorithms used, to identify all clicks exhibiting Smart ClickBot like patterns. We constructed a Bayesian classifier that automatically classifies server log data as being Smart ClickBot or not. We also introduce a Benchmark data set for Smart ClickBot. We disclose the results of our investigation of this bot to educate the security research community and provide information regarding the novelties of the attack.
ER  - 

TY  - CONF
JO  - Computing, Communication and Applications (ICCCA), 2012 International Conference on
TI  - Intrusion detection system for grid computing using SNORT
T2  - Computing, Communication and Applications (ICCCA), 2012 International Conference on
IS  - 
SN  - 
VO  - 
SP  - 1
EP  - 6
AU  - Kumar, M.
AU  - Hanumanthappa, M.
AU  - Kumar, T.V.S.
Y1  - 22-24 Feb. 2012
PY  - 2012
KW  - grid computing
KW  - public domain software
KW  - security of data
KW  - system monitoring
KW  - IDS
KW  - SNORT
KW  - attack behavior identification
KW  - communication mechanism requirement
KW  - configuration investigation
KW  - grid computing environment
KW  - grid intrusion problem
KW  - heterogeneous hosts compatibility requirement
KW  - logs investigation
KW  - network traffic investigation
KW  - open source network-based intrusion detection system
KW  - permission control
KW  - system maintenance
KW  - system updates
KW  - user actions investigation
KW  - Computers
KW  - IP networks
KW  - Intrusion detection
KW  - Operating systems
KW  - Prototypes
KW  - Grid Computing
KW  - IDS
KW  - Snort
VL  - 
JA  - Computing, Communication and Applications (ICCCA), 2012 International Conference on
DO  - 10.1109/ICCCA.2012.6179141
AB  - Because of distributed nature, grid computing environments are easy targets for intruders looking for possible vulnerabilities to exploit [1]. By impersonating legitimate users, the intruders can use a service's abundant resources maliciously. To combat attackers, intrusion-detection systems (IDSs) can offer additional security measures for these environments by investigating configurations, logs, network traffic, and user actions to identify typical attack behavior. However, IDS must be distributed to work in a grid computing environment. It must monitor each node and, when an attack occurs, alert other nodes in the environment. This kind of communication requires compatibility between heterogeneous hosts, various communication mechanisms, and permission control over system maintenance and updates. We present the problem of grid intrusion; analyze the requirements of a system to detect them. In this paper we are discussing how IDS can be implemented for grid computing environment.
ER  - 

TY  - CONF
JO  - Information and Knowledge Technology (IKT), 2014 6th Conference on
TI  - Fraud detection in Process Aware Information systems using MapReduce
T2  - Information and Knowledge Technology (IKT), 2014 6th Conference on
IS  - 
SN  - 
VO  - 
SP  - 88
EP  - 91
AU  - Mardani, S.
AU  - Akbari, M.K.
AU  - Sharifian, S.
Y1  - 27-29 May 2014
PY  - 2014
KW  - business data processing
KW  - data handling
KW  - fraud
KW  - information systems
KW  - parallel processing
KW  - security of data
KW  - MapReduce
KW  - business environment
KW  - business processes
KW  - fraud detection
KW  - process aware information systems
KW  - Computational modeling
KW  - Fraud detection
KW  - Map-reduce Mechanism
KW  - Outlier Detection
VL  - 
JA  - Information and Knowledge Technology (IKT), 2014 6th Conference on
DO  - 10.1109/IKT.2014.7030339
AB  - Today some companies use Process Aware Information Systems to support their business processes. These Systems adopt processes to business environment quickly. Since system allows to users to change processes, system is vulnerable to fraud. Thus we need a balance between security and flexibility. The solution can be a fraud detection system, but the increasing growth of data in logs causes some difficulties. In this paper we proposed an approach that use map-reduce mechanism to implement a fraud detection method in a distributed way.
ER  - 

TY  - CONF
JO  - Internet Technology And Secured Transactions, 2012 International Conference for
TI  - Honeydoop - a system for on-demand virtual high interaction honeypots
T2  - Internet Technology And Secured Transactions, 2012 International Conference for
IS  - 
SN  - 
VO  - 
SP  - 743
EP  - 747
AU  - Kulkarni, S.
AU  - Mutalik, M.
AU  - Kulkarni, P.
AU  - Gupta, T.
Y1  - 10-12 Dec. 2012
PY  - 2012
KW  - data analysis
KW  - information retrieval
KW  - security of data
KW  - telecommunication traffic
KW  - Honeydoop
KW  - IDS alerts
KW  - data analyzer Hadoop
KW  - dormant honeypots
KW  - information analysis
KW  - information retrieval
KW  - network traffic analysis
KW  - on-demand virtual high interaction honeypots
KW  - virtual honeypots
KW  - Databases
KW  - Organizations
KW  - Switches
KW  - Hadoop
KW  - Honeypots
KW  - Virtual Honeypots
VL  - 
JA  - Internet Technology And Secured Transactions, 2012 International Conference for
DO  - 
AB  - On demand allocation of honeypots at right places on the network and at right time would considerably make the network more secure and harder to sneak into. This paper proposes an idea of dynamically creating, modifying and managing virtual honeypots-Honeydoop. Honeydoop is a system of dynamically creating, modifying and managing virtual honeypots. It combines the concept of honeypots and uses big data analyzer Hadoop for quick information retrieval and analysis. The goal of the system is to create evanescent honeypots at right places and times, on demand, to achieve better security in this ever changing environment. The system finds the machines on the network which attackers are interested in using IDS alerts and network traffic analysis. Virtual honeypots replicating those systems are then created and deployed on the network. Suspicious traffic destined for the target system is then redirected to the newly created honeypot. Dormant honeypots are deleted periodically. Honeydoop can also be used to analyze existing honeypot logs.
ER  - 

TY  - CONF
JO  - Advances in Computing, Communications and Informatics (ICACCI), 2013 International Conference on
TI  - Architectural design for iCloudIDM Layer-II (iCloudIDM-LII) Subsystem of eCloudIDS generic security framework
T2  - Advances in Computing, Communications and Informatics (ICACCI), 2013 International Conference on
IS  - 
SN  - 
VO  - 
SP  - 1668
EP  - 1674
AU  - Srinivasan, M.K.
AU  - Sarukesi, K.
AU  - Revathy, P.
Y1  - 22-25 Aug. 2013
PY  - 2013
KW  - business data processing
KW  - cloud computing
KW  - resource allocation
KW  - security of data
KW  - apt resource sharing
KW  - architectural design
KW  - business computing
KW  - cloud computing
KW  - corporate IT
KW  - eCloudIDS generic security framework
KW  - iCloudIDM layer-II subsystem
KW  - iCloudIDM-LII
KW  - identity management capabilities
KW  - maximum resource utilization
KW  - multitenancy nature
KW  - next-generation computing
KW  - next-generation generic security framework
KW  - sX-Engine
KW  - security concerns
KW  - two-tier expert engines
KW  - uX-Engine
KW  - Authentication
KW  - Cloud computing
KW  - Computer architecture
KW  - Organizations
KW  - Servers
KW  - Virtual machining
KW  - Acute Audit Repository
KW  - Cloud Computing
KW  - Cloud Generic Security Framework
KW  - Cloud Security
KW  - Cloud VM/Instance Monitor
KW  - H-log-H
KW  - Instance-M
KW  - Special Permission Audit Repository
KW  - Standard Audit Repository
KW  - State-of-the-art Cloud Computing Security Taxonomies
KW  - Warning Level Generator
KW  - eCloudIDS
KW  - eCloudIDS C3
KW  - iCloudIDM
KW  - iCloudIDM-LI
KW  - iCloudIDM-LII
KW  - sX-Engine
KW  - uX-Engine
VL  - 
JA  - Advances in Computing, Communications and Informatics (ICACCI), 2013 International Conference on
DO  - 10.1109/ICACCI.2013.6637432
AB  - Cloud computing is becoming a key driver in supporting all on-demand needs of corporate IT today. Cloud promises extensive mechanisms for apt resource sharing, maximum resource utilization, and true elasticity as compared to any of its early contestants. Even after recognizing its advantages, many organizations are reluctant to take up this next-generation computing, due to its severe security concerns. In today's world, whole business computing relies on the security of customers and their data. Due to its public and multi-tenancy nature, security concerns are very huge in capacity. eCloudIDS is a next-generation generic security framework architected with innovative hybrid two-tier expert engines, namely uX-Engine and sX-Engine, considered to be most suitable security solution for any cloud computing environments; precisely for public and hybrid clouds. This paper explores the architectural design facets of eCloudIDS security framework's iCloudIDM Subsystem Layer-II (iCloudIDM-LII) that extends the identity management capabilities at Cloud VM Level.
ER  - 

TY  - JOUR
JO  - Security & Privacy, IEEE
TI  - 10 Quick, Dirty, and Cheap Things to Improve Enterprise Security
T2  - Security & Privacy, IEEE
IS  - 2
SN  - 1540-7993
VO  - 8
SP  - 83
EP  - 85
AU  - McGovern, J.
AU  - Peterson, Gunnar
Y1  - March-April 2010
PY  - 2010
KW  - security of data
KW  - enterprise security
KW  - enterprise software
KW  - organization process
KW  - organization rechnology
KW  - organization techniques
KW  - revolutionary changes
KW  - software development
KW  - software security
KW  - Costs
KW  - Information security
KW  - Programming
KW  - Software tools
KW  - audit logging
KW  - enterprise software
KW  - security and privacy
KW  - software development
KW  - software security
KW  - threat modeling
VL  - 8
JA  - Security & Privacy, IEEE
DO  - 10.1109/MSP.2010.61
AB  - As software security has increasingly become an important part of information security programs, there have been some notable trends and successes of various tools, processes, and models. Because "building security in" is so different from how enterprise software has historically been developed, the changes might seem revolutionary. In the enterprise, revolutionary changes involve cost and complexity, as organizations figure out how to incorporate new techniques, processes, and technology. The paper shows an informal list that doesn't say, "simply reboot your entire enterprise software development and you are ready to begin secure coding." Instead, it describes how people with limited budgets and/or authority can make potentially big changes in their enterprise software's overall security. 10 low or no-cost ideas were discussed in this paper.
ER  - 

TY  - CONF
JO  - IT in Business, Industry and Government (CSIBIG), 2014 Conference on
TI  - Big Data: A security compliance model
T2  - IT in Business, Industry and Government (CSIBIG), 2014 Conference on
IS  - 
SN  - 
VO  - 
SP  - 1
EP  - 5
AU  - Gupta, A.
AU  - Verma, A.
AU  - Kalra, P.
AU  - Kumar, L.
Y1  - 8-9 March 2014
PY  - 2014
KW  - Big Data
KW  - authorisation
KW  - security of data
KW  - access control features
KW  - big data processing tool
KW  - data availability
KW  - data dealing devices
KW  - data generation
KW  - security compliance model
KW  - security features
KW  - Artificial neural networks
KW  - Computer hacking
KW  - Electromagnetic compatibility
KW  - Libraries
KW  - Monitoring
KW  - Switches
KW  - Big Data
KW  - Critical user log
KW  - Lookup Method
KW  - Mapping
KW  - Relationship analysis tool
KW  - Security Model
VL  - 
JA  - IT in Business, Industry and Government (CSIBIG), 2014 Conference on
DO  - 10.1109/CSIBIG.2014.7056963
AB  - This paper proposed a security compliance model that presents security and access control features which are employed at the time of origin of Big Data. The growth rate of data dealing devices and generation of data has increased exponentially which led to the massive explosion of data availability both in structured or unstructured form also known as big data. The challenge is to analyze this unstructured form of data and categories in a momentous form.The tool design to process big data without considering the security and access control feature in mind that its security is not inbuilt in these systems. Moreover, because of the huge size data it becomes a very tedious task to incorporate security feature at a later stage. So the security should be embedded in the initial or design stage.
ER  - 

TY  - CONF
JO  - Industrial and Information Systems (ICIIS), 2011 6th IEEE International Conference on
TI  - A novel mind map based approach for log data extraction
T2  - Industrial and Information Systems (ICIIS), 2011 6th IEEE International Conference on
IS  - 
SN  - 
VO  - 
SP  - 130
EP  - 135
AU  - Jayathilake, P.W.D.C.
Y1  - 16-19 Aug. 2011
PY  - 2011
KW  - data structures
KW  - program testing
KW  - data extraction
KW  - data semantics
KW  - data structure
KW  - log data extraction
KW  - log file format
KW  - log file syntax
KW  - novel mind map
KW  - software log file analysis
KW  - software testing
KW  - software troubleshooting
KW  - Data mining
KW  - Grammar
KW  - Information systems
KW  - Monitoring
KW  - Semantics
KW  - Software
KW  - XML
KW  - log analysis
KW  - log data extraction
KW  - log file modeling
KW  - mind map
VL  - 
JA  - Industrial and Information Systems (ICIIS), 2011 6th IEEE International Conference on
DO  - 10.1109/ICIINFS.2011.6038054
AB  - Software log file analysis helps immensely in software testing and troubleshooting. The first step in automated log file analysis is extracting log data. This requires decoding the log file syntax and interpreting data semantics. The expected output of this phase is an organization of the extracted data for further processing. Log data extractors can be developed using popular programming languages targeting one or few log file formats. Rather than repeating this process for each log file format, it is desirable to have a generic scheme for interpreting elements of a log file and filling a data structure suitable for further processing. The new log data extraction scheme introduced in this paper is an attempt to provide the advanced features demanded by modern log file analysis procedures. It is a generic scheme which is capable of handling both text and binary log files with complex structures and difficult syntax. Its output is a tree filled with the information of interest for the particular case.
ER  - 

TY  - CONF
JO  - Advanced Information Networking and Applications (WAINA), 2011 IEEE Workshops of International Conference on
TI  - Towards a Forensic Analysis for Multimedia Communication Services
T2  - Advanced Information Networking and Applications (WAINA), 2011 IEEE Workshops of International Conference on
IS  - 
SN  - 
VO  - 
SP  - 424
EP  - 429
AU  - Geneiatakis, D.
AU  - Keromytis, A.D.
Y1  - 22-25 March 2011
PY  - 2011
KW  - computer forensics
KW  - entropy
KW  - multimedia communication
KW  - entropy theory
KW  - forensic analysis
KW  - malicious user
KW  - message randomness
KW  - message segment
KW  - multimedia communication services
KW  - security mechanism
KW  - security violation
KW  - Conferences
KW  - Digital Forensic
KW  - Entropy
KW  - Multimedia Communication Services
KW  - Voice over IP
VL  - 
JA  - Advanced Information Networking and Applications (WAINA), 2011 IEEE Workshops of International Conference on
DO  - 10.1109/WAINA.2011.14
AB  - No matter how robust the employed security mechanisms are malicious users or attackers will always find a way to bypass them. In addition, National Institute of Security and Technology mentions "In conjunction with appropriate tools &amp; procedures, audit trail can assist in detecting security violation and flaws in applications". Until now, in Multimedia Communication Services (MCS), such as Voice over IP, audit trails are not utilized in security audits due to (a) the lack of the appropriate analysis tools and (b) privacy restrictions. In this paper we report on the analysis of MCS audit trail by employing a novel method for identifying "uncommon" traffic indicating non normal behaviour that does not violate users' privacy. We rely on entropy theory and the notion of "itself information" to quantify the randomness of specific message segments, and we also introduce the term "actual itself information" for the assessment of entire message randomness. To protect users' privacy we hash audit trail's data. For evaluating the applicability of our proposed method we utilize an audit trail of a real MCS provider published by honey pot project. Initial outcomes show the feasibility of employing such a method to recognize "uncommon" traffic, recorded in MCS audit trail.
ER  - 

TY  - CONF
JO  - Computer Science and Software Engineering (JCSSE), 2012 International Joint Conference on
TI  - Towards structured log analysis
T2  - Computer Science and Software Engineering (JCSSE), 2012 International Joint Conference on
IS  - 
SN  - 
VO  - 
SP  - 259
EP  - 264
AU  - Jayathilake, D.
Y1  - May 30 2012-June 1 2012
PY  - 2012
KW  - system monitoring
KW  - COBIT
KW  - FISMA
KW  - HIPAA
KW  - ISO 27001
KW  - PCI DSS
KW  - compliance evaluation
KW  - intrusion detection
KW  - log information
KW  - log management tools
KW  - recurring log analysis procedure automation
KW  - software log file analysis
KW  - structured log analysis
KW  - Data mining
KW  - Databases
KW  - Engines
KW  - Monitoring
KW  - Organizations
KW  - Software
KW  - Standards
KW  - log analysis
KW  - log data extraction
KW  - log management
KW  - mind map
KW  - structured logs
VL  - 
JA  - Computer Science and Software Engineering (JCSSE), 2012 International Joint Conference on
DO  - 10.1109/JCSSE.2012.6261962
AB  - Value of software log file analysis has been constantly increasing with the value of information to organizations. Log management tools still have a lot to deliver in order to empower their customers with the true strength of log information. In addition to the traditional uses such as testing software functional conformance, troubleshooting and performance benchmarking, log analysis has proven its capabilities in fields like intrusion detection and compliance evaluation. This is verified by the emphasis on log analysis in regulations like PCI DSS, FISMA, HIPAA and frameworks such as ISO 27001 and COBIT. In this paper we present an in depth analysis into current log analysis domains and common problems. A practical guide to the use of few popular log analysis tools is also included. Lack of proper support for structured analysis is identified as one major flaw in existing tools. After that, we describe a framework we developed for structured log analysis with the view of providing a solution to open problems in the domain. The core strength of the framework is its ability to handle many log file formats that are not well served by existing tools and providing sophisticated infrastructure for automating recurring log analysis procedures. We prove the usefulness of the framework with a simple experiment.
ER  - 

TY  - JOUR
JO  - Computer Graphics and Applications, IEEE
TI  - Countering security information overload through alert and packet visualization
T2  - Computer Graphics and Applications, IEEE
IS  - 2
SN  - 0272-1716
VO  - 26
SP  - 60
EP  - 70
AU  - Conti, G.
AU  - Abdullah, K.
AU  - Grizzard, J.
AU  - Stasko, J.
AU  - Copeland, J.A.
AU  - Ahamad, M.
AU  - Owen, Henry L.
AU  - Lee, C.P.
Y1  - March-April 2006
PY  - 2006
KW  - data visualisation
KW  - graphical user interfaces
KW  - interactive systems
KW  - security of data
KW  - alert visualization
KW  - end-to-end design
KW  - graphical techniques
KW  - interactive technique
KW  - network security visualization system
KW  - packet visualization
KW  - security professional
KW  - Computer security
KW  - Data security
KW  - Hardware
KW  - Humans
KW  - Information analysis
KW  - Information security
KW  - Intrusion detection
KW  - Protocols
KW  - Statistical analysis
KW  - Visualization
KW  - alert visualization
KW  - log visualization
KW  - network visualization
KW  - packet visualization
KW  - payload visualization
KW  - Computer Communication Networks
KW  - Computer Graphics
KW  - Information Storage and Retrieval
KW  - Signal Processing, Computer-Assisted
KW  - Software
KW  - User-Computer Interface
VL  - 26
JA  - Computer Graphics and Applications, IEEE
DO  - 10.1109/MCG.2006.30
AB  - This article presents a framework for designing network security visualization systems as well as results from the end-to-end design and implementation of two highly interactive systems. In this article, we provide multiple contributions: we present the results of our survey of security professionals, the design framework, and lessons learned from the design of our systems as well as an evaluation of their effectiveness. Our results indicate that both systems effectively present significantly more information when compared to traditional textual approaches. We believe that the interactive, graphical techniques that we present will have broad applications in other domains seeking to deal with information overload.
ER  - 

TY  - CONF
JO  - Computer Modelling and Simulation (UKSim), 2012 UKSim 14th International Conference on
TI  - Security Analysis and Implementation of 3-Level Security System Using Image Based Authentication
T2  - Computer Modelling and Simulation (UKSim), 2012 UKSim 14th International Conference on
IS  - 
SN  - 
VO  - 
SP  - 547
EP  - 552
AU  - Anand, S.
AU  - Jain, P.
AU  - Nitin
AU  - Rastogi, R.
Y1  - 28-30 March 2012
PY  - 2012
KW  - image coding
KW  - security of data
KW  - 3-level security system
KW  - Internet
KW  - Web development
KW  - automated email
KW  - brute-force attack
KW  - image based authentication
KW  - security analysis
KW  - tempest attack
KW  - text based passwords
KW  - thwarting shoulder attack
KW  - Authentication
KW  - Computer hacking
KW  - Electronic mail
KW  - Force
KW  - Image color analysis
KW  - Servers
KW  - AJAX
KW  - Image Based Authentication System (IBA)
KW  - Keystroke Logging
KW  - Shoulder Attack and Brute-force Attack
KW  - Tempest Attack
VL  - 
JA  - Computer Modelling and Simulation (UKSim), 2012 UKSim 14th International Conference on
DO  - 10.1109/UKSim.2012.83
AB  - Increasing security has always been an issue since Internet and Web Development came into existence, text based passwords is not enough to counter such problems, which is also an anachronistic approach now. Therefore, this demands the need for something more secure along with being more user-friendly. Therefore, we have tried to increase the security by involving a 3-level security approach, involving text based password at Level 1, Image Based Authentication at Level 2, and automated generated one-time password (received through an automated email to the authentic user) at Level 3. And an assiduous effort has been done for thwarting Shoulder attack, Tempest attack, and Brute-force attack at client side, through the use of unique image set in the IBA System.
ER  - 

TY  - CONF
JO  - Availability, Reliability and Security, 2008. ARES 08. Third International Conference on
TI  - Secure and Privacy-Friendly Logging for eGovernment Services
T2  - Availability, Reliability and Security, 2008. ARES 08. Third International Conference on
IS  - 
SN  - 
VO  - 
SP  - 1091
EP  - 1096
AU  - Wouters, K.
AU  - Simoens, K.
AU  - Lathouwers, D.
AU  - Preneel, B.
Y1  - 4-7 March 2008
PY  - 2008
KW  - data privacy
KW  - government data processing
KW  - security of data
KW  - eGovernment services
KW  - multiple logging servers
KW  - privacy-friendly logging
KW  - security
KW  - Access control
KW  - Availability
KW  - Context
KW  - Cryptography
KW  - Government
KW  - Legislation
KW  - Mechanical factors
KW  - Privacy
KW  - Protection
KW  - Security
KW  - Logging Services
KW  - eGovernment
VL  - 
JA  - Availability, Reliability and Security, 2008. ARES 08. Third International Conference on
DO  - 10.1109/ARES.2008.41
AB  - In this paper we present a scheme for building a logging- trail for processes related to eGovernment services. A citizen can reconstruct the trail of such a process and verify its status if he is the subject of that process. Reconstruction is based on hand-overs, special types of log events, that link data stored by multiple logging servers, which are not necessarily trusted. Our scheme is privacy-friendly in the sense that only the authorised subject, i.e. the citizen, can link the different log entries related to one specific process. The scheme is also auditable; it allows logging servers to show that they behave according to a certain policy.
ER  - 

TY  - CONF
JO  - Machine Learning and Cybernetics, 2007 International Conference on
TI  - Log-Based Recovery Scheme for Executing Untrusted Programs
T2  - Machine Learning and Cybernetics, 2007 International Conference on
IS  - 
SN  - 
VO  - 4
SP  - 2136
EP  - 2139
AU  - Hui-Jun Lu
AU  - Shu-Zhen Leng
Y1  - 19-22 Aug. 2007
PY  - 2007
KW  - security of data
KW  - system monitoring
KW  - Linux Security Modules
KW  - Linux operating system
KW  - anonymous programs
KW  - high level security systems
KW  - log-based recovery scheme
KW  - trustworthy programs
KW  - uncertified program
KW  - untrusted programs safe execution
KW  - Costs
KW  - Cryptography
KW  - Cybernetics
KW  - File systems
KW  - IP networks
KW  - Linux
KW  - Machine learning
KW  - Protection
KW  - Runtime
KW  - Security
KW  - LSM
KW  - Log-based recovery
KW  - Untrusted programs
VL  - 4
JA  - Machine Learning and Cybernetics, 2007 International Conference on
DO  - 10.1109/ICMLC.2007.4370497
AB  - In this paper, a recovery scheme for safe execution of untrusted programs is presented. In this scheme, when the effects of untrusted program execution is undesirable, system can be easily rolled back to the initial state where the checkpoint is set before the program executed. In high level security systems, only the trustworthy programs, whose names are listed in a whitelist, are allowed to execute. However forbidding all the anonymous programs is unacceptable. In order to reduce the risk of running the uncertified program, many solutions has been proposed to solve the problem, most of which can be categorized into three kinds: detection, protection or recovery. As a recovery scheme, the system doesn't change the program and its context at runtime, and just monitors the process of its execution, records the access it made to system resources, and simultaneously backs up the modification it made to file system. When the record shows the effect of the program is unexpected, the administrator can undo what the program has modified to the file system according to the record. We have implemented a prototype system for Linux operating system using Linux Security Modules (LSM), which can be integrated into other security modules seamlessly. Key advantages of our scheme are that it requires no changes to the untrusted programs or its execution context; it doesn't do anything to hinder the execution process, and only has negligible performance overhead.
ER  - 

TY  - CONF
JO  - Data Mining, 2009. ICDM '09. Ninth IEEE International Conference on
TI  - Execution Anomaly Detection in Distributed Systems through Unstructured Log Analysis
T2  - Data Mining, 2009. ICDM '09. Ninth IEEE International Conference on
IS  - 
SN  - 1550-4786
VO  - 
SP  - 149
EP  - 158
AU  - Qiang Fu
AU  - Jian-Guang Lou
AU  - Yi Wang
AU  - Jiang Li
Y1  - 6-9 Dec. 2009
PY  - 2009
KW  - distributed programming
KW  - security of data
KW  - execution anomaly detection
KW  - finite state automaton
KW  - large scale distributed systems
KW  - unstructured log analysis technique
KW  - Large-scale systems
KW  - Learning automata
KW  - Measurement
KW  - Timing
KW  - distributed system
KW  - finite state automaton
KW  - log analysis
KW  - problem diagnosis
VL  - 
JA  - Data Mining, 2009. ICDM '09. Ninth IEEE International Conference on
DO  - 10.1109/ICDM.2009.60
AB  - Detection of execution anomalies is very important for the maintenance, development, and performance refinement of large scale distributed systems. Execution anomalies include both work flow errors and low performance problems. People often use system logs produced by distributed systems for troubleshooting and problem diagnosis. However, manually inspecting system logs to detect anomalies is unfeasible due to the increasing scale and complexity of distributed systems. Therefore, there is a great demand for automatic anomalies detection techniques based on log analysis. In this paper, we propose an unstructured log analysis technique for anomalies detection. In the technique, we propose a novel algorithm to convert free form text messages in log files to log keys without heavily relying on application specific knowledge. The log keys correspond to the log-print statements in the source code which can provide cues of system execution behavior. After converting log messages to log keys, we learn a Finite State Automaton (FSA) from training log sequences to present the normal work flow for each system component. At the same time, a performance measurement model is learned to characterize the normal execution performance based on the log messages' timing information. With these learned models, we can automatically detect anomalies in newly input log files. Experiments on Hadoop and SILK show that the technique can effectively detect running anomalies.
ER  - 

TY  - CONF
JO  - Computer Security Applications Conference, 2004. 20th Annual
TI  - Extracting attack manifestations to determine log data requirements for intrusion detection
T2  - Computer Security Applications Conference, 2004. 20th Annual
IS  - 
SN  - 1063-9527
VO  - 
SP  - 158
EP  - 167
AU  - Barse, E.L.
AU  - Jonsson, E.
Y1  - 6-10 Dec. 2004
PY  - 2004
KW  - computer networks
KW  - security of data
KW  - statistical analysis
KW  - attack manifestation
KW  - intrusion detection
KW  - log data requirements
KW  - Application software
KW  - Computer networks
KW  - Computer security
KW  - Data engineering
KW  - Data mining
KW  - Guidelines
KW  - Intrusion detection
KW  - Packaging
KW  - Telecommunication traffic
KW  - Intrusion detection
KW  - attack manifestations
KW  - data collection
KW  - log data
VL  - 
JA  - Computer Security Applications Conference, 2004. 20th Annual
DO  - 10.1109/CSAC.2004.20
AB  - Log data adapted for intrusion detection is a little explored research issue despite its importance for successful and efficient detection of attacks and intrusions. This paper presents a starting point in the search for suitable log data by providing a framework for determining exactly which log data that can reveal a specific attack, i.e. the attack manifestations. An attack manifestation consists of the log entries added, changed or removed by the attack compared to normal behaviour. We demonstrate the use of the framework by studying attacks in different types of log data. This work provides a foundation for a fully automated attack analysis. It also provides some pointers for how to define a collection of log elements that are both sufficient and necessary for detection of a specific group of attacks. We believe that this lead to a log data source that is especially adapted for intrusion detection purposes.
ER  - 

TY  - CONF
JO  - eDemocracy & eGovernment (ICEDEG), 2014 First International Conference on
TI  - An authentication and auditing architecture for enhancing security on egovernment services
T2  - eDemocracy & eGovernment (ICEDEG), 2014 First International Conference on
IS  - 
SN  - 
VO  - 
SP  - 73
EP  - 76
AU  - Flores, D.A.
Y1  - 24-25 April 2014
PY  - 2014
KW  - Internet
KW  - Web sites
KW  - access control
KW  - digital signatures
KW  - government data processing
KW  - information systems
KW  - public administration
KW  - security of data
KW  - Internet platform
KW  - auditing control
KW  - citizen privacy
KW  - data tampering
KW  - database intrusion detection module
KW  - digital certificates
KW  - eGovernment security enhancement
KW  - eGovernment services
KW  - governmental Web sites
KW  - governmental information deployment
KW  - salting-based authentication module
KW  - unauthorised access
KW  - Access control
KW  - Authentication
KW  - Databases
KW  - Intrusion detection
KW  - Servers
KW  - Web sites
KW  - architecture
KW  - auditing
KW  - authentication
KW  - database
KW  - eGovernment
KW  - intrusion detection
KW  - log
KW  - salting
VL  - 
JA  - eDemocracy & eGovernment (ICEDEG), 2014 First International Conference on
DO  - 10.1109/ICEDEG.2014.6819952
AB  - eGovernment deploys governmental information and services for citizens and general society. As the Internet is being used as underlying platform for information exchange, these services are exposed to data tampering and unauthorised access as main threats against citizen privacy. These issues have been usually tackled by applying controls at application level, making authentication stronger and protecting credentials in transit using digital certificates. However, these efforts to enhance security on governmental web sites have been only focused on what malicious users can do from the outside, and not in what insiders can do to alter data straight on the databases. In fact, the lack of security controls at back-end level hinders every effort to find evidence and investigate events related to credential misuse and data tampering. Moreover, even though attackers can be found and prosecuted, there is no evidence and audit trails on the databases to link illegal activities with identities. In this article, a Salting-Based Authentication Module and a Database Intrusion Detection Module are proposed as enhancements to eGovernment security to provide better authentication and auditing controls.
ER  - 

TY  - CONF
JO  - Communication Systems and Network Technologies (CSNT), 2014 Fourth International Conference on
TI  - Integrating Machine Learning Techniques to Constitute a Hybrid Security System
T2  - Communication Systems and Network Technologies (CSNT), 2014 Fourth International Conference on
IS  - 
SN  - 
VO  - 
SP  - 1082
EP  - 1087
AU  - Singh, N.
AU  - Chandra, N.
Y1  - 7-9 April 2014
PY  - 2014
KW  - learning (artificial intelligence)
KW  - neural nets
KW  - security of data
KW  - support vector machines
KW  - ANN
KW  - SVM
KW  - artificial neural network
KW  - computer security
KW  - hybrid security system
KW  - machine learning techniques
KW  - support vector machine
KW  - Artificial neural networks
KW  - Intrusion detection
KW  - Neurons
KW  - Probabilistic logic
KW  - Support vector machines
KW  - Training
KW  - Artificial neural network
KW  - Host logs
KW  - Machine Learning
KW  - Network logs
KW  - Support vector machine
VL  - 
JA  - Communication Systems and Network Technologies (CSNT), 2014 Fourth International Conference on
DO  - 10.1109/CSNT.2014.221
AB  - Computer Security has been discussed and improvised in many forms and using different techniques as well as technologies. The enhancements keep on adding as the security remains the fastest updating unit in a computer system. In this paper we propose a model for securing the system along with the network and enhance it more by applying machine learning techniques SVM (support vector machine) and ANN (Artificial Neural Network). Both the techniques are used together to generate results which are appropriate for analysis purpose and thus, prove to be the milestone for security.
ER  - 

TY  - JOUR
JO  - Signal Processing Letters, IEEE
TI  - Log-Polar Based Scheme for Revealing Duplicated Regions in Digital Images
T2  - Signal Processing Letters, IEEE
IS  - 10
SN  - 1070-9908
VO  - 18
SP  - 559
EP  - 562
AU  - Qiumin Wu
AU  - Shuozhong Wang
AU  - Xinpeng Zhang
Y1  - Oct. 2011
PY  - 2011
KW  - copy protection
KW  - fast Fourier transforms
KW  - image processing
KW  - interpolation
KW  - security of data
KW  - 1D Fourier transform
KW  - LPFFT
KW  - digital image tampering
KW  - image blocks
KW  - interpolation operations
KW  - log-polar coordinates
KW  - log-polar fast Fourier transform
KW  - region duplication
KW  - scaling invariance
KW  - Complexity theory
KW  - Discrete Fourier transforms
KW  - Fast Fourier transforms
KW  - Interpolation
KW  - Materials
KW  - Image forensics
KW  - pseudo log-polar transform
KW  - regional duplication detection
KW  - rotation and scaling invariance
VL  - 18
JA  - Signal Processing Letters, IEEE
DO  - 10.1109/LSP.2011.2163507
AB  - Region duplication is a common type of digital image tampering. This paper presents a log-polar based approach to detect such forgery even if the copied area has been rotated and/or scaled. We compute log-polar fast Fourier transform (LPFFT) on image blocks to approximate the log-polar Fourier transform (LPFT). LPFFT is based on a nearly log-polar system where conversion to log-polar coordinates only involves 1-D Fourier transform and interpolation operations. In addition to rotation and scaling invariance, computation complexity of LPFFT is <i>O</i>(<i>n</i><sup>2</sup>log<i>n</i>) , much lower than <i>O</i>(<i>n</i><sup>4</sup>) of LPFT when <i>n</i> is large. Similarity of the LPFFT results between different blocks provides indication of image tampering. Experimental results show efficacy of the proposed method.
ER  - 

TY  - CONF
JO  - System Sciences (HICSS), 2015 48th Hawaii International Conference on
TI  - An Agent Based Approach to Perform Damage Assessment and Recovery Efficiently after a Cyber Attack to Ensure E-government Database Security
T2  - System Sciences (HICSS), 2015 48th Hawaii International Conference on
IS  - 
SN  - 1530-1605
VO  - 
SP  - 2272
EP  - 2279
AU  - Kurra, K.
AU  - Panda, B.
AU  - Wing-Ning Li
AU  - Yi Hu
Y1  - 5-8 Jan. 2015
PY  - 2015
KW  - database management systems
KW  - government data processing
KW  - security of data
KW  - software agents
KW  - agent based approach
KW  - critical information
KW  - cyber attack
KW  - data damage assessment
KW  - data damage recovery
KW  - data processing
KW  - data recovery time
KW  - e-government database security
KW  - e-government sites
KW  - minimal log access
KW  - system down time minimization
KW  - system functionality maintenance
KW  - system intrusion detection
KW  - Aging
KW  - Databases
KW  - Electronic government
KW  - History
KW  - Security
KW  - System recovery
KW  - Cyber Attack
KW  - Data Recovery
KW  - Database Transactions
KW  - Dependency Relationships
VL  - 
JA  - System Sciences (HICSS), 2015 48th Hawaii International Conference on
DO  - 10.1109/HICSS.2015.272
AB  - Databases that contain critical information become prime targets for attacks. E-Government sites often contain data that must remain available to its users with as little interruption as possible. In case an intrusion into such a system is detected, quick damage assessment and recovery of data is vital for maintaining system functionality. Conventional log based damage assessment methods require that the log of an affected database must be scanned from the occurrence of the attacking transaction to the end of the log. Hence, these methods result in slow recovery time since they have to unnecessarily process large volume of data, a lot of which may be unaffected by the attack. In this paper, we have provided a model that can expedite the damage assessment and recovery process by using minimal log access. Our approach uses multiple agents to perform damage assessment and recovery processes in parallel and, as a result, minimizes system down time.
ER  - 

TY  - CONF
JO  - Software Engineering, 2000. Proceedings of the 2000 International Conference on
TI  - Broad-spectrum studies of log file analysis
T2  - Software Engineering, 2000. Proceedings of the 2000 International Conference on
IS  - 
SN  - 0270-5257
VO  - 
SP  - 105
EP  - 114
AU  - Andrews, J.H.
AU  - Yingjun Zhang
Y1  - 2000
PY  - 2000
KW  - formal specification
KW  - formal verification
KW  - program testing
KW  - safety-critical software
KW  - log file analysis
KW  - noncritical systems
KW  - safety-critical systems
KW  - software simulation
KW  - system-level testing
KW  - test results checking
KW  - unit-level testing
KW  - Analytical models
KW  - Code standards
KW  - Computer science
KW  - Failure analysis
KW  - Inspection
KW  - Permission
KW  - Programming
KW  - Software safety
KW  - Software testing
KW  - System testing
VL  - 
JA  - Software Engineering, 2000. Proceedings of the 2000 International Conference on
DO  - 10.1109/ICSE.2000.870402
AB  - This paper reports on research into applying the technique of log file analysis for checking test results to a broad range of testing and other tasks. The studies undertaken included applying log file analysis to both unit- and system-level testing and to requirements of both safety-critical and non-critical systems, and the use of log file analysis in combination with other testing methods. The paper also reports on the technique of using log file analyzers to simulate the software under test, both in order to validate the analyzers and to clarify requirements. It also discusses practical issues to do with the completeness of the approach, and includes comparisons to other recently-published approaches to log file analysis
ER  - 

TY  - CONF
JO  - Visualization for Computer Security, 2005. (VizSEC 05). IEEE Workshop on
TI  - A user-centered look at glyph-based security visualization
T2  - Visualization for Computer Security, 2005. (VizSEC 05). IEEE Workshop on
IS  - 
SN  - 
VO  - 
SP  - 21
EP  - 28
AU  - Komlodi, A.
AU  - Rheingans, P.
AU  - Utkarsha Ayachit
AU  - Goodall, J.R.
AU  - Amit Joshi
Y1  - 26 Oct. 2005
PY  - 2005
KW  - data visualisation
KW  - security of data
KW  - telecommunication security
KW  - glyph-based security visualization
KW  - information visualization
KW  - intrusion detection toolkit
KW  - intrusion detection visualization tool
KW  - network protection
KW  - usability evaluation
KW  - user-centered design
KW  - Data visualization
KW  - Guidelines
KW  - Information analysis
KW  - Information security
KW  - Intrusion detection
KW  - Monitoring
KW  - Protection
KW  - Three dimensional displays
KW  - Usability
KW  - User centered design
VL  - 
JA  - Visualization for Computer Security, 2005. (VizSEC 05). IEEE Workshop on
DO  - 10.1109/VIZSEC.2005.1532062
AB  - This paper presents the intrusion detection toolkit (IDtk), an information visualization tool for intrusion detection (ID). IDtk was developed through a user-centered design process, in which we identified design guidelines to support ID users. ID analysts protect their networks by searching for evidence of attacks in ID system output, firewall and system logs, and other complex, textual data sources. Monitoring and analyzing these sources incurs a heavy cognitive load for analysts. The use of information visualization techniques offers a valuable addition to the toolkit of the ID analyst. Several visualization techniques for ID have been developed, but few usability or field studies have been completed to assess the needs of ID analysts and the usability and usefulness of these tools. We intended to fill this gap by applying a user-centered design process in the development and evaluation of IDtk, a 3D, glyph-based visualization tool that gives the user maximum flexibility in setting up how the visualization display represents ID data. The user can also customize whether the display is a simple, high-level overview to support monitoring, or a more complex 3D view allowing for viewing the data from multiple angles and thus supporting analysis and diagnosis. This flexibility was found crucial in our usability evaluation. In addition to describing the tool, we report the findings of our user evaluation and propose new guidelines for the design of information visualization tools for ID.
ER  - 

TY  - CONF
JO  - Network Operations and Management Symposium (NOMS), 2012 IEEE
TI  - Na&#x00EF;ve Bayesian filters for log file analysis: Despam your logs
T2  - Network Operations and Management Symposium (NOMS), 2012 IEEE
IS  - 
SN  - 1542-1201
VO  - 
SP  - 627
EP  - 630
AU  - Havens, R.W.
AU  - Lunt, B.
AU  - Chia-Chi Teng
Y1  - 16-20 April 2012
PY  - 2012
KW  - Bayes methods
KW  - data mining
KW  - information filtering
KW  - information filters
KW  - pattern clustering
KW  - system monitoring
KW  - unsolicited e-mail
KW  - Bayesian spam filters
KW  - Naive Bayesian filters
KW  - clustering technologies
KW  - computer systems
KW  - data mining
KW  - log entries categorization
KW  - log file analysis
KW  - outage relationships
KW  - system log files
KW  - troubleshooting
KW  - Bayesian methods
KW  - Information filters
KW  - Noise
KW  - Servers
KW  - Springs
KW  - Bayesian content filter
KW  - Bogofilter
KW  - Spam Assassin
KW  - Spam Bayes
KW  - log file analysis
KW  - spam filter
KW  - word chaining
VL  - 
JA  - Network Operations and Management Symposium (NOMS), 2012 IEEE
DO  - 10.1109/NOMS.2012.6211972
AB  - System log files are critical for troubleshooting complex modern computer systems. Systems can easily produce more log file entries than a human can realistically use. However, there are a number of good filtering and clustering technologies that are used in various areas of data mining. This research focuses on using very easily accessible Bayesian spam filters for categorizing log entries. Results of this research have confirmed that these filters can be effectively used to discover log entries related to known issues, and to effectively disprove outage relationships. Both of these techniques can be easily instrumented in a log analysis framework and provide administrators with much needed filtering for similar logs and thus, similar outages.
ER  - 

TY  - CONF
JO  - Computer Application and System Modeling (ICCASM), 2010 International Conference on
TI  - Research of the honeynet log based on the IDA analyzer
T2  - Computer Application and System Modeling (ICCASM), 2010 International Conference on
IS  - 
SN  - 
VO  - 7
SP  - V7-468
EP  - V7-470
AU  - Bai Wan-min
AU  - Wang Zhong-sheng
Y1  - 22-24 Oct. 2010
PY  - 2010
KW  - data analysis
KW  - data mining
KW  - file servers
KW  - security of data
KW  - Honeynet secure remote log server
KW  - IDA analyzer
KW  - data analysis
KW  - pre-log data mining
KW  - IP networks
KW  - Variable speed drives
KW  - Data mining
KW  - IDA Analyzer
KW  - intrusion model
KW  - logging servers
VL  - 7
JA  - Computer Application and System Modeling (ICCASM), 2010 International Conference on
DO  - 10.1109/ICCASM.2010.5619106
AB  - In this paper, building a prototype system and the Honeynet secure remote log server, based on the right on the remote log server log data analysis and mining, will Honeynet log mining analytical framework designed to achieve pre-processing of log data, through the IDA to the pre-log data mining, Finally, the results were analyzed to achieve the purpose of effectively preventing network attacks.
ER  - 

TY  - CONF
JO  - High Assurance Systems Engineering Symposium, 2008. HASE 2008. 11th IEEE
TI  - Small Logs for Transactional Services: Distinction is Much More Accurate than (Positive) Discrimination
T2  - High Assurance Systems Engineering Symposium, 2008. HASE 2008. 11th IEEE
IS  - 
SN  - 1530-2059
VO  - 
SP  - 97
EP  - 106
AU  - Biswas, D.
AU  - Gazagnaire, T.
AU  - Genest, B.
Y1  - 3-5 Dec. 2008
PY  - 2008
KW  - data privacy
KW  - middleware
KW  - security of data
KW  - software fault tolerance
KW  - system monitoring
KW  - transaction processing
KW  - finite state machines
KW  - logging
KW  - middleware aspects
KW  - privacy constraints
KW  - security constraints
KW  - transactional services
KW  - Automata
KW  - Horses
KW  - Middleware
KW  - Observability
KW  - Privacy
KW  - Research and development
KW  - Security
KW  - Systems engineering and theory
KW  - Testing
KW  - Web services
KW  - Small Logs
KW  - Web Services
KW  - minimal marker placement
KW  - observability
VL  - 
JA  - High Assurance Systems Engineering Symposium, 2008. HASE 2008. 11th IEEE
DO  - 10.1109/HASE.2008.24
AB  - For complex services, logging is an integral part of many middleware aspects, especially, transactions and monitoring. In the event of a failure, the log allows us to deduce the cause of failure (diagnosis), recover by compensating the logged actions (atomicity), etc. However, for heterogeneous services, logging all the actions is often impracticable due to privacy/security constraints. Also, logging is expensive in terms of both time and space. Thus, we are interested in determining a small number of actions that needs to be logged, to know with certainty the actual sequence of executed actions from any given partial log. We propose two heuristics to determine such a small set of transitions, with services modeled as finite state machines. The first one is based on (Positive) discrimination of transitions, using every observation to know (discriminate) that a maximal number of transitions occurred. We characterize it algebraically, giving a very fast algorithm. The second algorithm, the distinguishing algorithm, uses every observation to maximize the number of transitions which are ensured not to have occurred. We show experimentally that the second algorithm gives much more accurate results than the first one, although it is also slower (but still fast enough).
ER  - 

TY  - CONF
JO  - Computer Science & Education, 2009. ICCSE '09. 4th International Conference on
TI  - Vista event log file parsing based on XML technology
T2  - Computer Science & Education, 2009. ICCSE '09. 4th International Conference on
IS  - 
SN  - 
VO  - 
SP  - 1186
EP  - 1190
AU  - Huang Xiaoyu
AU  - Wu shunxiang
Y1  - 25-28 July 2009
PY  - 2009
KW  - XML
KW  - operating systems (computers)
KW  - security of data
KW  - tree data structures
KW  - Microsoft Windows Vista operating system
KW  - Windows NT operating system
KW  - binary XML technology
KW  - computer forensics
KW  - event log file parsing
KW  - tree data structure
KW  - Computer crime
KW  - Computer science
KW  - Digital filters
KW  - Forensics
KW  - Information filtering
KW  - Information filters
KW  - Magnetic heads
KW  - Operating systems
KW  - Space technology
KW  - XML
KW  - Binary XML
KW  - Vista Event log
KW  - file parsing
VL  - 
JA  - Computer Science & Education, 2009. ICCSE '09. 4th International Conference on
DO  - 10.1109/ICCSE.2009.5228462
AB  - Microsoft Windows Vista operating system provides a new design of event log service, which is totally different with Windows NT operating system. It uses binary XML technology to organize the data. The structure of the event log file is complex and the information is not directviewing. This paper propose a solution that we adopt XML technology to parse Vista event log file and to present the result intuitively. The result can be applied into further computer forensics.
ER  - 

TY  - CONF
JO  - Automated Software Engineering, 1998. Proceedings. 13th IEEE International Conference on
TI  - Testing using log file analysis: tools, methods, and issues
T2  - Automated Software Engineering, 1998. Proceedings. 13th IEEE International Conference on
IS  - 
SN  - 
VO  - 
SP  - 157
EP  - 166
AU  - Andrews, J.H.
Y1  - 13-16 Oct 1998
PY  - 1998
KW  - formal specification
KW  - program testing
KW  - program verification
KW  - software tools
KW  - specification languages
KW  - formal verification
KW  - log file analysis
KW  - logging policies
KW  - program testing
KW  - software tools
KW  - software verification
KW  - specification language
KW  - system faults
KW  - system-level testing
KW  - test oracles
KW  - unit-level testing
KW  - Computer science
KW  - Formal specifications
KW  - Formal verification
KW  - Identity-based encryption
KW  - Programming profession
KW  - Prototypes
KW  - Reactive power
KW  - System testing
VL  - 
JA  - Automated Software Engineering, 1998. Proceedings. 13th IEEE International Conference on
DO  - 10.1109/ASE.1998.732614
AB  - Large software systems often keep log files of events. Such log files can be analyzed to check whether a run of a program reveals faults in the system. We discuss how such log files can be used in software testing. We present a framework for automatically analyzing log files, and describe a language for specifying analyzer programs and an implementation of that language. The language permits compositional, compact specifications of software, which act as test oracles; we discuss the use and efficacy of these oracles for unit- and system-level testing in various settings. We explore methodological issues such as efficiency and logging policies, and the scope and limitations of the framework. We conclude that testing using log file analysis constitutes a useful methodology for software verification, somewhere between current testing practice and formal verification methodologies
ER  - 

TY  - CONF
JO  - Semantic Computing and Applications, 2008. IWSCA '08. IEEE International Workshop on
TI  - Anomaly Detection over Clustering Multi-dimensional Transactional Audit Streams
T2  - Semantic Computing and Applications, 2008. IWSCA '08. IEEE International Workshop on
IS  - 
SN  - 
VO  - 
SP  - 78
EP  - 80
AU  - Nam Hun Park
AU  - Won Suk Lee
Y1  - 10-11 July 2008
PY  - 2008
KW  - data mining
KW  - security of data
KW  - anomaly detection
KW  - data mining techniques
KW  - finite audit data set
KW  - multidimensional audit data stream
KW  - multidimensional transactional audit streams
KW  - statistics
KW  - Algorithm design and analysis
KW  - Application software
KW  - Clustering algorithms
KW  - Computer applications
KW  - Computer science
KW  - Conferences
KW  - Data mining
KW  - Feature extraction
KW  - Statistics
KW  - World Wide Web
KW  - Anomaly Detection
KW  - Log data stream
KW  - clustering
VL  - 
JA  - Semantic Computing and Applications, 2008. IWSCA '08. IEEE International Workshop on
DO  - 10.1109/IWSCA.2008.17
AB  - In anomaly detection, one important issue how to model the normal behavior of activities performed by a user is an important issue. To extract the normal behavior from the activities of a user, conventional data mining techniques are widely applied to a finite audit data set. However, these approaches can only model the static behavior of a user in the audit data set. This drawback can be overcome by viewing the continuous activities of a user as an audit data stream. This paper proposes an anomaly detection method that continuously models the normal behavior of a user over the multi-dimensional audit data stream. Each cluster represents the frequent range of the activities with respect to a set of features. As a result, without physically maintaining any historical activity of a user, the new activities of the user can be continuously reflected onto the on-going result. At the same time, various statistics of the activities related to the identified clusters are additionally modeled to improve the performance of anomaly detection. The proposed algorithm is analyzed by a series of experiments to identify various characteristics.
ER  - 

TY  - CONF
JO  - Computer Science and Software Engineering (JCSSE), 2011 Eighth International Joint Conference on
TI  - Distributed Honeypot log management and visualization of attacker geographical distribution
T2  - Computer Science and Software Engineering (JCSSE), 2011 Eighth International Joint Conference on
IS  - 
SN  - 
VO  - 
SP  - 23
EP  - 28
AU  - Visoottiviseth, V.
AU  - Jaralrungroj, U.
AU  - Phoomrungraungsuk, E.
AU  - Kultanon, P.
Y1  - 11-13 May 2011
PY  - 2011
KW  - Internet
KW  - database management systems
KW  - file servers
KW  - security of data
KW  - GeoPlot software
KW  - WHOIS database
KW  - Web interface
KW  - database server
KW  - distributed Honeypot Log management
KW  - geographical distribution attacker
KW  - log management server
KW  - Distributed Server
KW  - Honeyd
KW  - Honeynet
KW  - Honeypot
KW  - Management Server
KW  - Network Security
VL  - 
JA  - Computer Science and Software Engineering (JCSSE), 2011 Eighth International Joint Conference on
DO  - 10.1109/JCSSE.2011.5930083
AB  - Honeypot is a prominent technology that helps us learn new hacking techniques from attackers and intruders. The much information from multiple Honeypot servers, the more appropriate signatures we can generate. To ease the administrator to manage and monitor trace files from multiple Honeypot servers that are distributed in various locations at the same time, in this paper we design and implement a prototype of log management server to automatically and periodically collect log files from them. Information reported by each Honeypot server will be sent in secure manner to the log management server. The log management server then parses the information into the database server, where users can search for specific information through the web interface, such as searching based on one or two Honeypot servers. Moreover, the geographical distribution of attackers is visualized in the world map by utilizing the WHOIS database and GeoPlot software.
ER  - 

TY  - CONF
JO  - Cloud Computing Technology and Science (CloudCom), 2014 IEEE 6th International Conference on
TI  - PADM: Page Rank-Based Anomaly Detection Method of Log Sequences by Graph Computing
T2  - Cloud Computing Technology and Science (CloudCom), 2014 IEEE 6th International Conference on
IS  - 
SN  - 
VO  - 
SP  - 700
EP  - 703
AU  - Xiaoben Yan
AU  - Wei Zhou
AU  - Yun Gao
AU  - Zhang Zhang
AU  - Jizhong Han
AU  - Ge Fu
Y1  - 15-18 Dec. 2014
PY  - 2014
KW  - Web sites
KW  - graph theory
KW  - security of data
KW  - PADM
KW  - Page Rank-based anomaly detection method
KW  - graph computing
KW  - log anomaly detection method
KW  - log records
KW  - log sequences
KW  - software exception
KW  - software service company
KW  - training logs
KW  - Algorithm design and analysis
KW  - Markov processes
KW  - Scalability
KW  - Software
KW  - Testing
KW  - Time complexity
KW  - Training
KW  - anomaly detection
KW  - graph computing
KW  - graph representation
KW  - log sequences
KW  - pagerank value
VL  - 
JA  - Cloud Computing Technology and Science (CloudCom), 2014 IEEE 6th International Conference on
DO  - 10.1109/CloudCom.2014.70
AB  - With the popularity of various software applications in cloud computing, software exception becomes an important issue. How to detect the exceptions more quickly seems to be crucial for the software service company. To solve the above problem, this paper presents an efficient log anomaly detection method named PADM (Page Rank-based Anomaly Detection Method) based on the graph computing algorithm. In this method, the logs are transformed into a graph to represent the complex relationship between the log records, then we design an extended Page Rank algorithm based on the graph to get the importance score for each log. After that, we compare the scores to that of the training logs to determine whether they are abnormal or not. Finally, we compare PADM with other anomaly detection methods on the real logs, and the results show that it outperforms the currently widely used mechanisms with higher accuracy, lower time complexity and better scalability.
ER  - 

TY  - CONF
JO  - Hybrid Intelligent Systems (HIS), 2014 14th International Conference on
TI  - Applying non-negative matrix factorization methods to discover user;s resource access patterns for computer security tasks
T2  - Hybrid Intelligent Systems (HIS), 2014 14th International Conference on
IS  - 
SN  - 
VO  - 
SP  - 43
EP  - 48
AU  - Tsarev, D.
AU  - Kurynin, R.
AU  - Petrovskiy, M.
AU  - Mashechkin, I.
Y1  - 14-16 Dec. 2014
PY  - 2014
KW  - matrix decomposition
KW  - security of data
KW  - NMF-based approach
KW  - NMF-based method
KW  - computer security task
KW  - employee access need
KW  - nonnegative matrix factorization method
KW  - resource access pattern
KW  - Companies
KW  - Computational modeling
KW  - Computer security
KW  - Data models
KW  - Matrix decomposition
KW  - computer security
KW  - logs mining
KW  - missing values imputation
KW  - non-negative matrix factorization
KW  - singular value decomposition
KW  - user behavior modelling
KW  - weight of evidence
VL  - 
JA  - Hybrid Intelligent Systems (HIS), 2014 14th International Conference on
DO  - 10.1109/HIS.2014.7086172
AB  - In the paper we describe the NMF-based approach applied to the problem of determining an employee's access needs. The conducted research showed that the proposed NMF-based methods provide a useful analytical framework for processing and modeling employee's access needs data, and the obtained results demonstrate acceptable performance and provide descriptive representation model.
ER  - 

TY  - CONF
JO  - Machine Learning and Cybernetics, 2003 International Conference on
TI  - Intelligent query in intrusion detection audit system
T2  - Machine Learning and Cybernetics, 2003 International Conference on
IS  - 
SN  - 
VO  - 4
SP  - 2212
EP  - 2216 Vol.4
AU  - Fei Gao
AU  - Qiang Xue
AU  - Ji-Zhou Sun
Y1  - 2-5 Nov. 2003
PY  - 2003
KW  - XML
KW  - auditing
KW  - grammars
KW  - natural languages
KW  - query processing
KW  - security of data
KW  - XML
KW  - audit log file database
KW  - extension markup language
KW  - function unification grammar
KW  - intelligent query
KW  - intrusion detection system
KW  - natural language process technology
KW  - Intelligent systems
KW  - Internet
KW  - Intrusion detection
KW  - Libraries
KW  - Markup languages
KW  - Natural languages
KW  - Spatial databases
KW  - Terminology
KW  - Vocabulary
KW  - XML
VL  - 4
JA  - Machine Learning and Cybernetics, 2003 International Conference on
DO  - 10.1109/ICMLC.2003.1259874
AB  - With the development of Internet, the audit work of IDS (intrusion detection system) is becoming harder. The way of examining log file in text format cannot adapt to the serious situation. In this paper, the NLP (natural language process) technology is introduced to resolve this problem, which can provide a way to interact with audit log file database easily. The FUG (function unification grammar) in NLP is applied to intelligent query in IDS audit system, and XML (extension markup language) schema is utilized in expression of accidence, syntax, glossary library and grammar. At the same time, the feature structure is used to describe the structure of vocabulary, phrase and sentence. These measures can make the query system more intelligent, extendable and friendly.
ER  - 

TY  - CONF
JO  - System Sciences, 2009. HICSS '09. 42nd Hawaii International Conference on
TI  - Vote Selling, Voter Anonymity, and Forensic Logging of Electronic Voting Machines
T2  - System Sciences, 2009. HICSS '09. 42nd Hawaii International Conference on
IS  - 
SN  - 1530-1605
VO  - 
SP  - 1
EP  - 10
AU  - Peisert, S.
AU  - Bishop, M.
AU  - Yasinsac, A.
Y1  - 5-8 Jan. 2009
PY  - 2009
KW  - government data processing
KW  - security of data
KW  - e-voting system
KW  - electronic voting machine
KW  - forensic audit trail
KW  - forensic logging
KW  - vote selling
KW  - voter anonymity
KW  - Computer science
KW  - Electronic equipment testing
KW  - Electronic voting
KW  - Electronic voting systems
KW  - Forensics
KW  - Guidelines
KW  - NIST
KW  - Nominations and elections
KW  - Security
KW  - System testing
VL  - 
JA  - System Sciences, 2009. HICSS '09. 42nd Hawaii International Conference on
DO  - 10.1109/HICSS.2009.503
AB  - Much recent work has focused on the process of auditing the results of elections. Little work has focused on auditing the e-voting systems currently in use. The facilities for doing the former include the voter-verified paper audit trail; unfortunately, that VVPAT is not particularly helpful in tracking down the source of errors within e-voting systems. This paper discusses the need for a detailed forensic audit trail (FAT) to enable auditors to analyze the actions of e-voting systems, in order to demonstrate either the absence of problems or to find the causes of problems. We also discuss methods to prevent the use of the FAT as a covert channel for violating the necessary properties of secrecy of the ballot, so voters cannot sell their votes, and anonymity of the ballot, so a third party cannot associate a particular ballot with the voter who cast it.
ER  - 

TY  - CONF
JO  - Mass Storage Systems and Technologies, 2007. MSST 2007. 24th IEEE Conference on
TI  - Efficient Logging and Replication Techniques for Comprehensive Data Protection
T2  - Mass Storage Systems and Technologies, 2007. MSST 2007. 24th IEEE Conference on
IS  - 
SN  - 
VO  - 
SP  - 171
EP  - 184
AU  - Maohua Lu
AU  - Shibiao Lin
AU  - Tzi-cker Chiueh
Y1  - 24-27 Sept. 2007
PY  - 2007
KW  - local area networks
KW  - multicast communication
KW  - peripheral interfaces
KW  - security of data
KW  - storage area networks
KW  - telecommunication traffic
KW  - Mariner
KW  - commodity ATA disk
KW  - continuous data protection
KW  - disk update
KW  - gigabit Ethernet technologies
KW  - iSCSI-based storage system
KW  - low-latency disk writing
KW  - network traffic load
KW  - replication technique
KW  - storage area networks
KW  - track-based logging technique
KW  - transparent reliable multicast mechanism
KW  - Access protocols
KW  - Delay
KW  - Ethernet networks
KW  - Marine technology
KW  - Multicast protocols
KW  - Protection
KW  - Prototypes
KW  - Storage area networks
KW  - Telecommunication traffic
KW  - Transmission line measurements
VL  - 
JA  - Mass Storage Systems and Technologies, 2007. MSST 2007. 24th IEEE Conference on
DO  - 10.1109/MSST.2007.4367972
AB  - Mariner is an iSCSI-based storage system that is designed to provide comprehensive data protection on commodity ATA disk and gigabit Ethernet technologies while offering the same performance as those without any such protection. In particular, Mariner supports continuous data protection (CDP) that allows every disk update within a time window to be undoable, and local/remote mirroring to guard data against machine/site failures. To minimize the performance overhead associated with CDP, Mariner employs a modified track-based logging technique that unifies the long-term logging required for CDP and short-term logging for low-latency disk writes. This new logging technique strikes an optimal balance among log space utilization, disk write latency, and ease of historical data access. To reduce the performance penalty of physical data replication used in local/remote mirroring, Mariner features a modified two-phase commit protocol that in turn is built on top of a novel transparent reliable multicast (TRM) mechanism specifically designed for Ethernet-based storage area networks. Without flooding the network, TRM is able to keep the network traffic load of reliable N-way replication roughly at the same level as the no-replication case, regardless of the value of N. Empirical performance measurements on the first Mariner prototype, which is built from gigabit Ethernet and ATA disks, shows that the average end-to-end latency for a 4KByte iSCSI write is under 1.2 msec when data logging and replication are both turned on.
ER  - 

TY  - CONF
JO  - Information Communication and Embedded Systems (ICICES), 2014 International Conference on
TI  - Ascertaining security in online reputation systems using Rate Auditing Tool (RAT)
T2  - Information Communication and Embedded Systems (ICICES), 2014 International Conference on
IS  - 
SN  - 
VO  - 
SP  - 1
EP  - 5
AU  - Dhivyalakshmi, D.
AU  - Kalaivani, K.
AU  - Tamilarasi, V.
AU  - Bhavani, P.
Y1  - 27-28 Feb. 2014
PY  - 2014
KW  - information filtering
KW  - security of data
KW  - social networking (online)
KW  - RAT
KW  - geographical location
KW  - information security
KW  - logged in time
KW  - logged out time
KW  - rate auditing tool
KW  - rating manipulation
KW  - social network
KW  - Communities
KW  - Educational institutions
KW  - Monitoring
KW  - Radiation detectors
KW  - Security
KW  - Servers
KW  - Videos
KW  - Information security
KW  - information filtering
KW  - social network
VL  - 
JA  - Information Communication and Embedded Systems (ICICES), 2014 International Conference on
DO  - 10.1109/ICICES.2014.7033951
AB  - In our proposed model we implement Rate Auditing Tool (RAT) to monitor each and every rating manipulation. It checks whether the time logged in and logged out matches the stipulated time for viewing any videos or messages and giving appropriate rating. It monitors either the video provided is fully or atleast partly viewed thus accordingly their ratings given. It also checks whether the login in network which is providing rating is either from same location or its location varies each and every time from vast geographical location. It also checks whether from same ip address multiple logins giving ratings continuously. Thus by doing this the attacker is also detected and given counter measure to their attacks.
ER  - 

TY  - CONF
JO  - Database and Expert Systems Applications, 1999. Proceedings. Tenth International Workshop on
TI  - NIGELOG: protecting logging information by hiding multiple backups in directories
T2  - Database and Expert Systems Applications, 1999. Proceedings. Tenth International Workshop on
IS  - 
SN  - 
VO  - 
SP  - 874
EP  - 878
AU  - Takada, T.
AU  - Koike, H.
Y1  - 1999
PY  - 1999
KW  - back-up procedures
KW  - data encapsulation
KW  - object-oriented programming
KW  - security of data
KW  - software libraries
KW  - C++ class library
KW  - NIGELOG
KW  - UNIX applications
KW  - automatic logging information restoration
KW  - directories
KW  - intruders
KW  - log files
KW  - logging information protection
KW  - multiple backup hiding
KW  - standard software libraries
KW  - Protection
VL  - 
JA  - Database and Expert Systems Applications, 1999. Proceedings. Tenth International Workshop on
DO  - 10.1109/DEXA.1999.795297
AB  - This paper proposes a novel method to protect logging information more securely. The method, named NIGELOG, produces multiple backups of the logging information, hides them in arbitrary directories, and periodically moves these backups to other directories. Also, NIGELOG can automatically restore the original logging information by using the backups even if the original file is altered or deleted by intruders. NIGELOG is implemented as a C++ class library with some standard software libraries and does not require additional hardware such as write-once media. It is easier to apply NIGELOG to UNIX applications which produces log-files
ER  - 

TY  - CONF
JO  - Scalable Computing and Communications; Eighth International Conference on Embedded Computing, 2009. SCALCOM-EMBEDDEDCOM'09. International Conference on
TI  - A Method of Log File Analysis for Test Oracle
T2  - Scalable Computing and Communications; Eighth International Conference on Embedded Computing, 2009. SCALCOM-EMBEDDEDCOM'09. International Conference on
IS  - 
SN  - 
VO  - 
SP  - 351
EP  - 354
AU  - Dan Tu
AU  - Rong Chen
AU  - Zhenjun Du
AU  - Yaqing Liu
Y1  - 25-27 Sept. 2009
PY  - 2009
KW  - data structures
KW  - finite state machines
KW  - program debugging
KW  - program testing
KW  - data structure
KW  - log file analysis
KW  - software behavior
KW  - software debugging
KW  - software testing
KW  - test oracle
KW  - tuple state machine model
KW  - unit-testing level
KW  - Data structures
KW  - Debugging
KW  - Embedded computing
KW  - Failure analysis
KW  - History
KW  - Information analysis
KW  - Information science
KW  - Runtime
KW  - Software testing
KW  - System testing
KW  - log file analysis
KW  - software testing
KW  - state machine
KW  - test oracle
VL  - 
JA  - Scalable Computing and Communications; Eighth International Conference on Embedded Computing, 2009. SCALCOM-EMBEDDEDCOM'09. International Conference on
DO  - 10.1109/EmbeddedCom-ScalCom.2009.69
AB  - An oracle is a mechanism commonly used for determining whether a system has passed or failed a test, which is very important for software testing. A log file, used to record the history of software's behavior, provides a means for debugging and testing. Log file analysis (LFA) is a lately applied approach for test oracle, having shown its applicable prospect in software testing. To achieve the better feature of general purpose, we propose a practical test oracle framework based on log file analysis, presents a 7-tuple state machine model for the analyzer and designs a description language to describe the state machine. The data structures and the algorithm for the implementation of this method are also given. Experimental results show this method can be easily used to test software whether on system-testing level or unit-testing level, suitable for various software testing.
ER  - 

TY  - JOUR
JO  - Knowledge and Data Engineering, IEEE Transactions on
TI  - Protection of Database Security via Collaborative Inference Detection
T2  - Knowledge and Data Engineering, IEEE Transactions on
IS  - 8
SN  - 1041-4347
VO  - 20
SP  - 1013
EP  - 1027
AU  - Yu Chen
AU  - Chu, W.W.
Y1  - Aug. 2008
PY  - 2008
KW  - database management systems
KW  - groupware
KW  - inference mechanisms
KW  - probability
KW  - query processing
KW  - security of data
KW  - collaborative inference detection
KW  - data dependency
KW  - database schema
KW  - database security protection
KW  - inference probability
KW  - query log
KW  - query sequences
KW  - query-time inference violation detection system
KW  - semantic inference graph
KW  - semantic inference model
KW  - semantic knowledge
KW  - sensitive data content protection
KW  - task-sensitive collaboration levels
KW  - Inference engines
KW  - Security and Privacy Protection
VL  - 20
JA  - Knowledge and Data Engineering, IEEE Transactions on
DO  - 10.1109/TKDE.2007.190642
AB  - Malicious users can exploit the correlation among data to infer sensitive information from a series of seemingly innocuous data accesses. Thus, we develop an inference violation detection system to protect sensitive data content. Based on data dependency, database schema and semantic knowledge, we constructed a semantic inference model (SIM) that represents the possible inference channels from any attribute to the pre-assigned sensitive attributes. The SIM is then instantiated to a semantic inference graph (SIG) for query-time inference violation detection. For a single user case, when a user poses a query, the detection system will examine his/her past query log and calculate the probability of inferring sensitive information. The query request will be denied if the inference probability exceeds the prespecified threshold. For multi-user cases, the users may share their query answers to increase the inference probability. Therefore, we develop a model to evaluate collaborative inference based on the query sequences of collaborators and their task-sensitive collaboration levels. Experimental studies reveal that information authoritativeness, communication fidelity and honesty in collaboration are three key factors that affect the level of achievable collaboration. An example is given to illustrate the use of the proposed technique to prevent multiple collaborative users from deriving sensitive information via inference.
ER  - 

TY  - JOUR
JO  - Software Engineering, IEEE Transactions on
TI  - General test result checking with log file analysis
T2  - Software Engineering, IEEE Transactions on
IS  - 7
SN  - 0098-5589
VO  - 29
SP  - 634
EP  - 648
AU  - Andrews, J.H.
AU  - Yingjun Zhang
Y1  - July 2003
PY  - 2003
KW  - finite state machines
KW  - formal specification
KW  - program debugging
KW  - program testing
KW  - program verification
KW  - event-based debugging
KW  - lightweight formal methods
KW  - log file
KW  - safety verification
KW  - software testing
KW  - state-machine-based formalism
KW  - test oracles
KW  - unit testing
KW  - Automatic testing
KW  - Failure analysis
KW  - Humans
KW  - Inspection
KW  - Performance analysis
KW  - Real time systems
KW  - Software safety
KW  - Software testing
KW  - System testing
KW  - Writing
VL  - 29
JA  - Software Engineering, IEEE Transactions on
DO  - 10.1109/TSE.2003.1214327
AB  - We describe and apply a lightweight formal method for checking test results. The method assumes that the software under test writes a text log file; this log file is then analyzed by a program to see if it reveals failures. We suggest a state-machine-based formalism for specifying the log file analyzer programs and describe a language and implementation based on that formalism. We report on empirical studies of the application of log file analysis to random testing of units. We describe the results of experiments done to compare the performance and effectiveness of random unit testing with coverage checking and log file analysis to other unit testing procedures. The experiments suggest that writing a formal log file analyzer and using random testing is competitive with other formal and informal methods for unit testing.
ER  - 

TY  - CONF
JO  - Computer Software and Applications Conference (COMPSAC), 2012 IEEE 36th Annual
TI  - Domain Independent Event Analysis for Log Data Reduction
T2  - Computer Software and Applications Conference (COMPSAC), 2012 IEEE 36th Annual
IS  - 
SN  - 0730-3157
VO  - 
SP  - 225
EP  - 232
AU  - Kalamatianos, T.
AU  - Kontogiannis, K.
AU  - Matthews, P.
Y1  - 16-20 July 2012
PY  - 2012
KW  - data analysis
KW  - data reduction
KW  - program diagnostics
KW  - security of data
KW  - DARPA Intrusion Detection Evaluation 1999 data sets
KW  - KDD 1999 data sets
KW  - domain independent event analysis
KW  - large software systems
KW  - log analysis technique
KW  - log data reduction
KW  - run time behavior analysis
KW  - similarity score
KW  - Algorithm design and analysis
KW  - Analytical models
KW  - Intrusion detection
KW  - Software
KW  - Standards
KW  - Weight measurement
KW  - Software engineering
KW  - dynamic analysis
KW  - log analysis
KW  - log reduction
KW  - software maintenance
KW  - system understanding
VL  - 
JA  - Computer Software and Applications Conference (COMPSAC), 2012 IEEE 36th Annual
DO  - 10.1109/COMPSAC.2012.33
AB  - Analyzing the run time behavior of large software systems is a difficult and challenging task. Log analysis has been proposed as a possible solution. However, such an analysis poses unique challenges, mostly due to the volume and diversity of the logged data that is collected, thus making this analysis often intractable for practical purposes. In this paper, we present a log analysis technique that aims to compute a smaller, compared to the original, collection of events that relate to a given analysis objective. The technique is based on computing a similarity score between the logged events and a collection of significant events that we refer to as beacons. The major novelties of the proposed technique are that it is domain independent and that it does not require the use of a pre-existing training data set. The technique has been evaluated against the DARPA Intrusion Detection Evaluation 1999 and the KDD 1999 data sets with promising results.
ER  - 

TY  - CONF
JO  - Information Assurance Workshop, 2005. IAW '05. Proceedings from the Sixth Annual IEEE SMC
TI  - A log independent distributed database damage assessment model
T2  - Information Assurance Workshop, 2005. IAW '05. Proceedings from the Sixth Annual IEEE SMC
IS  - 
SN  - 
VO  - 
SP  - 302
EP  - 309
AU  - Jing Zhou
AU  - Panda, B.
AU  - Yi Hu
Y1  - 15-17 June 2005
PY  - 2005
KW  - data structures
KW  - distributed databases
KW  - security of data
KW  - transaction processing
KW  - data damage
KW  - data structures
KW  - distributed database
KW  - information attack
KW  - log independent damage assessment model
KW  - malicious transaction
KW  - transaction relationship
KW  - Computer science
KW  - Data security
KW  - Data structures
KW  - Database systems
KW  - Distributed databases
KW  - Information security
KW  - Information systems
KW  - Intrusion detection
KW  - Transaction databases
KW  - USA Councils
VL  - 
JA  - Information Assurance Workshop, 2005. IAW '05. Proceedings from the Sixth Annual IEEE SMC
DO  - 10.1109/IAW.2005.1495967
AB  - In a distributed database system, damage assessment after an information attack is a complicated task due to intricate transaction relationships among distributed sites. In these systems, when any sub-transaction reads a damaged data at any site the entire transaction of which the sub-transaction is a part is considered affected by the damage. Hence, the data items updated by that transaction irrespective of sites are also considered damaged. To control spread of damage, accurate damage assessment and appropriate recovery procedures must be performed as soon as an attack is detected. This research focuses on damage assessment procedures for distributed database systems and uses pre-developed data structures for fast and accurate result. The method presented in this paper quickly identifies all affected transactions and all damaged data items without any log access.
ER  - 

TY  - CONF
JO  - Availability, Reliability and Security, 2009. ARES '09. International Conference on
TI  - Blue Gene/L Log Analysis and Time to Interrupt Estimation
T2  - Availability, Reliability and Security, 2009. ARES '09. International Conference on
IS  - 
SN  - 
VO  - 
SP  - 173
EP  - 180
AU  - Taerat, N.
AU  - Naksinehaboon, N.
AU  - Chandler, C.
AU  - Elliott, J.
AU  - Leangsuksun, C.B.
AU  - Ostrouchov, G.
AU  - Scott, S.L.
AU  - Engelmann, C.
Y1  - 16-19 March 2009
PY  - 2009
KW  - parallel machines
KW  - system recovery
KW  - systems analysis
KW  - Blue Gene/L log analysis
KW  - Blue Gene/L supercomputer
KW  - application-level failure
KW  - duplicate log message
KW  - high performance computing
KW  - log file analysis
KW  - system-level failure
KW  - temporal filtering
KW  - time-to-interrupt estimation
KW  - Computer architecture
KW  - Failure analysis
KW  - Information filtering
KW  - Information filters
KW  - Large-scale systems
KW  - Mission critical systems
KW  - Power system modeling
KW  - Power system reliability
KW  - Predictive models
KW  - Supercomputers
VL  - 
JA  - Availability, Reliability and Security, 2009. ARES '09. International Conference on
DO  - 10.1109/ARES.2009.105
AB  - System- and application-level failures could be characterized by analyzing relevant log files. The resulting data might then be used in numerous studies on and future developments for the mission-critical and large scale computational architecture, including fields such as failure prediction, reliability modeling, performance modeling and power awareness. In this paper, system logs covering a six month period of the Blue Gene/L supercomputer were obtained and subsequently analyzed. Temporal filtering was applied to remove duplicated log messages. Optimistic and pessimistic perspectives were exerted on filtered log information to observe failure behavior within the system. Further, various time to repair factors were applied to obtain application time to interrupt, which will be exploited in further resilience modeling research.
ER  - 

TY  - CONF
JO  - Electronic Systems, Signal Processing and Computing Technologies (ICESC), 2014 International Conference on
TI  - Detection of Malicious Transaction in Database Using Log Mining Approach
T2  - Electronic Systems, Signal Processing and Computing Technologies (ICESC), 2014 International Conference on
IS  - 
SN  - 
VO  - 
SP  - 262
EP  - 265
AU  - Pathan, A.C.
AU  - Potey, M.A.
Y1  - 9-11 Jan. 2014
PY  - 2014
KW  - SQL
KW  - data mining
KW  - relational databases
KW  - security of data
KW  - SQL
KW  - anomalous database transactions
KW  - automatic discovery
KW  - data mining
KW  - data sequence rules
KW  - domain dependency rules
KW  - intrusion detection
KW  - log mining approach
KW  - malicious transaction detection
KW  - query database
KW  - query structures
KW  - relational databases
KW  - Computers
KW  - Data mining
KW  - Database systems
KW  - Intrusion detection
KW  - Training
KW  - Data Mining
KW  - Database security
KW  - Intrusion Detection
VL  - 
JA  - Electronic Systems, Signal Processing and Computing Technologies (ICESC), 2014 International Conference on
DO  - 10.1109/ICESC.2014.50
AB  - Data mining is the process of finding correlations in the relational databases. There are different techniques for identifying malicious database transactions. Many existing approaches which profile is SQL query structures and database user activities to detect intrusion, the log mining approach is the automatic discovery for identifying anomalous database transactions. Mining of the Data is very helpful to end users for extracting useful business information from large database. Multi-level and multi-dimensional data mining are employed to discover data item dependency rules, data sequence rules, domain dependency rules, and domain sequence rules from the database log containing legitimate transactions. Database transactions that do not comply with the rules are identified as malicious transactions. The log mining approach can achieve desired true and false positive rates when the confidence and support are set up appropriately. The implemented system incrementally maintain the data dependency rule sets and optimize the performance of the intrusion detection process.
ER  - 

TY  - CONF
JO  - Application of Information and Communication Technologies (AICT), 2012 6th International Conference on
TI  - New novel idea for Cloud Computing: How can we use Kalman filter in security of Cloud Computing
T2  - Application of Information and Communication Technologies (AICT), 2012 6th International Conference on
IS  - 
SN  - 
VO  - 
SP  - 1
EP  - 5
AU  - Darbandi, M.
AU  - Shahbazi, P.
AU  - Setayesh, S.
AU  - Granmo, O.-C.
Y1  - 17-19 Oct. 2012
PY  - 2012
KW  - Kalman filters
KW  - cloud computing
KW  - security of data
KW  - Internet
KW  - Kalman filter
KW  - cloud computing security
KW  - hardware configurations
KW  - network crashing
KW  - network hanging
KW  - software configurations
KW  - virtual image
KW  - Cloud computing
KW  - Companies
KW  - Computers
KW  - Educational institutions
KW  - Hardware
KW  - Kalman filters
KW  - Cloud computing and its influences
KW  - Estimation and prediction
KW  - Kalman Filter
KW  - Security
VL  - 
JA  - Application of Information and Communication Technologies (AICT), 2012 6th International Conference on
DO  - 10.1109/ICAICT.2012.6398466
AB  - Cloud is a virtual image about some amount of undefined powers, that is widespread and had unknown power and inexact amount of hardware and software configurations, and because of we have not any information about clouds location and time dimensions and also the amounts of its sources we tell that Cloud Computing. This technology presents lots of abilities and opportunities such as processing power, storage and accessing it from everywhere, supporting, working - team group - with the latest versions of software and etc., by the means of internet. On the other hand, in such a large scale networks we should consider the reliability and powerfulness of such networks in facing with events such as high amount of users that may login to their profiles simultaneously, or for example if we have the ability to predict about what times that we would have the most crowd in network, or even users prefer to use which part of the Cloud Computing more than other parts - which software or hardware configuration. With knowing such information, we can avoid accidental crashing or hanging of the network that may be cause by logging of too much users. In this paper we propose Kalman Filter that can be used for estimating the amounts of users and software's that run on cloud computing or other similar platforms at a certain time. After introducing this filter, at the end of paper, we talk about some potentials of this filter in cloud computing platform. In this paper we demonstrate about how we can use Kalman filter in estimating and predicting of our target, by the means of several examples on Kalman filter.
ER  - 

TY  - CONF
JO  - Advanced Information Networking and Applications, 2006. AINA 2006. 20th International Conference on
TI  - A Distributed-Log-based IP Traceback Scheme to Defeat DDoS Attacks
T2  - Advanced Information Networking and Applications, 2006. AINA 2006. 20th International Conference on
IS  - 
SN  - 1550-445X
VO  - 2
SP  - 25
EP  - 32
AU  - Yinan Jing
AU  - Jingtao Li
AU  - Xueping Wang
AU  - Xiaochun Xiao
AU  - Zhang, G.
Y1  - 18-20 April 2006
PY  - 2006
KW  - IP networks
KW  - Internet
KW  - quality of service
KW  - security of data
KW  - telecommunication security
KW  - DDoS attack
KW  - IP traceback scheme
KW  - Internet protocol
KW  - distributed denial-of-service
KW  - log-based scheme
KW  - Algorithm design and analysis
KW  - Analytical models
KW  - Computer crime
KW  - Convergence
KW  - Degradation
KW  - Filters
KW  - Information science
KW  - Internet
KW  - Intrusion detection
KW  - Testing
VL  - 2
JA  - Advanced Information Networking and Applications, 2006. AINA 2006. 20th International Conference on
DO  - 10.1109/AINA.2006.22
AB  - Distributed denial-of-service attacks have become the major threat to Internet today. IP traceback is one of the most effective techniques to defeat these attacks by identifying attack sources even in the presence of IP spoofing. Because of low marking packet utilization, the convergence time of traditional probabilistic packet marking (PPM) schemes is still too long. In order to shorten the convergence time, a distributed-log-based IP traceback scheme is proposed. Theoretical analysis and simulation results show that this scheme not only can converge more quickly than previous PPM schemes, but also has much less log overhead than other log-based schemes
ER  - 

TY  - CONF
JO  - New Technologies, Mobility and Security (NTMS), 2014 6th International Conference on
TI  - An Efficient Network Log Anomaly Detection System Using Random Projection Dimensionality Reduction
T2  - New Technologies, Mobility and Security (NTMS), 2014 6th International Conference on
IS  - 
SN  - 
VO  - 
SP  - 1
EP  - 5
AU  - Juvonen, A.
AU  - Hamalainen, T.
Y1  - March 30 2014-April 2 2014
PY  - 2014
KW  - digital signatures
KW  - security of data
KW  - telecommunication traffic
KW  - Mahalanobis distance
KW  - anomaly score
KW  - data point
KW  - intrusion attempts
KW  - intrusion detection systems
KW  - log lines
KW  - network data structure
KW  - network log anomaly detection system
KW  - network services
KW  - network traffic structure
KW  - numerical data matrix
KW  - random projection dimensionality reduction
KW  - real-world network data
KW  - signature-based intrusion detection
KW  - Data mining
KW  - Data visualization
KW  - Feature extraction
KW  - Intrusion detection
KW  - Principal component analysis
KW  - Real-time systems
VL  - 
JA  - New Technologies, Mobility and Security (NTMS), 2014 6th International Conference on
DO  - 10.1109/NTMS.2014.6814006
AB  - Network traffic is increasing all the time and network services are becoming more complex and vulnerable. To protect these networks, intrusion detection systems are used. Signature-based intrusion detection cannot find previously unknown attacks, which is why anomaly detection is needed. However, many new systems are slow and complicated. We propose a log anomaly detection framework which aims to facilitate quick anomaly detection and also provide visualizations of the network traffic structure. The system preprocesses network logs into a numerical data matrix, reduces the dimensionality of this matrix using random projection and uses Mahalanobis distance to find outliers and calculate an anomaly score for each data point. Log lines that are too different are flagged as anomalies. The system is tested with real-world network data, and actual intrusion attempts are found. In addition, visualizations are created to represent the structure of the network data. We also perform computational time evaluation to ensure the performance is feasible. The system is fast, finds intrusion attempts and does not need clean training data.
ER  - 

TY  - CONF
JO  - Southeastcon, 2009. SOUTHEASTCON '09. IEEE
TI  - BlackBerry IPD parsing for open source forensics
T2  - Southeastcon, 2009. SOUTHEASTCON '09. IEEE
IS  - 
SN  - 
VO  - 
SP  - 195
EP  - 199
AU  - Fairbanks, K.
AU  - Atreya, K.
AU  - Owen, H.
Y1  - 5-8 March 2009
PY  - 2009
KW  - file organisation
KW  - mobile computing
KW  - mobile handsets
KW  - public domain software
KW  - security of data
KW  - BlackBerry
KW  - IPD parser
KW  - Sleuth Kit
KW  - backup file forensics tool
KW  - interactive pager
KW  - open source forensics tool
KW  - restore file forensics tool
KW  - Forensics
KW  - Handheld computers
VL  - 
JA  - Southeastcon, 2009. SOUTHEASTCON '09. IEEE
DO  - 10.1109/SECON.2009.5174075
AB  - In this paper, we present a framework for an open source BlackBerry Inter@ctive Pager Backup/Restore (IPD) file forensics tool. Our reasoning for developing an open source version of an IPD parser is to enhance the available open source forensic tools; an example of this category of tools is the Sleuth Kit. One intention of this work is to make users of BlackBerrys aware of the vulnerability of their information on their computers. Commercial tools such as the ABC Amber BlackBerry Converter application [6] presently exist. That commercial tool is able to gather the messages, contacts, SMS records, memos, call logs, and the task list from an IPD file. It then exports these records in a variety of forms. Another commercial tool by Paraben [4] can export data into closed source forensic tool kits such as Encase and FTK [5]. While still a work in progress, the preliminary results indicate that the method developed for extracting information is valid. The end goal of this research is to produce a model that can be adopted by open source forensic practitioners in their examinations and toolkit development.
ER  - 

TY  - CONF
JO  - Intelligent System Design and Engineering Application (ISDEA), 2012 Second International Conference on
TI  - System Design of Unified Auditing and Monitoring Based on Complex Network
T2  - Intelligent System Design and Engineering Application (ISDEA), 2012 Second International Conference on
IS  - 
SN  - 
VO  - 
SP  - 1144
EP  - 1147
AU  - Liu Lianzhong
AU  - Li Chunfang
AU  - Li Xiangyu
Y1  - 6-7 Jan. 2012
PY  - 2012
KW  - auditing
KW  - business data processing
KW  - database management systems
KW  - design engineering
KW  - management information systems
KW  - security of data
KW  - MIS system design
KW  - business software
KW  - complex network
KW  - database
KW  - external business logic relationship
KW  - large scale information system
KW  - log integrated managing system
KW  - management information system
KW  - running parameter integrated managing system
KW  - unified auditing and monitoring system design
KW  - Business
KW  - Complex networks
KW  - Databases
KW  - Monitoring
KW  - Security
KW  - Servers
KW  - Software
KW  - Database
KW  - Information Security Audit
KW  - Software Engineering
KW  - complex network
VL  - 
JA  - Intelligent System Design and Engineering Application (ISDEA), 2012 Second International Conference on
DO  - 10.1109/ISdea.2012.557
AB  - Audits are performed to ascertain the validity and reliability of information, also to provide an assessment of a system's internal control. Analyzed here is an incremental design of unified auditing and monitoring platform (UAM) in view point of business software. In fact, UAM is an integrated managing system of logs and running parameters. Database is the core of all kinds of management information system (MIS), and the more business logic implemented in database, the less coding work left to programmers. In this paper, complex network is introduced into the database table designing to visualization the internal associations which denotes the external business logic relationship. Though an example is discussed, but the approach can be widely used in various MIS system design, especially in large scale information system.
ER  - 

TY  - CONF
JO  - Systematic Approaches to Digital Forensic Engineering, 2008. SADFE '08. Third International Workshop on
TI  - Finding the Evidence in Tamper-Evident Logs
T2  - Systematic Approaches to Digital Forensic Engineering, 2008. SADFE '08. Third International Workshop on
IS  - 
SN  - 
VO  - 
SP  - 69
EP  - 75
AU  - Sandler, D.
AU  - Derr, K.
AU  - Crosby, S.
AU  - Wallach, D.S.
Y1  - 22-22 May 2008
PY  - 2008
KW  - data recording
KW  - query processing
KW  - security of data
KW  - Querifier
KW  - flexible pattern-matching language
KW  - forensic scrutiny
KW  - hash chaining
KW  - secure log
KW  - suspicious activity
KW  - tamper-evident chronological records
KW  - tamper-evident logs
KW  - Business
KW  - Data structures
KW  - Digital forensics
KW  - Forgery
KW  - Humans
KW  - Law
KW  - Legal factors
KW  - Power engineering and energy
KW  - Resists
KW  - Runtime
KW  - hash chaining
KW  - predicate logic
KW  - query processing
KW  - secure logs
KW  - tamper evidence
VL  - 
JA  - Systematic Approaches to Digital Forensic Engineering, 2008. SADFE '08. Third International Workshop on
DO  - 10.1109/SADFE.2008.22
AB  - Secure logs are powerful tools for building systems that must resist forgery, prove temporal relationships, and stand up to forensic scrutiny. The proofs of order and integrity encoded in these tamper-evident chronological records, typically built using hash chaining, may be used by applications to enforce operating constraints or sound alarms at suspicious activity. However, existing research stops short of discussing how one might go about automatically determining whether a given secure log satisfies a given set of constraints on its records. In this paper, we discuss our work on Querifier, a tool that accomplishes this. It can be used offline as an analyzer for static logs, or online during the runtime of a logging application. Querifier rules are written in a flexible pattern-matching language that adapts to arbitrary log structures; given a set of rules and available log data, Querifier presents evidence of correctness and offers counterexamples if desired. We describe Querfier's implementation and offer early performance results.
ER  - 

TY  - CONF
JO  - Wireless Communications and Mobile Computing Conference (IWCMC), 2012 8th International
TI  - Log-based intrusion detection for MANET
T2  - Wireless Communications and Mobile Computing Conference (IWCMC), 2012 8th International
IS  - 
SN  - 
VO  - 
SP  - 697
EP  - 702
AU  - Alattar, M.
AU  - Sailhan, F.
AU  - Bourgeois, J.
Y1  - 27-31 Aug. 2012
PY  - 2012
KW  - mobile ad hoc networks
KW  - routing protocols
KW  - security of data
KW  - telecommunication security
KW  - IDAR
KW  - MANET
KW  - OLSR protocol
KW  - ad hoc networks
KW  - ad hoc routing protocols
KW  - advanced intrusion detection
KW  - bandwidth usage
KW  - log based distributed intrusion detector
KW  - log based intrusion detection
KW  - Intrusion detection
KW  - Mobile ad hoc networks
KW  - Routing
KW  - Routing protocols
KW  - Topology
KW  - Intrusion detection
KW  - MANETs
KW  - misuse
KW  - routing protocols
VL  - 
JA  - Wireless Communications and Mobile Computing Conference (IWCMC), 2012 8th International
DO  - 10.1109/IWCMC.2012.6314289
AB  - Ad hoc networks operate mostly over open environments and are hence vulnerable to a large number of threats. This calls for providing advanced intrusion detection. To meet this requirement, we introduce IDAR, a signature- and log-based distributed intrusion detector dedicated to ad hoc routing protocols. Contrary to existing systems that observe packets, IDAR analyses the logs generated by the OLSR protocol and identifies patterns of misuse. This detector copes with the resource-constraints of devices by providing distributed detection. In particular, depending on the level of suspicion/gravity involved, in-depth cooperative investigation is launched. Simulation shows limited bandwidth usage, high detection and low false positives.
ER  - 

TY  - CONF
JO  - Communications, 2009. ICC '09. IEEE International Conference on
TI  - Preprocessing DNS Log Data for Effective Data Mining
T2  - Communications, 2009. ICC '09. IEEE International Conference on
IS  - 
SN  - 1938-1883
VO  - 
SP  - 1
EP  - 5
AU  - Snyder, M.E.
AU  - Sundaram, R.
AU  - Thakur, M.
Y1  - 14-18 June 2009
PY  - 2009
KW  - Internet
KW  - data mining
KW  - security of data
KW  - statistical analysis
KW  - telecommunication traffic
KW  - DNS servers
KW  - Internet traffic
KW  - bandwidth attacks
KW  - data mining
KW  - data-intensive problem
KW  - domain name service log data
KW  - statistical patterns
KW  - Communications Society
KW  - Computer science
KW  - Data mining
KW  - Information science
KW  - Interpolation
KW  - Network servers
KW  - Peer to peer computing
KW  - USA Councils
KW  - Web and internet services
KW  - Web server
VL  - 
JA  - Communications, 2009. ICC '09. IEEE International Conference on
DO  - 10.1109/ICC.2009.5199359
AB  - The domain name service (DNS) provides a critical function in directing Internet traffic. Defending DNS servers from bandwidth attacks is assisted by the ability to effectively mine DNS log data for statistical patterns. Processing DNS log data can be classified as a data-intensive problem, and as such presents challenges unique to this class of problem. When problems occur in capturing log data, or when the DNS server experiences an outage (scheduled or unscheduled), the normal pattern of traffic for that server becomes clouded. Simple linear interpolation of the holes in the data does not preserve features such as peaks in traffic (which can occur during an attack, making them of particular interest). We demonstrate a method for estimating values for missing portions of time sensitive DNS log data. This method would be suitable for use with a variety of datasets containing time series values where certain portions are missing.
ER  - 

TY  - CONF
JO  - Information Engineering and Electronic Commerce (IEEC), 2010 2nd International Symposium on
TI  - Research on Privacy Preserving Data in Web Log Mining
T2  - Information Engineering and Electronic Commerce (IEEC), 2010 2nd International Symposium on
IS  - 
SN  - 
VO  - 
SP  - 1
EP  - 4
AU  - Chang-bin Jiang
AU  - Li Chen
Y1  - 23-25 July 2010
PY  - 2010
KW  - Internet
KW  - data mining
KW  - data privacy
KW  - evolutionary computation
KW  - security of data
KW  - Web data mining
KW  - Web log mining
KW  - cloud model
KW  - evolutionary algorithm
KW  - privacy preserving mining model
KW  - private data protection
KW  - Clouds
KW  - Data mining
KW  - Data preprocessing
KW  - Data privacy
KW  - Electronic mail
KW  - Entropy
KW  - Evolutionary computation
KW  - Protection
KW  - Technology management
KW  - Web server
VL  - 
JA  - Information Engineering and Electronic Commerce (IEEC), 2010 2nd International Symposium on
DO  - 10.1109/IEEC.2010.5533261
AB  - Researches on preserving private data in the application of web data mining possess practical value. Through introducing basic concepts of web log mining and private data protection, this paper analyzes the status quo of privacy preservation in web log mining, and then it puts forward privacy preserving mining model based on evolutionary algorithm of cloud model, combining with evolutionary algorithm and cloud model theory. This model utilizes digital features of cloud and transformation between its qualitative concept and quantitative value expression. Thus, this model effectively conceals sensitive data, realizes web log mining based on privacy preservation. Results of the experiment reveal the feasibility and superiority of applying evolutionary algorithm of cloud model to privacy preservation in web log mining.
ER  - 

TY  - CONF
JO  - Autonomic Computing, 2005. ICAC 2005. Proceedings. Second International Conference on
TI  - Mining Logs Files for Computing System Management
T2  - Autonomic Computing, 2005. ICAC 2005. Proceedings. Second International Conference on
IS  - 
SN  - 
VO  - 
SP  - 309
EP  - 310
AU  - Wei Peng
AU  - Tao Li
AU  - Sheng Ma
Y1  - 13-16 June 2005
PY  - 2005
KW  - Bayes methods
KW  - data mining
KW  - data structures
KW  - message passing
KW  - object-oriented programming
KW  - asynchronous data collection
KW  - computing system management
KW  - context information
KW  - data analysis
KW  - data reporting
KW  - data representation
KW  - domain knowledge
KW  - knowledge acquisition
KW  - log file mining
KW  - log format heterogeneity
KW  - log format inconsistency
KW  - operating policies
KW  - operating rules
KW  - software components
KW  - system log file analysis
KW  - system maintenance
KW  - system monitoring
KW  - temporal characteristics
KW  - text message categorization
KW  - text mining
KW  - Computer science
KW  - Computerized monitoring
KW  - Data analysis
KW  - Knowledge acquisition
KW  - Knowledge management
KW  - Machine learning
KW  - Performance analysis
KW  - Pressing
KW  - Technology management
KW  - Vocabulary
VL  - 
JA  - Autonomic Computing, 2005. ICAC 2005. Proceedings. Second International Conference on
DO  - 10.1109/ICAC.2005.40
AB  - With advancement in science and technology, computing systems become increasingly more difficult to monitor, manage and maintain. Traditional approaches to system management have been largely based on domain experts through a knowledge acquisition process to translate domain knowledge into operating rules and policies. This has been experienced as a cumbersome, labor intensive, and error prone process. There is thus a pressing need for automatic and efficient approaches to monitor and manage complex computing systems. A popular approach to system management is based on analyzing system log files. However, several new aspects of the system log data have been less emphasized in existing analysis methods and posed several challenges. The aspects include disparate formats and relatively short text messages in data reporting, asynchronous data collection, and temporal characteristics in data representation. First, a typical computing system contains different devices with different software components, possibly from different providers. These various components have multiple ways to report events, conditions, errors and alerts. The heterogeneity and inconsistency of log formats make it difficult to automate problem determination. To perform automated analysis, we need to categorize the text messages with disparate formats into common situations. Second, text messages in the log files are relatively short with a large vocabulary size. Third, each text message usually contains a timestamp. The temporal characteristics provide additional context information of the messages and can be used to facilitate data analysis. In this paper, we apply text mining to automatically categorize the messages into a set of common categories, and propose two approaches of incorporating temporal information to improve the categorization performance
ER  - 

TY  - CONF
JO  - Computational Intelligence and Communication Networks (CICN), 2013 5th International Conference on
TI  - Deportment of Logs for Securing the Host System
T2  - Computational Intelligence and Communication Networks (CICN), 2013 5th International Conference on
IS  - 
SN  - 
VO  - 
SP  - 355
EP  - 359
AU  - Chauhan, P.
AU  - Singh, N.
AU  - Chandra, N.
Y1  - 27-29 Sept. 2013
PY  - 2013
KW  - security of data
KW  - system monitoring
KW  - host observation
KW  - host system security
KW  - log deportment
KW  - log files
KW  - malicious activity
KW  - Computers
KW  - Intrusion detection
KW  - Organizations
KW  - Servers
KW  - Software
KW  - XML
KW  - Host/System logs
KW  - Intrusion Detection System
KW  - Log management
VL  - 
JA  - Computational Intelligence and Communication Networks (CICN), 2013 5th International Conference on
DO  - 10.1109/CICN.2013.80
AB  - Logs are the files which contain the information about all the events occurring on the system. Logs have been playing a vital role in providing all kinds of information which can be used for several purposes like detecting a suspicious behaviour over the system. The aim of this paper is to study, analyse and generate results by observing host. Log files consist of different header information which can be further used to determine if any kind of malicious activity is discovered then that activity can be traced and blocked.
ER  - 

TY  - CONF
JO  - IT Convergence and Security (ICITCS), 2013 International Conference on
TI  - Implementation of Logging for Information Tracking on Network
T2  - IT Convergence and Security (ICITCS), 2013 International Conference on
IS  - 
SN  - 
VO  - 
SP  - 1
EP  - 4
AU  - Maeta, A.
AU  - Takahashi, K.
AU  - Kawamura, T.
AU  - Sugahara, K.
Y1  - 16-18 Dec. 2013
PY  - 2013
KW  - human factors
KW  - information dissemination
KW  - security of data
KW  - human factors
KW  - information diffusion
KW  - information leakage cases
KW  - logging implementation
KW  - network information tracking
KW  - sensitive information flow
KW  - system calls
KW  - Computers
KW  - Educational institutions
KW  - Kernel
KW  - Linux
KW  - Receivers
KW  - Security
KW  - Sockets
VL  - 
JA  - IT Convergence and Security (ICITCS), 2013 International Conference on
DO  - 10.1109/ICITCS.2013.6717841
AB  - In recent years, information leakage cases happen frequently and are one of big problems. 70% of such a cases are caused by human factors such as mismanagements and/or careless operations. Such a factors can be removed by the control of information flow and/or the check of computer operations. Therefore, some studies to control information flows have been proposed. However, almost studies focus on single computer. Thus, it is insufficient in recent working situations which exchange a lot of information through network. In this paper, we propose a framework to track sensitive information flow on multiple computers at kernel. The framework hooks system calls which may cause the diffusion of information, maintains their logs, and uses their logs to trace information. As the result, we can trace information on multiple computers. It would be useful for the control of the operations which uses sensitive information.
ER  - 

TY  - CONF
JO  - Software Engineering, 2010 ACM/IEEE 32nd International Conference on
TI  - Analysis of execution log files
T2  - Software Engineering, 2010 ACM/IEEE 32nd International Conference on
IS  - 
SN  - 0270-5257
VO  - 2
SP  - 409
EP  - 412
AU  - Nagappan, M.
Y1  - 2-8 May 2010
PY  - 2010
KW  - cloud computing
KW  - system monitoring
KW  - systems analysis
KW  - computational cloud system
KW  - dissertation research
KW  - execution log file analysis
KW  - log management
KW  - operational profiles
KW  - proactively prevent issues
KW  - Algorithm design and analysis
KW  - Arrays
KW  - Data mining
KW  - Fault diagnosis
KW  - Heuristic algorithms
KW  - Security
KW  - Software
KW  - analysis of textual data
KW  - diagnosis
KW  - fault isolation
KW  - log files
VL  - 2
JA  - Software Engineering, 2010 ACM/IEEE 32nd International Conference on
DO  - 10.1145/1810295.1810405
AB  - Log analysis can be used to find problems, define operational profiles, and even pro-actively prevent issues. The goal of my dissertation research is to investigate log management and analysis techniques suited for very large and very complex logs, such as those we might expect in a computational cloud system.
ER  - 

TY  - CONF
JO  - MILITARY COMMUNICATIONS CONFERENCE, 2011 - MILCOM 2011
TI  - Scaling data-plane logging in large scale networks
T2  - MILITARY COMMUNICATIONS CONFERENCE, 2011 - MILCOM 2011
IS  - 
SN  - 2155-7578
VO  - 
SP  - 1308
EP  - 1314
AU  - Arefin, A.
AU  - Khurshid, A.
AU  - Caesar, M.
AU  - Nahrstedt, K.
Y1  - 7-10 Nov. 2011
PY  - 2011
KW  - program debugging
KW  - security of data
KW  - tree data structures
KW  - wide area networks
KW  - Click software router
KW  - ISP networks
KW  - anomalous network behavior debugging
KW  - anomalous network behavior detection
KW  - control overhead
KW  - data-plane logging
KW  - distributed logging facility
KW  - failures
KW  - fine-grained behaviors
KW  - fine-grained data-plane behavior
KW  - large scale networks
KW  - log compression
KW  - logged information
KW  - memory requirement
KW  - military backbone networks
KW  - packet-level forwarding decisions
KW  - route changes
KW  - trace-driven simulations
KW  - tree-based data structure
KW  - wide area networks
KW  - Data structures
KW  - Gold
KW  - Monitoring
KW  - Nonvolatile memory
KW  - Random access memory
KW  - Reliability
KW  - Timing
VL  - 
JA  - MILITARY COMMUNICATIONS CONFERENCE, 2011 - MILCOM 2011
DO  - 10.1109/MILCOM.2011.6127483
AB  - Understanding and troubleshooting wide area networks (such as military backbone networks and ISP networks) are challenging tasks due to their large, distributed, and highly dynamic nature. Building a system that can record and replay fine-grained behaviors of such networks would simplify this problem by allowing operators to recreate the sequence and precise ordering of events (e.g., packet-level forwarding decisions, route changes, failures) taking place in their networks. However, doing this at large scales seems intractable due to the vast amount of information that would need to be logged. In this paper, we propose a scalable and reliable framework to monitor fine-grained data-plane behavior within a large network. We give a feasible architecture for a distributed logging facility, a tree-based data structure for log compression and show how this logged information helps network operators to detect and debug anomalous behavior of the network. Experimental results obtained through trace-driven simulations and Click software router experiments show that our design is lightweight in terms of processing time, memory requirement and control overhead, yet still achieves over 99% precision in capturing network events.
ER  - 

TY  - CONF
JO  - Informatics (PCI), 2011 15th Panhellenic Conference on
TI  - Log File Analysis of E-commerce Systems in Rich Internet Web 2.0 Applications
T2  - Informatics (PCI), 2011 15th Panhellenic Conference on
IS  - 
SN  - 
VO  - 
SP  - 222
EP  - 226
AU  - Aivalis, C.J.
AU  - Boucouvalas, A.C.
Y1  - Sept. 30 2011-Oct. 2 2011
PY  - 2011
KW  - Internet
KW  - consumer behaviour
KW  - electronic commerce
KW  - file organisation
KW  - software engineering
KW  - Web 2.0 applications
KW  - Web analytic technologies
KW  - Web development languages
KW  - customer behavior
KW  - e-commerce
KW  - exacter measurements
KW  - hybrid solution
KW  - log file analysis
KW  - operational data
KW  - page tagging
KW  - rich Internet application
KW  - software development
KW  - Browsers
KW  - Databases
KW  - Google
KW  - Java
KW  - Web servers
KW  - E-Commerce
KW  - GWT
KW  - JavaScript
KW  - Log File Analysis
KW  - Rich Internet Applications
KW  - Web Analytics
VL  - 
JA  - Informatics (PCI), 2011 15th Panhellenic Conference on
DO  - 10.1109/PCI.2011.31
AB  - This paper describes the implications of the new trends in web development languages on log file analysis for e-commerce. The new trends in Software Development and the available Web Analytics technologies are explained. The focus is placed on the diversifications introduced to the traces left on the system that by Rich Internet Applications (RIAs). Finally a novel hybrid solution is proposed that is based on the junction of log files with operational data and page tagging, which allows even exacter measurements of customer behavior. It allows a customization of the Analysis Tool and survives the shift of the technologies.
ER  - 

TY  - CONF
JO  - Digital Information Management (ICDIM), 2011 Sixth International Conference on
TI  - Applying multi-correlation for improving forecasting in cyber security
T2  - Digital Information Management (ICDIM), 2011 Sixth International Conference on
IS  - 
SN  - Pending
VO  - 
SP  - 179
EP  - 186
AU  - Pontes, E.
AU  - Guelfi, A.E.
AU  - Kofuji, S.T.
AU  - Silva, A.A.A.
Y1  - 26-28 Sept. 2011
PY  - 2011
KW  - operating systems (computers)
KW  - security of data
KW  - cyber security forecasting
KW  - cyber space
KW  - distributed intrusion forecasting system
KW  - event analysis system
KW  - historical series reliability
KW  - intrusion detection and prevention system
KW  - multicorrelation
KW  - operating system
KW  - Correlation
KW  - Forecasting
KW  - Internet
KW  - Logic gates
KW  - Prototypes
KW  - Security
KW  - Sensors
KW  - allert correlation
KW  - attacks prediction
KW  - false positives
KW  - intrusion forecasting
VL  - 
JA  - Digital Information Management (ICDIM), 2011 Sixth International Conference on
DO  - 10.1109/ICDIM.2011.6093323
AB  - Currently, defense of the cyber space is mostly based on detection and/or blocking of attacks (Intrusion Detection and Prevention System - IDPS). But, a significant improvement for IDPS is the employment of forecasting techniques in a Distributed Intrusion Forecasting System (DIFS), which enables the capability for predicting attacks. Notwithstanding, during our earlier works, one of the issues we have faced was the huge amount of alerts produced by IDPS, several of them were false positives. Checking the veracity of alerts through other sources (multi-correlation), e.g. logs taken from the operating system (OS), is a way of reducing the number of false alerts, and, therefore, improving data (historical series) to be used by the DIFS. The goal of this paper is to propose a two stage system which allows: (1) employment of an Event Analysis System (EAS) for making multi-correlation between alerts from an IDPS with the OS' logs; and (2) applying forecasting techniques on data generated by the EAS. Tests applied on laboratory by the use of the two stage system allow concluding about the improvement of the historical series reliability, and the consequent improvement of the forecasts accuracy.
ER  - 

TY  - CONF
JO  - Control Conference, 2007. CCC 2007. Chinese
TI  - NBTRL: A Software Platform for Network Background Traffic Replay Based on Log
T2  - Control Conference, 2007. CCC 2007. Chinese
IS  - 
SN  - 
VO  - 
SP  - 607
EP  - 611
AU  - Zhao Kuo
AU  - Tang Kuo
AU  - Chu Jianfeng
AU  - Hu Liang
Y1  - July 26 2007-June 31 2007
PY  - 2007
KW  - information retrieval
KW  - program testing
KW  - security of data
KW  - IDS testing
KW  - NBTRL
KW  - information extraction module
KW  - intrusion detection system
KW  - network background traffic replay
KW  - packet preprocessing module
KW  - replay module
KW  - software platform
KW  - Communication system traffic control
KW  - Computer displays
KW  - Computer networks
KW  - Data mining
KW  - Information security
KW  - Intrusion detection
KW  - Pervasive computing
KW  - Software testing
KW  - Telecommunication traffic
KW  - Traffic control
KW  - IDS Testing
KW  - Log
KW  - Traffic Replay
VL  - 
JA  - Control Conference, 2007. CCC 2007. Chinese
DO  - 10.1109/CHICC.2006.4347201
AB  - While the use of intrusion detection system (IDS), which monitors passively specific computing resources, and reports anomalous or intrusive activities, is becoming ubiquitous in today's network, evaluating IDS performance has been found to be challenging. Most IDS testing approaches are faced with selections with regard to their use of background traffic, which plays an important role in IDS testing. This paper presents the design and implementation of NBTRL, a software platform for network background traffic replay based on log files applied to IDS testing. NBTRL consists of information extraction module, packet preprocessing module and replay module. This software platform can extract traffic information from log files, control the speed of background traffic replay, process truncated packets, modify MAC address or IP address of packets, and provide a flexible and reusable experimental environment for IDS testing.
ER  - 

TY  - CONF
JO  - Advanced Computer Theory and Engineering (ICACTE), 2010 3rd International Conference on
TI  - System anomaly detection in distributed systems through MapReduce-Based log analysis
T2  - Advanced Computer Theory and Engineering (ICACTE), 2010 3rd International Conference on
IS  - 
SN  - 2154-7491
VO  - 6
SP  - V6-410
EP  - V6-413
AU  - Yan Liu
AU  - Wei Pan
AU  - Ning Cao
AU  - Guangwei Qiao
Y1  - 20-22 Aug. 2010
PY  - 2010
KW  - distributed processing
KW  - pattern clustering
KW  - public domain software
KW  - security of data
KW  - software maintenance
KW  - system monitoring
KW  - Hadoop
KW  - K-means clustering algorithm
KW  - MapReduce-based log analysis
KW  - open source distributed file system
KW  - problem diagnosis
KW  - random access file
KW  - system anomaly detection
KW  - Algorithm design and analysis
KW  - Clustering algorithms
KW  - Data mining
KW  - Distributed databases
KW  - Graphical user interfaces
KW  - Monitoring
KW  - Programming
KW  - K-means
KW  - MapReduce
KW  - anomaly detection
KW  - distributed system
KW  - log analysis
VL  - 6
JA  - Advanced Computer Theory and Engineering (ICACTE), 2010 3rd International Conference on
DO  - 10.1109/ICACTE.2010.5579173
AB  - System anomaly detection is very important for development, maintenance and performance refinement in large scale distributed systems. It's a good way to obtain the troubleshooting and problem diagnosis by analyzing system logs produced by distributed systems. However, due to the increasing scale and complexity of distributed systems, the size of logs must be very large. Thus, it's inefficient for common methods to analyze system logs on single node. Therefore, there is a great demand to adopt a distributed method for anomaly detection techniques based on log analysis. In this paper, a MapReduce-Based Framework is implemented to analyze the distributed log for detecting anomaly. The framework is built on top of Hadoop, an open source distributed file system and MapReduce implementation. We first make use of Random Access File to realize an incremental way for aggregating system logs from each node of the monitored cluster, and collect them to the analysis cluster. Then, we apply the K-means clustering algorithm to integrate the collected logs. After that, we implement a MapReduce-Based algorithm to parser these clustered log files. Furthermore, in order to make the best use of this collected data, a flexible and powerful way is utilized to display monitoring and analysis results. Thus, we can monitor system status of large distributed cluster and detect its anomalies.
ER  - 

TY  - CONF
JO  - Computer and Information Technology, 2005. CIT 2005. The Fifth International Conference on
TI  - Distributed-log-based scheme for IP traceback
T2  - Computer and Information Technology, 2005. CIT 2005. The Fifth International Conference on
IS  - 
SN  - 
VO  - 
SP  - 711
EP  - 715
AU  - Yi-Nan Jing
AU  - Peng Tu
AU  - Xue-Ping Wang
AU  - Gen-Du Zhang
Y1  - 21-23 Sept. 2005
PY  - 2005
KW  - IP networks
KW  - packet switching
KW  - probability
KW  - security of data
KW  - telecommunication security
KW  - MAC-enhanced hierarchical IP traceback system
KW  - distributed denial-of-service attack
KW  - distributed-log-based scheme
KW  - probabilistic packet marking scheme
KW  - Analytical models
KW  - Computer crime
KW  - Computer hacking
KW  - Degradation
KW  - Filtering
KW  - Filters
KW  - Information technology
KW  - Internet
KW  - Intrusion detection
KW  - Testing
VL  - 
JA  - Computer and Information Technology, 2005. CIT 2005. The Fifth International Conference on
DO  - 10.1109/CIT.2005.99
AB  - IP traceback is one of the most effective techniques to defeat the denial-of-service attacks and distributed denial-of-service attacks. And based on previous research, available probabilistic packet marking (PPM) schemes have more advantages than other IP traceback techniques. But the traditional schemes have too low marking packets utilization. In this paper, a new distributed-log-based scheme (DLS), which combines PPM and logging techniques, is proposed to utilize marking packets sufficiently. And, theoretical analysis and simulation results have proven that this scheme can converge more quickly than others. Based on this scheme the origin of an attack path can be traced by only several packets. Moreover, a MAC-enhanced hierarchical IP traceback system (HITS) is proposed to supply a gap of end-host schemes. We believe that MAC-enhanced HITS can be deployed and managed more conveniently and securely than end-host schemes. And the traceback results educed by it are more credible and authoritative.
ER  - 

TY  - CONF
JO  - MILCOM 2000. 21st Century Military Communications Conference Proceedings
TI  - Bilateral anonymity and prevention of abusing logged Web addresses
T2  - MILCOM 2000. 21st Century Military Communications Conference Proceedings
IS  - 
SN  - 
VO  - 1
SP  - 435
EP  - 439 vol.1
AU  - Demuth, T.
AU  - Rieke, A.
Y1  - 2000
PY  - 2000
KW  - Internet
KW  - file servers
KW  - information resources
KW  - online front-ends
KW  - security of data
KW  - telecommunication security
KW  - ISP
KW  - Internet service provider
KW  - URL
KW  - WWW
KW  - Web browser
KW  - Web pages
KW  - World Wide Web
KW  - address history
KW  - bilateral anonymity
KW  - client anonymity
KW  - content providers
KW  - data security
KW  - e-mail
KW  - logged Web address
KW  - logged Web address abuse prevention
KW  - server anonymity
KW  - temporarily valid Web address
KW  - Data security
KW  - History
KW  - Protection
KW  - Tail
KW  - Uniform resource locators
KW  - Web and internet services
KW  - Web pages
KW  - Web sites
KW  - Whales
KW  - World Wide Web
VL  - 1
JA  - MILCOM 2000. 21st Century Military Communications Conference Proceedings
DO  - 10.1109/MILCOM.2000.904990
AB  - A lot of effort has been taken to hide the content of a message from eavesdroppers. However, often not only the content, but also the address and identity of sender and/or receiver of the message are of interest for attackers. For that reason, several approaches were developed to guarantee anonymity in the case of email. A lot of services offer users to access Web pages unrecognised or without the risk of being backtracked, respectively. This kind of anonymity is called user or &ldquo;client anonymity&rdquo;. However, there are only a few offers that provide an equivalent protection for content providers, although this feature is desirable for many situations in which the identity of a publisher or content provider is to be hidden. This property is called server anonymity. The term &ldquo;server anonymity&rdquo; is explained in detail with the help of an existing system fulfilling some hundreds of thousand user requests per day. We also describe our experiences in providing such a system with respect to misuse. Furthermore there is another sensitive fact. While browsing Web pages, the used URLs are logged both by the Web client (Web browser) which is used and the Internet service provider (ISP), or any other instance or organisation that is involved in the communication. Hence the ISP can investigate the content a user is interested in afterwards simply by reusing the logged URLs. The same problem results from the behaviour of regular Web browsers to build an address history and local copies (browser cache) of the visited Web pages. We demonstrate a way of preventing the reuse of logged Web addresses by introducing the concept of temporarily valid Web addresses
ER  - 

TY  - CONF
JO  - Internet Technology and Applications, 2010 International Conference on
TI  - Developing a SSH Dictionary Attack Defense System in the Multi Platform Environments through the Analyzing Log
T2  - Internet Technology and Applications, 2010 International Conference on
IS  - 
SN  - 
VO  - 
SP  - 1
EP  - 4
AU  - Yen-Ning Su
AU  - Yueh-Hung Chen
AU  - Guang-Han Chung
AU  - Wu, B.J.
Y1  - 20-22 Aug. 2010
PY  - 2010
KW  - security of data
KW  - SSH dictionary attack defense system
KW  - current detection systems
KW  - detection functions
KW  - multiplatform environments
KW  - Computer science
KW  - Dictionaries
KW  - Education
KW  - Presses
KW  - Protocols
KW  - Security
KW  - Servers
VL  - 
JA  - Internet Technology and Applications, 2010 International Conference on
DO  - 10.1109/ITAPP.2010.5566560
AB  - The researcher discussed the SSH dictionary attack defense system in the multi platform environments through the analyzing log. The study introduced the current formats and threats of the SSH dictionary attack. Then, the research explained the types of the SSH dictionary attack defense system, and compared the functions and differences between the traditional and current detection systems. Moreover, the study based on the research of Y.N. Su and Y. H. Chen (2010), the SSH dictionary attack defense system, and advanced the detection functions in the multi platforms. According to those literatures, the study developed the SSH dictionary attack and detection system in the multi platforms environments using analyzing log . The contributions of the study were to provide an easy way for detection, and to get the list of the SSH dictionary attack sources by typing into the defection system in order to achieve the purpose of the SSH dictionary attack system in the multi platform defection environments.
ER  - 

TY  - CONF
JO  - Signal Processing, 2008. ICSP 2008. 9th International Conference on
TI  - Security of registration data of fingerprint image with a server by use of the fractional Fourier transform
T2  - Signal Processing, 2008. ICSP 2008. 9th International Conference on
IS  - 
SN  - 
VO  - 
SP  - 2070
EP  - 2073
AU  - Iwai, R.
AU  - Yoshimura, H.
Y1  - 26-29 Oct. 2008
PY  - 2008
KW  - Fourier transforms
KW  - fingerprint identification
KW  - image coding
KW  - image registration
KW  - security of data
KW  - access control
KW  - data processing
KW  - data registration
KW  - fingerprint authentication
KW  - fractional Fourier transform
KW  - image decoding
KW  - image fingerprint
KW  - indolence management
KW  - Access control
KW  - Authentication
KW  - Data analysis
KW  - Data processing
KW  - Data security
KW  - Decoding
KW  - Fingerprint recognition
KW  - Fourier transforms
KW  - Identity management systems
KW  - Image matching
VL  - 
JA  - Signal Processing, 2008. ICSP 2008. 9th International Conference on
DO  - 10.1109/ICOSP.2008.4697552
AB  - Recently, the personal identities by the fingerprint authentication have been increasing everywhere, for example, in the automatic logging into a PC, the access control and the diligence &amp; indolence management in an office, and so on. In the fingerprint authentication, the fingerprint does not have to be remembered and there is no worry to be lost like a password, if once the information is registered with a server. The fingerprint, however, cannot be changed like a password if the information leaks out from the server. Therefore, the data processing is necessary not to be able to be decoded by another person. In this study, we use the fractional Fourier transform (FRT) as the method of data processing and analyze the fundamental properties of the decoded image in comparison with the original image.
ER  - 

TY  - CONF
JO  - Communication Systems Networks and Digital Signal Processing (CSNDSP), 2010 7th International Symposium on
TI  - An e-shop log file analysis toolbox
T2  - Communication Systems Networks and Digital Signal Processing (CSNDSP), 2010 7th International Symposium on
IS  - 
SN  - 
VO  - 
SP  - 289
EP  - 294
AU  - Boucouvalas, A.C.
AU  - Aivalis, C.J.
Y1  - 21-23 July 2010
PY  - 2010
KW  - Internet
KW  - consumer behaviour
KW  - customer satisfaction
KW  - electronic commerce
KW  - file servers
KW  - performance evaluation
KW  - profitability
KW  - retail data processing
KW  - administrators
KW  - built-in performance measuring mechanisms
KW  - customer behavioral patterns
KW  - e-commerce applications
KW  - e-commerce log file analyzer
KW  - e-shop log file analysis toolbox
KW  - less visited pages
KW  - log file data
KW  - performance decay
KW  - profitability
KW  - user interaction
KW  - user satisfaction
KW  - web server architectures
KW  - Databases
KW  - IP networks
KW  - Robots
KW  - Service oriented architecture
KW  - Valves
KW  - Web server
VL  - 
JA  - Communication Systems Networks and Digital Signal Processing (CSNDSP), 2010 7th International Symposium on
DO  - 
AB  - Most e-commerce applications are implemented without any significant means of built-in performance measuring mechanisms, although responsiveness is a major factor for high revenue. Often, overall response times increase without the administrators even noticing. It is therefore crucial to have a precise measuring system that also reports customer behavioral patterns. We created a customizable e-commerce log file analyzer that measures performance mainly through combining a mix of log file data with e-shop information. It is capable of supporting multiple e-shops and can perform cross comparisons between them. It is easy to use and the software is extendable in order to support different web server architectures. It displays patterns taken by the user interaction with the e-shop and allows the administrators to locate less visited pages and make improvements or promote them better. This way an e-shop may benefit since it may lead to higher user satisfaction and profitability. The administrator can regularly study the readings and take the appropriate actions, if overall performance decay is observed.
ER  - 

TY  - CONF
JO  - Crime Detection and Prevention (ICDP 2009), 3rd International Conference on
TI  - A real time solution for face logging
T2  - Crime Detection and Prevention (ICDP 2009), 3rd International Conference on
IS  - 
SN  - 
VO  - 
SP  - 1
EP  - 6
AU  - Del Bimbo, A.
AU  - Dini, F.
AU  - Lisanti, G.
Y1  - 3-3 Dec. 2009
PY  - 2009
KW  - data loggers
KW  - face recognition
KW  - security of data
KW  - video signal processing
KW  - video surveillance
KW  - face image grabbing
KW  - face logging
KW  - face recognition
KW  - intrusion detection
KW  - intrusion logger
KW  - video surveillance
KW  - Videosurveillance
KW  - face detection
KW  - face logging
KW  - face tracking
KW  - quality estimation
VL  - 
JA  - Crime Detection and Prevention (ICDP 2009), 3rd International Conference on
DO  - 10.1049/ic.2009.0238
AB  - An intrusion logger is a video surveillance application designed to detect intrusion events and document them by storing times-tamped images in a log. Face loggers, in particular, are focused in grabbing imagery of intruders face, and are typically used to provide inputs to a face recognition system. Commonly, intrusion loggers are supposed to signal an alarm condition as the intrusion occurs, and are therefore required to run in real time and to work continuously for long time periods. In this paper we present a face logging solution capable of detecting and tracking several targets in real time, grabbing face images and evaluating their quality in order to store only the best for each detected target. An evaluation of the performance on a specifically designed test set is provided.
ER  - 

TY  - CONF
JO  - Enterprise Distributed Object Computing Conference Workshops, 2008 12th
TI  - Automated Privacy Audits Based on Pruning of Log Data
T2  - Enterprise Distributed Object Computing Conference Workshops, 2008 12th
IS  - 
SN  - 
VO  - 
SP  - 175
EP  - 182
AU  - Accorsi, R.
AU  - Stocker, T.
Y1  - 16-16 Sept. 2008
PY  - 2008
KW  - data privacy
KW  - records management
KW  - security of data
KW  - tree data structures
KW  - automated privacy audits
KW  - log data pruning
KW  - proof of concept
KW  - trees
KW  - Authorization
KW  - Control systems
KW  - Data analysis
KW  - Data privacy
KW  - Intrusion detection
KW  - Performance analysis
KW  - Runtime
KW  - Telematics
KW  - Tree data structures
KW  - XML
VL  - 
JA  - Enterprise Distributed Object Computing Conference Workshops, 2008 12th
DO  - 10.1109/EDOCW.2008.18
AB  - This paper presents a novel approach to automated audits based on the pruning of log data represented as trees. Events, recorded as a sequential list of entries, are interpreted as nodes of a tree. The audit consists in removing the nodes that are compliant with the policy, so that the remaining tree consists only of the violations of the policy. Besides presenting the method, this paper demonstrates that the resultant method is more efficient than usual audit approaches by analyzing its theoretical complexity and the runtime figures obtained by a proof of concept.
ER  - 

TY  - CONF
JO  - Software Reliability Engineering, 2008. ISSRE 2008. 19th International Symposium on
TI  - Automated Identification of Failure Causes in System Logs
T2  - Software Reliability Engineering, 2008. ISSRE 2008. 19th International Symposium on
IS  - 
SN  - 1071-9458
VO  - 
SP  - 117
EP  - 126
AU  - Mariani, L.
AU  - Pastore, F.
Y1  - 10-14 Nov. 2008
PY  - 2008
KW  - software fault tolerance
KW  - system monitoring
KW  - failure causes automated identification
KW  - log files
KW  - system administrators
KW  - system logs
KW  - Databases
KW  - Event detection
KW  - Expert systems
KW  - Failure analysis
KW  - Information analysis
KW  - Inspection
KW  - Law
KW  - Legal factors
KW  - Software reliability
KW  - System testing
KW  - anomaly detection
KW  - automated analysis
KW  - k behavior
KW  - log file analysis
KW  - software debugging
KW  - software fault diagnosis
KW  - software quality
KW  - software tools
VL  - 
JA  - Software Reliability Engineering, 2008. ISSRE 2008. 19th International Symposium on
DO  - 10.1109/ISSRE.2008.48
AB  - Log files are commonly inspected by system administrators and developers to detect suspicious behaviors and diagnose failure causes. Since size of log files grows fast, thus making manual analysis impractical, different automatic techniques have been proposed to analyze log files. Unfortunately, accuracy and effectiveness of these techniques are often limited by the unstructured nature of logged messages and the variety of data that can be logged.This paper presents a technique to automatically analyze log files and retrieve important information to identify failure causes. The technique automatically identifies dependencies between events and values in logs corresponding to legal executions, generates models of legal behaviors and compares log files collected during failing executions with the generated models to detect anomalous event sequences that are presented to users. Experimental results show the effectiveness of the technique in supporting developers and testers to identify failure causes.
ER  - 

TY  - CONF
JO  - Cognitive Infocommunications (CogInfoCom), 2013 IEEE 4th International Conference on
TI  - LibSearchNet: Analyses of library log files to identify search flows
T2  - Cognitive Infocommunications (CogInfoCom), 2013 IEEE 4th International Conference on
IS  - 
SN  - 
VO  - 
SP  - 543
EP  - 548
AU  - Csernoch, M.
AU  - Bujdoso, G.
AU  - Borbely, M.
AU  - Dani, E.
AU  - Nemethi-Takacs, M.
AU  - Koltay, K.
AU  - Balazs, L.
Y1  - 2-5 Dec. 2013
PY  - 2013
KW  - Internet
KW  - data analysis
KW  - digital libraries
KW  - information retrieval
KW  - virtual reality
KW  - 3D virtual spaces
KW  - LibSearchNet
KW  - library log file analysis
KW  - library systems
KW  - mental search representation
KW  - next generation library users
KW  - nonlibrary educated users
KW  - online libraries
KW  - user activity pattern
KW  - Conferences
KW  - Educational institutions
KW  - Libraries
KW  - Search problems
KW  - Software
KW  - Surface treatment
KW  - Three-dimensional displays
KW  - VirCA
KW  - log file analysis
KW  - search net
KW  - user- and system-launched searches
KW  - users' mental search lexikon
VL  - 
JA  - Cognitive Infocommunications (CogInfoCom), 2013 IEEE 4th International Conference on
DO  - 10.1109/CogInfoCom.2013.6719307
AB  - The online libraries of the next generation library users would mean creating 3D virtual spaces where they would navigate as they do in traditional libraries. To achieve this goal the surface has to be built and this new environment has offer options with which non-library educated users would navigate effectively. Reaching this state the activities of present day online library users and library systems should be thoroughly analyzed. The sources of the analyses are the library log files. Well-designed log files would reveal, on one hand, the patterns of the users' activities which are crucial in building algorithms for the mental representation of searches. On the other hand, the analyses of these log files would shed light on the operation of the system.
ER  - 

TY  - CONF
JO  - Conference on Computational Intelligence and Multimedia Applications, 2007. International Conference on
TI  - Detection of Region Duplication Forgery in Digital Images Using Wavelets and Log-Polar Mapping
T2  - Conference on Computational Intelligence and Multimedia Applications, 2007. International Conference on
IS  - 
SN  - 
VO  - 3
SP  - 371
EP  - 377
AU  - Myrna, A.N.
AU  - Venkateshmurthy, M.G.
AU  - Patil, C.G.
Y1  - 13-15 Dec. 2007
PY  - 2007
KW  - image processing
KW  - security of data
KW  - wavelet transforms
KW  - copy-move forgery
KW  - digital images
KW  - exhaustive search
KW  - log-polar mapping
KW  - phase correlation
KW  - region duplication forgery
KW  - wavelet transform
KW  - Biomedical imaging
KW  - Computational intelligence
KW  - Digital forensics
KW  - Digital images
KW  - Educational institutions
KW  - Forgery
KW  - Layout
KW  - Sorting
KW  - Statistics
KW  - Wavelet transforms
VL  - 3
JA  - Conference on Computational Intelligence and Multimedia Applications, 2007. International Conference on
DO  - 10.1109/ICCIMA.2007.271
AB  - Due to the abundantly available imaging technologies, manipulation of digital images has become a serious problem nowadays, in various fields like medical imaging, digital forensics, journalism, scientific publications, etc. In this paper, we concentrate on detection of a specific category of digital image forgery known as region duplication forgery or copy-move forgery, which is done by copying a block of an image and pasting it on to some other block of the same image. We present a novel approach based on the application of wavelet transform that detects and localizes such forgeries. Our technique works by first applying wavelet transform to the input image to yield a reduced dimension representation. We then perform exhaustive search to identify the similar blocks in the image by mapping them to log-polar coordinates and using phase correlation as the similarity criterion. This is done only once at the lowest resolution of the wavelet transform. Only the matched blocks are carried for comparison to the next level. This drastically reduces the time needed for the detection process. This approach works even if the pasted region has undergone transformations like translation and rotation.
ER  - 

TY  - CONF
JO  - Advances in Engineering, Science and Management (ICAESM), 2012 International Conference on
TI  - Decision equations for efficient search algorithms for network security
T2  - Advances in Engineering, Science and Management (ICAESM), 2012 International Conference on
IS  - 
SN  - 
VO  - 
SP  - 285
EP  - 289
AU  - Asaithambi, V.
AU  - Zackariah, N.
AU  - Nirmala, K.
Y1  - 30-31 March 2012
PY  - 2012
KW  - network theory (graphs)
KW  - search problems
KW  - security of data
KW  - trees (mathematics)
KW  - BFS
KW  - DFID
KW  - DFS
KW  - Smurf attacks
KW  - breadth first search
KW  - decision equations
KW  - denial-of-service attacks
KW  - depth first iterative deepening
KW  - depth first search
KW  - graph
KW  - network security
KW  - search algorithms
KW  - tree
KW  - BFS
KW  - DFID
KW  - DFS
KW  - DLS
KW  - UCS
KW  - goal node
KW  - graph
KW  - network
KW  - node
VL  - 
JA  - Advances in Engineering, Science and Management (ICAESM), 2012 International Conference on
DO  - 
AB  - Generally to secure a network from denial-of-service to Smurf attacks, hackers that perpetrate exploits, it is necessary to perform the tasks like Searching for multiple strings in packet payloads, approximate string matching, IP traceback via probabilistic marking, IP traceback via logging, detecting worms, etc. To execute the tasks there are many algorithms used. The search algorithms like Breadth First Search (BFS), Depth First Search (DFS), Depth First Iterative Deepening (DFID) etc are used to traverse any network like graph or a tree. The performance of one search algorithm may be better than the performance of any other search algorithms for search a particular node of a network. The algorithms, like BFS, DFS and DFID can be used to traverse any tree like network entirely. But it is necessary to determine a best suitable algorithm to search the goal node instead of using any one of them randomly. Then the identified algorithm can be used to traverse the network to reach the goal node.
ER  - 

TY  - CONF
JO  - Consumer Electronics (GCCE), 2014 IEEE 3rd Global Conference on
TI  - 3DPIN: Enhancing security with 3D display
T2  - Consumer Electronics (GCCE), 2014 IEEE 3rd Global Conference on
IS  - 
SN  - 
VO  - 
SP  - 129
EP  - 130
AU  - Mun-Kyu Lee
AU  - Jin Bok Kim
AU  - Franklin, M.K.
Y1  - 7-10 Oct. 2014
PY  - 2014
KW  - security of data
KW  - three-dimensional displays
KW  - user interfaces
KW  - 3D spot
KW  - PIN
KW  - brute-force search
KW  - glass free 3D display
KW  - handheld game consoles
KW  - passwords
KW  - personal identification numbers
KW  - security enhancement
KW  - shoulder surfing attacker
KW  - smartphones
KW  - user interface
KW  - Authentication
KW  - Haptic interfaces
KW  - Pins
KW  - Smart phones
KW  - Three-dimensional displays
KW  - Wheels
VL  - 
JA  - Consumer Electronics (GCCE), 2014 IEEE 3rd Global Conference on
DO  - 10.1109/GCCE.2014.7031090
AB  - Passwords and PINs are convenient and ubiquitous, yet they are quite vulnerable to attackers who stand near the user. This problem may be partially resolved by changing the user interface, but previous solutions of this kind still give shoulder-surfing attackers a significant advantage over brute-force search. We propose a novel solution based on 3D, especially suitable for glasses-free 3D displays found in many smartphones and handheld game consoles. A user at the &#x201C;3D spot&#x201D; may log in easily, while nearby shoulder-surfers gain no advantage. We perform a detailed experimental analysis of the usability of our scheme.
ER  - 

TY  - CONF
JO  - Dynamic Analysis, 2007. WODA '07. Fifth International Workshop on
TI  - Industrial Evaluation of a Log File Analysis Methodology
T2  - Dynamic Analysis, 2007. WODA '07. Fifth International Workshop on
IS  - 
SN  - 
VO  - 
SP  - 4
EP  - 4
AU  - Yantzi, D.J.
AU  - Andrews, J.H.
Y1  - 20-26 May 2007
PY  - 2007
KW  - program testing
KW  - commercial software
KW  - industrial evaluation
KW  - log file analysis methodology
KW  - Application software
KW  - Computer industry
KW  - Computer languages
KW  - Computer science
KW  - Information analysis
KW  - Instruments
KW  - Laboratories
KW  - Pattern analysis
KW  - Software testing
KW  - Writing
VL  - 
JA  - Dynamic Analysis, 2007. WODA '07. Fifth International Workshop on
DO  - 10.1109/WODA.2007.7
AB  - Test result evaluation programs often take the form of log file analyzers, which analyze text logs of events that have happened during testing. Previously, we proposed a methodology for deriving logging instrumentation and state-based log file analyzer programs from requirements. In this paper, we report on an industrial evaluation in which the methodology was carried out on two pieces of commercial software in a commercial setting. We report on our quantitative and qualitative observations and on recommendations for improving the methodology and the tools used.
ER  - 

TY  - CONF
JO  - Dependable Systems and Networks Workshop (DSN-W), 2013 43rd Annual IEEE/IFIP Conference on
TI  - Predicting job completion times using system logs in supercomputing clusters
T2  - Dependable Systems and Networks Workshop (DSN-W), 2013 43rd Annual IEEE/IFIP Conference on
IS  - 
SN  - 2325-6648
VO  - 
SP  - 1
EP  - 8
AU  - Xin Chen
AU  - Charng-Da Lu
AU  - Pattabiraman, K.
Y1  - 24-27 June 2013
PY  - 2013
KW  - cloud computing
KW  - computer centres
KW  - data mining
KW  - fault tolerant computing
KW  - hidden Markov models
KW  - parallel machines
KW  - security of data
KW  - software packages
KW  - system monitoring
KW  - HMM
KW  - HPC clusters
KW  - anomaly diagnosis
KW  - automatic failure prediction model
KW  - cloud computing clusters
KW  - commercial off-the-shelf components
KW  - commodity PC clusters logs
KW  - data centers
KW  - hidden Markov model
KW  - job completion time prediction
KW  - lack-of-organization
KW  - lack-of-semantic consistency
KW  - log mining
KW  - supercomputing clusters
KW  - system logs
KW  - Absorption
KW  - Computational modeling
KW  - Hardware
KW  - Hidden Markov models
KW  - Kernel
KW  - Markov processes
KW  - Training
KW  - Hidden Markov Model
KW  - Log Analysis
KW  - Prediction
VL  - 
JA  - Dependable Systems and Networks Workshop (DSN-W), 2013 43rd Annual IEEE/IFIP Conference on
DO  - 10.1109/DSNW.2013.6615513
AB  - Most large systems such as HPC/cloud computing clusters and data centers are built from commercial off-the-shelf components. System logs are usually the main source of choice to gain insights into the system issues. Therefore, mining logs to diagnose anomalies has been an active research area. Due to the lack of organization and semantic consistency in commodity PC clusters' logs, what constitutes a fault or an error is subjective and thus building an automatic failure prediction model from log messages is hard. In this paper we sidestep the difficulty by asking a different question: Given the concomitant system log messages of a running job, can we predict the job's remaining time? We adopt Hidden Markov Model (HMM) coupled with frequency analysis to achieve this. Our HMM approach can predict 75% of jobs' remaining times with an error of less than 200 seconds.
ER  - 

TY  - CONF
JO  - Computer Software and Applications, 2008. COMPSAC '08. 32nd Annual IEEE International
TI  - Pattern and Policy Driven Log Analysis for Software Monitoring
T2  - Computer Software and Applications, 2008. COMPSAC '08. 32nd Annual IEEE International
IS  - 
SN  - 0730-3157
VO  - 
SP  - 108
EP  - 111
AU  - Razavi, A.
AU  - Kontogiannis, Kostas
Y1  - July 28 2008-Aug. 1 2008
PY  - 2008
KW  - object-oriented programming
KW  - program diagnostics
KW  - risk analysis
KW  - security of data
KW  - Viterbi algorithm
KW  - component-based software
KW  - industrial software systems
KW  - pattern driven log analysis
KW  - pattern recognition
KW  - policy driven log analysis
KW  - software monitoring
KW  - system auditing
KW  - system diagnosis
KW  - system maintenance
KW  - system monitoring
KW  - system risk
KW  - system threat profile
KW  - Application software
KW  - Collaborative software
KW  - Computer industry
KW  - Context modeling
KW  - Monitoring
KW  - Pattern analysis
KW  - Pattern matching
KW  - Pattern recognition
KW  - Risk analysis
KW  - Software systems
KW  - Software Auditing
KW  - Software Monitoring
KW  - Trace Analysis
VL  - 
JA  - Computer Software and Applications, 2008. COMPSAC '08. 32nd Annual IEEE International
DO  - 10.1109/COMPSAC.2008.81
AB  - The component-based nature of large industrial software systems that consist of a number of diverse collaborating applications, pose significant challenges with respect to system maintenance, monitoring, auditing, and diagnosing. In this context, a monitoring and diagnostic system interprets log data to recognize patterns of significant events that conform to specific threat models. Threat models have been used by the software industry for analyzing and documenting a systempsilas risks in order to understand a systempsilas threat profile. In this paper, we propose a framework whereby patterns of significant events are represented as expressions of a specialized monitoring language that are used to annotate specific threat models. An approximate matching technique that is based on the Viterbi algorithm is then used to identify whether system generated events, fit the given patterns. The technique has been applied and evaluated considering threat models and monitoring policies in logs that have been obtained from multi-user MS-Windows based systems.
ER  - 

TY  - CONF
JO  - e-Business (ICE-B), 2011 Proceedings of the International Conference on
TI  - A proposal of &#x201C;identity commons&#x201D; for service creation using communication log: Position paper
T2  - e-Business (ICE-B), 2011 Proceedings of the International Conference on
IS  - 
SN  - 
VO  - 
SP  - 1
EP  - 4
AU  - Otsuki, M.
AU  - Sonehara, N.
Y1  - 18-21 July 2011
PY  - 2011
KW  - data protection
KW  - security of data
KW  - IdC system
KW  - communication log
KW  - identity commons
KW  - personal ID
KW  - personal data protection
KW  - service creation
KW  - Companies
KW  - Educational institutions
KW  - Licenses
KW  - Privacy
KW  - Proposals
KW  - Security
KW  - Anonymization
KW  - Information loss
KW  - Privacy
KW  - Utility
VL  - 
JA  - e-Business (ICE-B), 2011 Proceedings of the International Conference on
DO  - 
AB  - In this paper we consider the possibility of introducing Identity Commons (IdC) system. This will work as an ID bank to balance the utilization and protection of personal data. This paper first describes current problems on the utilization and protection of personal ID. It then proposes the IdC System. Finally, it makes some considerations regarding some of the institutional and technical challenges that need to be addressed in order to share personal data, combine them with related services and ensure the flexible use of them.
ER  - 

TY  - CONF
JO  - Information Assurance, 2005. Proceedings. Third IEEE International Workshop on
TI  - Forensic analysis of file system intrusions using improved backtracking
T2  - Information Assurance, 2005. Proceedings. Third IEEE International Workshop on
IS  - 
SN  - 
VO  - 
SP  - 154
EP  - 163
AU  - Sitaraman, S.
AU  - Venkatesan, S.
Y1  - 23-24 March 2005
PY  - 2005
KW  - backtracking
KW  - data flow analysis
KW  - data flow graphs
KW  - file organisation
KW  - program slicing
KW  - security of data
KW  - BackTracker
KW  - backtracking
KW  - dataflow analysis
KW  - dependency graph
KW  - dynamic slicing
KW  - false positives
KW  - file system intrusions
KW  - forensic analysis
KW  - intrusion detection systems
KW  - search space
KW  - search time
KW  - storage space
KW  - suspicious contents
KW  - system administration
KW  - system alert
KW  - Computer science
KW  - Data analysis
KW  - Event detection
KW  - File systems
KW  - Flow graphs
KW  - Forensics
KW  - Information analysis
KW  - Intrusion detection
KW  - Linux
KW  - Real time systems
KW  - Backtracking
KW  - Data Flow Analysis
KW  - Dynamic Slicing
KW  - File System
KW  - Intrusion Detection
VL  - 
JA  - Information Assurance, 2005. Proceedings. Third IEEE International Workshop on
DO  - 10.1109/IWIA.2005.9
AB  - Intrusion detection systems alert the system administrators of intrusions but, in most cases, do not provide details about which system events are relevant to the intrusion and how the system events are related. We consider intrusions of file systems. Existing tools, like BackTracker, help the system administrator backtrack from the detection point, which is a file with suspicious contents, to possible entry points of the intrusion by providing a graph containing dependency information between the various files and processes that could be related to the detection point. We improve such backtracking techniques by logging certain additional parameters of the file system during normal operations (real-time) and examining the logged information during the analysis phase. In addition, we use dataflow analysis within the processes related to the intrusion to prune unwanted paths from the dependency graph. This results in significant reduction in search space, search time, and false positives. We also analyze the effort required in terms of storage space and search time.
ER  - 

TY  - CONF
JO  - Communications, 2004 and the 5th International Symposium on Multi-Dimensional Mobile Communications Proceedings. The 2004 Joint Conference of the 10th Asia-Pacific Conference on
TI  - Implementation and overhead analysis of a log-based intrusion recovery module on Linux file system
T2  - Communications, 2004 and the 5th International Symposium on Multi-Dimensional Mobile Communications Proceedings. The 2004 Joint Conference of the 10th Asia-Pacific Conference on
IS  - 
SN  - 
VO  - 2
SP  - 548
EP  - 552 vol.2
AU  - Jae-Kook Lee
AU  - Hyong-Shik Kim
Y1  - 29 Aug.-1 Sept. 2004
PY  - 2004
KW  - Linux
KW  - computer viruses
KW  - file organisation
KW  - security of data
KW  - LBIRM
KW  - Linux file system
KW  - hacker damage
KW  - log-based intrusion recovery module
KW  - overhead analysis
KW  - Computer hacking
KW  - Computer science
KW  - Computer worms
KW  - File systems
KW  - Intrusion detection
KW  - Invasive software
KW  - Linux
KW  - Protection
KW  - Reliability engineering
KW  - Tiles
VL  - 2
JA  - Communications, 2004 and the 5th International Symposium on Multi-Dimensional Mobile Communications Proceedings. The 2004 Joint Conference of the 10th Asia-Pacific Conference on
DO  - 10.1109/APCC.2004.1391774
AB  - Most people always want to get reliable information, even if there are intrusions that illegitimate hacker damages to file systems through operations modifying, appending, and deleting files. To accomplish this, we need the recovery function with transparent to user and return to its former state. This paper proposes design and implementation of a log-based intrusion recovery module (LBIRM). It can keep the previous contents of designated files as a chain of logs and recover the damaged file by using the log.
ER  - 

TY  - CONF
JO  - Systems Man and Cybernetics (SMC), 2010 IEEE International Conference on
TI  - An effective log mining approach for database intrusion detection
T2  - Systems Man and Cybernetics (SMC), 2010 IEEE International Conference on
IS  - 
SN  - 1062-922X
VO  - 
SP  - 2299
EP  - 2306
AU  - Yi Hu
AU  - Campan, A.
AU  - Walden, J.
AU  - Vorobyeva, I.
AU  - Shelton, J.
Y1  - 10-13 Oct. 2010
PY  - 2010
KW  - SQL
KW  - data mining
KW  - relational databases
KW  - security of data
KW  - database intrusion detection
KW  - database user activities
KW  - log mining approach
KW  - malicious database transactions
KW  - multidimensional data dependency
KW  - multilevel data dependency
KW  - profile SQL query structures
KW  - Databases
KW  - Data Mining
KW  - Database Security
KW  - Intrusion Detection
VL  - 
JA  - Systems Man and Cybernetics (SMC), 2010 IEEE International Conference on
DO  - 10.1109/ICSMC.2010.5641988
AB  - Organizations spend a significant amount of resources securing their servers and network perimeters. However, these mechanisms are not sufficient for protecting databases. In this paper, we present a new technique for identifying malicious database transactions. Compared to many existing approaches which profile SQL query structures and database user activities to detect intrusions, the novelty of this approach is the automatic discovery and use of essential data dependencies, namely, multi-dimensional and multi-level data dependencies, for identifying anomalous database transactions. Since essential data dependencies reflect semantic relationships among data items and are less likely to change than SQL query structures or database user behaviors, they are ideal for profiling data correlations for identifying malicious database activities.
ER  - 

TY  - CONF
JO  - Cognitive Infocommunications (CogInfoCom), 2013 IEEE 4th International Conference on
TI  - LibSearchNet: Library log file initiatives - As a part of semantic library interface development for the VirCA 3D virtual collaboration arena
T2  - Cognitive Infocommunications (CogInfoCom), 2013 IEEE 4th International Conference on
IS  - 
SN  - 
VO  - 
SP  - 567
EP  - 572
AU  - Bujdoso, G.
AU  - Csernoch, M.
AU  - Borbely, M.
AU  - Dani, E.
AU  - Nemethi-Takacs, M.
AU  - Koltay, K.
AU  - Balazs, L.
Y1  - 2-5 Dec. 2013
PY  - 2013
KW  - Internet
KW  - data analysis
KW  - digital libraries
KW  - information needs
KW  - information retrieval
KW  - virtual reality
KW  - LibSearchNet
KW  - VirCA 3D virtual collaboration arena
KW  - intelligent library
KW  - library log file initiatives
KW  - library requirements
KW  - library systems
KW  - library user behavior analysis
KW  - online search
KW  - semantic design
KW  - semantic library interface development
KW  - user needs prediction
KW  - user searching behavior
KW  - Artificial intelligence
KW  - Collaboration
KW  - Conferences
KW  - Libraries
KW  - Semantics
KW  - Software
KW  - Three-dimensional displays
KW  - VirCA
KW  - intelligent library systems
KW  - log file analysis
KW  - log file initiatives
VL  - 
JA  - Cognitive Infocommunications (CogInfoCom), 2013 IEEE 4th International Conference on
DO  - 10.1109/CogInfoCom.2013.6719312
AB  - In this paper we examine the possibilities for analyzing the behavior of library users. We point out that the softwares that we can use cannot fulfill library requirements. There are many data given for on-line searches in library systems hidden from the analyzers. After examining the possibilities and some log files produced by the library systems, we propose how log files could give more usable information for the semantic design of the intelligent library in the VirCA virtual collaboration system and of other on-line systems, too. These data can be applied as input information for intelligent systems in learning user searching behavior and help them predict user needs more precisely.
ER  - 

TY  - CONF
JO  - Computational and Information Sciences (ICCIS), 2013 Fifth International Conference on
TI  - Research and Application of Data Mining Based on RoboCup Soccer Logs
T2  - Computational and Information Sciences (ICCIS), 2013 Fifth International Conference on
IS  - 
SN  - 
VO  - 
SP  - 367
EP  - 370
AU  - Cheng Ze-Kai
AU  - Liu Qian
AU  - Qin Feng
Y1  - 21-23 June 2013
PY  - 2013
KW  - data mining
KW  - file organisation
KW  - mobile robots
KW  - multi-robot systems
KW  - sport
KW  - RoboCup soccer logs
KW  - data mining method
KW  - initiative moving strategy
KW  - knowledge discovery
KW  - log file analysis
KW  - mass data
KW  - Data mining
KW  - Data models
KW  - Data preprocessing
KW  - Educational institutions
KW  - Games
KW  - Knowledge representation
KW  - Market research
KW  - RoboCup
KW  - data mining
KW  - log files
KW  - sequential pattern initiative moves
VL  - 
JA  - Computational and Information Sciences (ICCIS), 2013 Fifth International Conference on
DO  - 10.1109/ICCIS.2013.104
AB  - RoboCup is an international academic competition which has seen a rapid development in recent years. Generally, developers use the way of engineering to develop a team but ignore the log files of RoboCup Soccer. Data mining is used for knowledge discovering from mass data. In this paper, the method of Data Mining based on the log files will be used to design the initiative moving strategy. The results showed that the improved initiative moving strategy was successful and the way of using Data Mining method to analyze log files could improve the efficiency of development.
ER  - 

TY  - CONF
JO  - Grid Computing, 2007 8th IEEE/ACM International Conference on
TI  - Log summarization and anomaly detection for troubleshooting distributed systems
T2  - Grid Computing, 2007 8th IEEE/ACM International Conference on
IS  - 
SN  - 
VO  - 
SP  - 226
EP  - 234
AU  - Gunter, D.
AU  - Tierney, B.L.
AU  - Brown, A.
AU  - Swany, M.
AU  - Bresnahan, J.
AU  - Schopf, Jennifer M.
Y1  - 19-21 Sept. 2007
PY  - 2007
KW  - grid computing
KW  - middleware
KW  - security of data
KW  - system monitoring
KW  - system recovery
KW  - Grid middleware
KW  - anomaly detection
KW  - checking user certificates
KW  - end-to-end distributed software stack
KW  - log summarization
KW  - system failures
KW  - system monitoring tools
KW  - troubleshooting distributed systems
KW  - Condition monitoring
KW  - Debugging
KW  - Degradation
KW  - Instruments
KW  - Laboratories
KW  - Middleware
KW  - Permission
KW  - Software performance
KW  - Software tools
KW  - Testing
VL  - 
JA  - Grid Computing, 2007 8th IEEE/ACM International Conference on
DO  - 10.1109/GRID.2007.4354137
AB  - Today's system monitoring tools are capable of detecting system failures such as host failures, OS errors, and network partitions in near-real time. Unfortunately, the same cannot yet be said of the end-to-end distributed software stack. Any given action, for example, reliably transferring a directory of files, can involve a wide range of complex and interrelated actions across multiple pieces of software: checking user certificates and permissions, getting details for all files, performing third-party transfers, understanding re-try policy decisions, etc. We present an infrastructure for troubleshooting complex middleware, a general purpose technique for configurable log summarization, and an anomaly detection technique that works in near-real time on running Grid middleware. We present results gathered using this infrastructure from instrumented Grid middleware and applications running on the Emulab testbed. From these results, we analyze the effectiveness of several algorithms at accurately detecting a variety of performance anomalies.
ER  - 

TY  - CONF
JO  - Software Engineering (ICSE), 2012 34th International Conference on
TI  - Log-based testing
T2  - Software Engineering (ICSE), 2012 34th International Conference on
IS  - 
SN  - 0270-5257
VO  - 
SP  - 1591
EP  - 1594
AU  - Elyasov, A.
Y1  - 2-9 June 2012
PY  - 2012
KW  - Internet
KW  - program testing
KW  - diagnosis framework
KW  - future Internet application continuous testing
KW  - log-based testing
KW  - logging framework
KW  - rewriting
KW  - software testing
KW  - Automation
KW  - Graphical user interfaces
KW  - Instruments
KW  - Internet
KW  - Libraries
KW  - Software testing
KW  - instrumentation
KW  - log file analysis
KW  - rewriting
VL  - 
JA  - Software Engineering (ICSE), 2012 34th International Conference on
DO  - 10.1109/ICSE.2012.6227029
AB  - This thesis presents an ongoing research on using logs for software testing. We propose a complex and generic logging and diagnosis framework, that can be efficiently used for continuous testing of future Internet applications. To simplify the diagnosis of logs we suggest to reduce its size by means of rewriting.
ER  - 

TY  - CONF
JO  - Computer Research and Development (ICCRD), 2011 3rd International Conference on
TI  - Web log cleaning for mining of web usage patterns
T2  - Computer Research and Development (ICCRD), 2011 3rd International Conference on
IS  - 
SN  - 
VO  - 2
SP  - 490
EP  - 494
AU  - Aye, T.T.
Y1  - 11-13 March 2011
PY  - 2011
KW  - Internet
KW  - data mining
KW  - information retrieval
KW  - Web log cleaning
KW  - Web usage pattern mining
KW  - World Wide Web
KW  - data cleaning algorithms
KW  - data mining
KW  - field extraction
KW  - Algorithm design and analysis
KW  - Cleaning
KW  - Data preprocessing
KW  - Web mining
KW  - Web servers
KW  - Data Preprocessing
KW  - Log File Analysis
KW  - Web Usage Mining
VL  - 2
JA  - Computer Research and Development (ICCRD), 2011 3rd International Conference on
DO  - 10.1109/ICCRD.2011.5764181
AB  - Web usage mining (WUM) is a type of Web mining, which exploits data mining techniques to extract valuable information from navigation behavior of World Wide Web users. The data should be preprocessed to improve the efficiency and ease of the mining process. So it is important to define before applying data mining techniques to discover user access patterns from Web log. The main task of data preprocessing is to prune noisy and irrelevant data, and to reduce data volume for the pattern discovery phase. This paper mainly focus on data preprocessing stage of the first phase of Web usage mining with activities like field extraction and data cleaning algorithms. Field extraction algorithm performs the process of separating fields from the single line of the log file. Data cleaning algorithm eliminates inconsistent or unnecessary items in the analyzed data.
ER  - 

TY  - CONF
JO  - Advanced Computing and Communications, 2006. ADCOM 2006. International Conference on
TI  - Evaluating an Obstacle Avoidance Strategy to Ant Colony Optimization Algorithm for Classification in Event Logs
T2  - Advanced Computing and Communications, 2006. ADCOM 2006. International Conference on
IS  - 
SN  - 
VO  - 
SP  - 628
EP  - 629
AU  - Chandrasekar, R.
AU  - Suresh, R.K.
AU  - Ponnambalam, S.G.
Y1  - 20-23 Dec. 2006
PY  - 2006
KW  - data mining
KW  - optimisation
KW  - pattern classification
KW  - security of data
KW  - ant colony optimization algorithm
KW  - ant-miner algorithm
KW  - classification
KW  - event log files
KW  - obstacle avoidance strategy
KW  - rule formation process
KW  - training set
KW  - Accuracy
KW  - Ant colony optimization
KW  - Change detection algorithms
KW  - Classification algorithms
KW  - Educational institutions
KW  - Information technology
KW  - Intrusion detection
KW  - Mechanical engineering
KW  - Performance evaluation
KW  - Testing
VL  - 
JA  - Advanced Computing and Communications, 2006. ADCOM 2006. International Conference on
DO  - 10.1109/ADCOM.2006.4289972
AB  - Classification using ant colony optimization (ACO) algorithm provides a very good technique for users to understand the data obtained from event log files, which can further help in building a system profile and determining whether intrusions have taken place in the system. To evaluate the obstacle avoidance strategy, the parameters used are along the lines of simplicity of rules formed, number of terms present in the rules and also the predictive accuracy of the test data on the training set using the rules obtained. We have tried to analyze changes in the rule formation process for different thresholds, and for different times within the process of generating rules. We show through our evaluation that the obstacle avoidance strategy to ACO performs better than the popular ant-miner algorithm by building simple rules with an improved predictive accuracy.
ER  - 

TY  - CONF
JO  - Information Technology: Coding and Computing, 2005. ITCC 2005. International Conference on
TI  - Dynamic learning of automata from the call stack log for anomaly detection
T2  - Information Technology: Coding and Computing, 2005. ITCC 2005. International Conference on
IS  - 
SN  - 
VO  - 1
SP  - 774
EP  - 779 Vol. 1
AU  - Zhen Liu
AU  - Bridges, S.M.
Y1  - 4-6 April 2005
PY  - 2005
KW  - data handling
KW  - finite state machines
KW  - learning systems
KW  - program diagnostics
KW  - security of data
KW  - anomaly detection
KW  - call stack information
KW  - dynamic learning
KW  - hybrid push down automaton
KW  - learning algorithm
KW  - static analysis
KW  - Bridges
KW  - Change detection algorithms
KW  - Computer science
KW  - Computerized monitoring
KW  - Data analysis
KW  - Heuristic algorithms
KW  - Information analysis
KW  - Learning automata
KW  - Power system modeling
KW  - Training data
VL  - 1
JA  - Information Technology: Coding and Computing, 2005. ITCC 2005. International Conference on
DO  - 10.1109/ITCC.2005.136
AB  - Anomaly detection based on monitoring of sequences of system calls has proved to be an effective approach for detection of previously unknown attacks on programs. This paper describes a new model for profiling normal program behavior that can be used to detect intrusions that change application execution flow. The model (hybrid push down automaton, HPDA) incorporates call stack information and can be learned by dynamic analysis of training data captured from the call stack log. The learning algorithm uses call stack information maintained by the program to build a finite state automaton. When compared to other approaches including VtPath which also uses call stack information, the HPDA model produces a more compact and general representation of control flow, handles recursion naturally, can be learned with less training data, and has a lower false positive rate when used for anomaly detection. In addition, dynamic learning can also be used to supplement a model acquired from static analysis.
ER  - 

TY  - CONF
JO  - Data Mining, 2008. ICDM '08. Eighth IEEE International Conference on
TI  - Alert Detection in System Logs
T2  - Data Mining, 2008. ICDM '08. Eighth IEEE International Conference on
IS  - 
SN  - 1550-4786
VO  - 
SP  - 959
EP  - 964
AU  - Oliner, A.J.
AU  - Aiken, A.
AU  - Stearley, J.
Y1  - 15-19 Dec. 2008
PY  - 2008
KW  - entropy
KW  - security of data
KW  - Nodeinfo
KW  - alert detection
KW  - anomaly detection
KW  - information entropy
KW  - message terms
KW  - system logs
KW  - unsupervised algorithm
KW  - Costs
KW  - Data mining
KW  - Detection algorithms
KW  - Fault detection
KW  - Information entropy
KW  - Laboratories
KW  - Personnel
KW  - Production systems
KW  - Supercomputers
KW  - USA Councils
KW  - anomaly detection
KW  - fault detection
KW  - hpc
KW  - information theory
KW  - log analysis
VL  - 
JA  - Data Mining, 2008. ICDM '08. Eighth IEEE International Conference on
DO  - 10.1109/ICDM.2008.132
AB  - We present Nodeinfo, an unsupervised algorithm for anomaly detection in system logs. We demonstrate Nodeinfo's effectiveness on data from four of the world's most powerful supercomputers: using logs representing over 746 million processor-hours, in which anomalous events called alerts were manually tagged for scoring, we aim to automatically identify the regions of the log containing those alerts. We formalize the alert detection task in these terms, describe how Nodeinfo uses the information entropy of message terms to identify alerts, and present an online version of this algorithm, which is now in production use. This is the first work to investigate alert detection on (several) publicly-available supercomputer system logs, thereby providing a reproducible performance baseline.
ER  - 

TY  - CONF
JO  - System Sciences, 2003. Proceedings of the 36th Annual Hawaii International Conference on
TI  - Verifying trustworthiness requirements in distributed systems with formal log-file analysis
T2  - System Sciences, 2003. Proceedings of the 36th Annual Hawaii International Conference on
IS  - 
SN  - 
VO  - 
SP  - 10 pp.
EP  - 
AU  - Ulrich, A.
AU  - Hallal, H.
AU  - Petrenko, A.
AU  - Boroday, S.
Y1  - 6-9 Jan. 2003
PY  - 2003
KW  - formal specification
KW  - formal verification
KW  - reasoning about programs
KW  - system monitoring
KW  - systems re-engineering
KW  - concurrent system
KW  - distributed systems
KW  - formal log-file analysis
KW  - formally specification
KW  - off-the-shelf model checker
KW  - requirement descriptions
KW  - system specification
KW  - trustworthiness requirements verification
KW  - Flow graphs
KW  - Interleaved codes
KW  - Intrusion detection
KW  - Large-scale systems
KW  - Lattices
KW  - Monitoring
KW  - Performance analysis
KW  - Protection
KW  - System testing
KW  - Visualization
VL  - 
JA  - System Sciences, 2003. Proceedings of the 36th Annual Hawaii International Conference on
DO  - 10.1109/HICSS.2003.1174915
AB  - The paper reports on an analysis technology based on the tracing approach to test trustworthy requirements of a distributed system. The system under test is instrumented such that it generates events at runtime to enable reasoning about the implementation of these requirements in a later step. Specifically, an event log collected during a system run is converted into a specification of the system. The (trustworthy) requirements of the system must be formally specified by an expert who has sufficient knowledge about the behaviour of the system. The reengineered model of the system and the requirement descriptions are then processed by an off-the-shelf model checker. The model checker generates scenarios that visualize fulfilments or violations of the requirements. A complex example of a concurrent system serves as a case study.
ER  - 

TY  - CONF
JO  - Communication Systems and Network Technologies (CSNT), 2011 International Conference on
TI  - Analysis of Different Clustering Approaches for Grouping Users on the Basis of GSM Call Logs
T2  - Communication Systems and Network Technologies (CSNT), 2011 International Conference on
IS  - 
SN  - 
VO  - 
SP  - 1
EP  - 4
AU  - Deshpande, S.S.
AU  - Chauhan, N.A.
Y1  - 3-5 June 2011
PY  - 2011
KW  - cellular radio
KW  - fraud
KW  - mobile computing
KW  - mobility management (mobile radio)
KW  - pattern clustering
KW  - security of data
KW  - telecommunication computing
KW  - telecommunication security
KW  - GSM call logs
KW  - cellular system
KW  - clustering approach
KW  - fraud detection
KW  - location-aware data
KW  - mobile communication network
KW  - mobile computing
KW  - mobile users
KW  - mobility prediction
KW  - pervasive computing
KW  - serving cell
KW  - social group discovery
KW  - user context discovery
KW  - user grouping
KW  - user location
KW  - Base stations
KW  - Context
KW  - GSM
KW  - Mobile communication
KW  - Mobile computing
KW  - Mobile handsets
KW  - Prediction algorithms
KW  - Affinity Model
KW  - Block Crediting
KW  - Model Formulation
KW  - Velocity trap
VL  - 
JA  - Communication Systems and Network Technologies (CSNT), 2011 International Conference on
DO  - 10.1109/CSNT.2011.162
AB  - Analysis of Location-aware data plays an important role in pervasive and mobile computing. In cellular systems (e.g., GSM) the serving cell is easily available as an indication of the user location, without any additional hardware or network services. With this location data and other context variables groups of mobile users who are advertised to the found together can be traced. The objective of this paper is to review the works carried out by different group of researchers using varied techniques in Discovering User context, Mobility Prediction of Mobile users, Discovering social groups, Fraud detection in mobile communications networks etc.
ER  - 

TY  - CONF
JO  - Software Reliability Engineering Workshops (ISSREW), 2013 IEEE International Symposium on
TI  - Reliability feedback through system log analysis
T2  - Software Reliability Engineering Workshops (ISSREW), 2013 IEEE International Symposium on
IS  - 
SN  - 
VO  - 
SP  - 82
EP  - 83
AU  - Vinod, K.
AU  - Pandit, P.
AU  - Ramachandra, M.
Y1  - 4-7 Nov. 2013
PY  - 2013
KW  - data analysis
KW  - feedback
KW  - file organisation
KW  - health care
KW  - research and development
KW  - software reliability
KW  - system monitoring
KW  - systems analysis
KW  - healthcare industry
KW  - information technology
KW  - reliability feedback
KW  - research and development data analysis
KW  - system log file analysis
KW  - volume deployment mode
KW  - Computer crashes
KW  - Industries
KW  - Magnetic resonance imaging
KW  - Measurement
KW  - Medical services
KW  - Monitoring
KW  - Reliability
VL  - 
JA  - Software Reliability Engineering Workshops (ISSREW), 2013 IEEE International Symposium on
DO  - 10.1109/ISSREW.2013.6688877
AB  - The healthcare industry has taken a significant pie in the information technology where a great progress is being shown in analyzing the research and development data collected over the years. Yet, most of the potential for value creation is still unclaimed. <sup>[1]</sup> In a typical healthcare system, the actual usage of the system can be determined when one transitions from the monitoring or a limited release phase of the project to the volume deployment mode. An early, if not a continuous feedback, can be ensured when the systems are usually beta tested at the selected sites. This is where we bring in the system log file analysis to play a major role in determining the reliability of the deployed system to receive an incessant and established feedback.
ER  - 

TY  - CONF
JO  - Internet Multimedia Systems Architecture and Application (IMSAA), 2011 IEEE 5th International Conference on
TI  - A mechanism to recover voice from logged VoIP sessions based on RTP
T2  - Internet Multimedia Systems Architecture and Application (IMSAA), 2011 IEEE 5th International Conference on
IS  - 
SN  - 
VO  - 
SP  - 1
EP  - 5
AU  - Choudhury, P.
AU  - Kumar, K.R.P.
AU  - Athithan, G.
AU  - Nandi, S.
Y1  - 12-13 Dec. 2011
PY  - 2011
KW  - Internet telephony
KW  - security of data
KW  - signalling protocols
KW  - Internet protocol networks
KW  - RTP
KW  - SIP/ H.323
KW  - SSRC-multiplexing
KW  - Voice over IP
KW  - active stream injection attack
KW  - dominant signaling protocols
KW  - logged VoIP sessions
KW  - media transfer
KW  - Codecs
KW  - Decoding
KW  - IP networks
KW  - Media
KW  - Multiplexing
KW  - Payloads
KW  - Protocols
KW  - RTP packet injection
KW  - SSRC multiplexing
KW  - VoIP
KW  - Voice recovery
VL  - 
JA  - Internet Multimedia Systems Architecture and Application (IMSAA), 2011 IEEE 5th International Conference on
DO  - 10.1109/IMSAA.2011.6156339
AB  - The convergence of voice and data on Internet Protocol (IP) networks has led to tremendous growth of Voice over IP (VoIP). While SIP/ H.323 are dominant signaling protocols, RTP is widely used for media transfer. Certain situations may require logging of RTP sessions along with the signaling information for call and voice analysis. Recovering voice from RTP packets requires explicit handling of RTP stack and decoding. In this paper, we propose a scheme that uses softphones with appropriate codec support for voice recovery from logged RTP packets. It is done using inherent provisions of RTP, specifically SSRC-multiplexing. The scheme also suggests the possibility of an active stream injection attack in RTP based VoIP.
ER  - 

TY  - CONF
JO  - Quantitative Evaluation of Systems (QEST), 2010 Seventh International Conference on the
TI  - An Evaluation of Entropy Based Approaches to Alert Detection in High Performance Cluster Logs
T2  - Quantitative Evaluation of Systems (QEST), 2010 Seventh International Conference on the
IS  - 
SN  - 
VO  - 
SP  - 69
EP  - 78
AU  - Makanju, A.
AU  - Zincir-Heywood, A.N.
AU  - Milios, E.E.
Y1  - 15-18 Sept. 2010
PY  - 2010
KW  - entropy
KW  - pattern clustering
KW  - security of data
KW  - HPC logs
KW  - Nodeinfo framework
KW  - Sandia National Laboratories
KW  - automatic alert detection
KW  - computational cost metric
KW  - detection accuracy metric
KW  - entropy based approach
KW  - false positive rate metric
KW  - high performance cluster logs
KW  - manual alert detection
KW  - performance metrics
KW  - Accuracy
KW  - Computational efficiency
KW  - Computers
KW  - Data mining
KW  - Entropy
KW  - Kernel
KW  - Transforms
KW  - Algorithms
KW  - Modeling and Assessment
KW  - Network Operations Management Systems
VL  - 
JA  - Quantitative Evaluation of Systems (QEST), 2010 Seventh International Conference on the
DO  - 10.1109/QEST.2010.16
AB  - Manual alert detection on modern high performance clusters (HPC) is cumbersome given their increasing complexity and size of their logs. The ability to automatically detect such alerts quickly and accurately with little or no human intervention is therefore desirable. The entropy-based approach of the Nodeinfo framework, which is in production use at Sandia National Laboratories, is one approach to automatic alert detection in HPC logs. In this work, we perform a comparative evaluation of three entropy based techniques, which are modifications to Nodeinfo. We evaluate these systems using three performance metrics, namely (i) Computational cost, (ii) detection accuracy, and (iii) false positive rate. Our results show that there is still room for improvement in entropy based approaches to the task of alert detection. We also show experimentally that it is possible to detect 100% of all alerts while maintaining an effective false positive rate of 0% using an entropy based approach. Our work suggests that entropy based approaches are viable for automatic alert detection in HPC and can improve the dependability of such systems if applied.
ER  - 

TY  - CONF
JO  - Mobile Data Management (MDM), 2013 IEEE 14th International Conference on
TI  - eCloudIDS Tier-1 iCloudIDM Layer-I (iCloudIDM-LI) Subsystem Design and Implementation through User-centric Identity Management Approach for Secure Cloud Computing Environment
T2  - Mobile Data Management (MDM), 2013 IEEE 14th International Conference on
IS  - 
SN  - 
VO  - 2
SP  - 206
EP  - 211
AU  - Srinivasan, M.K.
AU  - Sarukesi, K.
AU  - Revathy, P.
Y1  - 3-6 June 2013
PY  - 2013
KW  - cloud computing
KW  - resource allocation
KW  - security of data
KW  - ubiquitous computing
KW  - cloud computing environment security
KW  - eCloudIDS Tier-1 iCloudIDM layer-I subsystem design
KW  - eCloudIDS architecture tier-1 iCloudIDM
KW  - innovative hybrid two-tier expert engines
KW  - next-generation computing paradigm
KW  - next-generation security system
KW  - pay-as-you-use business model
KW  - resource sharing
KW  - resource utilization
KW  - sX-Engine
KW  - subsystem layer-I
KW  - uX-Engine
KW  - user-centric identity management approach
KW  - Business
KW  - Cloud computing
KW  - Computational modeling
KW  - Computer architecture
KW  - Engines
KW  - Libraries
KW  - Security
KW  - Acute Audit Repository
KW  - Cloud Computing
KW  - Cloud Security
KW  - Cloud VM/Instance Monitor
KW  - H-log-H
KW  - Special Permission Audit Repository
KW  - St
KW  - Standard Audit Repository
KW  - Warning Level Generator
KW  - eCloudIDS
KW  - eCloudIDS C3
KW  - iCloudIDM
KW  - iCloudIDM-LI
KW  - sX-Engine
KW  - uX-Engine
VL  - 2
JA  - Mobile Data Management (MDM), 2013 IEEE 14th International Conference on
DO  - 10.1109/MDM.2013.95
AB  - Cloud computing making ubiquitous influence in today's ever growing and on-demand IT world. Cloud delivers the next-generation computing paradigm that showcases the possibilities of apt resource sharing, true elasticity, and maximum resource utilization as compared to any of its early competitors. In addition to its technicality, corporates are also fascinated with its attractive tag of `pay-as-you-use' business model. After perceiving its advantages, many organizations fear in stepping into this modern computing technology, due to its severe security concerns. In today's world, whole business computing relies on the security of customers and their data. Consequently, due to the public and multi-tenancy nature of the cloud, security concerns and its velocity are huge in volume. eCloudIDS is a next-generation security system designed with innovative hybrid two-tier expert engines, namely uX-Engine and sX-Engine, which is considered as a most suitable security solution for cloud computing environments; precisely for public cloud. This paper explores the design and implementation of eCloudIDS architecture's Tier-1 iCloudIDM (formerly known as Behavioral Analyzer) Subsystem Layer-I (iCloudIDM-LI) through user-centric identity management approach.
ER  - 

TY  - JOUR
JO  - Security & Privacy, IEEE
TI  - The Kerf toolkit for intrusion analysis
T2  - Security & Privacy, IEEE
IS  - 6
SN  - 1540-7993
VO  - 2
SP  - 42
EP  - 52
AU  - Aslam, J.
AU  - Bratus, S.
AU  - Kotz, D.
AU  - Peterson, R.
AU  - Tofel, B.
AU  - Rus, D.
Y1  - Nov.-Dec. 2004
PY  - 2004
KW  - computer network management
KW  - data structures
KW  - security of data
KW  - software packages
KW  - telecommunication security
KW  - Kerf toolkit
KW  - correlation tools
KW  - data-representation tools
KW  - integrated front end
KW  - network-based intrusions
KW  - package
KW  - post-attack intrusion analysis
KW  - security
KW  - system administrators
KW  - Computer security
KW  - Cryptography
KW  - Data privacy
KW  - Data security
KW  - Databases
KW  - Indexing
KW  - Information analysis
KW  - Information security
KW  - Power system security
KW  - Telecommunication traffic
KW  - 65
KW  - Intrusion analysis
KW  - attack forensics
KW  - log alerts
KW  - remote logging
VL  - 2
JA  - Security & Privacy, IEEE
DO  - 10.1109/MSP.2004.113
AB  - Network-based intrusions have become a significant security concern. To aid system administrators with post-attack intrusion analysis, the Kerf toolkit provides an integrated front end and powerful correlation and data-representation tools, all in one package.
ER  - 

TY  - CONF
JO  - Dependable, Autonomic and Secure Computing (DASC), 2013 IEEE 11th International Conference on
TI  - A New Approach to Building a Multi-tier Direct Access Knowledgebase for IDS/SIEM Systems
T2  - Dependable, Autonomic and Secure Computing (DASC), 2013 IEEE 11th International Conference on
IS  - 
SN  - 
VO  - 
SP  - 118
EP  - 123
AU  - Azodi, A.
AU  - Jaeger, D.
AU  - Feng Cheng
AU  - Meinel, C.
Y1  - 21-22 Dec. 2013
PY  - 2013
KW  - knowledge based systems
KW  - natural language processing
KW  - security of data
KW  - IDS-SIEM system
KW  - event information analysis
KW  - event information processing
KW  - event normalisation
KW  - intrusion detection
KW  - multitier direct access knowledge-based system
KW  - natural language processing
KW  - security analytics
KW  - security event information system
KW  - security event monitoring
KW  - Algorithm design and analysis
KW  - Data mining
KW  - Indexes
KW  - Security
KW  - Servers
KW  - Standards
KW  - Event Correlation
KW  - Event Normalisation
KW  - Intrusion Detection
KW  - Natural Language Processing
KW  - Security Analytics
KW  - Security Event Information System
KW  - Security Event Monitor- ing
VL  - 
JA  - Dependable, Autonomic and Secure Computing (DASC), 2013 IEEE 11th International Conference on
DO  - 10.1109/DASC.2013.48
AB  - Looking at current IDS and SIEM systems, we observe heavy processing power dedicated solely to answering a simple question, What is the format of the log line that the IDS (or SIEM) system should process next? Due to the apparent difficulties of uniquely identifying a log line at run-time, most systems today do little or no normalisation of the events they receive. Indeed these systems often rely on popular search engine applications for processing and analysing the event information they receive, which results in slower and far less accurate event correlations. In this process, a large list of tokenisers is usually created in order to find an answer to the above posted question. The tokenisers are run against the log lines, until a match is found. The appropriate log line can then be passed on to the correct extraction module for further processing. This process is currently the standard procedure of most IDS and SIEM systems. To address this problem and to optimise and improve the said process, this paper describes a method for detecting the exact type and format of a read log line in the first place. The method presented performs in an efficient manner, while it is less resource hungry. The proposed detection system is described and implemented, its pros and cons are analysed and weighed against methods currently implemented by popular IDS and SIEM systems for solving this task.
ER  - 

TY  - CONF
JO  - Information Technology: New Generations (ITNG), 2012 Ninth International Conference on
TI  - CIDD: A Cloud Intrusion Detection Dataset for Cloud Computing and Masquerade Attacks
T2  - Information Technology: New Generations (ITNG), 2012 Ninth International Conference on
IS  - 
SN  - 
VO  - 
SP  - 397
EP  - 402
AU  - Kholidy, H.A.
AU  - Baiardi, F.
Y1  - 16-18 April 2012
PY  - 2012
KW  - cloud computing
KW  - security of data
KW  - statistics
KW  - user interfaces
KW  - CIDD dataset
KW  - UNIX user
KW  - Windows user
KW  - audit time
KW  - behavior based audit data
KW  - cloud computing
KW  - cloud intrusion detection dataset
KW  - host based attack
KW  - knowledge based audit data
KW  - log analyzer and correlator system
KW  - masquerade attack
KW  - network based attack
KW  - statistic table
KW  - user IP address
KW  - user requirement
KW  - virtual machines
KW  - Cloud computing
KW  - Correlators
KW  - IP networks
KW  - Intrusion detection
KW  - Testing
KW  - Training
KW  - attacks
KW  - cloud
KW  - computing
KW  - dataset
KW  - intrusion detection
KW  - masquerade
KW  - security
VL  - 
JA  - Information Technology: New Generations (ITNG), 2012 Ninth International Conference on
DO  - 10.1109/ITNG.2012.97
AB  - Masquerade attacks pose a serious threat for cloud system due to the massive amount of resource of these systems. Lack of datasets for cloud computing hinders the building of efficient intrusion detection of these attacks. Current dataset cannot be used due to the heterogeneity of user requirements, the distinct operating systems installed in the VMs, and the data size of Cloud systems. This paper presents a Cloud Intrusion Detection Dataset (CIDD) that is the first one for cloud systems and that consists of both knowledge and behavior based audit data collected from both UNIX and Windows users. With respect to current datasets, CIDD has real instances of host and network based attacks and masquerades, and provides complete diverse audit parameters to build efficient detection techniques. The final statistic tables for each user are built by Log Analyzer and Correlator System (LACS) that parses and analyzes user's binary log files, and correlates audits data according to user IP address(es) and audit time. We describe in details the components and the architecture of LACS and CIDD, and the attacks distribution in CIDD.
ER  - 

TY  - JOUR
JO  - Dependable and Secure Computing, IEEE Transactions on
TI  - Analysis of Computer Intrusions Using Sequences of Function Calls
T2  - Dependable and Secure Computing, IEEE Transactions on
IS  - 2
SN  - 1545-5971
VO  - 4
SP  - 137
EP  - 150
AU  - Peisert, S.
AU  - Bishop, M.
AU  - Karin, S.
AU  - Marzullo, K.
Y1  - April-June 2007
PY  - 2007
KW  - security of data
KW  - anomaly detection
KW  - computer intrusion detection
KW  - forensic analysis
KW  - function call sequence
KW  - unauthorized access
KW  - Computer aided instruction
KW  - Computer crime
KW  - Computer security
KW  - Forensics
KW  - Humans
KW  - Information security
KW  - Intrusion detection
KW  - Kernel
KW  - Needles
KW  - Testing
KW  - Security
KW  - anomaly detection
KW  - auditing
KW  - design
KW  - forensic analysis
KW  - hacking).
KW  - intrusion detection
KW  - logging
KW  - management
KW  - unauthorized access (for example
VL  - 4
JA  - Dependable and Secure Computing, IEEE Transactions on
DO  - 10.1109/TDSC.2007.1003
AB  - This paper demonstrates the value of analyzing sequences of function calls for forensic analysis. Although this approach has been used for intrusion detection (that is, determining that a system has been attacked), its value in isolating the cause and effects of the attack has not previously been shown. We also look for not only the presence of unexpected events but also the absence of expected events. We tested these techniques using reconstructed exploits in su, ssh, and lpr, as well as proof-of-concept code, and, in all cases, were able to detect the anomaly and the nature of the vulnerability.
ER  - 

TY  - CONF
JO  - Intelligent Information Technology and Security Informatics (IITSI), 2010 Third International Symposium on
TI  - The Design and Implementation of Host-Based Intrusion Detection System
T2  - Intelligent Information Technology and Security Informatics (IITSI), 2010 Third International Symposium on
IS  - 
SN  - 
VO  - 
SP  - 595
EP  - 598
AU  - Lin Ying
AU  - Zhang Yan
AU  - Ou Yang-jia
Y1  - 2-4 April 2010
PY  - 2010
KW  - backpropagation
KW  - data analysis
KW  - neural nets
KW  - security of data
KW  - BP neural network
KW  - anomaly detection
KW  - backpropagation
KW  - host-based intrusion detection system
KW  - log file analysis
KW  - Computer displays
KW  - Computer security
KW  - Decoding
KW  - Information analysis
KW  - Information technology
KW  - Intrusion detection
KW  - Neural networks
KW  - Pattern matching
KW  - Protection
KW  - Telecommunication traffic
KW  - BP neural network
KW  - HIDS
KW  - Log analysis
KW  - OSSEC
KW  - intrusion detection
KW  - intrusion detection system
VL  - 
JA  - Intelligent Information Technology and Security Informatics (IITSI), 2010 Third International Symposium on
DO  - 10.1109/IITSI.2010.127
AB  - Intrusion detection is the process of identifying and responding to suspicious activities targeted at computing and communication resources, and it has become the mainstream of information assurance as the dramatic increase in the number of attacks. Intrusion detection system (IDS) monitors and collects data from a target system that should be protected, processes and correlates the gathered information, and initiates responses when evidence of an intrusion is detected. In this paper, we designed and implemented a host-based intrusion detection system, which combines two detection technologies, one is log file analysis technology and the other is BP neural network technology. Log file analysis is an approach of misuse detection, and BP neural network is an approach of anomaly detection. By combination of these two kinds of detection technologies, the HIDS that we have implemented can effectively improve the efficiency and accuracy of intrusion detection.
ER  - 

TY  - CONF
JO  - Services (SERVICES), 2011 IEEE World Congress on
TI  - TrustCloud: A Framework for Accountability and Trust in Cloud Computing
T2  - Services (SERVICES), 2011 IEEE World Congress on
IS  - 
SN  - 
VO  - 
SP  - 584
EP  - 588
AU  - Ko, R.K.L.
AU  - Jagadpramana, P.
AU  - Mowbray, M.
AU  - Pearson, S.
AU  - Kirchberg, M.
AU  - Qianhui Liang
AU  - Bu Sung Lee
Y1  - 4-9 July 2011
PY  - 2011
KW  - cloud computing
KW  - security of data
KW  - TrustCloud
KW  - cloud computing accountability
KW  - data distribution
KW  - data security
KW  - large-scale virtualization
KW  - policy-based approach
KW  - preventive controls
KW  - Cloud computing
KW  - Law
KW  - Monitoring
KW  - Operating systems
KW  - Privacy
KW  - Security
KW  - Servers
KW  - accountability
KW  - auditability
KW  - continuous auditing
KW  - data provenance
KW  - governance
KW  - logging
KW  - monitoring
KW  - trust in cloud computing
VL  - 
JA  - Services (SERVICES), 2011 IEEE World Congress on
DO  - 10.1109/SERVICES.2011.91
AB  - The key barrier to widespread uptake of cloud computing is the lack of trust in clouds by potential customers. While preventive controls for security and privacy are actively researched, there is still little focus on detective controls related to cloud accountability and audit ability. The complexity resulting from large-scale virtualization and data distribution carried out in current clouds has revealed an urgent research agenda for cloud accountability, as has the shift in focus of customer concerns from servers to data. This paper discusses key issues and challenges in achieving a trusted cloud through the use of detective controls, and presents the Trust Cloud framework, which addresses accountability in cloud computing via technical and policy-based approaches.
ER  - 

TY  - CONF
JO  - Computer Architecture, 2008. ISCA '08. 35th International Symposium on
TI  - Flexible Hardware Acceleration for Instruction-Grain Program Monitoring
T2  - Computer Architecture, 2008. ISCA '08. 35th International Symposium on
IS  - 
SN  - 1063-6897
VO  - 
SP  - 377
EP  - 388
AU  - Chen, S.
AU  - Kozuch, M.
AU  - Strigkos, T.
AU  - Falsafi, B.
AU  - Gibbons, P.B.
AU  - Mowry, T.C.
AU  - Ramachandran, V.
AU  - Ruwase, O.
AU  - Ryan, M.
AU  - Vlachos, E.
Y1  - 21-25 June 2008
PY  - 2008
KW  - program debugging
KW  - program diagnostics
KW  - security of data
KW  - dynamic binary instrumentation
KW  - flexible hardware acceleration
KW  - flexible hardware solution
KW  - instruction-grain program monitoring
KW  - security attacks
KW  - software-only tools
KW  - Acceleration
KW  - Computer bugs
KW  - Costs
KW  - Data security
KW  - Filters
KW  - Hardware
KW  - Instruments
KW  - Monitoring
KW  - Proposals
KW  - Target tracking
KW  - Hardware Acceleration
KW  - Idempotent Filter
KW  - Inheritance Tracking
KW  - Instruction-grain Program Monitoring
KW  - LBA
KW  - Lifeguards
KW  - Log-Based Architectures
KW  - Metadata-TLB
VL  - 
JA  - Computer Architecture, 2008. ISCA '08. 35th International Symposium on
DO  - 10.1109/ISCA.2008.20
AB  - Instruction-grain program monitoring tools, which check and analyze executing programs at the granularity of individual instructions, are invaluable for quickly detecting bugs and security attacks and then limiting their damage (via containment and/or recovery). Unfortunately, their fine-grain nature implies very high monitoring overheads for software-only tools, which are typically based on dynamic binary instrumentation. Previous hardware proposals either focus on mechanisms that target specific bugs or address only the cost of binary instrumentation. In this paper, we propose a flexible hardware solution for accelerating a wide range of instruction-grain monitoring tools. By examining a number of diverse tools (for memory checking, security tracking, and data race detection), we identify three significant common sources of overheads and then propose three novel hardware techniques for addressing these overheads: Inheritance Tracking, Idempotent Filters, and Metadata-TLBs. Together, these constitute a general-purpose hardware acceleration framework. Experimental results show our framework reduces overheads by 2-3X over the previous state-of-the-art, while supporting the needed flexibility.
ER  - 

TY  - CONF
JO  - Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology (ECTI-CON), 2013 10th International Conference on
TI  - An analysis of suitable parameters for efficiently applying K-means clustering to large TCPdump data set using Hadoop framework
T2  - Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology (ECTI-CON), 2013 10th International Conference on
IS  - 
SN  - 
VO  - 
SP  - 1
EP  - 6
AU  - Therdphapiyanak, J.
AU  - Piromsopa, K.
Y1  - 15-17 May 2013
PY  - 2013
KW  - data mining
KW  - distributed processing
KW  - pattern clustering
KW  - security of data
KW  - Apache Mahout/Hadoop framework
KW  - KDD'99 competition
KW  - ROC curve
KW  - TCPdump data set
KW  - anomaly detection model
KW  - detection rate
KW  - distributed system framework
KW  - false alarm rate table
KW  - k-means algorithm
KW  - k-means clustering
KW  - large-scale log analysis
KW  - suitable parameters
KW  - Accuracy
KW  - Algorithm design and analysis
KW  - Clustering algorithms
KW  - Distributed databases
KW  - Indexes
KW  - Intrusion detection
KW  - Partitioning algorithms
KW  - Distributed log analysis
KW  - Hadoop
KW  - IDS
KW  - Intrusion Detection System
KW  - K-means algorithm
KW  - KDD'99
KW  - Log analysis
KW  - Mahout
KW  - Security
VL  - 
JA  - Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology (ECTI-CON), 2013 10th International Conference on
DO  - 10.1109/ECTICon.2013.6559650
AB  - In this paper, we determined the appropriate number of clusters and the proper amount of entries for applying K-means clustering to TCPdump data set using Apache Mahout/Hadoop framework. We aim at finding suitable configuration for efficiently analyzing large data set in limited amount of time. Our implementation applied Hadoop for large-scale log analysis with data set from KDD'99 competition as test data. With the distributed system framework, we can analyze a whole data set of KDD'99 by first applying our preprocessing. In addition, we use an anomaly detection model for log analysis. A key challenge is to make anomaly detection work more accurately. For the Kmeans algorithm, a key challenge is to set the appropriate number of the initial cluster (K). Moreover, we discuss whether the number of entries in log files affects the accuracy and detection rate of the system or not. Therefore, our implementation and experimental results describe the appropriate number of cluster and the proper amount of entries in log files. Finally, we show the result of our experiments with accuracy rate and number of initial cluster (K) graph, ROC curve and detection rate and false alarm rate table.
ER  - 

TY  - CONF
JO  - e-Education, e-Business, e-Management, and e-Learning, 2010. IC4E '10. International Conference on
TI  - Methods for Efficient Digital Evidences Collecting of Business Proceses and Users Activity in eLearning Enviroments
T2  - e-Education, e-Business, e-Management, and e-Learning, 2010. IC4E '10. International Conference on
IS  - 
SN  - 
VO  - 
SP  - 126
EP  - 130
AU  - Azemovic, J.
AU  - Music, D.
Y1  - 22-24 Jan. 2010
PY  - 2010
KW  - computer aided instruction
KW  - computer crime
KW  - computer forensics
KW  - database management systems
KW  - forensic science
KW  - business proceses
KW  - computer crime act
KW  - database systems
KW  - digital evidences collecting
KW  - elearning enviroments
KW  - forensic analysis
KW  - users activity
KW  - Computer crime
KW  - Data security
KW  - Databases
KW  - Digital forensics
KW  - Educational institutions
KW  - Electronic learning
KW  - Force measurement
KW  - Information systems
KW  - Information technology
KW  - Medical diagnostic imaging
KW  - audit logging
KW  - database
KW  - digital evidence
KW  - digital forensics
VL  - 
JA  - e-Education, e-Business, e-Management, and e-Learning, 2010. IC4E '10. International Conference on
DO  - 10.1109/IC4E.2010.92
AB  - Contemporary information systems such as: eLearning, eUnivesity, eVoting, eHealth, etc., are frequently used and misused for irregular data changes (data tampering). Those facts force us to reconsider our security measures and find a way to improve them. Proving a computer crime act require very complicated processes which are based on digital evidence collecting, forensic analysis and investigation process. Forensic analysis of database systems is very specific and demanding task and therefore presents main inspiration for our research. In this paper we present the fact that classical methods of collecting digital evidence are not appropriate and efficient. In order to improve efficiency we propose combination of well know technology independent methods from database world and their application in field of forensic science. Also we are proposed some new research direction in this area.
ER  - 

TY  - CONF
JO  - Computer Research and Development, 2010 Second International Conference on
TI  - Securing Database by Using Collaborative Inference Detection
T2  - Computer Research and Development, 2010 Second International Conference on
IS  - 
SN  - 
VO  - 
SP  - 260
EP  - 264
AU  - Swami, K.S.
AU  - Sawant, A.A.
Y1  - 7-10 May 2010
PY  - 2010
KW  - database management systems
KW  - groupware
KW  - inference mechanisms
KW  - knowledge acquisition
KW  - probability
KW  - security of data
KW  - collaborative inference detection
KW  - data dependency
KW  - database schema
KW  - database security
KW  - inference probability
KW  - inference violation detection system
KW  - knowledge acquisition
KW  - query-time inference violation detection
KW  - semantic inference graph
KW  - semantic inference model
KW  - semantic knowledge
KW  - Collaboration
KW  - Collaborative work
KW  - Data analysis
KW  - Data security
KW  - Databases
KW  - Educational institutions
KW  - History
KW  - Information security
KW  - Information technology
KW  - Knowledge acquisition
KW  - Computation methodologies
KW  - Data dependency analysis
KW  - Database security
KW  - Inference engines
KW  - Knowledge processing
VL  - 
JA  - Computer Research and Development, 2010 Second International Conference on
DO  - 10.1109/ICCRD.2010.47
AB  - Database is a major component of global information infrastructure. Thus securing these databases is very important thing. Thus, we develop an inference violation detection system to protect sensitive data content. Based on Knowledge acquisition, data dependency, database schema, and semantic knowledge, we constructed a semantic inference model (SIM) that represents the possible inference channels from any attribute to the pre-assigned sensitive attributes. The SIM is then instantiated to a semantic inference graph (SIG) for query-time inference violation detection. For a single user case, when a user poses a query, the detection system will examine users past query log and calculate the probability of inferring sensitive information. The query request will be denied if the inference probability exceeds the pre-specified threshold. For multiuser cases, the users may share their query answers to increase the inference probability. Therefore, we develop a model for evaluating collaborative inference based on the query sequences of collaborators and their task-sensitive collaboration levels. Experimental studies reveal that information authoritativeness, communication fidelity, and honesty in collaboration are three key factors that affect the level of achievable collaboration. In order to prevent an adversary from inferring information from a database, inference analyst must be able to detect and prevent possible inferences.
ER  - 

TY  - CONF
JO  - Science and Information Conference (SAI), 2014
TI  - Enhancing detection rate in database intrusion detection system
T2  - Science and Information Conference (SAI), 2014
IS  - 
SN  - 
VO  - 
SP  - 556
EP  - 563
AU  - Rao, U.P.
AU  - Singh, N.K.
AU  - Amin, A.R.
AU  - Sahu, K.
Y1  - 27-29 Aug. 2014
PY  - 2014
KW  - data protection
KW  - database management systems
KW  - security of data
KW  - advanced security solutions
KW  - database IDS
KW  - database intrusion detection system
KW  - database protection
KW  - detection rate enhancement
KW  - high false positive rate probability
KW  - high profile matching constraint
KW  - malicious insider
KW  - Data mining
KW  - Databases
KW  - Feature extraction
KW  - Intrusion detection
KW  - Organizations
KW  - Pattern matching
KW  - Audit Log
KW  - DBMS
KW  - Database Security
KW  - Intrusion Detection System
KW  - Transaction Profile
VL  - 
JA  - Science and Information Conference (SAI), 2014
DO  - 10.1109/SAI.2014.6918241
AB  - Protecting the database from malicious insider is the major challenge for the organizations now-a-days. The organizations are eager to adopt the advanced security solutions to prevent the theft of data. Most of the existing database IDS (intrusion detection system) apply high profile matching constraint. Due to this there is probability of high false positive rate. In this paper we propose a novel database IDS which reduces the false positive and false negative rates. This approach provides flexibility in profile matching constraint.
ER  - 

TY  - JOUR
JO  - Security & Privacy, IEEE
TI  - Building Reliable and Secure Virtual Machines Using Architectural Invariants
T2  - Security & Privacy, IEEE
IS  - 5
SN  - 1540-7993
VO  - 12
SP  - 82
EP  - 85
AU  - Cuong Pham
AU  - Estrada, Z.J.
AU  - Phuong Cao
AU  - Kalbarczyk, Z.
AU  - Iyer, R.K.
Y1  - Sept.-Oct. 2014
PY  - 2014
KW  - security of data
KW  - virtual machines
KW  - HyperTap
KW  - architectural invariants
KW  - continuous VM monitoring
KW  - data logging
KW  - event-driven VM monitoring
KW  - hardware architectural invariants properties
KW  - hypervisor-level monitoring framework
KW  - reliable virtual machines
KW  - secure virtual machines
KW  - Computer architecture
KW  - Computer security
KW  - Linux
KW  - Monitoring
KW  - Virtual machine monitors
KW  - Virtual machining
KW  - HyperTap
KW  - hang detection
KW  - hangs
KW  - hardware architectural invariants
KW  - hardware-assisted virtualization
KW  - privilege escalation
KW  - reliability
KW  - rootkits
KW  - security
KW  - virtual machines
KW  - virtual-machine introspection
VL  - 12
JA  - Security & Privacy, IEEE
DO  - 10.1109/MSP.2014.87
AB  - HyperTap is a hypervisor-level monitoring framework for virtual machines (VMs). It uses hardware architectural invariants properties defined and enforced by a hardware platform to establish the root of trust for logging data and events. HyperTap also supports continuous, event-driven VM monitoring, which enables both capturing the system state and responding rapidly to actions of interest.
ER  - 

TY  - CONF
JO  - Collaborative Computing: Networking, Applications and Worksharing (Collaboratecom), 2013 9th International Conference Conference on
TI  - Android keylogging threat
T2  - Collaborative Computing: Networking, Applications and Worksharing (Collaboratecom), 2013 9th International Conference Conference on
IS  - 
SN  - 
VO  - 
SP  - 545
EP  - 552
AU  - Mohsen, F.
AU  - Shehab, M.
Y1  - 20-23 Oct. 2013
PY  - 2013
KW  - Android (operating system)
KW  - mobile computing
KW  - security of data
KW  - Android keylogging threat
KW  - Android platform
KW  - Android soft keyboards
KW  - Android standard keyboard
KW  - KeyboardView class
KW  - security risks
KW  - third-party keyboards
KW  - Androids
KW  - Humanoid robots
KW  - Keyboards
KW  - Malware
KW  - Mobile communication
KW  - Smart phones
KW  - Software
KW  - keyboard logging
KW  - mobile apps
KW  - mobile security
VL  - 
JA  - Collaborative Computing: Networking, Applications and Worksharing (Collaboratecom), 2013 9th International Conference Conference on
DO  - 
AB  - The openness of Android platform has attracted users, developers and attackers. Android offers bunch of capabilities and flexibilities, for instance, developers can write their own keyboard service-similar to Android soft keyboards-using the KeyboardView class. This class is available since api level 3.0 and can be part of the layout of an activity. Users prefer to download and install third-party keyboards that offer better experience and capabilities. However, there are security risks related to users installing and using these custom keyboards. Attackers can build or take advantage of existing third-party keyboards to create keyloggers to spy on smartphones users. Third-party keyboard once activated would substitute the Android standard keyboard, so all keys events pass this app. As results, many attacks can be launched identified by the permissions granted to these apps. The objective of this paper is to present these attacks, analyze their causes, and provide possible solutions.
ER  - 

TY  - JOUR
JO  - Security & Privacy, IEEE
TI  - Detecting Fraud on Websites
T2  - Security & Privacy, IEEE
IS  - 6
SN  - 1540-7993
VO  - 9
SP  - 80
EP  - 85
AU  - Fly, R.
Y1  - Nov.-Dec. 2011
PY  - 2011
KW  - Web sites
KW  - security of data
KW  - Web sites
KW  - online fraud detection
KW  - Behavioral science
KW  - Computer crime
KW  - Internet
KW  - Online services
KW  - computer security
KW  - fraud detection
KW  - http requests
KW  - logging events
KW  - website security
VL  - 9
JA  - Security & Privacy, IEEE
DO  - 10.1109/MSP.2011.161
AB  - Beyond some specific vertical markets that have been dealing with fraud for ages such as financial institutions and retailers most software and services have zero ability to detect someone committing fraud against them or their users. Most sites happily take whatever requests and input users give them, thinking it's legitimate.Fortunately, there are fairly straightforward things every website can do to detect online fraud.
ER  - 

TY  - CONF
JO  - Advanced Information Networking and Applications - Workshops, 2008. AINAW 2008. 22nd International Conference on
TI  - Detecting Masqueraders Using High Frequency Commands as Signatures
T2  - Advanced Information Networking and Applications - Workshops, 2008. AINAW 2008. 22nd International Conference on
IS  - 
SN  - 
VO  - 
SP  - 596
EP  - 601
AU  - Ming Dong Wan
AU  - Han-Ching Wu
AU  - Ying-Wei Kuo
AU  - Marshall, J.
AU  - Huang, S.-H.S.
Y1  - 25-28 March 2008
PY  - 2008
KW  - computer networks
KW  - digital signatures
KW  - security of data
KW  - telecommunication security
KW  - high frequency commands
KW  - masquerader detection
KW  - network intruders
KW  - stolen passwords
KW  - user computer accounts
KW  - user signatures
KW  - Application software
KW  - Artificial intelligence
KW  - Computer networks
KW  - Computer science
KW  - Data security
KW  - Databases
KW  - Electronic mail
KW  - Frequency
KW  - Intrusion detection
KW  - Monitoring
KW  - Intrusion Detection
KW  - Masqueraders
KW  - Network Security.
KW  - Profiles
KW  - Signatures
VL  - 
JA  - Advanced Information Networking and Applications - Workshops, 2008. AINAW 2008. 22nd International Conference on
DO  - 10.1109/WAINA.2008.38
AB  - Network intruders commonly use stolen passwords or other means to log into legitimate users' computer accounts. To prevent this from happening, it is important that we are able to distinguish a user as a true user or a masquerader. Uniqueness of user command has been used in the past as signature of users. This project explores the high frequency commands to see if they work well as signatures. Experimental result was provided to show that they work as well as the Uniqueness method. Besides, the comparisons with other methods were also presented.
ER  - 

TY  - CONF
JO  - Availability, Reliability and Security (ARES), 2011 Sixth International Conference on
TI  - LiDSec- A Lightweight Pseudonymization Approach for Privacy-Preserving Publishing of Textual Personal Information
T2  - Availability, Reliability and Security (ARES), 2011 Sixth International Conference on
IS  - 
SN  - 
VO  - 
SP  - 603
EP  - 608
AU  - Rawassizadeh, R.
AU  - Heurix, J.
AU  - Khosravipour, S.
AU  - A Min Tjoa
Y1  - 22-26 Aug. 2011
PY  - 2011
KW  - data privacy
KW  - security of data
KW  - LiDSec
KW  - data consumers
KW  - data providers
KW  - emails
KW  - life events
KW  - life logging
KW  - lightweight pseudonymization approach
KW  - personal archive
KW  - personal information recording
KW  - personal information sharing
KW  - privacy-preserving publishing
KW  - sensor network
KW  - social networking activities
KW  - textual personal information
KW  - Data privacy
KW  - Data structures
KW  - Graphical user interfaces
KW  - Privacy
KW  - Prototypes
KW  - Social network services
KW  - Usability
KW  - Personal Information
KW  - Privacy
KW  - Pseudonymization
KW  - Security
VL  - 
JA  - Availability, Reliability and Security (ARES), 2011 Sixth International Conference on
DO  - 10.1109/ARES.2011.93
AB  - Sharing personal information benefits both data providers and data consumers in many ways. Recent advances in sensor networks and personal archives enable users to record personal information including emails, social networking activities, or life events (life logging). These information objects are usually privacy sensitive and thus need to be protected adequately when being shared. In this work, we present a lightweight pseudonymization framework which allows users to benefit from sharing their personal information while still preserving their privacy. Furthermore, this approach increases the data owners' awareness of what information they are sharing, thus rendering data publishing more transparent.
ER  - 

TY  - CONF
JO  - Computer Information Systems and Industrial Management Applications (CISIM), 2010 International Conference on
TI  - Extra-Tree: A model to organize execution traces of Web services
T2  - Computer Information Systems and Industrial Management Applications (CISIM), 2010 International Conference on
IS  - 
SN  - 
VO  - 
SP  - 497
EP  - 501
AU  - Sinha, S.
AU  - Sinha, S.K.
AU  - Purkayastha, B.S.
Y1  - 8-10 Oct. 2010
PY  - 2010
KW  - Web services
KW  - security of data
KW  - trees (mathematics)
KW  - Web services
KW  - coarse-grained level
KW  - distributed computing paradigm
KW  - extra-tree model
KW  - nonlinear model
KW  - Business
KW  - Computational modeling
KW  - Computers
KW  - Security
KW  - Service oriented architecture
KW  - XML
KW  - Audit Trail
KW  - BPEL
KW  - Execution Trace
KW  - Extra-Tree
KW  - Security
KW  - Web service
VL  - 
JA  - Computer Information Systems and Industrial Management Applications (CISIM), 2010 International Conference on
DO  - 10.1109/CISIM.2010.5643460
AB  - In this paper, a non-linear model called Extra-Tree is proposed to organize execution traces of orchestrate Web services. This proposed model provides us a secured logging system which records the history of all the suspicious or malicious activities from the initiation of the Web service to the completion of the Web service. The main focus of this paper is to organize execution traces of Web services in a distributed computing paradigm in the form of a tree. One of the special characteristics of the model is that the execution traces of Web services can be retrieved from the coarse-grained level to the fine-grained level of the tree as per requirement.
ER  - 

TY  - CONF
JO  - Mechatronic Science, Electric Engineering and Computer (MEC), 2011 International Conference on
TI  - Removable media auditbased on drive layer
T2  - Mechatronic Science, Electric Engineering and Computer (MEC), 2011 International Conference on
IS  - 
SN  - 
VO  - 
SP  - 1754
EP  - 1756
AU  - Yang Pengying
Y1  - 19-22 Aug. 2011
PY  - 2011
KW  - digital storage
KW  - multimedia computing
KW  - security of data
KW  - computer storage
KW  - drive layer
KW  - log transaction
KW  - mobile storage media
KW  - removable media audit
KW  - removable storage medium
KW  - trusted computing theory
KW  - Authorization
KW  - Media
KW  - Mobile communication
KW  - Servers
KW  - Universal Serial Bus
KW  - Writing
KW  - Media audit
KW  - mobile storage
KW  - security
VL  - 
JA  - Mechatronic Science, Electric Engineering and Computer (MEC), 2011 International Conference on
DO  - 10.1109/MEC.2011.6025821
AB  - A solution to use removable storage medium safely is put forward based on the study about trusted computing theory. This solution manages removable media through C/S model, registers mobile storage media used by user in the server, assigns a unique authorization number to authorize it can be used in trusted region; The client consists of the application and driver, the application is used to authorize mobile storage media and record the transaction log, then send control words to the driver after authentication, and the driver makes the mobile storage media at three states: read-only, disabled and normal. So it can control users use mobile storage media safely, prevent leakage of sensitive data in the network effectively.
ER  - 

TY  - CONF
JO  - Trust, Security and Privacy in Computing and Communications (TrustCom), 2013 12th IEEE International Conference on
TI  - A Mutual Nonrepudiation Protocol for Cloud Storage with Interchangeable Accesses of a Single Account from Multiple Devices
T2  - Trust, Security and Privacy in Computing and Communications (TrustCom), 2013 12th IEEE International Conference on
IS  - 
SN  - 
VO  - 
SP  - 439
EP  - 446
AU  - Gwan-Hwan Hwang
AU  - Jenn-Zjone Peng
AU  - Wei-Sian Huang
Y1  - 16-18 July 2013
PY  - 2013
KW  - cloud computing
KW  - protocols
KW  - security of data
KW  - auditing protocol
KW  - chain-hashing scheme
KW  - client devices
KW  - cloud storage
KW  - hash tree
KW  - interchangeable accesses
KW  - logging attestations
KW  - multiple client devices
KW  - multiple devices
KW  - mutual nonrepudiation protocol
KW  - service provider
KW  - service-level agreement
KW  - signed messages
KW  - Cloud computing
KW  - Cryptography
KW  - Digital signatures
KW  - Frequency shift keying
KW  - Performance evaluation
KW  - Protocols
KW  - Nonrepudiation
KW  - SLA
KW  - cloud security
KW  - cloud storage
KW  - hash tree
KW  - service-level agreement
VL  - 
JA  - Trust, Security and Privacy in Computing and Communications (TrustCom), 2013 12th IEEE International Conference on
DO  - 10.1109/TrustCom.2013.55
AB  - Obtaining mutual nonrepudiation between the user and service provider is crucial in cloud storage. One of the solutions for mutual nonrepudiation is based on logging attestations, which are signed messages. For every request, clients and service provider exchange attestations. These attestations will be used in an auditing protocol to verify their behavior. The chain-hashing scheme chains attestations and stores them in service provider for supporting write serializability and read freshness of files. However, the chain-hashing scheme is inefficient when files in an account can be accessed by multiple client devices interchangeably. In this paper we first show that the chain-hashing scheme cannot resist roll-back attack from service provider unless client devices keep all the attestations or there exists a way to broadcast the last attestation to all the client devices. We propose a scheme that can guarantee mutual nonrepudiation between the user and service provider without requiring the client devices to exchange any messages, and each client device only has to store the last attestation it received. We also propose how to apply the hash tree to remove accumulated attestations. The results from related experiments demonstrate the feasibility of the proposed scheme. A service provider of cloud storage can use the proposed scheme to provide a mutual nonrepudiation guarantee in their service-level agreement.
ER  - 

TY  - CONF
JO  - Applications and the Internet (SAINT), 2010 10th IEEE/IPSJ International Symposium on
TI  - Masquerade Detection in Network Environments
T2  - Applications and the Internet (SAINT), 2010 10th IEEE/IPSJ International Symposium on
IS  - 
SN  - 
VO  - 
SP  - 38
EP  - 44
AU  - Strasburg, C.
AU  - Krishnan, S.
AU  - Dorman, K.
AU  - Basu, S.
AU  - Wong, J.S.
Y1  - 19-23 July 2010
PY  - 2010
KW  - Internet
KW  - data privacy
KW  - security of data
KW  - support vector machines
KW  - GUI interaction profiling
KW  - Internet connected system
KW  - anonymized summary data
KW  - command line
KW  - data accessibility
KW  - data privacy
KW  - graphical user interface
KW  - host based approache
KW  - masquerade detection
KW  - network environment
KW  - network statistics
KW  - server log analysis
KW  - support vector machine classification
KW  - system call
KW  - tag network
KW  - user network profile
KW  - Data models
KW  - Electronic mail
KW  - Hidden Markov models
KW  - IP networks
KW  - Support vector machines
KW  - Training
KW  - Training data
KW  - D.4.6. Security and Privacy Protection
KW  - H.1.2. User/Machine Systems
KW  - K.6.5. Security and Protection
VL  - 
JA  - Applications and the Internet (SAINT), 2010 10th IEEE/IPSJ International Symposium on
DO  - 10.1109/SAINT.2010.66
AB  - As reliance on Internet connected systems expands, the threat of damage from malicious actors, especially undetected actors, rises. Masquerade attacks, where one individual or system poses as another, are among the most harmful and difficult to detect types of intrusion. Previous efforts to detect masquerade attacks have focused on host-based approaches, including command line, system call, and GUI interaction profiling but when host data is not accessible or legal/ethical restrictions apply, these methods are infeasible. In this work, we present an approach to masquerade detection using only basic network statistics. We use server log analysis to tag network events with the associated user and build user network profiles By utilizing only antagonized summary data, we limit the privacy impact of masquerade detection while avoiding the data accessibility issues associated with host-based approaches. We compile 90 days of NetFlow data from over 50 users and show the user profile are unique, and likely useful for detecting masqueraders. Finally, we apply Support Vector Machine (SVM) classification to demonstrate feasibility of masquerade detection using network data.
ER  - 

TY  - CONF
JO  - Advanced Information Networking and Applications (AINA), 2014 IEEE 28th International Conference on
TI  - Protecting Run-Time Filters for Network Intrusion Detection Systems
T2  - Advanced Information Networking and Applications (AINA), 2014 IEEE 28th International Conference on
IS  - 
SN  - 1550-445X
VO  - 
SP  - 116
EP  - 122
AU  - Valgenti, V.C.
AU  - Hai Sun
AU  - Min Sik Kim
Y1  - 13-16 May 2014
PY  - 2014
KW  - filters
KW  - security of data
KW  - telecommunication traffic
KW  - NIDS performance
KW  - anomaly detector
KW  - crafted packets
KW  - filtering solution
KW  - malicious traffic
KW  - multigigabit line-speeds
KW  - network intrusion detection systems
KW  - network packets
KW  - packet inspection
KW  - run-time filters
KW  - run-time filters protection
KW  - Automata
KW  - Detectors
KW  - Inspection
KW  - Intrusion detection
KW  - Limiting
KW  - Matched filters
KW  - Sociology
KW  - Deep Packet Inspection
KW  - Filters
KW  - IDS
KW  - Intrusion Detection
KW  - Network Security
KW  - Run-time Filters
KW  - Security
VL  - 
JA  - Advanced Information Networking and Applications (AINA), 2014 IEEE 28th International Conference on
DO  - 10.1109/AINA.2014.19
AB  - Network Intrusion Detection Systems (NIDS) examine millions of network packets searching for malicious traffic. Multi-gigabit line-speeds combined with growing databases of rules lead to dropped packets as the load exceeds the capacity of the device. Several areas of research have attempted to mitigate this problem through improving packet inspection efficiency, increasing resources, or reducing the examined population. A popular method for reducing the population examined is to employ run-time filters that can provide a quick check to determine that a given network packet cannot match a particular rule set. While this technique is an excellent method for reducing the population under examination, rogue elements can trivially bypass such filters with specially crafted packets and render the run-time filters effectively useless. Since the filtering comes at the cost of extra processing a filtering solution could actually perform worse than a non-filtered solution under such pandemic circumstances. To defend against such attacks, it is necessary to consider run-time filters as an independent anomaly detector capable of detecting attacks against itself. Such anomaly detection, together with judicious rate-limiting of traffic forwarded to full packet inspection, allows the detection, logging, and mitigation of attacks targeted at the filters while maintaining the overall improvements in NIDS performance garnered from using run-time filters.
ER  - 

TY  - JOUR
JO  - Visualization and Computer Graphics, IEEE Transactions on
TI  - Interactive Exploration of Data Traffic with Hierarchical Network Maps
T2  - Visualization and Computer Graphics, IEEE Transactions on
IS  - 6
SN  - 1077-2626
VO  - 12
SP  - 1440
EP  - 1449
AU  - Mansmann, F.
AU  - Vinnik, S.
Y1  - Nov.-Dec. 2006
PY  - 2006
KW  - IP networks
KW  - Internet
KW  - data visualisation
KW  - data warehouses
KW  - inference mechanisms
KW  - learning (artificial intelligence)
KW  - security of data
KW  - telecommunication security
KW  - telecommunication traffic
KW  - Internet
KW  - analytical reasoning
KW  - anomaly detection
KW  - data traffic
KW  - data warehouse technology
KW  - hierarchical network map
KW  - interactive visual exploration
KW  - interactive visualization
KW  - machine learning
KW  - multidimensional modeling
KW  - statistics
KW  - system log
KW  - very large data set
KW  - Business communication
KW  - Data visualization
KW  - Displays
KW  - Government
KW  - IP networks
KW  - Machine learning
KW  - Multidimensional systems
KW  - Remote monitoring
KW  - Statistical analysis
KW  - Telecommunication traffic
KW  - Data and knowledge visualization
KW  - information visualization
KW  - network security.
KW  - visual analytics
KW  - Algorithms
KW  - Computer Communication Networks
KW  - Computer Graphics
KW  - Computer Simulation
KW  - Information Storage and Retrieval
KW  - Signal Processing, Computer-Assisted
KW  - User-Computer Interface
VL  - 12
JA  - Visualization and Computer Graphics, IEEE Transactions on
DO  - 10.1109/TVCG.2006.98
AB  - Network communication has become indispensable in business, education and government. With the pervasive role of the Internet as a means of sharing information across networks, its misuse for destructive purposes, such as spreading malicious code, compromising remote hosts, or damaging data through unauthorized access, has grown immensely in the recent years. The classical way of monitoring the operation of large network systems is by analyzing the system logs for detecting anomalies. In this work, we introduce hierarchical network map, an interactive visualization technique for gaining a deeper insight into network flow behavior by means of user-driven visual exploration. Our approach is meant as an enhancement to conventional analysis methods based on statistics or machine learning. We use multidimensional modeling combined with position and display awareness to view source and target data of the hosts in a hierarchical fashion with the ability to interactively change the level of aggregation or apply filtering. The interdisciplinary approach integrating data warehouse technology, information visualization and decision support brings about the benefit of efficiently collecting the input data and aggregating over very large data sets, visualizing the results and providing interactivity to facilitate analytical reasoning
ER  - 

TY  - CONF
JO  - Trust, Security and Privacy in Computing and Communications (TrustCom), 2013 12th IEEE International Conference on
TI  - Mitigating Malicious Updates: Prevention of Insider Threat to Databases
T2  - Trust, Security and Privacy in Computing and Communications (TrustCom), 2013 12th IEEE International Conference on
IS  - 
SN  - 
VO  - 
SP  - 781
EP  - 788
AU  - Ragavan, H.
AU  - Panda, B.
Y1  - 16-18 July 2013
PY  - 2013
KW  - database management systems
KW  - graph theory
KW  - security of data
KW  - attack prevention models
KW  - data items monitoring
KW  - database
KW  - dependency graph
KW  - dependency relationships
KW  - insider threat prevention
KW  - logs
KW  - malicious operations
KW  - malicious update mitigation
KW  - write operations
KW  - Data models
KW  - Databases
KW  - Delays
KW  - Monitoring
KW  - Organizations
KW  - Remuneration
KW  - Security
KW  - Database
KW  - Dependency Graph
KW  - Insider Threat
KW  - Log
VL  - 
JA  - Trust, Security and Privacy in Computing and Communications (TrustCom), 2013 12th IEEE International Conference on
DO  - 10.1109/TrustCom.2013.95
AB  - Insider threats cause serious damage to data in any organization and is considered as a grave issue. In spite of the presence of threat prevention mechanisms, insiders can continue to attack a database by figuring out the dependency relationships among data items. Thus, examining write operations performed by an insider by taking advantage of dependencies aids in mitigating insider threats. We have developed two attack prevention models, which involve logs and dependency graphs respectively, to monitor data items and prevent malicious operations on them. The developed algorithms have been implemented on a simulated database and the results show that the models effectively mitigate insider threats arising from write operations.
ER  - 

TY  - JOUR
JO  - Parallel and Distributed Systems, IEEE Transactions on
TI  - Tracing Worm Break-In and Contaminations via Process Coloring: A Provenance-Preserving Approach
T2  - Parallel and Distributed Systems, IEEE Transactions on
IS  - 7
SN  - 1045-9219
VO  - 19
SP  - 890
EP  - 902
AU  - Xuxian Jiang
AU  - Buchholz, F.
AU  - Walters, A.
AU  - Dongyan Xu
AU  - Yi-Min Wang
AU  - Spafford, E.H.
Y1  - July 2008
PY  - 2008
KW  - Internet
KW  - network servers
KW  - operating systems (computers)
KW  - security of data
KW  - color-based worm warning generation
KW  - networked servers
KW  - process coloring
KW  - provenance-preserving approach
KW  - real-time monitoring
KW  - self-propagating worm attacks
KW  - tamper-resistant log collection
KW  - virtualization-based implementation
KW  - (viruses
KW  - Security and Protection
KW  - Servers
KW  - Trojan horses)
KW  - worms
VL  - 19
JA  - Parallel and Distributed Systems, IEEE Transactions on
DO  - 10.1109/TPDS.2007.70765
AB  - To detect and investigate self-propagating worm attacks against networked servers, the following capabilities are desirable: 1) raising timely alerts to trigger a worm investigation, 2) determining the break-in point of a worm, i.e., the vulnerable service from which the worm infiltrates the victim, and 3) identifying all contaminations inflicted by the worm during its residence in the victim. In this paper, we argue that the worm break-in provenance information has not been exploited in achieving these capabilities and thus propose process coloring, a new approach that preserves worm break-in provenance information and propagates it along operating- system-level information flows. More specifically, process coloring assigns a "color," a unique systemwide identifier, to each remotely accessible server process. The color will be either inherited by spawned child processes or diffused transitively through process actions. Process coloring achieves three new capabilities: color-based worm warning generation, break-in point identification, and log file partitioning. The virtualization-based implementation enables more tamper-resistant log collection, storage, and real-time monitoring. Beyond the overhead introduced by virtualization, process coloring only incurs very small additional system overhead. Experiments with real-world worms demonstrate the advantages of processing coloring over non-provenance-preserving tools.
ER  - 

TY  - CONF
JO  - Information Networking (ICOIN), 2014 International Conference on
TI  - Web attack detection using entropy-based analysis
T2  - Information Networking (ICOIN), 2014 International Conference on
IS  - 
SN  - 
VO  - 
SP  - 244
EP  - 247
AU  - Threepak, T.
AU  - Watcharapupong, A.
Y1  - 10-12 Feb. 2014
PY  - 2014
KW  - entropy
KW  - security of data
KW  - AVG-SD
KW  - Shannon entropy analysis
KW  - Web access logging text
KW  - Web attack detection
KW  - Web attacking scripts
KW  - Web request anomaly behaviors
KW  - entropy-based analysis
KW  - intrusive events
KW  - standard deviation
KW  - Complexity theory
KW  - Entropy
KW  - Equations
KW  - Intrusion detection
KW  - Mathematical model
KW  - Standards
KW  - Anomaly Detection
KW  - Entropy Analysis
KW  - Information Security
VL  - 
JA  - Information Networking (ICOIN), 2014 International Conference on
DO  - 10.1109/ICOIN.2014.6799699
AB  - Web attacks are increases both magnitude and complexity. In this paper, we try to use the Shannon entropy analysis to detect these attacks. Our approach examines web access logging text using the principle that web attacking scripts usually have more sophisticated request patterns than legitimate ones. Risk level of attacking incidents are indicated by the average (AVG) and standard deviation (SD) of each entropy period, i.e., Alpha and Beta lines which are equal to AVG-SD and AVG-2*SD, respectively. They represent boundaries in detection scheme. As the result, our technique is not only used as high accurate procedure to investigate web request anomaly behaviors, but also useful to prune huge application access log files and focus on potential intrusive events. The experiments show that our proposed process can detect anomaly requests in web application system with proper effectiveness and low false alarm rate.
ER  - 

TY  - JOUR
JO  - Learning Technologies, IEEE Transactions on
TI  - Disengagement Detection in Online Learning: Validation Studies and Perspectives
T2  - Learning Technologies, IEEE Transactions on
IS  - 2
SN  - 1939-1382
VO  - 4
SP  - 114
EP  - 124
AU  - Cocea, M.
AU  - Weibelzahl, S.
Y1  - April-June 2011
PY  - 2011
KW  - Internet
KW  - hypermedia markup languages
KW  - intelligent tutoring systems
KW  - HTML-Tutor
KW  - Web-based learning environment
KW  - disengagement detection
KW  - e-learning systems
KW  - effective learning
KW  - efficacious instruction
KW  - intelligent tutoring systems
KW  - log-file analysis
KW  - online learning
KW  - Decision support systems
KW  - disengagement prediction
KW  - e-Learning
KW  - educational data mining
KW  - log-file analysis.
VL  - 4
JA  - Learning Technologies, IEEE Transactions on
DO  - 10.1109/TLT.2010.14
AB  - Learning environments aim to deliver efficacious instruction, but rarely take into consideration the motivational factors involved in the learning process. However, motivational aspects like engagement play an important role in effective learning-engaged learners gain more. E-Learning systems could be improved by tracking students' disengagement that, in turn, would allow personalized interventions at appropriate times in order to reengage students. This idea has been exploited several times for Intelligent Tutoring Systems, but not yet in other types of learning environments that are less structured. To address this gap, our research looks at online learning-content-delivery systems using educational data mining techniques. Previously, several attributes relevant for disengagement prediction were identified by means of log-file analysis on HTML-Tutor, a web-based learning environment. In this paper, we investigate the extendibility of our approach to other systems by studying the relevance of these attributes for predicting disengagement in a different e-learning system. To this end, two validation studies were conducted indicating that the previously identified attributes are pertinent for disengagement prediction, and two new meta-attributes derived from log-data observations improve prediction and may potentially be used for automatic log-file annotation.
ER  - 

TY  - CONF
JO  - Advance Computing Conference (IACC), 2015 IEEE International
TI  - A novel disengagement detection strategy for online learning using quasi framework
T2  - Advance Computing Conference (IACC), 2015 IEEE International
IS  - 
SN  - 
VO  - 
SP  - 634
EP  - 638
AU  - Sundar, P.V.P.
AU  - Senthil Kumar, A.V.
Y1  - 12-13 June 2015
PY  - 2015
KW  - Internet
KW  - computer aided instruction
KW  - Internet
KW  - disengagement detection strategy
KW  - log file analysis
KW  - online learning
KW  - quasiframework
KW  - Benchmark testing
KW  - Indexes
KW  - Proposals
KW  - Reliability
KW  - Disengagement Detection
KW  - EDM
KW  - Log File Analysis
KW  - Online Learning
KW  - Quasi
VL  - 
JA  - Advance Computing Conference (IACC), 2015 IEEE International
DO  - 10.1109/IADCC.2015.7154784
AB  - The online learning gains more popularity in recent days; its key success is delivering content over internet and can be accessed by students from anywhere and anytime. In general, attraction is the quality of arousing interest. Similarly, motivation is the other hand to support for learning. Since, the online learning has less control over students compared to the conventional teaching method. Therefore engagement of student gets more importance on online learning. Most of the learning systems stores learners activities in log files and their profile related informations in database. Usually log file analysis alone could not have enough data to find out disengagement. Thus we integrate the log file information with database and develop a novel disengagement detection strategy using quasi framework. This study result reveals that quasi framework is effective in term of quality compared to previous proposals.
ER  - 

TY  - CONF
JO  - Systems, Man, and Cybernetics (SMC), 2011 IEEE International Conference on
TI  - Premature silent workflow termination in publicly posted composite documents
T2  - Systems, Man, and Cybernetics (SMC), 2011 IEEE International Conference on
IS  - 
SN  - 1062-922X
VO  - 
SP  - 1292
EP  - 1297
AU  - Balinsky, H.
AU  - Liqun Chen
AU  - Simske, S.
Y1  - 9-12 Oct. 2011
PY  - 2011
KW  - auditing
KW  - business data processing
KW  - document handling
KW  - security of data
KW  - workflow management software
KW  - audit information
KW  - cross-organizational workflows
KW  - early detection mechanism
KW  - mandatory document logging
KW  - premature silent workflow termination
KW  - publicly posted composite documents
KW  - Access control
KW  - Cryptography
KW  - Document delivery
KW  - Handwriting recognition
KW  - Marine vehicles
KW  - Receivers
KW  - Silicon
KW  - audit
KW  - composite documents
KW  - embedded access control
KW  - premature silent workflow termination
KW  - workflow
KW  - workflow recovery procedure
VL  - 
JA  - Systems, Man, and Cybernetics (SMC), 2011 IEEE International Conference on
DO  - 10.1109/ICSMC.2011.6083838
AB  - Publicly Posted Composite Documents (PPCDs) address the problem of composite documents with different formats and varied sensitivity participating in cross-organizational workflows distributed over potentially non-secure channels. An early version of the PPCD, however, is susceptible to silent workflow termination, wherein a document shipped by one workflow participant is never received by the subsequent participant, causing the workflow to terminate without notification. In the current paper, we extend the PPCD solution to provide an early detection mechanism for a potential workflow termination, to provide workflow recovery procedures, and to provide for the often mandatory document logging and audit information.
ER  - 

TY  - CONF
JO  - Advanced Information Networking and Applications, 2009. AINA '09. International Conference on
TI  - Masquerade Detection Using Command Prediction and Association Rules Mining
T2  - Advanced Information Networking and Applications, 2009. AINA '09. International Conference on
IS  - 
SN  - 1550-445X
VO  - 
SP  - 552
EP  - 559
AU  - Han-Ching Wu
AU  - Huang, S.-H.S.
Y1  - 26-29 May 2009
PY  - 2009
KW  - data mining
KW  - data privacy
KW  - security of data
KW  - association rules mining
KW  - computer systems
KW  - data privacy
KW  - intrusion detection approaches
KW  - masquerade detection
KW  - Accuracy
KW  - Application software
KW  - Association rules
KW  - Computer networks
KW  - Computer science
KW  - Computer security
KW  - Data mining
KW  - Electronic mail
KW  - Frequency
KW  - Intrusion detection
KW  - Association Rule Mining
KW  - Intrusion Detection
KW  - Masqueraders
KW  - Network Security
VL  - 
JA  - Advanced Information Networking and Applications, 2009. AINA '09. International Conference on
DO  - 10.1109/AINA.2009.38
AB  - Masqueraders commonly impersonate legitimate userpsilas account to gain access to computer systems that they are not authorized to enter. Normally users exhibit some regularity in their behavior such as command usage. We propose a new approach to mine user command associations. Since each user may have different usage behavior, using the built behavior pattern to predict a masqueraderpsilas next command will result in low success rate. We devise an algorithm to identify masqueraders by evaluating the accuracy of the predictions. Furthermore our detection method can be used in real-time without having to wait for a log of a large number of commands. Experimental results show that the association rules mining performs very well in detecting masqueraders.
ER  - 

TY  - CONF
JO  - Advance Computing Conference, 2009. IACC 2009. IEEE International
TI  - A Web Intrusion Detection Mechanism based on Feature based Data Clustering
T2  - Advance Computing Conference, 2009. IACC 2009. IEEE International
IS  - 
SN  - 
VO  - 
SP  - 1124
EP  - 1129
AU  - Das, D.
AU  - Sharma, U.
AU  - Bhattacharyya, D.K.
Y1  - 6-7 March 2009
PY  - 2009
KW  - Internet
KW  - SQL
KW  - Web sites
KW  - hypermedia markup languages
KW  - pattern clustering
KW  - security of data
KW  - 'packet arrival factor'
KW  - HTML
KW  - Internet services
KW  - KDD CUP99
KW  - PHP
KW  - SQL
KW  - WWW site
KW  - Web based attacks
KW  - Web intrusion detection mechanism
KW  - Web layer log file
KW  - attack rules
KW  - data clustering
KW  - data retrieving
KW  - e-business operation
KW  - hamming edit distance
KW  - misuse detection
KW  - query based projected clustering
KW  - script languages
KW  - unauthorized access
KW  - Databases
KW  - Feature extraction
KW  - HTML
KW  - Information retrieval
KW  - Intrusion detection
KW  - Phase detection
KW  - Testing
KW  - Web and internet services
KW  - Web server
KW  - World Wide Web
KW  - Web intrusion
KW  - attack labeling
KW  - query based projected clustering
KW  - sql injection
KW  - web layer log matching
VL  - 
JA  - Advance Computing Conference, 2009. IACC 2009. IEEE International
DO  - 10.1109/IADCC.2009.4809172
AB  - Web is one of the most popular internet services in today's world. In today's world, web servers and web based applications are the popular corporate applications and become the targets of the attackers. A Large number of Web applications, especially those deployed for companies to e-business operation involve high reliability, efficiency and confidentiality. Such applications are written in script languages like PHP embedded in HTML allowing establish the connection to databases, retrieving data and putting them in WWW site. In order to detect known attacks, misuse detection of web based attacks consists of attack rules and descriptions. As misuse detection considers predefined signatures for intrusion detection, here we have proposed two phases of intrusion detection mechanism. In the first phase we have used web host based intrusion detection with matching mechanism using 'Hamming Edit Distance'. We have considered here. the web layer log file for matching. This phase has been tested with our university intranet web server's log file. We have tested successfully the SQL injection for unauthorized access. We proposed a 'Query based projected clustering' for unsupervised anomaly detection and also a 'packet arrival factor' for intrusion detection in the second phase. We tested the scheme in this phase using KDD CUP99. In this phase while testing our scheme, we have extracted the feature dataset with protocol 'tcp' and services 'http'. Both the phases of our scheme found working successfully and an evaluated threshold has been proposed for better result.
ER  - 

TY  - CONF
JO  - Communication Networks and Services Research Conference, 2005. Proceedings of the 3rd Annual
TI  - Generating representative traffic for intrusion detection system benchmarking
T2  - Communication Networks and Services Research Conference, 2005. Proceedings of the 3rd Annual
IS  - 
SN  - 
VO  - 
SP  - 112
EP  - 117
AU  - Kayacik, H.G.
AU  - Zincir-Heywood, N.
Y1  - 16-18 May 2005
PY  - 2005
KW  - Internet
KW  - benchmark testing
KW  - file servers
KW  - online front-ends
KW  - security of data
KW  - telecommunication security
KW  - telecommunication traffic
KW  - Web browser
KW  - Web server
KW  - benchmarking
KW  - data driven fashion
KW  - intrusion detection system testing
KW  - system traffic
KW  - Delay
KW  - Exponential distribution
KW  - Intrusion detection
KW  - Probability distribution
KW  - Telecommunication traffic
KW  - Testing
KW  - Training data
KW  - Uniform resource locators
KW  - Web pages
KW  - Web server
KW  - Intrusion Detection
KW  - Markov Models
KW  - Network Security
KW  - Self-Organizing Maps
KW  - Traffic Modeling
VL  - 
JA  - Communication Networks and Services Research Conference, 2005. Proceedings of the 3rd Annual
DO  - 10.1109/CNSR.2005.35
AB  - In this paper, a modeling and simulation framework is proposed for generating data for training and testing intrusion detection systems. The framework can develop models of Web usage from Web server logs in a data driven fashion and the actual traffic is generated by employing the Web browser installed on the host. Additionally, we employed an intrusion detection system as a traffic analyzer to validate the synthetic data that the framework generated and compared it against the standard intrusion detection system benchmark data, namely KDD 99 datasets.
ER  - 

TY  - JOUR
JO  - Industrial Informatics, IEEE Transactions on
TI  - Classification of Disturbances and Cyber-Attacks in Power Systems Using Heterogeneous Time-Synchronized Data
T2  - Industrial Informatics, IEEE Transactions on
IS  - 3
SN  - 1551-3203
VO  - 11
SP  - 650
EP  - 662
AU  - Shengyi Pan
AU  - Morris, T.
AU  - Adhikari, U.
Y1  - June 2015
PY  - 2015
KW  - data mining
KW  - pattern classification
KW  - phasor measurement
KW  - power engineering computing
KW  - power system security
KW  - security of data
KW  - sensor fusion
KW  - classification
KW  - common path
KW  - critical system states
KW  - heterogeneous synchrophasor data
KW  - heterogeneous time-synchronized data
KW  - network event monitor logs
KW  - path-mining algorithm
KW  - pattern extraction
KW  - power system cyber-attacks
KW  - power system disturbances
KW  - relay logs
KW  - sequential pattern mining approach
KW  - synchrophasor measurements
KW  - system logs
KW  - Classification algorithms
KW  - Data mining
KW  - Monitoring
KW  - Power systems
KW  - Relays
KW  - Sensors
KW  - Transmission line measurements
KW  - Common Paths
KW  - Common paths
KW  - Cyber-attack Detection
KW  - Disturbances
KW  - Symmetric and Unsymmetrical Faults
KW  - Synchrophasor Data and Device Log Mining
KW  - cyber-attack detection
KW  - disturbances
KW  - symmetric and unsymmetrical faults
KW  - synchrophasor data and device log mining
VL  - 11
JA  - Industrial Informatics, IEEE Transactions on
DO  - 10.1109/TII.2015.2420951
AB  - Visualization and situational awareness are of vital importance for power systems, as the earlier a power-system event such as a transmission line fault or cyber-attack is identified, the quicker operators can react to avoid unnecessary loss. Accurate time-synchronized data, such as system measurements and device status, provide benefits for system state monitoring. However, the time-domain analysis of such heterogeneous data to extract patterns is difficult due to the existence of transient phenomena in the analyzed measurement waveforms. This paper proposes a sequential pattern mining approach to accurately extract patterns of power-system disturbances and cyber-attacks from heterogeneous time-synchronized data, including synchrophasor measurements, relay logs, and network event monitor logs. The term common path is introduced. A common path is a sequence of critical system states in temporal order that represent individual types of disturbances and cyber-attacks. Common paths are unique signatures for each observed event type. They can be compared to observed system states for classification. In this paper, the process of automatically discovering common paths from labeled data logs is introduced. An included case study uses the common path-mining algorithm to learn common paths from a fusion of heterogeneous synchrophasor data and system logs for three types of disturbances (in terms of faults) and three types of cyber-attacks, which are similar to or mimic faults. The case study demonstrates the algorithm's effectiveness at identifying unique paths for each type of event and the accompanying classifier's ability to accurately discern each type of event.
ER  - 

TY  - CONF
JO  - Systematic Approaches to Digital Forensic Engineering, 2009. SADFE '09. Fourth International IEEE Workshop on
TI  - Analysing Access Control Specifications
T2  - Systematic Approaches to Digital Forensic Engineering, 2009. SADFE '09. Fourth International IEEE Workshop on
IS  - 
SN  - 
VO  - 
SP  - 22
EP  - 33
AU  - Probst, C.W.
AU  - Hansen, R.R.
Y1  - 21-21 May 2009
PY  - 2009
KW  - authorisation
KW  - computer crime
KW  - access control configuration
KW  - access control specification analysis
KW  - cyber crime analysis
KW  - log file analysis
KW  - nontechnical application
KW  - online analysis
KW  - prosecuting crime
KW  - surveillance system
KW  - Access control
KW  - Companies
KW  - Conferences
KW  - Control system analysis
KW  - Control systems
KW  - Digital forensics
KW  - Information systems
KW  - Measurement standards
KW  - Network servers
KW  - Surveillance
KW  - access control
KW  - log file analysis
VL  - 
JA  - Systematic Approaches to Digital Forensic Engineering, 2009. SADFE '09. Fourth International IEEE Workshop on
DO  - 10.1109/SADFE.2009.13
AB  - When prosecuting crimes, the main question to answer is often who had a motive and the possibility to commit the crime. When investigating cyber crimes, the question of possibility is often hard to answer, as in a networked system almost any location can be accessed from almost anywhere. The most common tool to answer this question, analysis of log files, faces the problem that the amount of logged data may be overwhelming. This problems gets even worse in the case of insider attacks, where the attacker's actions usually will be logged as permissible, standard actions-if they are logged at all. Recent events have revealed intimate knowledge of surveillance and control systems on the side of the attacker, making it often impossible to deduce the identity of an inside attacker from logged data. In this work we present an approach that analyses the access control configuration to identify the set of credentials needed to reach a certain location in a system. This knowledge allows to identify a set of (inside) actors who have the possibility to commit an insider attack at that location. This has immediate applications in analysing log files, but also non-technical applications such as identifying possible suspects, or, beyond cyber crimes, picking the "best" actor for a certain task. We also sketch an online analysis that identifies where an actor can be located based on observed actions.
ER  - 

TY  - CONF
JO  - Software Security and Reliability-Companion (SERE-C), 2014 IEEE Eighth International Conference on
TI  - On Coverage-Based Attack Profiles
T2  - Software Security and Reliability-Companion (SERE-C), 2014 IEEE Eighth International Conference on
IS  - 
SN  - 
VO  - 
SP  - 5
EP  - 6
AU  - Rivers, A.T.
AU  - Vouk, M.A.
AU  - Williams, L.A.
Y1  - June 30 2014-July 2 2014
PY  - 2014
KW  - Internet
KW  - security of data
KW  - Web request signatures
KW  - attack patterns
KW  - coverage-based attack profiles
KW  - cyber attacks
KW  - hypergeometric process model
KW  - production Web server
KW  - zero-day exploits
KW  - Computational modeling
KW  - Equations
KW  - IP networks
KW  - Mathematical model
KW  - Software
KW  - Software reliability
KW  - Testing
KW  - attack
KW  - coverage
KW  - models
KW  - profile
KW  - security
VL  - 
JA  - Software Security and Reliability-Companion (SERE-C), 2014 IEEE Eighth International Conference on
DO  - 10.1109/SERE-C.2014.15
AB  - Automated cyber attacks tend to be schedule and resource limited. The primary progress metric is often "coverage" of pre-determined "known" vulnerabilities that may not have been patched, along with possible zero-day exploits (if such exist). We present and discuss a hypergeometric process model that describes such attack patterns. We used web request signatures from the logs of a production web server to assess the applicability of the model.
ER  - 

TY  - CONF
JO  - Trust, Security and Privacy in Computing and Communications (TrustCom), 2013 12th IEEE International Conference on
TI  - A Comprehensive Approach to Abusing Locality in Shared Web Hosting Servers
T2  - Trust, Security and Privacy in Computing and Communications (TrustCom), 2013 12th IEEE International Conference on
IS  - 
SN  - 
VO  - 
SP  - 1620
EP  - 1625
AU  - Mirheidari, S.A.
AU  - Arshad, S.
AU  - Khoshkdahan, S.
AU  - Jalili, R.
Y1  - 16-18 July 2013
PY  - 2013
KW  - Internet
KW  - Web sites
KW  - security of data
KW  - Web site management
KW  - comprehensive secure configuration
KW  - network technology
KW  - securing mechanisms
KW  - server maintenance
KW  - shared Web hosting servers
KW  - social interaction
KW  - Authentication
KW  - Force
KW  - Linux
KW  - Operating systems
KW  - Process control
KW  - Servers
KW  - CSRF Token Poisoning
KW  - Convenient Phishing
KW  - Data Confidentiality Violation
KW  - Data Integrity Violation
KW  - Fast Brute Force
KW  - Intensive LFI
KW  - Log Poisoning
KW  - Log Snooping
KW  - Session Poisoning
KW  - Session Snooping
KW  - Shared Web Hosting
VL  - 
JA  - Trust, Security and Privacy in Computing and Communications (TrustCom), 2013 12th IEEE International Conference on
DO  - 10.1109/TrustCom.2013.200
AB  - With the growing of network technology along with the need of human for social interaction, using websites nowadays becomes critically important which leads in the increasing number of websites and servers. One popular solution for managing these large numbers of websites is using shared web hosting servers in order to decrease the overall cost of server maintenance. Despite affordability, this solution is insecure and risky according to high amount of reported defaces and attacks during recent years. In this paper, we introduce top ten most common attacks in shared web hosting servers which can occur because of the nature and bad configuration in these servers. Moreover, we present several simple scenarios that are capable of penetrating these kinds of servers even with the existence of several securing mechanisms. Finally, we provide a comprehensive secure configuration for confronting these attacks.
ER  - 

TY  - CONF
JO  - Information and Communications Technologies (IETICT 2013), IET International Conference on
TI  - Towards practical intrusion tolerant systems
T2  - Information and Communications Technologies (IETICT 2013), IET International Conference on
IS  - 
SN  - 
VO  - 
SP  - 280
EP  - 287
AU  - Wenbing Zhao
Y1  - 27-29 April 2013
PY  - 2013
KW  - fault tolerance
KW  - security of data
KW  - acceptance testing
KW  - append-only logging policy
KW  - building mission-critical systems
KW  - intrusion attack
KW  - intrusion tolerant systems
KW  - state management
KW  - Acceptance Test
KW  - Fault and Intrusion Tolerance
KW  - Logging and Checkpointing
KW  - Replication
VL  - 
JA  - Information and Communications Technologies (IETICT 2013), IET International Conference on
DO  - 10.1049/cp.2013.0063
AB  - In this paper, we propose a new approach for building mission-critical systems with an emphasis on intrusion tolerance. The fundamental mechanisms employed in this approach includes: (1) The separation of execution and state management, which enables the use of a single process to manage application requests, thereby reducing runtime overhead and enables highly concurrent executions. (2) The append-only logging policy, which protects the state of the system against an intrusion attack and ensures a clean state for the system to fallback on during recovery. (3) The use of acceptance testing as a way of verifying the integrity of the execution of application requests.
ER  - 

TY  - CONF
JO  - Internet Security (WorldCIS), 2012 World Congress on
TI  - Smart crawlers for flash-crowd DDoS: The attacker's perspective
T2  - Internet Security (WorldCIS), 2012 World Congress on
IS  - 
SN  - 
VO  - 
SP  - 37
EP  - 44
AU  - Drinfeld, D.
AU  - Vlajic, N.
Y1  - 10-12 June 2012
PY  - 2012
KW  - Web sites
KW  - security of data
KW  - software agents
KW  - Web site
KW  - attacking bots
KW  - bot customization
KW  - flash-crowd DDoS attack
KW  - smart crawler
KW  - Computer crime
KW  - Estimation
KW  - Google
KW  - Humans
KW  - Internet
KW  - Servers
KW  - Web pages
KW  - bot design
KW  - flash crowd attack
KW  - layer-7 DDoS
KW  - web page popularity estimation
KW  - web-log analysis
VL  - 
JA  - Internet Security (WorldCIS), 2012 World Congress on
DO  - 
AB  - Flash-crowd DDoS attacks - in which the attacking bots aim to appear indistinguishable from the regular visitors to the victim web-site - have only recently been identified in the literature. While generally seen as the most advanced and most potent type of DDoS, flash crowd attacks are only partially understood, and their practical viability is still very much unclear. To the best of our knowledge, this is the first study that takes the perspective of a potential attacker interested in executing a flash crowd DDoS, and looks at the challenges of designing a botnet that would carry out that execution effectively. The results of our study demonstrate that, through the use of some popular readily available Internet tools, the attacker is likely to succeed in harvesting critical information about any perspective victim site, and thus be in the position to customize his bots (i.e., make them behave very close to how a typical human visitor to the given site would behave). Clearly, better bot customization would imply more powerful and harder-to-defend-against DDoS attacks.
ER  - 

TY  - CONF
JO  - Telecommunications and Multimedia (TEMU), 2014 International Conference on
TI  - Future proof analytics techniques for web 2.0 applications
T2  - Telecommunications and Multimedia (TEMU), 2014 International Conference on
IS  - 
SN  - 
VO  - 
SP  - 214
EP  - 219
AU  - Aivalis, C.J.
AU  - Boucouvalas, A.C.
Y1  - 28-30 July 2014
PY  - 2014
KW  - Internet
KW  - electronic commerce
KW  - file servers
KW  - ALF
KW  - RIAs
KW  - Web 2.0 applications
KW  - Web analytics applications
KW  - Web server
KW  - access log file
KW  - behavioral measuring mechanisms
KW  - business perspective
KW  - contemporary Web analytics application
KW  - database
KW  - e-commerce
KW  - extraction techniques
KW  - future proof analytics techniques
KW  - near real time support techniques
KW  - rich Internet applications
KW  - user interface
KW  - Business
KW  - Data mining
KW  - Databases
KW  - Measurement
KW  - Real-time systems
KW  - Web servers
KW  - E-Commerce
KW  - Log File Analysis
KW  - Real Time
KW  - Tagging Systems
KW  - Web Analytics
VL  - 
JA  - Telecommunications and Multimedia (TEMU), 2014 International Conference on
DO  - 10.1109/TEMU.2014.6917763
AB  - This paper describes the components that constitute a contemporary web analytics application and discusses the impact and the implications of the technology paradigm shift towards rich Internet applications (RIAs) on web analytics applications in Web 2.0. The performance and behavioral measuring mechanisms collect data from various source points. These sources depend heavily on the implementation characteristics of each web application. For example, while the amount of detail data registered in the access log file (ALF) by the web server is sufficient for any non RIA application, it is heavily reduced for any RIA, since the model of interaction between client and server is heavily shifted towards the client [1]. In RIAs most processing and preparation of data is performed locally, at the client site by the browser, so the client handles all key clicks, as well as the main interaction with the user interface of the application. Communication with the server mainly covers transfers and requests of data to and from the database of the web application. The two basic technologies used for analytics, tagging and log file analysis are described and discussed. A hybrid solution that addresses the issue of the reduced ALF is proposed. Additionally extraction techniques for specific crucial data needed in order to scrutinize the performance operationally, as well as from the business perspective of the web application enriches the analyzer's database and provides measurements and data using terms and descriptions familiar to the operational personnel.
ER  - 

TY  - CONF
JO  - Self-Adaptation and Self-Organizing Systems Workshops (SASOW), 2013 IEEE 7th International Conference on
TI  - Ants, To-Go: A Portable Demonstration of Large Infrastructure Cyber Defense
T2  - Self-Adaptation and Self-Organizing Systems Workshops (SASOW), 2013 IEEE 7th International Conference on
IS  - 
SN  - 
VO  - 
SP  - 11
EP  - 12
AU  - Fink, G.A.
AU  - Fligg, A.K.
AU  - Haack, J.N.
Y1  - 9-13 Sept. 2013
PY  - 2013
KW  - data visualisation
KW  - multi-agent systems
KW  - security of data
KW  - virtual machines
KW  - ABCD
KW  - Ants-to-Go
KW  - PNNL
KW  - Pacific Northwest National Laboratory
KW  - agents
KW  - ant-based cyber defense
KW  - cyber infrastructure defense
KW  - decentralized adaptive systems
KW  - distributed logs
KW  - large-scale visualization
KW  - physical machines
KW  - self-organizing systems
KW  - virtual machines
KW  - Conferences
KW  - Laboratories
KW  - Malware
KW  - Operating systems
KW  - Smart grids
KW  - Virtual machining
KW  - Visualization
KW  - Ant-based
KW  - computer security
KW  - cyber defense
VL  - 
JA  - Self-Adaptation and Self-Organizing Systems Workshops (SASOW), 2013 IEEE 7th International Conference on
DO  - 10.1109/SASOW.2013.20
AB  - Creating a self-organizing system of agents to defend large cyber infrastructures presents many challenges, one of which is demonstrating the system without trying to host it on a large real net-work of tens of thousands of machines. This abstract describes a portable demonstration of PNNL's Ant-Based Cyber Defense (ABCD) that can run on one or a few physical machines with sufficient resources. We have chosen to run the framework on hundreds of virtual machines whose number is limited only by the available memory and processing power. We collect the distributed logs and visualize the results on a large-scale visualization created to represent up to a million nodes. Our approach should be useful for other decentralized adaptive and self-organizing systems that span large numbers of physical machines.
ER  - 

TY  - CONF
JO  - Interactive Mobile and Computer Aided Learning (IMCL), 2012 International Conference on
TI  - A coding style-based plagiarism detection
T2  - Interactive Mobile and Computer Aided Learning (IMCL), 2012 International Conference on
IS  - 
SN  - 
VO  - 
SP  - 180
EP  - 186
AU  - Arabyarmohamady, S.
AU  - Moradi, H.
AU  - Asadpour, M.
Y1  - 6-8 Nov. 2012
PY  - 2012
KW  - Internet
KW  - computer science education
KW  - document handling
KW  - program verification
KW  - security of data
KW  - Internet
KW  - code development
KW  - coding style extraction
KW  - coding style-based plagiarism detection
KW  - feature extraction
KW  - plagiarized code detection
KW  - programming code
KW  - style level checker module
KW  - Conferences
KW  - Encoding
KW  - Feature extraction
KW  - Internet
KW  - Plagiarism
KW  - Vectors
KW  - Watermarking
KW  - Plagiarism detection
KW  - author identification
KW  - software forensics
KW  - source code
VL  - 
JA  - Interactive Mobile and Computer Aided Learning (IMCL), 2012 International Conference on
DO  - 10.1109/IMCL.2012.6396471
AB  - In this paper a plagiarism detection framework is proposed based on coding style. Furthermore, the typical style-based approach is improved to better detect plagiarism in programming codes. The plagiarism detection is performed in two phases: in the first phase the main features representing a coding style are extracted. In the second phase the extracted features are used in three different modules to detect the plagiarized codes and to determine the giver and takers of the codes. The extracted features for each code developer are kept in a history log, i.e. a user profile as his/her style of coding, and would be used to determine the change in coding style. The user profile allows the system to detect if a code is truly developed by the claimed developer or it is written by another person, having another style. Furthermore, the user profile allows determining the code giver and code taker when two codes are similar by comparing the codes' styles with the style of the programmers. Also if a code is copied from the internet or developed by a third party, then the style of who claims the ownership of the code is normally less proficient in coding than the third party and can be detected. The difference between the style levels is done through the style level checker module in the proposed framework. The proposed framework has been implemented and tested and the results are compared to Moss which shows comparable performance in detecting plagiarized codes.
ER  - 

TY  - CONF
JO  - Systems, Man and Cybernetics, 2005 IEEE International Conference on
TI  - Understanding the performance of enterprise applications
T2  - Systems, Man and Cybernetics, 2005 IEEE International Conference on
IS  - 
SN  - 
VO  - 3
SP  - 2825
EP  - 2829 Vol. 3
AU  - Li, Q.
AU  - Bauer, M.
Y1  - 10-12 Oct. 2005
PY  - 2005
KW  - Internet
KW  - business data processing
KW  - file servers
KW  - commercial enterprise application
KW  - log file analysis
KW  - performance analysis
KW  - Application software
KW  - Bandwidth
KW  - Companies
KW  - Computer science
KW  - Databases
KW  - Humans
KW  - Information analysis
KW  - Network servers
KW  - Performance analysis
KW  - Web server
KW  - Enterprise applications
KW  - log file analysis
KW  - performance analysis
KW  - visualization
VL  - 3
JA  - Systems, Man and Cybernetics, 2005 IEEE International Conference on
DO  - 10.1109/ICSMC.2005.1571578
AB  - Companies have gradually abandoned internally developed enterprise applications in favor of commercial ones. These systems are often comprised of different, multi-vendor components, including, Web servers, databases and applications. For most organizations, performance problems are often addressed by acquiring larger servers or increasing network bandwidth. While the performance of an individual component or application can be and is analyzed, the overall performance of the integrated suite of applications remains largely ad hoc and determining the cause of problems becomes trial and error. We report on initial work to address these problems by studying a commercial enterprise application, understanding what information is important, how such information can be gathered and analyzed and what kinds of tools would be beneficial. We report on an analysis tool and some results of using that tool and on the potential of the approach and methodology for larger enterprise environments.
ER  - 

TY  - CONF
JO  - Cloud Computing and Big Data (CloudCom-Asia), 2013 International Conference on
TI  - A Survey on Cloud Accountability
T2  - Cloud Computing and Big Data (CloudCom-Asia), 2013 International Conference on
IS  - 
SN  - 
VO  - 
SP  - 627
EP  - 632
AU  - Xianghan Zheng
AU  - Hui Ye
AU  - Chunming Tang
AU  - Chunming Rong
AU  - Guolong Chen
Y1  - 16-19 Dec. 2013
PY  - 2013
KW  - cloud computing
KW  - security of data
KW  - cloud accountability
KW  - cloud accountability framework
KW  - cloud computing technology
KW  - cloud data
KW  - cloud service company
KW  - cloud service providers
KW  - data confidentiality
KW  - remote machines
KW  - user privacy
KW  - Access control
KW  - Authentication
KW  - Cloud computing
KW  - Cryptography
KW  - Educational institutions
KW  - Monitoring
KW  - accountability
KW  - cloud computing
KW  - data sharing
KW  - logging
VL  - 
JA  - Cloud Computing and Big Data (CloudCom-Asia), 2013 International Conference on
DO  - 10.1109/CLOUDCOM-ASIA.2013.29
AB  - Recent advances in cloud computing technology now allow users to access data wherever they are. However, with these advances come some costs: cloud data is often stored and manipulated by remote machines that users have no access to. Moreover, cloud service providers often claim that their data is safe and secure, but users rarely have ways to make sure. Based on recent academic and industrial research, this paper surveys various approaches cloud service companies can make to ensure proper accountability. The paper focuses on issues such as cloud accountability framework, data confidentiality, and user privacy. More importantly, we also put forth some efficiency advice.
ER  - 

TY  - CONF
JO  - Networking, Architecture and Storage (NAS), 2011 6th IEEE International Conference on
TI  - Scalable Index Update for Block-Level Continuous Data Protection
T2  - Networking, Architecture and Storage (NAS), 2011 6th IEEE International Conference on
IS  - 
SN  - 
VO  - 
SP  - 372
EP  - 381
AU  - Maohua Lu
AU  - Simha, D.
AU  - Tzi-cker Chiueh
Y1  - 28-30 July 2011
PY  - 2011
KW  - network servers
KW  - security of data
KW  - storage area networks
KW  - block level continuous data protection system
KW  - disk block update operation
KW  - disk update
KW  - flexible recovery time objective
KW  - high performance index update mechanism
KW  - index structures
KW  - large data backup application
KW  - multiple index structure
KW  - multiple index update operation
KW  - network storage server
KW  - performance measurement
KW  - recovery point objective
KW  - scalable index update
KW  - sequential disk I/O
KW  - Delay
KW  - Home appliances
KW  - Indexes
KW  - Servers
KW  - Storage area networks
KW  - Throughput
KW  - I/O batching
KW  - continuous data protection
KW  - high-performance logging
KW  - random index updates
VL  - 
JA  - Networking, Architecture and Storage (NAS), 2011 6th IEEE International Conference on
DO  - 10.1109/NAS.2011.54
AB  - A block-level continuous data protection (CDP) system logs every disk update to a network storage server it protects, so as to support more flexible recovery time objective (RTO) and recovery point objective (RPO). To provide efficient access to historical snapshots, block-level CDP systems maintain multiple index structures, each of which needs to be updated whenever a disk block update operation is logged. Because these index structures are too large to be held in memory, updating their on-disk versions in real time becomes a major performance bottleneck that prevents existing CDP systems from scaling to large data backup applications. This paper describes the design and implementation of a high-performance index update mechanism that logs index updates, batches them in memory, and commits them using mostly sequential disk I/O. Sequential commit greatly reduces the cost of bringing in and writing back each on-disk index page. Update batching further amortizes this cost over multiple index update operations. Empirical performance measurements demonstrate that the proposed technique improves the index update throughput by more than an order of magnitude and reduces the performance overhead associated with index updates from 95% to under 15%.
ER  - 

TY  - CONF
JO  - Cluster, Cloud and Grid Computing (CCGrid), 2015 15th IEEE/ACM International Symposium on
TI  - Towards Provenance-Based Anomaly Detection in MapReduce
T2  - Cluster, Cloud and Grid Computing (CCGrid), 2015 15th IEEE/ACM International Symposium on
IS  - 
SN  - 
VO  - 
SP  - 647
EP  - 656
AU  - Cong Liao
AU  - Squicciarini, A.
Y1  - 4-7 May 2015
PY  - 2015
KW  - data analysis
KW  - parallel processing
KW  - security of data
KW  - Hadoop
KW  - MapReduce computation
KW  - computational provenance system
KW  - data tampering
KW  - provenance-based anomaly detection
KW  - Access control
KW  - Cloud computing
KW  - Containers
KW  - Distributed databases
KW  - Monitoring
KW  - Yarn
KW  - MapReduce
KW  - computation integrity
KW  - logging
KW  - provenance
VL  - 
JA  - Cluster, Cloud and Grid Computing (CCGrid), 2015 15th IEEE/ACM International Symposium on
DO  - 10.1109/CCGrid.2015.16
AB  - MapReduce enables parallel and distributed processing of vast amount of data on a cluster of machines. However, such computing paradigm is subject to threats posed by malicious and cheating nodes or compromised user submitted code that could tamper data and computation since users maintain little control as the computation is carried out in a distributed fashion. In this paper, we focus on the analysis and detection of anomalies during the process of MapReduce computation. Accordingly, we develop a computational provenance system that captures provenance data related to MapReduce computation within the MapReduce framework in Hadoop. In particular, we identify a set of invariants against aggregated provenance information, which are later analyzed to uncover anomalies indicating possible tampering of data and computation. We conduct a series of experiments to show the efficiency and effectiveness of our proposed provenance system.
ER  - 

TY  - CONF
JO  - Distributed Computing Systems (ICDCS), 2012 IEEE 32nd International Conference on
TI  - Tiresias: Online Anomaly Detection for Hierarchical Operational Network Data
T2  - Distributed Computing Systems (ICDCS), 2012 IEEE 32nd International Conference on
IS  - 
SN  - 1063-6927
VO  - 
SP  - 173
EP  - 182
AU  - Chi-Yao Hong
AU  - Caesar, M.
AU  - Duffield, N.
AU  - Jia Wang
Y1  - 18-21 June 2012
PY  - 2012
KW  - IP networks
KW  - security of data
KW  - IP network
KW  - ISP operator
KW  - Tier-1 ISP
KW  - Tiresias
KW  - anomalous event detection
KW  - anomalous event tracking
KW  - customer care call logs
KW  - data sparseness
KW  - data volatility
KW  - detection accuracy
KW  - equipment system logs
KW  - hierarchical operational data
KW  - hierarchical operational network data
KW  - hierarchical structure
KW  - high-impact aggregate
KW  - management data
KW  - manual inspection
KW  - network problem detection
KW  - online anomaly detection
KW  - online detection algorithm
KW  - space complexity
KW  - time complexity
KW  - Accuracy
KW  - Aggregates
KW  - Charge coupled devices
KW  - Computer crashes
KW  - Forecasting
KW  - Time series analysis
KW  - Vegetation
KW  - anomaly detection
KW  - log analysis
KW  - operational network data
KW  - time series analysis
VL  - 
JA  - Distributed Computing Systems (ICDCS), 2012 IEEE 32nd International Conference on
DO  - 10.1109/ICDCS.2012.30
AB  - Operational network data, management data such as customer care call logs and equipment system logs, is a very important source of information for network operators to detect problems in their networks. Unfortunately, there is lack of efficient tools to automatically track and detect anomalous events on operational data, causing ISP operators to rely on manual inspection of this data. While anomaly detection has been widely studied in the context of network data, operational data presents several new challenges, including the volatility and sparseness of data, and the need to perform fast detection (complicating application of schemes that require offline processing or large/stable data sets to converge). To address these challenges, we propose Tiresias, an automated approach to locating anomalous events on hierarchical operational data. Tiresias leverages the hierarchical structure of operational data to identify high-impact aggregates (e.g., locations in the network, failure modes) likely to be associated with anomalous events. To accommodate different kinds of operational network data, Tiresias consists of an online detection algorithm with low time and space complexity, while preserving high detection accuracy. We present results from two case studies using operational data collected at a large commercial IP network operated by a Tier-1 ISP: customer care call logs and set-top box crash logs. By comparing with a reference set verified by the ISP's operational group, we validate that Tiresias can achieve &gt;;94% accuracy in locating anomalies. Tiresias also discovered several previously unknown anomalies in the ISP's customer care cases, demonstrating its effectiveness.
ER  - 

TY  - CONF
JO  - Intelligent Systems Design and Applications (ISDA), 2013 13th International Conference on
TI  - Rough set theory approach for filtering spams from boundary messages in a chat system
T2  - Intelligent Systems Design and Applications (ISDA), 2013 13th International Conference on
IS  - 
SN  - 
VO  - 
SP  - 28
EP  - 34
AU  - Roy, S.S.
AU  - Charaborty, S.
AU  - Sourav, S.
AU  - Abraham, A.
Y1  - 8-10 Dec. 2013
PY  - 2013
KW  - Internet
KW  - electronic messaging
KW  - information filtering
KW  - rough set theory
KW  - security of data
KW  - unsolicited e-mail
KW  - Web logs
KW  - Web-services
KW  - boundary messages
KW  - chat system
KW  - e-mail
KW  - forums
KW  - refreshing spam discovery technology
KW  - rough set theory approach
KW  - spam filtering
KW  - unsolicited contents
KW  - Approximation methods
KW  - Arrays
KW  - Drugs
KW  - Unsolicited electronic mail
KW  - rough set
KW  - spam filtering
KW  - web logs
VL  - 
JA  - Intelligent Systems Design and Applications (ISDA), 2013 13th International Conference on
DO  - 10.1109/ISDA.2013.6920763
AB  - This paper purports a refreshing spam discovery technology for chat system based on rough set theory. Nowadays, spam is very much allied with a huge chunk of data transferred through internet involving all disturbing and unsolicited contents received via different web-services such as chat systems, e-mail, forums and web logs. In this paper, we have reviewed various past research works of filtering SPAM and propose a novel filtering technique for SPAM especially for chat system with the support of classical rough set theory. Simulation results clearly indicate that our proposed method, can achieve higher accuracy in spam detection as compared to the existing strategies.
ER  - 

TY  - CONF
JO  - Cybernetics (CYBCONF), 2015 IEEE 2nd International Conference on
TI  - Detection of Internet robots using a Bayesian approach
T2  - Cybernetics (CYBCONF), 2015 IEEE 2nd International Conference on
IS  - 
SN  - 
VO  - 
SP  - 365
EP  - 370
AU  - Suchacka, Grazyna
AU  - Sobkow, Mariusz
Y1  - 24-26 June 2015
PY  - 2015
KW  - Bayes methods
KW  - Correlation
KW  - Euclidean distance
KW  - Internet
KW  - Robots
KW  - Testing
KW  - Bayesian approach
KW  - Bayesian statistics
KW  - Internet robot
KW  - Matlab
KW  - Web bot
KW  - Web mining
KW  - Web robot detection
KW  - Web server
KW  - Web traffic
KW  - cluster analysis
KW  - correlation analysis
KW  - data mining
KW  - e-commerce
KW  - log file analysis
VL  - 
JA  - Cybernetics (CYBCONF), 2015 IEEE 2nd International Conference on
DO  - 10.1109/CYBConf.2015.7175961
AB  - A large part of Web traffic on e-commerce sites is generated not by human users but by Internet robots: search engine crawlers, shopping bots, hacking bots, etc. In practice, not all robots, especially the malicious ones, disclose their identities to a Web server and thus there is a need to develop methods for their detection and identification. This paper proposes the application of a Bayesian approach to robot detection based on characteristics of user sessions. The method is applied to the Web traffic from a real e-commerce site. Results show that the classification model based on the cluster analysis with the Ward's method and the weighted Euclidean metric is very effective in robot detection, even obtaining accuracy of above 90%.
ER  - 

TY  - CONF
JO  - Intelligent Systems Design and Applications (ISDA), 2011 11th International Conference on
TI  - Discovering and analyzing patterns of usage to detect usability problems in web applications
T2  - Intelligent Systems Design and Applications (ISDA), 2011 11th International Conference on
IS  - 
SN  - 2164-7143
VO  - 
SP  - 575
EP  - 580
AU  - Vargas, A.
AU  - Weffers, H.
AU  - da Rocha, H.V.
Y1  - 22-24 Nov. 2011
PY  - 2011
KW  - Internet
KW  - Web applications
KW  - WebHint method
KW  - online application
KW  - successive usability analysis
KW  - usage patterns
KW  - Context
KW  - Data analysis
KW  - Data mining
KW  - Electronic mail
KW  - Portfolios
KW  - Postal services
KW  - Usability
KW  - log file analysis
KW  - pattern mining
KW  - remote usability evaluation
KW  - sequence mining
KW  - usability evaluation
KW  - usage mining
KW  - user behavior analysis
VL  - 
JA  - Intelligent Systems Design and Applications (ISDA), 2011 11th International Conference on
DO  - 10.1109/ISDA.2011.6121717
AB  - In this paper we describe two usability studies in which the interaction of users with an online application was remotely and automatically captured and analyzed. The usability studies were performed using the WebHint method for usability analysis of web applications. We evaluate two different versions of the application in order to observe the applicability of the method for successive usability analysis. The results show how the WebHint method can be used as an alternative approach to carry out successive evaluations of the usability of an application in order to analyze the evolution of different versions of its interface.
ER  - 

TY  - CONF
JO  - Cognitive Methods in Situation Awareness and Decision Support (CogSIMA), 2015 IEEE International Inter-Disciplinary Conference on
TI  - Simple event correlator - Best practices for creating scalable configurations
T2  - Cognitive Methods in Situation Awareness and Decision Support (CogSIMA), 2015 IEEE International Inter-Disciplinary Conference on
IS  - 
SN  - 
VO  - 
SP  - 96
EP  - 100
AU  - Vaarandi, R.
AU  - Blumbergs, B.
AU  - C&#x0327;al&#x0131;s&#x0327;kan, E.
Y1  - 9-12 March 2015
PY  - 2015
KW  - public domain software
KW  - system monitoring
KW  - SEC
KW  - event correlation
KW  - open source event correlation tool
KW  - prominent monitoring technique
KW  - simple event correlator
KW  - situational awareness
KW  - Conferences
KW  - Context
KW  - Correlation
KW  - IP networks
KW  - Monitoring
KW  - Pattern matching
KW  - Reactive power
KW  - Simple Event Correlator
KW  - event correlation
KW  - event processing
KW  - log file analysis
VL  - 
JA  - Cognitive Methods in Situation Awareness and Decision Support (CogSIMA), 2015 IEEE International Inter-Disciplinary Conference on
DO  - 10.1109/COGSIMA.2015.7108181
AB  - During the past two decades, event correlation has emerged as a prominent monitoring technique, and is essential for achieving better situational awareness. Since its introduction in 2001 by one of the authors of this paper, Simple Event Correlator (SEC) has become a widely used open source event correlation tool. During the last decade, a number of papers have been published that describe the use of SEC in various environments. However, recent SEC versions have introduced a number of novel features not discussed in existing works. This paper fills this gap and provides an up-to-date coverage of best practices for creating scalable SEC configurations.
ER  - 

TY  - CONF
JO  - Web-Age Information Management, 2008. WAIM '08. The Ninth International Conference on
TI  - Research on Malicious Transaction Processing Method of Database System
T2  - Web-Age Information Management, 2008. WAIM '08. The Ninth International Conference on
IS  - 
SN  - 
VO  - 
SP  - 435
EP  - 440
AU  - Chi Chen
AU  - Deng-Guo Feng
AU  - Min Zhang
AU  - De-sheng Zhang
AU  - He-qun Xian
Y1  - 20-22 July 2008
PY  - 2008
KW  - database management systems
KW  - security of data
KW  - system recovery
KW  - transaction processing
KW  - DBMS
KW  - DOS attack
KW  - data recovery algorithm
KW  - database system
KW  - defensive information warfare
KW  - information attacks
KW  - malicious committed transactions
KW  - malicious transaction processing method
KW  - transaction recovery log
KW  - Computer crime
KW  - Data security
KW  - Database systems
KW  - Error correction
KW  - Hardware
KW  - Information management
KW  - Information security
KW  - Laboratories
KW  - Military computing
KW  - Transaction databases
VL  - 
JA  - Web-Age Information Management, 2008. WAIM '08. The Ninth International Conference on
DO  - 10.1109/WAIM.2008.27
AB  - Recovery from information attacks is difficult because DBMS is not designed to deal with malicious committed transactions. A few existing methods developed for this purpose rely on operation logs, which can't express the dependency between different transactions directly. These methods usually use rollback mechanism and abandon results of innocent transactions to maintain correctness, which may indeed be used as an approach to realize DOS attack. Hence, it's necessary to find out the malicious transaction and subsequent transactions depending on it precisely. In this paper, the definition of transaction recovery log is presented and each log item records the actions taken in one transaction, by which, we can calculate transactions' dependency directly. Based on the log model and the algorithm for log's creation, the dependency calculation and data recovery algorithm are studied, which are proofed to be complete and correct. Using transaction recovery log and the algorithm, database system can significantly enhance the performance of recovery for defensive information warfare.
ER  - 

TY  - CONF
JO  - Parallel Processing Workshops, 2009. ICPPW '09. International Conference on
TI  - Towards a Side Access Free Data Grid Resource by Means of Infrastructure Clouds
T2  - Parallel Processing Workshops, 2009. ICPPW '09. International Conference on
IS  - 
SN  - 1530-2016
VO  - 
SP  - 198
EP  - 205
AU  - Huemer, D.
AU  - A Min Tjoa
AU  - Descher, M.
AU  - Feilhauer, T.
AU  - Masser, P.
Y1  - 22-25 Sept. 2009
PY  - 2009
KW  - grid computing
KW  - medical administrative data processing
KW  - security of data
KW  - virtual machines
KW  - HIPAA
KW  - audit trails
KW  - data access
KW  - data interaction
KW  - data modification
KW  - data sharing
KW  - infrastructure clouds
KW  - medical data
KW  - side-access problem
KW  - side-access-free single point of access
KW  - traceability problem
KW  - virtual grid computing environments
KW  - virtual machine security
KW  - Authentication
KW  - Authorization
KW  - Biomedical engineering
KW  - Clouds
KW  - Data security
KW  - Grid computing
KW  - Information security
KW  - Interactive systems
KW  - Parallel processing
KW  - Protection
KW  - Grid Computing
KW  - Single Point of Access
KW  - cloud computing
VL  - 
JA  - Parallel Processing Workshops, 2009. ICPPW '09. International Conference on
DO  - 10.1109/ICPPW.2009.56
AB  - Sharing data within grid environments always results in losing control over data usage. If the shared information is medical data, this violates laws as specified in HIPAA, because access to such data must be reconstructible and needs to be tightly controlled. To comply with these regulations it must be guaranteed, that each data access and modification is recorded to produce audit trails. The provided solution introduces a new concept for virtual grid computing environments consisting of secure virtual machines, hosting data and indemnifying thorough audit trails by logging every data interaction, solving the problem of traceability and side-access.
ER  - 

TY  - CONF
JO  - IT Security Incident Management and IT Forensics, 2009. IMF '09. Fifth International Conference on
TI  - Analysis of Download Accelerator Plus (DAP) for Forensic Artefacts
T2  - IT Security Incident Management and IT Forensics, 2009. IMF '09. Fifth International Conference on
IS  - 
SN  - 
VO  - 
SP  - 142
EP  - 152
AU  - Yasin, M.
AU  - Wahla, M.A.
AU  - Kausar, Firdous
Y1  - 15-17 Sept. 2009
PY  - 2009
KW  - file organisation
KW  - security of data
KW  - DAP forensic artefact collector
KW  - download accelerator plus
KW  - download path
KW  - downloaded files
KW  - forensic artefacts
KW  - install location
KW  - log files
KW  - menu extensions
KW  - software records
KW  - windows registry entries
KW  - Conference management
KW  - Digital audio players
KW  - File servers
KW  - Forensics
KW  - History
KW  - Information analysis
KW  - Information security
KW  - Performance analysis
KW  - Technology management
KW  - Web server
VL  - 
JA  - IT Security Incident Management and IT Forensics, 2009. IMF '09. Fifth International Conference on
DO  - 10.1109/IMF.2009.11
AB  - Download accelerator plus (DAP) is one of the most popular download managers due to its free availability, download speed and versatility. This software records download activities across multiple files which include history, registry, RAM, swap and temporary files. This paper analyzes (a) the log files (with .DAT extension), (b) windows registry entries, and (c) RAM and swap files from forensic view point. We also look at tools and techniques for extracting evidence. This research work describes a number of traces left behind after the use of DAP such as install location, download path, downloaded files and menu extensions to name a few, enabling digital investigators to search and interpret download activities. Moreover the study is supported by a tool, DAP forensic artefact collector (DAPFAC), that assists forensic examiners by providing valuable information which is retrieved from the windows registry and history files on the basis of analysis performed. The widespread use of DAP makes this analysis, an attractive option, ranging from law enforcement agencies to employees monitoring manager.
ER  - 

TY  - CONF
JO  - Engineering of Complex Computer Systems, 2002. Proceedings. Eighth IEEE International Conference on
TI  - MNEMOSYNE: designing and implementing network short-term memory
T2  - Engineering of Complex Computer Systems, 2002. Proceedings. Eighth IEEE International Conference on
IS  - 
SN  - 
VO  - 
SP  - 91
EP  - 100
AU  - Vigna, Giovanni
AU  - Mitchell, A.
Y1  - 2-4 Dec. 2002
PY  - 2002
KW  - computer network management
KW  - real-time systems
KW  - security of data
KW  - telecommunication security
KW  - telecommunication traffic
KW  - MNEMOSYNE
KW  - computer network monitoring
KW  - cross-stream queries
KW  - dynamic remote reconfiguration
KW  - incident analysis
KW  - logging features
KW  - multiple streams
KW  - network security
KW  - network short-term memory
KW  - network traffic logs
KW  - performance evaluation
KW  - real-time intrusion detection
KW  - resource consumption problem
KW  - sliding window
KW  - Application software
KW  - Computer architecture
KW  - Computer network reliability
KW  - Data security
KW  - Forensics
KW  - Intrusion detection
KW  - Maintenance
KW  - Monitoring
KW  - Telecommunication traffic
KW  - Throughput
VL  - 
JA  - Engineering of Complex Computer Systems, 2002. Proceedings. Eighth IEEE International Conference on
DO  - 10.1109/ICECCS.2002.1181501
AB  - Network traffic logs play an important role in incident analysis. With the increasing throughput of network links, maintaining a complete log of all network activity has become a task that requires an enormous amount of resources. We propose an approach to network monitoring that mitigates the resource consumption problem while still providing effective support to evidence collection and incident analysis. The approach relies on a tool, called MNEMOSYNE, that maintains a sliding window containing the traffic that has been recently seen on a network link. MNEMOSYNE provides improved logging features, such as multiple streams, support for cross-stream queries, and dynamic remote reconfiguration. By integrating MNEMOSYNE with real-time intrusion detection capability, it is possible to provide incident analysis functionality and effective evidence collection, without having to maintain complete traffic logs. This paper describes the MNEMOSYNE tool, its architecture, and presents the results of the quantitative evaluation of its performance.
ER  - 

TY  - JOUR
JO  - Computer Graphics and Applications, IEEE
TI  - Detecting flaws and intruders with visual data analysis
T2  - Computer Graphics and Applications, IEEE
IS  - 5
SN  - 0272-1716
VO  - 24
SP  - 27
EP  - 35
AU  - Soon Tee Teoh
AU  - Kwan-Liu Ma
AU  - Wu, S.F.
AU  - Jankun-Kelly, T.J.
Y1  - Sept.-Oct. 2004
PY  - 2004
KW  - data analysis
KW  - data mining
KW  - data visualisation
KW  - security of data
KW  - data mining
KW  - flaws detection
KW  - human interaction
KW  - intruders
KW  - log-file analysis
KW  - machine-learning
KW  - visual data analysis
KW  - Application software
KW  - Computer networks
KW  - Data analysis
KW  - Data mining
KW  - Data security
KW  - Data visualization
KW  - Humans
KW  - Intrusion detection
KW  - Performance analysis
KW  - Visual analytics
KW  - Internet routing stability
KW  - information visualization
KW  - intrusion detection
KW  - network visualization
KW  - visual data mining
KW  - Computer Communication Networks
KW  - Computer Graphics
KW  - Computer Security
KW  - Database Management Systems
KW  - Databases, Factual
KW  - Information Storage and Retrieval
KW  - Online Systems
KW  - Software
KW  - User-Computer Interface
VL  - 24
JA  - Computer Graphics and Applications, IEEE
DO  - 10.1109/MCG.2004.26
AB  - The task of sifting through large amounts of data to find useful information spawned the field of data mining. Most data mining approaches are based on machine-learning techniques, numerical analysis, or statistical modeling. They use human interaction and visualization only minimally. Such automatic methods can miss some important features of the data. Incorporating human perception into the data mining process through interactive visualization can help us better understand the complex behaviors of computer network systems. This article describes visual-analytics-based solutions and outlines a visual exploration process for log analysis. Three log-file analysis applications demonstrate our approach's effectiveness in discovering flaws and intruders in network systems.
ER  - 

TY  - CONF
JO  - Applications and the Internet, 2008. SAINT 2008. International Symposium on
TI  - A Generalized Feature Extraction Scheme to Detect 0-Day Attacks via IDS Alerts
T2  - Applications and the Internet, 2008. SAINT 2008. International Symposium on
IS  - 
SN  - 
VO  - 
SP  - 55
EP  - 61
AU  - Jungsuk Song
AU  - Takakura, H.
AU  - Yongjin Kwon
Y1  - July 28 2008-Aug. 1 2008
PY  - 2008
KW  - feature extraction
KW  - security of data
KW  - support vector machines
KW  - telecommunication security
KW  - unsupervised learning
KW  - 0-day attack detection
KW  - cyber attacks
KW  - generalized feature extraction
KW  - intrusion detection system alerts
KW  - one-class SVM
KW  - raw traffic data
KW  - support vector machines
KW  - unknown attack detection
KW  - unsupervised learning
KW  - Computer security
KW  - Data security
KW  - Feature extraction
KW  - Informatics
KW  - Information security
KW  - Internet
KW  - Intrusion detection
KW  - Support vector machines
KW  - Telecommunication traffic
KW  - Unsupervised learning
VL  - 
JA  - Applications and the Internet, 2008. SAINT 2008. International Symposium on
DO  - 10.1109/SAINT.2008.85
AB  - Intrusion detection system (IDS) has played an important role as a device to defend our networks from cyber attacks. However, since it still suffers from detecting an unknown attack, i.e., 0-day attack, the ultimate challenge in intrusion detection field is how we can exactly identify such an attack. Unlike the existing approaches that investigate raw traffic data, we introduced a feature extraction method in order to detect such an attack from IDS alerts [J. Song et al., 2007]. However, there is a problem that it can be only applied to limited IDS products. In this paper, we present a generalized version of the feature extraction method. To this end, we define new 7 features using only the basic 6 features of IDS alerts; detection time, source address and port, destination address and port, and signature name. In order to detect 0-day attack from IDS alerts with new 7 features, we apply an unsupervised learning technique, One-class SVM, to them. We evaluated our method over the log data of IDS that is deployed in Kyoto University, and our experimental results show that it has capability to detect not only a type of 0-day attack detected in our previous study, but also several different types of 0-day attack.
ER  - 

TY  - CONF
JO  - Availability, Reliability and Security, 2008. ARES 08. Third International Conference on
TI  - A Revised Taxonomy of Data Collection Mechanisms with a Focus on Intrusion Detection
T2  - Availability, Reliability and Security, 2008. ARES 08. Third International Conference on
IS  - 
SN  - 
VO  - 
SP  - 624
EP  - 629
AU  - Larson, U.E.
AU  - Jonsson, E.
AU  - Lindskog, S.
Y1  - 4-7 March 2008
PY  - 2008
KW  - data acquisition
KW  - security of data
KW  - data collection mechanisms
KW  - detection engine
KW  - intrusion detection
KW  - log data quality
KW  - Availability
KW  - Computer science
KW  - Computer security
KW  - Data engineering
KW  - Data security
KW  - Guidelines
KW  - Intrusion detection
KW  - Reliability engineering
KW  - Taxonomy
KW  - Terminology
KW  - Data collection
KW  - Intrusion detection
KW  - Taxonomy
VL  - 
JA  - Availability, Reliability and Security, 2008. ARES 08. Third International Conference on
DO  - 10.1109/ARES.2008.38
AB  - Surprisingly few data collection mechanisms have been used for intrusion detection, and most systems rely on network and system call data as input to the detection engine. Even though the quality of log data is vital to the detection process and heavily dependent on the collection mechanism, no extensive survey or taxonomy has been conducted within the detection field. In this paper, we propose a revised taxonomy which provides a unified terminology and a framework in which data collection mechanisms can be systematically inspected, evaluated, and compared. Since the taxonomy is derived from existing mechanisms, it also provides a useful overview of different types of mechanisms. The paper also suggests areas within data collection where additional work is required.
ER  - 

TY  - JOUR
JO  - Computer Graphics and Applications, IEEE
TI  - Hierarchical visualization of network intrusion detection data
T2  - Computer Graphics and Applications, IEEE
IS  - 2
SN  - 0272-1716
VO  - 26
SP  - 40
EP  - 47
AU  - Itoh, T.
AU  - Takakura, H.
AU  - Sawada, A.
AU  - Koyamada, K.
Y1  - March-April 2006
PY  - 2006
KW  - IP networks
KW  - data visualisation
KW  - security of data
KW  - telecommunication security
KW  - IP addresses
KW  - bioactive chemical visualization
KW  - black square icons
KW  - branch nodes
KW  - data leaf nodes
KW  - hierarchical data visualization technique
KW  - job distribution
KW  - network intrusion detection system log files
KW  - parallel-computing environment
KW  - rectangular borders
KW  - Computer displays
KW  - Computer networks
KW  - Computer security
KW  - Data security
KW  - Data structures
KW  - Data visualization
KW  - Graphical user interfaces
KW  - Intrusion detection
KW  - Large-scale systems
KW  - Mesh generation
KW  - Hierarchical data visualization
KW  - IP address space
KW  - Intrusion detection system
KW  - Rectangle packing
KW  - Computer Communication Networks
KW  - Computer Graphics
KW  - Information Storage and Retrieval
KW  - Signal Processing, Computer-Assisted
KW  - Software
KW  - User-Computer Interface
VL  - 26
JA  - Computer Graphics and Applications, IEEE
DO  - 10.1109/MCG.2006.34
AB  - A technique for visualizing intrusion-detection system log files using hierarchical data based on IP addresses represents the number of incidents for thousands of computers in one display space. Our technique applies a hierarchical data visualization technique that represents leaf nodes as black square icons and branch nodes as rectangular borders enclosing the icons. This representation style visualizes thousands of hierarchical data leaf nodes equally in one display space. We applied the technique to bioactive chemical visualization and job distribution in parallel-computing environments.
ER  - 

TY  - CONF
JO  - Systems, Man and Cybernetics, 2003. IEEE International Conference on
TI  - Intrusion behavior detection through visualization
T2  - Systems, Man and Cybernetics, 2003. IEEE International Conference on
IS  - 
SN  - 1062-922X
VO  - 3
SP  - 2507
EP  - 2513 vol.3
AU  - Erbacher, R.F.
Y1  - 5-8 Oct. 2003
PY  - 2003
KW  - computer crime
KW  - data visualisation
KW  - reliability
KW  - user interfaces
KW  - computer intrusions
KW  - computer log information
KW  - hackers
KW  - intrusion behavior detection
KW  - log file analysis
KW  - network intrusions
KW  - network log information
KW  - reliability
KW  - user behavior analysis
KW  - visualization techniques
KW  - Computer hacking
KW  - Computer network reliability
KW  - Computer networks
KW  - Computer science
KW  - Computer security
KW  - Forensics
KW  - Information analysis
KW  - Intrusion detection
KW  - Pattern matching
KW  - Visualization
VL  - 3
JA  - Systems, Man and Cybernetics, 2003. IEEE International Conference on
DO  - 10.1109/ICSMC.2003.1244260
AB  - As computer and network intrusions become more and more of a concern, the need for better capabilities to assist in the detection and analysis of intrusions also increases. We propose a methodology for analyzing network and computer log information visually based on the analysis of user behavior. Each user's behavior is the key to determining their intent and overriding goals, whether they attempt to hide their actions or not. Proficient hackers will attempt to hide their ultimate goal, which hinders the reliability of log file analysis. Visually analyzing the user's behavior, however, is much more adaptable and difficult to counteract. This paper will discuss how user behavior can be exhibited within the visualization techniques, the capabilities provided by the environment, typical characteristics users should look out for (i.e., how unusual behavior exhibits itself), and exploration paradigms effective for identifying the meaning behind the user's behavior.
ER  - 

TY  - JOUR
JO  - Computers, IEEE Transactions on
TI  - Data hiding in 2-color images
T2  - Computers, IEEE Transactions on
IS  - 7
SN  - 0018-9340
VO  - 51
SP  - 873
EP  - 878
AU  - Yu-Chee Tseng
AU  - Hsiang-Kuang Pan
Y1  - Jul 2002
PY  - 2002
KW  - data encapsulation
KW  - image colour analysis
KW  - security of data
KW  - coding
KW  - critical information hiding
KW  - cryptography
KW  - data hiding
KW  - data security
KW  - host binary image processing
KW  - image block
KW  - prisoners problem
KW  - steganography scheme
KW  - two color images
KW  - Computer Society
KW  - Cryptography
KW  - Data encapsulation
KW  - Data security
KW  - Image quality
KW  - Information security
KW  - Information services
KW  - Pixel
KW  - Steganography
KW  - Web sites
VL  - 51
JA  - Computers, IEEE Transactions on
DO  - 10.1109/TC.2002.1017706
AB  - In an earlier paper (Chen et al., 2000), we proposed a steganography scheme for hiding a piece of critical information in a host binary image. That scheme ensures that, in each m &times; n image block of the host image, as many as [log<sub>2</sub> (mn + 1)] bits can be hidden in the block by changing at most 2 bits in the block. We propose a new scheme that improves (Chen et al., 2000) in its capability to maintain higher quality of the host image after data hiding by sacrificing some data hiding space. The new scheme can still offer a good data hiding ratio. It ensures that, for any bit that is modified in the host image, the bit is adjacent to another bit which has a value equal to the former's new value. Thus, the hiding effect is quite invisible
ER  - 

TY  - CONF
JO  - Integrated Network Management Proceedings, 2001 IEEE/IFIP International Symposium on
TI  - Proactive detection of distributed denial of service attacks using MIB traffic variables-a feasibility study
T2  - Integrated Network Management Proceedings, 2001 IEEE/IFIP International Symposium on
IS  - 
SN  - 
VO  - 
SP  - 609
EP  - 622
AU  - Cabrera, J.B.D.
AU  - Lewis, L.
AU  - Xinzhou Qin
AU  - Wenke Lee
AU  - Prasanth, R.K.
AU  - Ravichandran, B.
AU  - Mehra, R.K.
Y1  - 2001
PY  - 2001
KW  - distributed databases
KW  - security of data
KW  - statistical analysis
KW  - telecommunication computing
KW  - telecommunication network management
KW  - telecommunication security
KW  - telecommunication traffic
KW  - time series
KW  - MIB traffic variables
KW  - anomaly detection scheme
KW  - attacker machine monitoring
KW  - attacking behavior
KW  - automated procedure
KW  - datasets
KW  - distributed denial of service attacks
KW  - management information base
KW  - network management systems
KW  - proactive detection
KW  - research test bed
KW  - statistical signatures
KW  - statistical tests
KW  - time series
KW  - Computer crime
KW  - Computer network management
KW  - Data security
KW  - Information security
KW  - Intrusion detection
KW  - Monitoring
KW  - Samarium
KW  - Technology management
KW  - Testing
KW  - USA Councils
VL  - 
JA  - Integrated Network Management Proceedings, 2001 IEEE/IFIP International Symposium on
DO  - 10.1109/INM.2001.918069
AB  - We propose a methodology for utilizing network management systems for the early detection of distributed denial of service (DDoS) attacks. Although there are quite a large number of events that are prior to an attack (e.g. suspicious log-ons, start of processes, addition of new files, sudden shifts in traffic, etc.), in this work we depend solely on information from MIB (management information base) traffic variables collected from the systems participating in the attack. Three types of DDoS attacks were effected on a research test bed, and MIB variables were recorded. Using these datasets, we show how there are indeed MIB-based precursors of DDoS attacks that render it possible to detect them before the target is shut down. Most importantly, we describe how the relevant MIB variables at the attacker can be extracted automatically using statistical tests for causality. It is shown that statistical tests applied in the time series of MIB traffic at the target and the attacker are effective in extracting the correct variables for monitoring in the attacker machine. Following the extraction of these key variables at the attacker, it is shown that an anomaly detection scheme, based on a simple model of the normal rate of change of the key MIBs can be used to determine statistical signatures of attacking behavior. These observations suggest the possibility of an entirely automated procedure centered on network management systems for detecting precursors of distributed denial of service attacks, and responding to them
ER  - 

TY  - CONF
JO  - Distributed Computing Systems Workshops, 2005. 25th IEEE International Conference on
TI  - Forensix: a robust, high-performance reconstruction system
T2  - Distributed Computing Systems Workshops, 2005. 25th IEEE International Conference on
IS  - 
SN  - 
VO  - 
SP  - 155
EP  - 162
AU  - Goel, A.
AU  - Feng, W.-C.
AU  - Maier, D.
AU  - Feng, W.-C.
AU  - Walpole, J.
Y1  - 6-10 June 2005
PY  - 2005
KW  - database management systems
KW  - security of data
KW  - system recovery
KW  - Forensix
KW  - append-only storage
KW  - automated analysis
KW  - database technology
KW  - forensic analysis
KW  - high-performance reconstruction system
KW  - human-intensive task
KW  - kernel event level
KW  - Computer networks
KW  - Computerized monitoring
KW  - Costs
KW  - Forensics
KW  - Humans
KW  - Kernel
KW  - Performance analysis
KW  - Real time systems
KW  - Robustness
KW  - Storage automation
VL  - 
JA  - Distributed Computing Systems Workshops, 2005. 25th IEEE International Conference on
DO  - 10.1109/ICDCSW.2005.62
AB  - When computer intrusions occur, one of the most costly, time-consuming, and human-intensive tasks is the analysis and recovery of the compromised system. At a time when the cost of human resources dominates the cost of CPU, network, and storage resources, we argue that computing systems should, in fact, be built with automated analysis and recovery as a primary goal. Towards this end, we describe the design, implementation, and evaluation of Forensix: a robust, high-precision reconstruction and analysis system for supporting the computer equivalent of "TiVo". Forensix uses three key mechanisms to improve the accuracy and reduce the human overhead of performing forensic analysis. First it performs comprehensive monitoring of the execution of a target system at the kernel event level, giving a high-resolution, application-independent view of all activity. Second, it streams the kernel event information, in real-time, to append-only storage on a separate, hardened, logging machine, making the system resilient to a wide variety of attacks. Third, it uses database technology to support high-level querying of the archived log, greatly reducing the human cost of performing forensic analysis.
ER  - 

TY  - CONF
JO  - Aerospace Computer Security Applications Conference, 1988., Fourth
TI  - A mandatory access control mechanism for the Unix file system
T2  - Aerospace Computer Security Applications Conference, 1988., Fourth
IS  - 
SN  - 
VO  - 
SP  - 173
EP  - 177
AU  - Thomas, T.
Y1  - 12-16 Dec 1988
PY  - 1988
KW  - Unix
KW  - security of data
KW  - BSD Unix
KW  - MAC design
KW  - Network File System support
KW  - Systems V
KW  - file name hiding
KW  - mandatory access control mechanism
KW  - traditional Unix file system
KW  - Access control
KW  - Data security
KW  - Erbium
KW  - File systems
VL  - 
JA  - Aerospace Computer Security Applications Conference, 1988., Fourth
DO  - 10.1109/ACSAC.1988.113437
AB  - The design of a mandatory access control (MAC) mechanism for the Unix file system is described. The design is simple, compatible with AT&amp;T's Systems V and Berkeley's BSD Unix with Sun Microsystem's Network File System support, and it avoids some of the deficiencies present in approaches done to date. The MAC design introduces the concept of file name hiding. The design eliminates the need for partitioned directories and the need to log out and then log in again to use upgraded directories. The author briefly describes the traditional Unix file system. Approaches to adding a mandatory access control mechanism to the Unix file system are detailed, and problems with the approaches are examined. Finally, the proposed approach is described, including an explanation of how it solves the deficiencies of the previous approaches
ER  - 

TY  - JOUR
JO  - Computer Graphics and Applications, IEEE
TI  - Intrusion and misuse detection in large-scale systems
T2  - Computer Graphics and Applications, IEEE
IS  - 1
SN  - 0272-1716
VO  - 22
SP  - 38
EP  - 47
AU  - Erbacher, R.F.
AU  - Walker, K.L.
AU  - Frincke, D.A.
Y1  - Jan/Feb 2002
PY  - 2002
KW  - data visualisation
KW  - safety systems
KW  - security of data
KW  - computer systems
KW  - consumer confidence
KW  - cyberterrorism
KW  - electronic purchases
KW  - glyph metaphor
KW  - information visualization techniques
KW  - intrusion detection
KW  - large-scale systems
KW  - misuse detection
KW  - network based world
KW  - network-based economic resources
KW  - system integrity
KW  - textual log information
KW  - Bandwidth
KW  - Computer networks
KW  - Computer security
KW  - Data security
KW  - Data visualization
KW  - Humans
KW  - Intelligent networks
KW  - Intrusion detection
KW  - Large-scale systems
KW  - NIST
VL  - 22
JA  - Computer Graphics and Applications, IEEE
DO  - 10.1109/38.974517
AB  - Attacks and misuses of computer systems are major concerns in today's network-based world. With the growing concern with regard to cyberterrorism there is a need for new tools and techniques to monitor networks and systems for intrusions and misuse. The goal must be to identify an attack before an organization incurs damage, loses information (theft or otherwise), or has its integrity impugned. With today's network-based economic resources, a successful attack will negatively impact consumer confidence and decrease consumers' willingness to make electronic purchases. The authors present information visualization techniques based on a glyph metaphor for visually representing textual log information
ER  - 

TY  - CONF
JO  - Network and System Security, 2009. NSS '09. Third International Conference on
TI  - Characterising the Evolution in Scanning Activity of Suspicious Hosts
T2  - Network and System Security, 2009. NSS '09. Third International Conference on
IS  - 
SN  - 
VO  - 
SP  - 344
EP  - 350
AU  - Wahid, A.
AU  - Leckie, C.
AU  - Chenfeng Zhou
Y1  - 19-21 Oct. 2009
PY  - 2009
KW  - Internet
KW  - security of data
KW  - telecommunication security
KW  - DShield repository
KW  - Internet
KW  - aggregation interval
KW  - distributed IDS
KW  - evolution characterisation
KW  - imminent multistage attack detection
KW  - stable size
KW  - suspicious host scanning activity
KW  - temporal property
KW  - Aggregates
KW  - Computer science
KW  - Computer security
KW  - Computerized monitoring
KW  - Internet
KW  - Intrusion detection
KW  - Recruitment
KW  - Size measurement
KW  - Software engineering
KW  - Spatiotemporal phenomena
VL  - 
JA  - Network and System Security, 2009. NSS '09. Third International Conference on
DO  - 10.1109/NSS.2009.13
AB  - The early detection of multistage attacks like DDoS and coordinated spamming poses a major challenge for existing counter-measures based on reactive blacklists. One approach to addressing this challenge would be to profile hosts that engage in scanning activity and predict their future actions. However, this requires understanding how hosts evolve their scanning behaviour. In order to address this issue we have analysed logs from the DShield repository of globally distributed IDS alerts corresponding to the first 15 days of January 2005. We first clustered hosts using similarities in the spatial breadth (targeted DShield subscribers) and depth (targeted destination ports) of their scanning activity during aggregation intervals of one day at a time. We then analysed temporal properties like popularity, volatility, lifetime and transition of these clusters to infer how they evolved over time. We found persistent clusters with stable sizes. However, they were highly volatile with a consistent turn-over of hosts everyday. This was caused by the lifetime of hosts in each cluster mostly being one day. Nevertheless, we came across a non-trivial number of hosts that appeared everyday while belonging to the same cluster or transitioning from one cluster to another. Based on these findings, it is plausible that suspicious hosts can be profiled for long periods of time to predict an imminent multistage attack.
ER  - 

TY  - CONF
JO  - Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing, 2007. SNPD 2007. Eighth ACIS International Conference on
TI  - Risk Evaluation for Host System Based on Theory of Evidence
T2  - Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing, 2007. SNPD 2007. Eighth ACIS International Conference on
IS  - 
SN  - 
VO  - 1
SP  - 120
EP  - 124
AU  - Li Chun-yan
AU  - Guo Yi-zun
Y1  - July 30 2007-Aug. 1 2007
PY  - 2007
KW  - case-based reasoning
KW  - operating system kernels
KW  - risk analysis
KW  - security of data
KW  - Dempster-Shafer theory
KW  - data analysis
KW  - data mining
KW  - kernel operating system
KW  - risk evaluation
KW  - Artificial intelligence
KW  - Data security
KW  - Distributed computing
KW  - Educational institutions
KW  - Electric breakdown
KW  - Frequency
KW  - Information security
KW  - Kernel
KW  - Software engineering
KW  - Telecommunication traffic
VL  - 1
JA  - Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing, 2007. SNPD 2007. Eighth ACIS International Conference on
DO  - 10.1109/SNPD.2007.83
AB  - Bring forwards a new approach to evaluate the secure level for a host system. Kernel files of OS is frangible against intruders and virus, we can log suspicious events that access the kernel, and the threaten level of each event can be defined. By mining and analyzing the data in log, using the theory of evidence, we can calculate the risk level for the whole host system. On the base of the evaluation result, the administrator can decide to enhance or lower the host defense level.
ER  - 

TY  - CONF
JO  - Education Technology and Computer Science (ETCS), 2010 Second International Workshop on
TI  - Statistic and Analysis for Host-Based Syslog
T2  - Education Technology and Computer Science (ETCS), 2010 Second International Workshop on
IS  - 
SN  - 
VO  - 2
SP  - 277
EP  - 280
AU  - Gu Zhaojun
AU  - Wang Chao
Y1  - 6-7 March 2010
PY  - 2010
KW  - backpropagation
KW  - security of data
KW  - statistical analysis
KW  - anomaly detection
KW  - back propagation
KW  - data preprocessing
KW  - host secure
KW  - host-based syslog
KW  - intrusion detection
KW  - local host syslog
KW  - log collection tool
KW  - neural network
KW  - regular expression
KW  - rule matching
KW  - secure state model
KW  - statistics
KW  - syslog audit
KW  - syslog protocol
KW  - syslog record
KW  - system information
KW  - Chaos
KW  - Computer science
KW  - Computer science education
KW  - Data security
KW  - Educational technology
KW  - Equations
KW  - Information analysis
KW  - Neural networks
KW  - Pattern matching
KW  - Statistical analysis
KW  - Analysis
KW  - Anomaly detection
KW  - BP NN
KW  - Regular Expression
KW  - Statistic
KW  - Syslog
VL  - 2
JA  - Education Technology and Computer Science (ETCS), 2010 Second International Workshop on
DO  - 10.1109/ETCS.2010.128
AB  - Syslog audit acts an important role in keeping host secure. This paper studied Host-based syslog, and constructed a secure state model for host performing normally from the angle of anomaly detection. Through deep research on syslog protocol, a log collection tool was created for collecting remote or local host syslog. Because different segments of syslog imply different system information, the model separated every segment from a syslog record with Regular Expression, then it made data preprocessing and statistic with rule matching, in the end it conducted analysis by BP (Back Propagation) NN (Neural Network). The result indicates that not only can it achieve Host-based intrusion and anomaly detection, but also it is a high efficient and intelligent method.
ER  - 

TY  - JOUR
JO  - Security & Privacy, IEEE
TI  - Training Johnny to Authenticate (Safely)
T2  - Security & Privacy, IEEE
IS  - 1
SN  - 1540-7993
VO  - 10
SP  - 37
EP  - 45
AU  - Herzberg, A.
AU  - Margulies, R.
Y1  - Jan.-Feb. 2012
PY  - 2012
KW  - Internet
KW  - image recognition
KW  - mobile computing
KW  - security of data
KW  - user interfaces
KW  - Web authentication scenario
KW  - adaptive authentication mechanism
KW  - fallback authentication
KW  - image memorization
KW  - image recognition
KW  - impersonation attacks detection
KW  - interactive image
KW  - login bookmark
KW  - login ceremony
KW  - mobile authentication scenario
KW  - negative training function
KW  - nonworking buttons
KW  - phishing
KW  - site-based login mechanism
KW  - user log-in
KW  - user training
KW  - Access control
KW  - Authentication
KW  - Browsers
KW  - Computer security
KW  - Electronic mail
KW  - Privacy
KW  - Training
KW  - fallback authentication
KW  - forcing functions
KW  - graphical passwords
KW  - human factors
KW  - long-term user study
KW  - memorability
KW  - password reset
KW  - phishing
KW  - training
VL  - 10
JA  - Security & Privacy, IEEE
DO  - 10.1109/MSP.2011.129
AB  - The authors present the results of a long-term user study of site-based login mechanisms that train users to log in safely. Interactive site-identifying images received 70 percent detection rates, which is significantly better than the 20 percent received by the typical login ceremony. They also found that combining login bookmarks with interactive images and nonworking buttons or links (called negative training functions) achieved the best detection rates (82 percent) and overall resistance rates (93 percent). Because interactive custom images provide effective user training against phishing, the authors extended its authentication usages. The authors present an adaptive authentication mechanism based on recognition of multiple custom images, which can be used for different Web and mobile authentication scenarios. The mechanism relies on memorization of the custom images on each primary login, adaptively increasing the authentication difficulty on detection of impersonation attacks, and recognizing all images for fallback authentication.
ER  - 

TY  - CONF
JO  - Computer Software and Applications Conference, 2001. COMPSAC 2001. 25th Annual International
TI  - A novel intrusion detection system model for securing web-based database systems
T2  - Computer Software and Applications Conference, 2001. COMPSAC 2001. 25th Annual International
IS  - 
SN  - 0730-3157
VO  - 
SP  - 249
EP  - 254
AU  - Shu Wenhui
AU  - Tan, T.D.H.
Y1  - 2001
PY  - 2001
KW  - Internet
KW  - database management systems
KW  - information resources
KW  - safety systems
KW  - security of data
KW  - alarm
KW  - database protection
KW  - information resources
KW  - intrusion detection system
KW  - web server
KW  - web-based database systems
KW  - Data analysis
KW  - Data security
KW  - Database systems
KW  - Error analysis
KW  - Information security
KW  - Information systems
KW  - Intrusion detection
KW  - Logic
KW  - Protection
KW  - Web server
VL  - 
JA  - Computer Software and Applications Conference, 2001. COMPSAC 2001. 25th Annual International
DO  - 10.1109/CMPSAC.2001.960624
AB  - Intrusion detection (ID) has become an important technology for protecting information resources and databases from malicious attacks and information leakage. This paper proposes a novel two-layer mechanism to detect intrusions against a web-based database service. Layer one builds historical profiles based on audit trails and other log data provided by the web server and database server. Pre-alarms will be triggered if anomalies occurred. Layer two makes further analysis on the pre-alarms generated from Layer one. Such methods integrates the alarm context with the alarms themselves rather than a simple "analysis in isolation". This can reduce the error rates, especially false positives and greatly improve the accuracy of intrusion detection, alarm notification and hence more effective incident handling
ER  - 

TY  - CONF
JO  - Security and Privacy, 1995. Proceedings., 1995 IEEE Symposium on
TI  - Holding intruders accountable on the Internet
T2  - Security and Privacy, 1995. Proceedings., 1995 IEEE Symposium on
IS  - 
SN  - 
VO  - 
SP  - 39
EP  - 49
AU  - Staniford-Chen, S.
AU  - Heberlein, L.T.
Y1  - 8-10 May 1995
PY  - 1995
KW  - Internet
KW  - authorisation
KW  - information services
KW  - security of data
KW  - statistical analysis
KW  - Internet
KW  - algorithm
KW  - connection chain
KW  - intruder tracing
KW  - local area network
KW  - multiple machines
KW  - multivariate statistics
KW  - principal component analysis
KW  - thumbprints
KW  - Assembly
KW  - Computer crime
KW  - Computer networks
KW  - Internet
KW  - Law
KW  - Legal factors
KW  - Local area networks
KW  - Personnel
KW  - Security
KW  - Statistical analysis
VL  - 
JA  - Security and Privacy, 1995. Proceedings., 1995 IEEE Symposium on
DO  - 10.1109/SECPRI.1995.398921
AB  - This paper addresses the problem of tracing intruders who obscure their identity by logging through a chain of multiple machines. After discussing previous approaches to this problem, we introduce thumbprints which are short summaries of the content of a connection. These can be compared to determine whether two connections contain the same text and are therefore likely to be part of the same connection chain. We enumerate the properties a thumbprint needs to have to work in practice, and then define a class of local thumbprints which have the desired properties. A methodology from multivariate statistics called principal component analysis is used to infer the best choice of thumbprinting parameters from data. Currently our thumbprints require 24 bytes per minute per connection. We develop an algorithm to compare these thumbprints which allows for the possibility that data may leak from one time-interval to the next. We present experimental data showing that our scheme works on a local area network
ER  - 

TY  - CONF
JO  - Information Security and Assurance, 2008. ISA 2008. International Conference on
TI  - IDEA: A New Intrusion Detection Data Source
T2  - Information Security and Assurance, 2008. ISA 2008. International Conference on
IS  - 
SN  - 
VO  - 
SP  - 15
EP  - 19
AU  - Mahoney, W.
AU  - Sousan, W.
Y1  - 24-26 April 2008
PY  - 2008
KW  - security of data
KW  - computer systems
KW  - data source
KW  - intrusion detection automata system
KW  - network traffic
KW  - Automata
KW  - Computerized monitoring
KW  - DSL
KW  - Domain specific languages
KW  - Information security
KW  - Instruments
KW  - Intrusion detection
KW  - Open source software
KW  - Operating systems
KW  - Telecommunication traffic
KW  - instrumentation
KW  - intrusion detection
KW  - open-source
VL  - 
JA  - Information Security and Assurance, 2008. ISA 2008. International Conference on
DO  - 10.1109/ISA.2008.32
AB  - In the context of computer systems, an intrusion is generally considered to be a harmful endeavor to prevent others from legitimate use of that system, to obtain data which is not normally available to the intruder, or to plant data or disrupt data already existent on the machines. Traditionally intrusion detection has relied on two data sources: various log files which record user's activity, and network traffic which contains potential threats. This research presents a system which we call IDEA; the Intrusion DEtection Automata system. We utilize a third source of data for intrusion detection in the form of an instrumented process. Open source software is recompiled using a modified compiler we have created, and the resulting executable program generates the data as it runs. An external monitoring facility then checks the behavior of the program against known good execution paths. These paths are specified either using a domain specific language and hand-written rules, or by running the software in a learning mode and capturing the normal behavior for later comparison.
ER  - 

TY  - CONF
JO  - Computers, Networks, Systems and Industrial Engineering (CNSI), 2011 First ACIS/JNU International Conference on
TI  - Development of Integrated Insider Attack Detection System Using Intelligent Packet Filtering
T2  - Computers, Networks, Systems and Industrial Engineering (CNSI), 2011 First ACIS/JNU International Conference on
IS  - 
SN  - 
VO  - 
SP  - 65
EP  - 69
AU  - Jung-Sook Kim
Y1  - 23-25 May 2011
PY  - 2011
KW  - security of data
KW  - software packages
KW  - IT outsourcing
KW  - TCP tunneling
KW  - Telnet
KW  - administrator moral problems
KW  - gateway
KW  - integrated insider attack detection system
KW  - intelligent packet filtering
KW  - legacy servers
KW  - malicious purposes
KW  - multiple root accounts
KW  - packet logging
KW  - root accounts
KW  - software package
KW  - Engines
KW  - Filtering
KW  - Hardware
KW  - Home appliances
KW  - Program processors
KW  - Security
KW  - Servers
KW  - Integrated Insider Attack Detection
KW  - Intelligent agent
KW  - Packet Filtering
KW  - TCP tunneling
KW  - provisioning
VL  - 
JA  - Computers, Networks, Systems and Industrial Engineering (CNSI), 2011 First ACIS/JNU International Conference on
DO  - 10.1109/CNSI.2011.4
AB  - External threats to the cyber-infrastructure of an organization are constantly evolving. The greatest threat, however, is the problem of insiders who misuse their privileges for malicious purposes. These days, private information has often been leaked because of increased IT outsourcing, administrator's moral problems, multiple root accounts, and root accounts shared by many users, etc. Accordingly, organizations have employed insider attack detection systems to protect their critical information from break-ins by insider attack and hackers. In this paper, we developed an integrated insider attack detection system which was composed of a minimized hardware appliance and a software package using TCP tunneling. It could be configured as a gateway between users and the legacy servers in order to protect the important internal information in the legacy servers. And it could control the access of users on the servers, who were connected by Telnet or FTP, and would block the theft of confidential information using intelligent packet filtering. Also, it should provide an audit using the packet logging on the legacy servers.
ER  - 

TY  - JOUR
JO  - Internet Computing, IEEE
TI  - Tracing network attacks to their sources
T2  - Internet Computing, IEEE
IS  - 2
SN  - 1089-7801
VO  - 6
SP  - 20
EP  - 26
AU  - Baba, T.
AU  - Matsuda, S.
Y1  - Mar/Apr 2002
PY  - 2002
KW  - Internet
KW  - security of data
KW  - IP packets
KW  - IP traceback architecture
KW  - Internet
KW  - adjacent forwarding nodes
KW  - denial-of-service attacks
KW  - network attacks tracing
KW  - Computer crime
KW  - Computer networks
KW  - Computer security
KW  - IP networks
KW  - Information security
KW  - Internet
KW  - Intrusion detection
KW  - Protection
KW  - Prototypes
KW  - TCPIP
VL  - 6
JA  - Internet Computing, IEEE
DO  - 10.1109/4236.991439
AB  - An IP traceback architecture in which routers log data about packets and adjacent forwarding nodes lets us trace IP packets to their sources, even when the source IP address is forged
ER  - 

TY  - CONF
JO  - INFOCOM 2001. Twentieth Annual Joint Conference of the IEEE Computer and Communications Societies. Proceedings. IEEE
TI  - Secure and invisible data hiding in 2-color images
T2  - INFOCOM 2001. Twentieth Annual Joint Conference of the IEEE Computer and Communications Societies. Proceedings. IEEE
IS  - 
SN  - 0743-166X
VO  - 2
SP  - 887
EP  - 896 vol.2
AU  - Yu-Chee Tseng
AU  - Hsiang-Kuang Pan
Y1  - 2001
PY  - 2001
KW  - data encapsulation
KW  - image coding
KW  - image colour analysis
KW  - security of data
KW  - 2-color images
KW  - data hiding ratio
KW  - digital media
KW  - host binary image
KW  - image block
KW  - image coding
KW  - invisible data hiding
KW  - secure data hiding
KW  - steganography
KW  - Computer science
KW  - Cryptography
KW  - Data encapsulation
KW  - Data security
KW  - Image coding
KW  - Image processing
KW  - Information security
KW  - Pixel
KW  - Steganography
KW  - Terminology
VL  - 2
JA  - INFOCOM 2001. Twentieth Annual Joint Conference of the IEEE Computer and Communications Societies. Proceedings. IEEE
DO  - 10.1109/INFCOM.2001.916280
AB  - In an earlier paper, we propose a steganography scheme for hiding a piece of critical information in a host binary image. That scheme ensures that in each m&times;n image block of the host image, as many as [log<sub>2</sub>(mn+1)] bits can be hidden in the block by changing at most 2 bits in the block. As a sequel of that work, in this paper we propose a revised scheme that can maintain higher quality of the host image by sacrificing some data hiding space. The new scheme can still offer a good data hiding ratio. It ensures that for any bit that is modified in the host image, the bit is adjacent to another bit which has a value equal to the former's new value. Thus, the hiding effect is quite invisible
ER  - 

TY  - CONF
JO  - Security and Privacy, 2008. SP 2008. IEEE Symposium on
TI  - SybilLimit: A Near-Optimal Social Network Defense against Sybil Attacks
T2  - Security and Privacy, 2008. SP 2008. IEEE Symposium on
IS  - 
SN  - 1081-6011
VO  - 
SP  - 3
EP  - 17
AU  - Haifeng Yu
AU  - Gibbons, P.B.
AU  - Kaminsky, M.
AU  - Feng Xiao
Y1  - 18-22 May 2008
PY  - 2008
KW  - distributed processing
KW  - peer-to-peer computing
KW  - protocols
KW  - security of data
KW  - SybilGuard protocol
KW  - SybilLimit protocol
KW  - decentralized distributed system
KW  - fast mixing social network
KW  - malicious user
KW  - million-node system
KW  - multiple identity
KW  - near-optimal social network defense
KW  - peer-to-peer system
KW  - sybil attack
KW  - sybil nodes
KW  - Collaboration
KW  - Humans
KW  - Large-scale systems
KW  - National security
KW  - Peer to peer computing
KW  - Privacy
KW  - Protocols
KW  - Routing
KW  - Social network services
KW  - Voting
KW  - Sybil attack
KW  - SybilLimit
KW  - social networks
KW  - sybil identity
VL  - 
JA  - Security and Privacy, 2008. SP 2008. IEEE Symposium on
DO  - 10.1109/SP.2008.13
AB  - Decentralized distributed systems such as peer-to-peer systems are particularly vulnerable to sybil attacks, where a malicious user pretends to have multiple identities (called sybil nodes). Without a trusted central authority, defending against sybil attacks is quite challenging. Among the small number of decentralized approaches, our recent SybilGuard protocol [H. Yu et al., 2006] leverages a key insight on social networks to bound the number of sybil nodes accepted. Although its direction is promising, SybilGuard can allow a large number of sybil nodes to be accepted. Furthermore, SybilGuard assumes that social networks are fast mixing, which has never been confirmed in the real world. This paper presents the novel SybilLimit protocol that leverages the same insight as SybilGuard but offers dramatically improved and near-optimal guarantees. The number of sybil nodes accepted is reduced by a factor of ominus(radicn), or around 200 times in our experiments for a million-node system. We further prove that SybilLimit's guarantee is at most a log n factor away from optimal, when considering approaches based on fast-mixing social networks. Finally, based on three large-scale real-world social networks, we provide the first evidence that real-world social networks are indeed fast mixing. This validates the fundamental assumption behind SybilLimit's and SybilGuard's approach.
ER  - 

TY  - CONF
JO  - Information Assurance Workshop, 2004. Proceedings from the Fifth Annual IEEE SMC
TI  - NoSEBrEaK - attacking honeynets
T2  - Information Assurance Workshop, 2004. Proceedings from the Fifth Annual IEEE SMC
IS  - 
SN  - 
VO  - 
SP  - 123
EP  - 129
AU  - Dornseif, M.
AU  - Holz, T.
AU  - Klein, C.N.
Y1  - 10-11 June 2004
PY  - 2004
KW  - security of data
KW  - system monitoring
KW  - data security
KW  - honeynets
KW  - system monitoring
KW  - Communication channels
KW  - Condition monitoring
KW  - Cryptography
KW  - Information security
KW  - Kernel
KW  - Laboratories
KW  - Linux
KW  - Protection
KW  - Telecommunication traffic
KW  - Turning
VL  - 
JA  - Information Assurance Workshop, 2004. Proceedings from the Fifth Annual IEEE SMC
DO  - 10.1109/IAW.2004.1437807
AB  - It is usually assumed that honeynets are hard to detect and that attempts to detect or disable them can be unconditionally monitored. We scrutinize this assumption and demonstrate a method how a host in a honeynet can be completely controlled by an attacker without any substantial logging taking place.
ER  - 

TY  - CONF
JO  - Computational and Information Sciences (ICCIS), 2013 Fifth International Conference on
TI  - Research on Data Preprocessing Technology in Safety Equipment Linkage System
T2  - Computational and Information Sciences (ICCIS), 2013 Fifth International Conference on
IS  - 
SN  - 
VO  - 
SP  - 1713
EP  - 1716
AU  - Xiaorong Cheng
AU  - Hui Liu
Y1  - 21-23 June 2013
PY  - 2013
KW  - XML
KW  - security of data
KW  - XML language
KW  - data preprocessing efficiency
KW  - data preprocessing technology
KW  - extensible markup language
KW  - improved data cleaning method
KW  - linkage system data characteristics
KW  - log data format
KW  - safety equipment linkage system
KW  - Cleaning
KW  - Couplings
KW  - Data preprocessing
KW  - Databases
KW  - Safety
KW  - Security
KW  - XML
KW  - XML
KW  - cleaning data
KW  - data preprocessing
VL  - 
JA  - Computational and Information Sciences (ICCIS), 2013 Fifth International Conference on
DO  - 10.1109/ICCIS.2013.447
AB  - Data preprocessing plays an essential role in the process of safety equipment linkage system, directly influenced the quality and the results of the later analysis and decision-making. This paper describes the process of data preprocessing in detail, and according to the characteristics of the linkage system data, proposes an improved data cleaning method in order to improve the efficiency of data preprocessing. For each log data format is different, using XML language to describe of the log data, which provides a unified interface for safety equipment linkage system data access, and then solve the problem of heterogeneous data.
ER  - 

TY  - CONF
JO  - Dependable Systems and Networks, 2000. DSN 2000. Proceedings International Conference on
TI  - Benchmarking anomaly-based detection systems
T2  - Dependable Systems and Networks, 2000. DSN 2000. Proceedings International Conference on
IS  - 
SN  - 
VO  - 
SP  - 623
EP  - 630
AU  - Maxion, R.A.
AU  - Tan, K.M.C.
Y1  - 2000
PY  - 2000
KW  - calibration
KW  - failure analysis
KW  - security of data
KW  - software performance evaluation
KW  - anomaly-based detection systems
KW  - attacks
KW  - benchmarking
KW  - calibrated anomaly-injected data sets
KW  - computer security
KW  - data environment structure characterization
KW  - data logs
KW  - defects
KW  - dependability
KW  - detection performance
KW  - empirical methods
KW  - environmental regularity
KW  - faults
KW  - intrusion detection
KW  - performance
KW  - probabilistic detection algorithms
KW  - system behaviour perturbations
KW  - Application software
KW  - Benchmark testing
KW  - Bridges
KW  - Computer science
KW  - Computer security
KW  - Detectors
KW  - Electrocardiography
KW  - Fault detection
KW  - Intrusion detection
KW  - Plasma applications
VL  - 
JA  - Dependable Systems and Networks, 2000. DSN 2000. Proceedings International Conference on
DO  - 10.1109/ICDSN.2000.857599
AB  - Anomaly detection is a key element of intrusion detection and other detection systems in which perturbations of normal behavior suggest the presence of intentionally or unintentionally induced attacks, faults, defects, etc. Because most anomaly detectors are based on probabilistic algorithms that exploit the intrinsic structure (or regularity) embedded in data logs, a fundamental question is whether or not such structure influences detection performance. If detector performance is indeed a function of environmental regularity, it would be critical to match detectors to environmental characteristics. In intrusion-detection settings, however, this is not done, possibly because such characteristics are not easily ascertained. This paper introduces a metric for characterizing structure in data environments, and tests the hypothesis that intrinsic structure influences probabilistic detection. In a series of experiments, an anomaly detection algorithm was applied to a benchmark suite of 165 carefully calibrated, anomaly-injected data sets of varying structure. The results showed performance differences of as much as an order of magnitude, indicating that current approaches to anomaly detection may not be universally dependable
ER  - 

TY  - CONF
JO  - DARPA Information Survivability Conference &amp; Exposition II, 2001. DISCEX '01. Proceedings
TI  - Assuring the safety of opening email attachments
T2  - DARPA Information Survivability Conference &amp; Exposition II, 2001. DISCEX '01. Proceedings
IS  - 
SN  - 
VO  - 2
SP  - 257
EP  - 262 vol.2
AU  - Balzer, R.
Y1  - 2001
PY  - 2001
KW  - electronic mail
KW  - security of data
KW  - data security
KW  - email attachments
KW  - file system
KW  - inter-host communication
KW  - process spawning
KW  - process-specific rules
KW  - runtime behavior monitoring
KW  - safety
KW  - system registry
KW  - wrapper
KW  - Animation
KW  - Control systems
KW  - File systems
KW  - Middleware
KW  - Monitoring
KW  - Operating systems
KW  - Runtime
KW  - Safety
KW  - Security
KW  - Switches
VL  - 2
JA  - DARPA Information Survivability Conference &amp; Exposition II, 2001. DISCEX '01. Proceedings
DO  - 10.1109/DISCEX.2001.932177
AB  - A wrapper has been developed that monitors the runtime behavior of opened email attachments to ensure that these processes do not do anything harmful. The wrapper detects violations of process-specific rules establishing the acceptable (and safe) behavior of these processes relative to four resources: the file system, the system registry, inter-host communication, and process spawning. The wrapper can determine whether an operation is being performed by the native application or by active content within the email attachment and applies a different (and presumably more stringent) set of rules to the latter. When attempted violations are detected, the user is notified and informed of the severity of the violation. The user determines whether to allow or prohibit the offending operation. The violation, the user's response, and the initiating email and attachment-obtained from the email client-are logged
ER  - 

TY  - CONF
JO  - High Performance Computing and Communications, 2009. HPCC '09. 11th IEEE International Conference on
TI  - A Streaming Intrusion Detection System for Grid Computing Environments
T2  - High Performance Computing and Communications, 2009. HPCC '09. 11th IEEE International Conference on
IS  - 
SN  - 
VO  - 
SP  - 44
EP  - 51
AU  - Smith, M.
AU  - Schwarzer, F.
AU  - Harbach, M.
AU  - Noll, T.
AU  - Freisleben, B.
Y1  - 25-27 June 2009
PY  - 2009
KW  - database management systems
KW  - grid computing
KW  - security of data
KW  - grid computing environment
KW  - large scale system
KW  - single host database
KW  - streaming database intrusion detection system
KW  - streaming detection logic
KW  - temporal attack data
KW  - traditional log-file
KW  - Computer architecture
KW  - Computer science
KW  - Databases
KW  - Grid computing
KW  - High performance computing
KW  - Intrusion detection
KW  - Mathematics
KW  - Reconnaissance
KW  - Security
KW  - Telecommunication traffic
KW  - Grid
KW  - Intrustion Detection
KW  - PIPES
KW  - Streaming Database
VL  - 
JA  - High Performance Computing and Communications, 2009. HPCC '09. 11th IEEE International Conference on
DO  - 10.1109/HPCC.2009.99
AB  - In this paper, a novel architecture for a streaming intrusion detection system for Grid computing environments is presented. Detection mechanisms based on traditional log-files or single host databases are replaced by a streaming database approach. The streaming architecture allows processing of temporal attack data across multiple sites and offers the potential for performance benefits in large scale systems, since data is processed during its natural flow and only stored as long as necessary for analysis. Two cross-site example attacks in a Grid environment and the streaming detection logic for these attacks are presented to illustrate the approach. Experimental results of a prototypical implementation are presented.
ER  - 

TY  - CONF
JO  - Intelligence and Security Informatics, 2007 IEEE
TI  - Using Digital Chains of Custody on Constrained Devices to Verify Evidence
T2  - Intelligence and Security Informatics, 2007 IEEE
IS  - 
SN  - 
VO  - 
SP  - 8
EP  - 15
AU  - Bradford, P.G.
AU  - Ray, D.A.
Y1  - 23-24 May 2007
PY  - 2007
KW  - distributed algorithms
KW  - security of data
KW  - constrained device
KW  - custody digital chain
KW  - digital chain authentication
KW  - digital chain validation
KW  - digital data documentation
KW  - hash chain traversal algorithm
KW  - hash element
KW  - online algorithm
KW  - Computer science
KW  - Digital forensics
KW  - Hardware
VL  - 
JA  - Intelligence and Security Informatics, 2007 IEEE
DO  - 10.1109/ISI.2007.379522
AB  - A digital chain of custody is an account documenting digital data at a particular place and time. This paper gives a method of validating and authenticating a digital chain of custody using an algorithm by Jakobsson and by providing a new algorithm that compliments Jakobsson's algorithm. Our method assumes specialized hardware. The physical hardware is assumed to be memory and processor constrained. Our new algorithm is an online algorithm that generates a hash chain of n hash elements without knowledge of n before it starts. At the same time, as n grows our new algorithm stores only the [log n] pebbles which are inputs for Jakobsson's amortized hash chain traversal algorithm. Jakobsson's algorithm is then used to validate hash elements and thus the digital chain of custody. The complimentary algorithm presented here is used to generate and store the hash chain. The compact representation used by these algorithms is useful to store a large digital chain of custody on a small and constrained device. Our proposed method allows the use of constrained devices to validate complex and ephemeral data such as shipping manifests and handling logs.
ER  - 

TY  - CONF
JO  - Privacy, Security, Risk and Trust (PASSAT) and 2011 IEEE Third Inernational Conference on Social Computing (SocialCom), 2011 IEEE Third International Conference on
TI  - Insider Threat Detection Using Stream Mining and Graph Mining
T2  - Privacy, Security, Risk and Trust (PASSAT) and 2011 IEEE Third Inernational Conference on Social Computing (SocialCom), 2011 IEEE Third International Conference on
IS  - 
SN  - 
VO  - 
SP  - 1102
EP  - 1110
AU  - Parveen, P.
AU  - Evans, J.
AU  - Thuraisingham, Bhavani
AU  - Hamlen, K.W.
AU  - Khan, L.
Y1  - 9-11 Oct. 2011
PY  - 2011
KW  - data mining
KW  - graph theory
KW  - pattern classification
KW  - security of data
KW  - unsupervised learning
KW  - classification models
KW  - ensemble-based stream mining
KW  - graph mining
KW  - graph-based anomaly detection
KW  - insider threat detection
KW  - malicious insider activity
KW  - single-model methods
KW  - unsupervised learning
KW  - Conferences
KW  - Privacy
KW  - Security
KW  - Social network services
KW  - anomaly detection
KW  - ensemble
KW  - graph-based
KW  - insider threat
VL  - 
JA  - Privacy, Security, Risk and Trust (PASSAT) and 2011 IEEE Third Inernational Conference on Social Computing (SocialCom), 2011 IEEE Third International Conference on
DO  - 10.1109/PASSAT/SocialCom.2011.211
AB  - Evidence of malicious insider activity is often buried within large data streams, such as system logs accumulated over months or years. Ensemble-based stream mining leverages multiple classification models to achieve highly accurate anomaly detection in such streams even when the stream is unbounded, evolving, and unlabeled. This makes the approach effective for identifying insider threats who attempt to conceal their activities by varying their behaviors over time. This paper applies ensemble-based stream mining, unsupervised learning, and graph-based anomaly detection to the problem of insider threat detection, demonstrating that the ensemble-based approach is significantly more effective than traditional single-model methods.
ER  - 

TY  - CONF
JO  - Information Assurance Workshop, 2006 IEEE
TI  - Investigating the Effect of an Attack on a Distributed Database
T2  - Information Assurance Workshop, 2006 IEEE
IS  - 
SN  - 
VO  - 
SP  - 312
EP  - 317
AU  - Samara, R.
AU  - Panda, B.
Y1  - 21-23 June 2006
PY  - 2006
KW  - distributed databases
KW  - security of data
KW  - transaction processing
KW  - damage assessment
KW  - denial-of-service
KW  - distributed database
KW  - intricate transaction relationships
KW  - two-pass algorithm
KW  - Availability
KW  - Computer crime
KW  - Data security
KW  - Database systems
KW  - Distributed databases
KW  - Information retrieval
KW  - Performance evaluation
KW  - Protection
KW  - Real time systems
KW  - Transaction databases
VL  - 
JA  - Information Assurance Workshop, 2006 IEEE
DO  - 10.1109/IAW.2006.1652111
AB  - After an attack on a database system, evaluation of damage must be performed as soon the attack is identified. Otherwise, the initial damage would spread to other parts of the database via valid transactions, consequently resulting in denial-of-service. Damage assessment in a distributed database system is a complicated task due to intricate transaction relationships among distributed sites. In these systems, when any sub-transaction reads a damaged data at any site, the entire transaction of which the sub-transaction is a part, is considered affected by the damage. Hence, the data items updated by that transaction irrespective of sites are also considered damaged. This research focuses on damage assessment procedure for distributed database systems and uses a two-pass algorithm to obtain the final list of affected data items. The advantages of this method are: (1) the process is fully distributed in the sense that every site would execute the same algorithm, (2) the amount of data to be exchanged between the sites is minimized to the list of affected items at each site instead of the entire log, and (3) the local damage assessors can be executed in parallel at their respective sites
ER  - 

TY  - CONF
JO  - Cybernetics and Intelligent Systems (CIS), 2010 IEEE Conference on
TI  - Semi-supervised classification for intrusion Detection System in networks
T2  - Cybernetics and Intelligent Systems (CIS), 2010 IEEE Conference on
IS  - 
SN  - 
VO  - 
SP  - 120
EP  - 125
AU  - Chaudhari, N.S.
AU  - Tiwari, A.
AU  - Thakar, U.
AU  - Thomas, J.
Y1  - 28-30 June 2010
PY  - 2010
KW  - pattern classification
KW  - security of data
KW  - support vector machines
KW  - intrusion detection system
KW  - malicious data
KW  - semi-supervised classification
KW  - spherical decision boundaries
KW  - support vector machine
KW  - Clustering algorithms
KW  - Computer networks
KW  - Databases
KW  - Information security
KW  - Intrusion detection
KW  - Kernel
KW  - Monitoring
KW  - Support vector machine classification
KW  - Support vector machines
KW  - Telecommunication traffic
KW  - IDS
KW  - Kernel Method
KW  - Lagrange multipliers
KW  - Quadratic programming
KW  - Semi-Supervised classification
VL  - 
JA  - Cybernetics and Intelligent Systems (CIS), 2010 IEEE Conference on
DO  - 10.1109/ICCIS.2010.5518571
AB  - We propose a semi supervised classifier for intrusion detection. In our approach, we classify the data entering the computer network. To achieve this, we start with two broad classes of data namely, malicious data and good data. We use Support vector machine based classifier with spherical decision boundaries to classify a chosen subset of malicious data taken as training samples. In the Intrusion Detection System (IDS) database, all data identified as malicious data according to our classifier is included as signature (of attack). Using our classifier for testing the out-of-sample data samples, we observe that the accuracy of the system is 72% for web log data.
ER  - 

TY  - CONF
JO  - Advances in Social Networks Analysis and Mining (ASONAM), 2011 International Conference on
TI  - SPOT 1.0: Scoring Suspicious Profiles on Twitter
T2  - Advances in Social Networks Analysis and Mining (ASONAM), 2011 International Conference on
IS  - 
SN  - 
VO  - 
SP  - 377
EP  - 381
AU  - Perez, C.
AU  - Lemercier, M.
AU  - Birregah, B.
AU  - Corpel, A.
Y1  - 25-27 July 2011
PY  - 2011
KW  - security of data
KW  - social networking (online)
KW  - SPOT 1.0
KW  - data theft
KW  - identity theft
KW  - microblogging platform
KW  - scoring suspicious profiles on twitter
KW  - social network analysis
KW  - three-dimensional indicator
KW  - Databases
KW  - Equations
KW  - Security
KW  - Support vector machines
KW  - Twitter
KW  - Unsolicited electronic mail
KW  - Twitter
KW  - aggressiveness
KW  - level of danger
KW  - social network analysis
KW  - support vector machine
KW  - suspicious profiles
KW  - visibility
VL  - 
JA  - Advances in Social Networks Analysis and Mining (ASONAM), 2011 International Conference on
DO  - 10.1109/ASONAM.2011.63
AB  - Everyday more than fifty million messages are generated by about two hundred million profiles on Twitter. Some users attempt to exploit the success of this micro logging platform and its relative freedom to perform malicious actions that can lead to identity or data theft. This work aims to propose a framework to assess suspicious behavior on Twitter. We present a tool developed for Scoring Suspicious Profiles On Twitter (SPOT 1.0) through a three-dimensional indicator that involves the degree of aggressiveness, the visibility and the level of danger.
ER  - 

TY  - CONF
JO  - Applications and the Internet (SAINT), 2010 10th IEEE/IPSJ International Symposium on
TI  - Resisting Sybil Attack By Social Network and Network Clustering
T2  - Applications and the Internet (SAINT), 2010 10th IEEE/IPSJ International Symposium on
IS  - 
SN  - 
VO  - 
SP  - 15
EP  - 21
AU  - Ling Xu
AU  - Chainan, S.
AU  - Takizawa, H.
AU  - Kobayashi, H.
Y1  - 19-23 July 2010
PY  - 2010
KW  - computational complexity
KW  - distributed algorithms
KW  - peer-to-peer computing
KW  - security of data
KW  - social networking (online)
KW  - Sybil attacks
KW  - Sybil peers
KW  - Sybil resisting network clustering
KW  - distributed algorithm
KW  - malicious user
KW  - peer to peer systems
KW  - social network model
KW  - Clustering algorithms
KW  - Data models
KW  - Image edge detection
KW  - Peer to peer computing
KW  - Protocols
KW  - Security
KW  - Social network services
KW  - p2p
KW  - social network
KW  - sybil
VL  - 
JA  - Applications and the Internet (SAINT), 2010 10th IEEE/IPSJ International Symposium on
DO  - 10.1109/SAINT.2010.32
AB  - Peer to peer (P2P) systems are extremely vulnerable to Sybil attacks, in which a malicious user controls a large number of Sybil peers to collude to break the system laws. This paper proposes a distributed algorithm, named Sybil Resisting Network Clustering (SRNC), to resist the Sybil attack by preventing honest peers from communicating with Sybil Peers. SRNC is based on a social network model. In this model, honest peers and Sybil peers can be largely classified into two clusters, connected by a small number of edges, called attack edges. SRNC tries to explicitly detect attack edges, and then prohibits the communication over the detected edges. The performance of SRNC is evaluated by theoretical analysis and simulations. In particular, SRNC ensures theoretically that honest peers totally accept O(|AE|) Sybil peers. This is a O(log(n)) times improvement over SybilLimit, one of the conventional representative Sybil resisting algorithms, where n is the number of peers and |AE| is the number of attack edges in the system. The performance is then evaluated by simulations on data sets of real world social networks.
ER  - 

TY  - CONF
JO  - Wireless Communications, Vehicular Technology, Information Theory and Aerospace & Electronic Systems (VITAE), 2014 4th International Conference on
TI  - The un-polarized bit-channels in the wiretap polar coding scheme
T2  - Wireless Communications, Vehicular Technology, Information Theory and Aerospace & Electronic Systems (VITAE), 2014 4th International Conference on
IS  - 
SN  - 
VO  - 
SP  - 1
EP  - 5
AU  - Mirghasemi, H.
AU  - Belfiore, J.-C.
Y1  - 11-14 May 2014
PY  - 2014
KW  - encoding
KW  - security of data
KW  - Z-parameter
KW  - channel use number
KW  - unpolarized bit channel
KW  - wiretap channel
KW  - wiretap polar coding scheme
KW  - Decoding
KW  - Encoding
KW  - Mutual information
KW  - Noise measurement
KW  - Reliability
KW  - Security
KW  - Vectors
VL  - 
JA  - Wireless Communications, Vehicular Technology, Information Theory and Aerospace & Electronic Systems (VITAE), 2014 4th International Conference on
DO  - 10.1109/VITAE.2014.6934465
AB  - Polar coding theorems state that as the number of channel use, n, tends to infinity, the fraction of un-polarized bit-channels (the bit-channels whose Z parameters are in the interval (&#x03B4;(n), 1- &#x03B4; (n)), tends to zero. Consider two BEC channels W(z<sub>1</sub>) and W(z<sub>2</sub>). Motivated by polar coding scheme proposed for the wiretap channel, we investigate the number of bit-channels which are simultaneously un-polarized for both of W(z<sub>1</sub>) and W(z<sub>2</sub>). We show that for finite values of n, there is a considerable regime of (z<sub>1</sub>, Z<sub>2</sub>) where the set of (joint)un-polarized bit-channels is empty. We also show that for &#x03B3; &#x2264; 1/2 and &#x03B4; (n) = 2<sup>-n&#x03B3;</sup>, the number of un-polarized bit-channels is lower bounded by 2&#x03B3; log (n).
ER  - 

TY  - CONF
JO  - Cyberworlds (CW), 2011 International Conference on
TI  - An Entropy and Volume-Based Approach for Identifying Malicious Activities in Honeynet Traffic
T2  - Cyberworlds (CW), 2011 International Conference on
IS  - 
SN  - 
VO  - 
SP  - 23
EP  - 30
AU  - Sqalli, M.H.
AU  - Firdous, S.N.
AU  - Baig, Z.
AU  - Azzedin, F.
Y1  - 4-6 Oct. 2011
PY  - 2011
KW  - security of data
KW  - cybersecurity
KW  - entropy distributions
KW  - feature-based schemes
KW  - hacker community
KW  - honeynet traffic
KW  - malicious activity identification
KW  - volume-based approach
KW  - Educational institutions
KW  - Entropy
KW  - Feature extraction
KW  - IP networks
KW  - Organizations
KW  - Payloads
KW  - Security
KW  - Anomaly Detection
KW  - Cybersecurity
KW  - Entropy
KW  - Honeynet
VL  - 
JA  - Cyberworlds (CW), 2011 International Conference on
DO  - 10.1109/CW.2011.35
AB  - Honeynets are an increasingly popular choice deployed by organizations to lure attackers into a trap network, for collection and analysis of unauthorized network activity. A Honeynet captures substantial amount of data and logs for analysis in order to identify malicious activities perpetrated by the hacker community. The analysis of this large amount of data is a challenging task. Through this paper, we propose a technique based on the entropy and volume thresholds of selected network features to efficiently analyze Honeynet data, and identify malicious activities. Our technique consists of both feature-based and volume-based schemes to identify malicious activities in the Honeynet traffic. Through deployment of our proposed approach, a detailed analysis of various traffic features is conducted and the most appropriate features for Honeynet traffic are thereupon selected. The anomalies are identified using entropy distributions and volume distributions, along with their corresponding threshold levels. The proposed scheme proves to be effective in identifying most types of anomalies seen in Honeynet traffic.
ER  - 

TY  - CONF
JO  - Frontier of Computer Science and Technology, 2008. FCST '08. Japan-China Joint Workshop on
TI  - Tracking and Repairing Damaged Databases Using Before Image Table
T2  - Frontier of Computer Science and Technology, 2008. FCST '08. Japan-China Joint Workshop on
IS  - 
SN  - 
VO  - 
SP  - 36
EP  - 41
AU  - Meiyi Xie
AU  - Hong Zhu
AU  - Yucai Feng
AU  - Guanrong Hu
Y1  - 27-28 Dec. 2008
PY  - 2008
KW  - database management systems
KW  - security of data
KW  - system recovery
KW  - TPC-C benchmark
KW  - before image table
KW  - damage assessment approaches
KW  - damaged database repairing
KW  - extended recovery model
KW  - malicious attacks
KW  - post-intrusion database
KW  - Arm
KW  - Bismuth
KW  - Computer science
KW  - Data analysis
KW  - Data security
KW  - Database systems
KW  - Image analysis
KW  - Image databases
KW  - Investments
KW  - Transaction databases
KW  - BI table
KW  - damage assessment
KW  - damage spreading
KW  - intrusion tolerant
VL  - 
JA  - Frontier of Computer Science and Technology, 2008. FCST '08. Japan-China Joint Workshop on
DO  - 10.1109/FCST.2008.25
AB  - Traditional damage assessment approaches can only locate damage caused by reading corrupted data in a post-intrusion database. This paper indicates another kind of damage spreading pattern characterized by omitting maliciously deleted data, which was not considered in previous studies. An extended recovery model arms at this problem is presented and a novel approach based on the model is proposed to recover a damaged database from malicious attacks. This approach can track the damage spreading more completely by maintaining before image tables (BI tables) in databases and analyzing transactions' potential-read to the BI tables. BI tables are also used for damage repair without accessing database logs. Experimental evaluation of the overhead of this method based upon TPC-C benchmark is also presented.
ER  - 

TY  - CONF
JO  - Fuzzy Systems Conference, 2007. FUZZ-IEEE 2007. IEEE International
TI  - Detecting Abnormal Changes in E-mail Traffic Using Hierarchical Fuzzy Systems
T2  - Fuzzy Systems Conference, 2007. FUZZ-IEEE 2007. IEEE International
IS  - 
SN  - 1098-7584
VO  - 
SP  - 1
EP  - 6
AU  - Lim, M.J.-H.
AU  - Negnevitsky, M.
AU  - Hartnett, J.
Y1  - 23-26 July 2007
PY  - 2007
KW  - electronic mail
KW  - information retrieval
KW  - law administration
KW  - security of data
KW  - terrorism
KW  - Enron e-mail corpus
KW  - e-mail traffic abnormal change detection
KW  - forensic tool
KW  - hierarchical fuzzy system architecture
KW  - information extraction
KW  - law enforcement
KW  - terrorist attack
KW  - Australia
KW  - Data analysis
KW  - Data mining
KW  - Digital forensics
KW  - Electronic mail
KW  - Fuzzy systems
KW  - Information analysis
KW  - Law enforcement
KW  - Mobile communication
KW  - Terrorism
VL  - 
JA  - Fuzzy Systems Conference, 2007. FUZZ-IEEE 2007. IEEE International
DO  - 10.1109/FUZZY.2007.4295556
AB  - E-mail traffic analysis is an area of work that focuses on extracting information about the behaviour of e-mail users based on the sender, receiver, and date/time information taken from the header section of e-mail messages. Such work has applications for law enforcement where investigators and analysts require techniques to assist them with finding unusual or suspicious patterns from large amounts of communication log data. This paper describes work using hierarchical fuzzy systems to detect abnormal changes in e-mail traffic behaviour, through the fusion of e-mail traffic behaviour measurements. The paper focuses on the use of three different hierarchical fuzzy system architectures, to determine the effect that input variable groupings have on the abnormality ratings given to the communication links of suspect e-mail accounts. The case study demonstrates the use of the three hierarchical fuzzy system architectures for analysing suspect e-mail accounts belonging to the Enron e-mail corpus.
ER  - 

TY  - CONF
JO  - INFOCOM, 2010 Proceedings IEEE
TI  - Predictive Blacklisting as an Implicit Recommendation System
T2  - INFOCOM, 2010 Proceedings IEEE
IS  - 
SN  - 0743-166X
VO  - 
SP  - 1
EP  - 9
AU  - Soldo, F.
AU  - Anh Le
AU  - Markopoulou, A.
Y1  - 14-19 March 2010
PY  - 2010
KW  - Internet
KW  - recommender systems
KW  - security of data
KW  - time series
KW  - Internet
KW  - Netflix competition
KW  - attacker-victims interactions
KW  - implicit recommendation system
KW  - malicious traffic
KW  - multilevel prediction model
KW  - neighborhood models
KW  - predictive collaborative blacklisting
KW  - prolific attack sources
KW  - time series
KW  - Collaboration
KW  - Communications Society
KW  - History
KW  - Internet
KW  - Intrusion detection
KW  - Predictive models
KW  - Robustness
KW  - Security
KW  - Telecommunication traffic
KW  - Traffic control
VL  - 
JA  - INFOCOM, 2010 Proceedings IEEE
DO  - 10.1109/INFCOM.2010.5461982
AB  - A widely used defense practice against malicious traffic on the Internet is through blacklists: lists of prolific attack sources are compiled and shared. The goal of blacklists is to predict and block future attack sources. Existing blacklisting techniques have focused on the most prolific attack sources and, more recently, on collaborative blacklisting. In this paper, we formulate the problem of forecasting attack sources (also referred to as "predictive blacklisting") based on shared attack logs, as an implicit recommendation system. We compare the performance of existing approaches against the upper bound for prediction and we demonstrate that there is much room for improvement. Inspired by the recent NetFlix competition, we propose a multi-level collaborative filtering model that is adjusted and tuned specifically for the attack forecasting problem. Our model captures and combines various factors namely: attacker-victim history (using time-series) and attackers and/or victims interactions (using neighborhood models). We evaluate our combined method on one month of logs from Dshield.org and demonstrate that it improves significantly the prediction rate over state-of-the-art methods as well as the robustness against poisoning attacks.
ER  - 

TY  - CONF
JO  - Multimedia and Information Technology (MMIT), 2010 Second International Conference on
TI  - Alert Correlation Model Design Based on Self-regulate
T2  - Multimedia and Information Technology (MMIT), 2010 Second International Conference on
IS  - 
SN  - 
VO  - 1
SP  - 266
EP  - 269
AU  - Li Yang
AU  - Dong Xinfa
Y1  - 24-25 April 2010
PY  - 2010
KW  - correlation methods
KW  - data mining
KW  - knowledge acquisition
KW  - security of data
KW  - Apriori algorithm
KW  - active protection
KW  - alert correlation model
KW  - attack detection
KW  - attack forecasting
KW  - data mining technology
KW  - false alert rate
KW  - intrusion behaviors
KW  - intrusion detection system
KW  - intrusion knowledge
KW  - multistep attack
KW  - network intrusions
KW  - omission rate
KW  - passive detection
KW  - self-regulate
KW  - Association rules
KW  - Correlation
KW  - Data mining
KW  - Electronic mail
KW  - Information analysis
KW  - Information security
KW  - Information technology
KW  - Intrusion detection
KW  - Paper technology
KW  - Protection
VL  - 1
JA  - Multimedia and Information Technology (MMIT), 2010 Second International Conference on
DO  - 10.1109/MMIT.2010.60
AB  - The multi-step attack is one of the primary forms of the current network intrusions. How to detect these attacks is an important aspect of IDS (Intrusion Detection System) research. The correlation research in intrusion detection performs mainly on the following aspects: reducing the false alert rate and omission rate; detecting unknown attacks; attack forecasting. Especially the development of the third point perhaps improves the passive detection to the active protection. Through the study on patterns of the multi-step attack, a model of alert correlation which is based on self-regulate is designed. This paper describes the definition and classification of alert correlation. Also it introduces the association rules. To improve efficiency of IDS, the paper applies data mining technology to IDS In the paper we present a method of how to acquire the intrusion knowledge from the logs and detect the intrusion behaviors based on the improved Apriori algorithm.
ER  - 

TY  - CONF
JO  - Machine Learning and Cybernetics, 2005. Proceedings of 2005 International Conference on
TI  - A real-time architecture for NIDS based on sequence analysis
T2  - Machine Learning and Cybernetics, 2005. Proceedings of 2005 International Conference on
IS  - 
SN  - 
VO  - 3
SP  - 1893
EP  - 1896 Vol. 3
AU  - Qing-Hua Liu
AU  - Feng Zhaoi
AU  - Feng Zhao
Y1  - 18-21 Aug. 2005
PY  - 2005
KW  - computer networks
KW  - data mining
KW  - knowledge based systems
KW  - real-time systems
KW  - security of data
KW  - customer demands
KW  - data mining
KW  - historical network data
KW  - intelligent network intrusion detection systems
KW  - intrusion discovery
KW  - multidimensional item set
KW  - network data stream gathering
KW  - network events
KW  - network stream environment
KW  - real-time architecture
KW  - sequence analysis
KW  - sequence mining
KW  - sliding window
KW  - system logs
KW  - Data engineering
KW  - Data mining
KW  - Data security
KW  - Databases
KW  - Decision support systems
KW  - Intrusion detection
KW  - Multidimensional systems
KW  - Performance analysis
KW  - Real time systems
KW  - Windows
KW  - IDS
KW  - intrusion
KW  - real time
KW  - sequence
KW  - sliding window
VL  - 3
JA  - Machine Learning and Cybernetics, 2005. Proceedings of 2005 International Conference on
DO  - 10.1109/ICMLC.2005.1527254
AB  - Due to customers' demands, network intrusion detection systems (NIDS) are required more real time. Since traditional intelligent NIDS are constructed on the basis of historical network data and system logs, they are expensive and not real time in a network stream environment. This paper presents an improved real time model that based on sequence mining to accelerate the accuracy and efficiency. In this paper, multidimensional item set is used to describes network events, sliding window is used to gather network data stream, and sequence mining algorithms are applied to discover intrusions from normal network stream. Analysis and study on this model indicate that it provide a more accurate and efficient way to building real-time NIDS.
ER  - 

TY  - CONF
JO  - Fuzzy Systems, 2003. FUZZ '03. The 12th IEEE International Conference on
TI  - Classification of anomalous traces of privileged and parallel programs by neural networks
T2  - Fuzzy Systems, 2003. FUZZ '03. The 12th IEEE International Conference on
IS  - 
SN  - 
VO  - 2
SP  - 1225
EP  - 1230 vol.2
AU  - Zhen Liu
AU  - Bridges, S.M.
AU  - Vaughn, R.B.
Y1  - 25-28 May 2003
PY  - 2003
KW  - neural nets
KW  - parallel programming
KW  - security of data
KW  - anomalies
KW  - anomalous traces
KW  - connection based intrusion detection
KW  - neural networks
KW  - parallel programs
KW  - privileged programs
KW  - process based intrusion detection
KW  - system call logs
KW  - user based intrusion detection
KW  - Bridges
KW  - Computer crime
KW  - Computer science
KW  - Computer security
KW  - Computerized monitoring
KW  - Electronic commerce
KW  - Intrusion detection
KW  - Neural networks
KW  - Operating systems
KW  - Pattern recognition
VL  - 2
JA  - Fuzzy Systems, 2003. FUZZ '03. The 12th IEEE International Conference on
DO  - 10.1109/FUZZ.2003.1206606
AB  - The focus of intrusion detection has recently shifted from user-based and connection-based to process-based intrusion detection. Substantial research has been done in the analysis of system call logs using different methods including neural networks. Detection is based on the classification of short sequences as anomalous or normal. The classification of interest, however, is the status of the program trace, not just the short sequences. In this paper we report the results of a comparative study of three different methods for on-line classification of program traces based detection of anomalies in sequences of system calls by neural networks. These results demonstrate that methods that use information about the locality of anomalies are more effective than those that only look at the number of anomalies.
ER  - 

TY  - CONF
JO  - Systems and Informatics (ICSAI), 2012 International Conference on
TI  - Research on transaction dependency mechanism of self-healing database system
T2  - Systems and Informatics (ICSAI), 2012 International Conference on
IS  - 
SN  - 
VO  - 
SP  - 2357
EP  - 2360
AU  - Xiaoling Xia
AU  - Qingyu Ji
AU  - Jiajin Le
Y1  - 19-20 May 2012
PY  - 2012
KW  - database management systems
KW  - security of data
KW  - transaction processing
KW  - UTS
KW  - intertransaction reverse-dependency log
KW  - malicious attacks
KW  - rollback
KW  - selective recovery
KW  - selfhealing database system
KW  - transaction dependency mechanism
KW  - transaction operation log
KW  - undo transaction set
KW  - Computer science
KW  - Computers
KW  - Database systems
KW  - History
KW  - Intrusion detection
KW  - Selective recovery
KW  - Self-healing
KW  - before image data
KW  - suspect transaction
KW  - transaction reverse-dependency
VL  - 
JA  - Systems and Informatics (ICSAI), 2012 International Conference on
DO  - 10.1109/ICSAI.2012.6223528
AB  - Transaction Dependency is the base of the self-healing database system to do the selective recovery after malicious attacks. According to the dependency of transactions, self-healing database system needs undo only suspect transactions in the transaction log in order to reduce the impact of suspect transactions as much as possible. By analyzing the existing mechanism of logging, this paper presents a new transaction dependency mechanism which contains inter-transaction reverse-dependency log, tracing the reverse dependency of all the transactions, and transaction operation log, recording image dates of each transaction to rollback. The algorithm of UTS (Undo Transaction Set) is also presented. At last, we analyzed the performance and space overhead of this method compared with other methods.
ER  - 

TY  - CONF
JO  - Database Engineering and Applications Symposium, 2003. Proceedings. Seventh International
TI  - Identification of malicious transactions in database systems
T2  - Database Engineering and Applications Symposium, 2003. Proceedings. Seventh International
IS  - 
SN  - 1098-8068
VO  - 
SP  - 329
EP  - 335
AU  - Yi Hu
AU  - Panda, B.
Y1  - 16-18 July 2003
PY  - 2003
KW  - Petri nets
KW  - database management systems
KW  - security of data
KW  - transaction processing
KW  - DBMS
KW  - IDS system
KW  - Petri nets
KW  - anomaly activity detection
KW  - data dependency relationship
KW  - data item update
KW  - data update pattern modeling
KW  - database intrusion detection
KW  - database log
KW  - database management system
KW  - database system
KW  - information system
KW  - intrusion detection system
KW  - malicious transaction identification
KW  - misuse detection
KW  - post-write set
KW  - prewrite set
KW  - read set
KW  - system security
KW  - user transaction
KW  - Artificial intelligence
KW  - Computer science
KW  - Data engineering
KW  - Database systems
KW  - Information systems
KW  - Intrusion detection
KW  - Operating systems
KW  - Spatial databases
KW  - Transaction databases
KW  - USA Councils
VL  - 
JA  - Database Engineering and Applications Symposium, 2003. Proceedings. Seventh International
DO  - 10.1109/IDEAS.2003.1214946
AB  - Existing host-based intrusion detection systems (IDSs) use the operating system log or the application log to detect misuse or anomaly activities. These methods are not sufficient for detecting intrusion in database systems. In this paper, we describe a method for database intrusion detection by using data dependency relationships. Typically before a data item is updated in the database some other data items are read or written. And after the update other data items may also be written. These data items read or written in the course of updating one data item construct the read set, pre-write set, and the post-write set for this data item. The proposed method identifies malicious transactions by comparing these sets with data items read or written in user transactions. We have provided mechanisms for finding data dependency relationships among transactions and use Petri nets to model normal data update patterns at user task level. Using this method we ascertain more hidden anomalies in the database log.
ER  - 

TY  - CONF
JO  - Distributed Computing Systems Workshops (ICDCSW), 2013 IEEE 33rd International Conference on
TI  - Secure Cache Provision: Provable DDOS Prevention for Randomly Partitioned Services with Replication
T2  - Distributed Computing Systems Workshops (ICDCSW), 2013 IEEE 33rd International Conference on
IS  - 
SN  - 
VO  - 
SP  - 58
EP  - 63
AU  - Weibo Chu
AU  - Xiaohong Guan
AU  - Lui, J.C.S.
AU  - Zhongmin Cai
AU  - Xiaohong Shi
Y1  - 8-11 July 2013
PY  - 2013
KW  - cache storage
KW  - security of data
KW  - back-end nodes
KW  - cache size
KW  - popularity-based front-end cache
KW  - provable DDOS prevention
KW  - randomly partitioned cluster services
KW  - replication factor
KW  - secure cache provision
KW  - system overload
KW  - Analytical models
KW  - Computer crime
KW  - Educational institutions
KW  - Load management
KW  - Load modeling
KW  - Servers
KW  - Throughput
VL  - 
JA  - Distributed Computing Systems Workshops (ICDCSW), 2013 IEEE 33rd International Conference on
DO  - 10.1109/ICDCSW.2013.24
AB  - In this paper we show a small but fast popularity-based front-end cache can provide provable DDOS prevention for randomly partitioned cluster services with replication. To achieve this, we first give the best strategy for an adversary to overload the system, and then prove that the cache size is lower bounded by O(n log log n/ log d), where n is the number of back-end nodes and d is the replication factor. Since log log n/ log d &lt;; 2 holds for almost all the current clusters (i.e., the number of back-end nodes n &lt;; 10<sup>5</sup> and the replication factor d &#x2265; 3), this result implies an O(n) lower bound on the required cache size. Our analysis and results are well validated through extensive simulations.
ER  - 

TY  - CONF
JO  - Computational Intelligence in Cyber Security (CICS), 2013 IEEE Symposium on
TI  - [Front matter]
T2  - Computational Intelligence in Cyber Security (CICS), 2013 IEEE Symposium on
IS  - 
SN  - 
VO  - 
SP  - i
EP  - vi
Y1  - 16-19 April 2013
PY  - 2013
KW  - artificial intelligence
KW  - cloud computing
KW  - data visualisation
KW  - database management systems
KW  - program verification
KW  - radio networks
KW  - security of data
KW  - smart power grids
KW  - steganography
KW  - access control
KW  - audio encryption
KW  - author identification
KW  - cloud computing
KW  - complex networks
KW  - computational intelligence
KW  - database audit
KW  - forensic analysis algorithm
KW  - hybrid-network intrusion detection system
KW  - image visualization
KW  - malware classification
KW  - malware detection
KW  - memory corruption vulnerabilities
KW  - resilient hybrid overlay model
KW  - smart grid
KW  - static software checking
KW  - steganography
KW  - wireless networks
VL  - 
JA  - Computational Intelligence in Cyber Security (CICS), 2013 IEEE Symposium on
DO  - 10.1109/CICYBS.2013.6597197
AB  - The following topics are dealt with: access control; wireless networks; hybrid-network intrusion detection system; cloud computing; database audit; forensic analysis algorithm; complex networks; resilient hybrid overlay model; smart grid; malware classification; author identification; image visualization based malware detection; steganography; audio encryption; computational intelligence; static software checking; and memory corruption vulnerabilities.
ER  - 

TY  - CONF
JO  - Integrated Network Management (IM), 2011 IFIP/IEEE International Symposium on
TI  - On the use of weighted syslog time series for anomaly detection
T2  - Integrated Network Management (IM), 2011 IFIP/IEEE International Symposium on
IS  - 
SN  - 
VO  - 
SP  - 393
EP  - 398
AU  - Fukuda, K.
Y1  - 23-27 May 2011
PY  - 2011
KW  - document handling
KW  - natural languages
KW  - security of data
KW  - time series
KW  - wavelet transforms
KW  - Japanese R&amp;E network
KW  - RIDF
KW  - anomaly detection
KW  - core router log messages
KW  - network devices
KW  - network operation
KW  - residual inverse document frequency
KW  - wavelet-based abrupt change detection algorithm
KW  - weighted syslog time series
KW  - Entropy
KW  - Manuals
KW  - Switches
VL  - 
JA  - Integrated Network Management (IM), 2011 IFIP/IEEE International Symposium on
DO  - 10.1109/INM.2011.5990538
AB  - Finding unusual (anomalous) events in a network is a crucial task for network operation. The collection and analysis of the log messages from the network devices (i.e., router and switch) are a common method. However, the detection of the anomalous events is painful because of a huge amount and many types of log messages; the important event messages are sometimes hidden in a large number of relatively less important and usual event messages. Many researches have been devoted to find out anomalies accurately and quickly by using time series of log message. To construct the log time series for anomaly detection in order to highlight such hidden anomalies, this paper focuses on the effectiveness of using a global weight that is based on a global appearance of a message type in the all data set. We introduce two types of the global weight called residual inverse document frequency (RIDF) and entropy that are well-known method in information retrieval field. Then, we evaluate the performance improvement due to the global weights with a wavelet-based abrupt change detection algorithm through 1-year long collection of core router log messages taken from a Japanese R&amp;E network. Our main findings are (1) the global weights assign a low weight to less important and frequently appeared event messages, (2) they highlight unusual events by removing temporal correlation (i.e., periodic trend), and (3) the wavelet-based detection algorithm more accurately alarms the anomalous time period with the weighted time series.
ER  - 

TY  - JOUR
JO  - Parallel and Distributed Systems, IEEE Transactions on
TI  - A class of randomized strategies for low-cost comparison of file copies
T2  - Parallel and Distributed Systems, IEEE Transactions on
IS  - 2
SN  - 1045-9219
VO  - 2
SP  - 160
EP  - 170
AU  - Barbara, D.
AU  - Lipton, R.J.
Y1  - Apr 1991
PY  - 1991
KW  - algorithm theory
KW  - file organisation
KW  - security of data
KW  - differing pages
KW  - file copies
KW  - randomized signatures
KW  - randomized strategies
KW  - remotely located file copies
KW  - Broadcasting
KW  - Costs
KW  - Database systems
KW  - Distributed algorithms
KW  - Fault tolerance
KW  - Hardware
KW  - Humans
KW  - Sun
KW  - Transaction databases
KW  - Voting
VL  - 2
JA  - Parallel and Distributed Systems, IEEE Transactions on
DO  - 10.1109/71.89062
AB  - A class of algorithms that use randomized signatures to compare remotely located file copies is presented. A simple technique that sends on the order of 4<sup>f</sup>log(<e1>n</e1>) bits, where <e1>f</e1> is the number of differing pages that are to be diagnosed and <e1>n</e1> is the number of pages in the file, is described. A method to improve the bound in the number of bits sent, making them grow with <e1>f</e1> as <e1>f</e1>log(<e1>f</e1>) and with <e1>n</e1> as log(<e1>n</e1>)log(log( <e1>n</e1>)), and a class of algorithms in which the number of signatures grows with <e1>f</e1> as <e1>fr</e1><sup>f</sup>, where <e1>r </e1> can be made to approach 1, are also presented. A comparison of these techniques is discussed
ER  - 

TY  - JOUR
JO  - Parallel and Distributed Systems, IEEE Transactions on
TI  - Protecting BGP routes to top-level DNS servers
T2  - Parallel and Distributed Systems, IEEE Transactions on
IS  - 9
SN  - 1045-9219
VO  - 14
SP  - 851
EP  - 860
AU  - Lan Wang
AU  - Xiaoliang Zhao
AU  - Dan Pei
AU  - Bush, R.
AU  - Massey, D.
AU  - Lixia Zhang
Y1  - Sept. 2003
PY  - 2003
KW  - Internet
KW  - computer network reliability
KW  - security of data
KW  - telecommunication network routing
KW  - telecommunication security
KW  - transport protocols
KW  - BGP route protection
KW  - BGP routing logs
KW  - Domain Name System
KW  - IP addresses
KW  - Internet
KW  - data security
KW  - fault tolerance
KW  - heuristics
KW  - host names
KW  - malicious impostor
KW  - path-filter
KW  - path-filtering approach
KW  - redundancy
KW  - routing operations
KW  - top-level DNS servers
KW  - Current measurement
KW  - Domain Name System
KW  - IEEE news
KW  - Network servers
KW  - Protection
KW  - Redundancy
KW  - Routing
KW  - Testing
KW  - Web and internet services
KW  - Web server
VL  - 14
JA  - Parallel and Distributed Systems, IEEE Transactions on
DO  - 10.1109/TPDS.2003.1233708
AB  - The Domain Name System (DNS) is an essential part of the Internet infrastructure and provides fundamental services, such as translating host names into IP addresses for Internet communication. The DNS is vulnerable to a number of potential faults and attacks. In particular, false routing announcements can deny access to the DNS service or redirect DNS queries to a malicious impostor. Due to the hierarchical DNS design, a single fault or attack against the routes to any of the top-level DNS servers can disrupt Internet services to millions of users. We propose a path-filtering approach to protect the routes to the critical top-level DNS servers. Our approach exploits both the high degree of redundancy in top-level DNS servers and the observation that popular destinations, including top-level DNS servers, are well-connected via stable routes. Our path-filter restricts the potential top-level DNS server route changes to be within a set of established paths. Heuristics derived from routing operations are used to adjust the potential routes over time. We tested our path-filtering design against BGP routing logs and the results show that the design can effectively ensure correct routes to top-level DNS servers without impacting DNS service availability.
ER  - 

TY  - CONF
JO  - Cybersecurity Summit (WCS), 2011 Second Worldwide
TI  - The analysis of youths' searching behavior
T2  - Cybersecurity Summit (WCS), 2011 Second Worldwide
IS  - 
SN  - 
VO  - 
SP  - 1
EP  - 4
AU  - Chao Li
AU  - Bin Wu
AU  - Yuxiao Li
Y1  - 1-2 June 2011
PY  - 2011
KW  - Internet
KW  - behavioural sciences computing
KW  - information retrieval
KW  - pattern classification
KW  - pattern clustering
KW  - search engines
KW  - security of data
KW  - IP addresses
KW  - Internet era
KW  - cyber security
KW  - cyberspace environment
KW  - gambling
KW  - keywords filtering technology
KW  - pornography
KW  - search engine operators
KW  - searching log
KW  - youth searching behavior analysis
KW  - Classification algorithms
KW  - Clustering algorithms
KW  - Internet
KW  - Machine learning algorithms
KW  - Search engines
KW  - Text categorization
KW  - Training
KW  - bad informations
KW  - classify
KW  - cluster
KW  - searching behavior
KW  - shield
VL  - 
JA  - Cybersecurity Summit (WCS), 2011 Second Worldwide
DO  - 
AB  - In order to create a safe and harmonious cyberspace environment for youth and protect them from the impact of bad information such as pornography, violence and gambling, most of the current solutions are to shield the sites containing this information. However some youth will take the initiative to seek out this bad information. In this paper we propose a method to analyze the searching behavior of youth and then to shield those keywords that may impact their health over time. We suggest that search engine operators provide keywords filtering technology to limit the searching behavior of youth. Operators should return to warning users who search keywords that contain bad information and record the searching log as feedback to parents. Of course parents should provide their IP addresses to the search engine operators first, to allow operators to set their IP addresses within the scope of supervising. Relying on search logs we count and classify keywords to understand the trend of youths' searching behavior and the areas that they often are interested in. Then we cluster the keywords in each area to get some main clusters and frequent keywords. Using this method we can grasp youths' searching behavior and protect them from the impact of bad information effectively.
ER  - 

TY  - CONF
JO  - Parallel and Distributed Processing Symposium, 2005. Proceedings. 19th IEEE International
TI  - Contiguous Search in the Hypercube for Capturing an Intruder
T2  - Parallel and Distributed Processing Symposium, 2005. Proceedings. 19th IEEE International
IS  - 
SN  - 
VO  - 
SP  - 62
EP  - 62
AU  - Flocchini, P.
AU  - Miao Jim Huang
AU  - Luccio, F.L.
Y1  - 04-08 April 2005
PY  - 2005
KW  - computer network management
KW  - hypercube networks
KW  - security of data
KW  - software agents
KW  - collaborative software agents
KW  - computer network management
KW  - hostile intruder
KW  - hypercube network
KW  - intruder search
KW  - Cleaning
KW  - Collaborative software
KW  - Hypercubes
KW  - Information technology
KW  - Network topology
KW  - Protection
KW  - Read-write memory
KW  - Software agents
KW  - Software algorithms
KW  - Time measurement
VL  - 
JA  - Parallel and Distributed Processing Symposium, 2005. Proceedings. 19th IEEE International
DO  - 10.1109/IPDPS.2005.151
AB  - In this paper we consider the problem of searching for an intruder in a network. There is a team of collaborative software agents that are deployed to capture a hostile intruder (e.g., a virus). These agents asynchronously move along the network links and the intruder has the capability of escaping arbitrarily fast. We propose two different strategies for the solution of the problem in a widely studied topology: the hypercube network. In the first strategy one of the agents acts as a coordinator making the other agents move in a precise way; this strategy requires O(n log n) moves, a team of O(frac{n}{{nlog n}}) agents and runs in O(n log n) time steps. The second strategy is devised for a model where the agents are allowed to "see" the state of their neighbours. In this case, the computation is local, i.e., there is no need of a coordinator and agents can move automously. In this setting the solution requires frac{n}{2} agents, but is much faster (log n time steps), and requires the same number of moves (O(n log n)).
ER  - 

TY  - CONF
JO  - Secure Network Protocols, 2008. NPSec 2008. 4th Workshop on
TI  - Improving anonymity using social links
T2  - Secure Network Protocols, 2008. NPSec 2008. 4th Workshop on
IS  - 
SN  - 
VO  - 
SP  - 15
EP  - 20
AU  - Puttaswamy, K.P.N.
AU  - Sala, A.
AU  - Zhao, B.Y.
Y1  - 19-19 Oct. 2008
PY  - 2008
KW  - data privacy
KW  - routing protocols
KW  - security of data
KW  - social networking (online)
KW  - anonymous routing protocols
KW  - network communication
KW  - passive logging attacks
KW  - social links
KW  - social networks
KW  - user privacy
KW  - Acceleration
KW  - Degradation
KW  - Facebook
KW  - Performance analysis
KW  - Privacy
KW  - Protection
KW  - Protective relaying
KW  - Routing protocols
KW  - Social network services
KW  - Telecommunication traffic
VL  - 
JA  - Secure Network Protocols, 2008. NPSec 2008. 4th Workshop on
DO  - 10.1109/NPSEC.2008.4664875
AB  - Protecting user privacy in network communication is vital in todaypsilas open networking environment. Current anonymous routing protocols provide anonymity by forwarding traffic through a static path of randomly selected relay nodes. In practice, however, malicious relays can perform passive logging attacks to compromise the anonymity of a flow. This degradation is accelerated when nodes fail, forcing source node to reconstruct a path, and in doing so, leaking more information to passive loggers. This ldquopredecessor attackrdquo is highly effective and difficult to defend against on current systems. In this paper, we propose a highly effective approach to blocking predecessor attacks by leveraging trusted links from social networks. We first show how users can completely shield themselves from traditional logging attacks. We then propose a hybrid logging attack optimized for social networks, and perform detailed analysis to show that we can defend against it using optimized path selection techniques. Finally, we analyze detailed measurement traces from Facebook to show that our approach is indeed feasible given the user behavior in social networks today.
ER  - 

TY  - JOUR
JO  - Knowledge and Data Engineering, IEEE Transactions on
TI  - Straggler Identification in Round-Trip Data Streams via Newton's Identities and Invertible Bloom Filters
T2  - Knowledge and Data Engineering, IEEE Transactions on
IS  - 2
SN  - 1041-4347
VO  - 23
SP  - 297
EP  - 306
AU  - Eppstein, D.
AU  - Goodrich, M.T.
Y1  - Feb. 2011
PY  - 2011
KW  - computational complexity
KW  - filtering theory
KW  - polynomials
KW  - security of data
KW  - Newton identities
KW  - false deletion tolerance
KW  - high-bandwidth multicast data stream
KW  - invertible bloom filters
KW  - round-trip data streams
KW  - straggler identification problem
KW  - symmetric polynomials
KW  - Algorithm design and analysis
KW  - Complexity theory
KW  - Finite element methods
KW  - Newton method
KW  - Polynomials
KW  - Servers
KW  - Bloom filters
KW  - Newton's identities
KW  - Straggler identification
KW  - data streams.
VL  - 23
JA  - Knowledge and Data Engineering, IEEE Transactions on
DO  - 10.1109/TKDE.2010.132
AB  - In this paper, we study the straggler identification problem, in which an algorithm must determine the identities of the remaining members of a set after it has had a large number of insertion and deletion operations performed on it, and now has relatively few remaining members. The goal is to do this in o(n) space, where n is the total number of identities. Straggler identification has applications, for example, in determining the unacknowledged packets in a high-bandwidth multicast data stream. We provide a deterministic solution to the straggler identification problem that uses only O(d log n) bits, based on a novel application of Newton's identities for symmetric polynomials. This solution can identify any subset of d stragglers from a set of n O(log n)-bit identifiers, assuming that there are no false deletions of identities not already in the set. Indeed, we give a lower bound argument that shows that any small-space deterministic solution to the straggler identification problem cannot be guaranteed to handle false deletions. Nevertheless, we provide a simple randomized solution, using O(d log n log (1/&#x2208;)) bits that can maintain a multiset and solve the straggler identification problem, tolerating false deletions, where &#x2208; &gt; 0 is a user-defined parameter bounding the probability of an incorrect response. This randomized solution is based on a new type of Bloom filter, which we call the invertible Bloom filter.
ER  - 

TY  - CONF
JO  - Intelligence and Security Informatics (ISI), 2011 IEEE International Conference on
TI  - Leveraging social networks to detect anomalous insider actions in collaborative environments
T2  - Intelligence and Security Informatics (ISI), 2011 IEEE International Conference on
IS  - 
SN  - 
VO  - 
SP  - 119
EP  - 124
AU  - You Chen
AU  - Nyemba, S.
AU  - Wen Zhang
AU  - Malin, B.
Y1  - 10-12 July 2011
PY  - 2011
KW  - groupware
KW  - information systems
KW  - security of data
KW  - social networking (online)
KW  - CIS
KW  - ROC curve
KW  - SNAD
KW  - Wikipedia
KW  - access logs
KW  - anomalous insider actions detection
KW  - broad access privileges
KW  - collaborative environments
KW  - collaborative information systems
KW  - complex dynamic systems
KW  - detect threats
KW  - editing logs
KW  - electronic health record system
KW  - extensive empirical evaluation
KW  - insider threat detection models
KW  - leveraging social networks
KW  - shared tasks
KW  - specialized network anomaly detection
KW  - Analytical models
KW  - Atmospheric measurements
KW  - Automatic voltage control
KW  - Electronic publishing
KW  - Intrusion detection
KW  - Medical services
VL  - 
JA  - Intelligence and Security Informatics (ISI), 2011 IEEE International Conference on
DO  - 10.1109/ISI.2011.5984061
AB  - Collaborative information systems (CIS) enable users to coordinate efficiently over shared tasks. They are often deployed in complex dynamic systems that provide users with broad access privileges, but also leave the system vulnerable to various attacks. Techniques to detect threats originating from beyond the system are relatively mature, but methods to detect insider threats are still evolving. A promising class of insider threat detection models for CIS focus on the communities that manifest between users based on the usage of common subjects in the system. However, current methods detect only when a user's aggregate behavior is intruding, not when specific actions have deviated from expectation. In this paper, we introduce a method called specialized network anomaly detection (SNAD) to detect such events. SNAD assembles the community of users that access a particular subject and assesses if similarities of the community with and without a certain user are sufficiently different. We present a theoretical basis and perform an extensive empirical evaluation with the access logs of two distinct environments: those of a large electronic health record system (6,015 users, 130,457 patients and 1,327,500 accesses) and the editing logs of Wikipedia (2,388,955 revisors, 55,200 articles and 6,482,780 revisions). We compare SNAD with several competing methods and demonstrate it is significantly more effective: on average it achieves 20-30% greater area under an ROC curve.
ER  - 

TY  - CONF
JO  - Dependable Systems and Networks, 2000. DSN 2000. Proceedings International Conference on
TI  - Avoiding loss of fairness owing to process crashes in fair data exchange protocols
T2  - Dependable Systems and Networks, 2000. DSN 2000. Proceedings International Conference on
IS  - 
SN  - 
VO  - 
SP  - 631
EP  - 640
AU  - Peng Liu
AU  - Peng Ning
AU  - Jajodia, S.
Y1  - 2000
PY  - 2000
KW  - electronic commerce
KW  - electronic data interchange
KW  - fault tolerant computing
KW  - losses
KW  - protocols
KW  - security of data
KW  - system recovery
KW  - telecommunication security
KW  - data exchange systems
KW  - electronic commerce
KW  - fair exchange protocols
KW  - fairness loss risks
KW  - fairness-lossless recoverability
KW  - fault-tolerance correctness criterion
KW  - local system failures
KW  - message logging
KW  - mutually distrusted parties
KW  - performance
KW  - process crashes
KW  - protocol correctness
KW  - system recovery
KW  - trusted third party
KW  - Communication channels
KW  - Computer crashes
KW  - Contracts
KW  - Electronic commerce
KW  - Fault diagnosis
KW  - Information systems
KW  - Postal services
KW  - Protocols
VL  - 
JA  - Dependable Systems and Networks, 2000. DSN 2000. Proceedings International Conference on
DO  - 10.1109/ICDSN.2000.857600
AB  - Fair exchange between two or more potentially mutually distrusted parties has been identified as an important issue in electronic commerce. However, the correctness (fairness) of the existing fair exchange protocols that use a trusted third party (TTP) is based on the assumption that, during an exchange, there are no failures at any of the local systems involved in the exchange, which is too strong in many situations. This paper points out that (1) system failures could cause loss of fairness, and (2) existing fair exchange protocols that use TTPs cannot ensure fairness in presence of system failures. We present a systematic way to develop such data exchange systems that can recover from system failures without losing fairness. We identify a set of fairness loss risks caused by local system failures. We identify a fault-tolerance correctness criterion for fair data exchange, denoted &ldquo;fairness-lossless recoverability&rdquo;. A fairness-lossless recoverable fair exchange system is immune from the set of fairness loss risks. Standard message logging approaches are then studied and extended to achieve fairness-lossless recoverability with good performance
ER  - 

TY  - CONF
JO  - INFOCOM 2001. Twentieth Annual Joint Conference of the IEEE Computer and Communications Societies. Proceedings. IEEE
TI  - On the effectiveness of probabilistic packet marking for IP traceback under denial of service attack
T2  - INFOCOM 2001. Twentieth Annual Joint Conference of the IEEE Computer and Communications Societies. Proceedings. IEEE
IS  - 
SN  - 0743-166X
VO  - 1
SP  - 338
EP  - 347 vol.1
AU  - Kihong Park
AU  - Heejo Lee
Y1  - 2001
PY  - 2001
KW  - Internet
KW  - minimax techniques
KW  - optimisation
KW  - packet switching
KW  - probability
KW  - security of data
KW  - telecommunication security
KW  - telecommunication traffic
KW  - transport protocols
KW  - IP header
KW  - IP internets
KW  - IP traceback
KW  - Internet
KW  - attack volume
KW  - constrained minimax optimization problem
KW  - deterministic packet logging
KW  - deterministic packet marking
KW  - distributed denial of service attack
KW  - forgeable attack paths
KW  - messaging based schemes
KW  - optimal decision problem
KW  - path length
KW  - probabilistic packet marking
KW  - sampling constraints
KW  - source address
KW  - spoofed marking value
KW  - spoofed source IP address
KW  - spoofing
KW  - traffic volume
KW  - Computer crime
KW  - Computer networks
KW  - IP networks
KW  - Impedance
KW  - Intelligent networks
KW  - Minimax techniques
KW  - Pressing
KW  - Quality of service
KW  - Web and internet services
KW  - Web server
VL  - 1
JA  - INFOCOM 2001. Twentieth Annual Joint Conference of the IEEE Computer and Communications Societies. Proceedings. IEEE
DO  - 10.1109/INFCOM.2001.916716
AB  - Effective mitigation of denial of service (DoS) attack is a pressing problem on the Internet. In many instances, DoS attacks can be prevented if the spoofed source IP address is traced back to its origin which allows assigning penalties to the offending party or isolating the compromised hosts and domains from the rest of the network. IP traceback mechanisms based on probabilistic packet marking (PPM) have been proposed for achieving traceback of DoS attacks. We show that probabilistic packet marking-of interest due to its efficiency and implementability vis-a-vis deterministic packet marking and logging or messaging based schemes-suffers under spoofing of the marking field in the IP header by the attacker which can impede traceback by the victim. We show that there is a trade-off between the ability of the victim to localize the attacker and the severity of the DoS attack, which is represented as a function of the marking probability, path length, and traffic volume. The optimal decision problem-the victim can choose the marking probability whereas the attacker can choose the spoofed marking value, source address, and attack volume-can be expressed as a constrained minimax optimization problem, where the victim chooses the marking probability such that the number of forgeable attack paths is minimized. We show that the attacker's ability to hide his location is curtailed by increasing the marking probability, however, the latter is upper-bounded due to sampling constraints. In typical IP internets, the attacker's address can be localized to within 2-5 equally likely sites which renders PPM effective against single source attacks. Under distributed DoS attacks, the uncertainty achievable by the attacker can be amplified, which diminishes the effectiveness of PPM
ER  - 

TY  - CONF
JO  - Parallel Architectures, Algorithms and Networks, 2004. Proceedings. 7th International Symposium on
TI  - A marking scheme using Huffman codes for IP traceback
T2  - Parallel Architectures, Algorithms and Networks, 2004. Proceedings. 7th International Symposium on
IS  - 
SN  - 1087-4089
VO  - 
SP  - 421
EP  - 428
AU  - Choi, K.H.
AU  - Dai, H.K.
Y1  - 10-12 May 2004
PY  - 2004
KW  - Huffman codes
KW  - Internet
KW  - security of data
KW  - telecommunication security
KW  - Huffman codes
KW  - IP traceback
KW  - Internet
KW  - distributed denial-of-service attacks
KW  - link testing
KW  - marking scheme
KW  - probabilistic markings
KW  - Computer crime
KW  - Computer science
KW  - Encoding
KW  - Information filtering
KW  - Information filters
KW  - Network interfaces
KW  - Protocols
KW  - Robustness
KW  - Testing
KW  - Web and internet services
VL  - 
JA  - Parallel Architectures, Algorithms and Networks, 2004. Proceedings. 7th International Symposium on
DO  - 10.1109/ISPAN.2004.1300516
AB  - In (distributed) denial of service attack ((D)DoS), attackers send a huge number of packets with spoofed source addresses to disguise themselves toward a target host or network Various IP traceback techniques such as link testing, marking, and logging to find out the real source of attacking packets have been proposed. We present a marking scheme (with marking and traceback algorithms) in which a router marks a packet with a link that the packet came through. Links of a router are represented by Huffman codes according to the traffic distribution among the links. If the packet runs out of space allotted for the marking field in the packet header, then the router stores the marking field in the router's local memory along with a message digest of the packet. We analyze the memory requirement of routers to store marking fields, compare the scheme with other existing techniques, and address practical issues to deploy the scheme in the Internet. The scheme marks every packet, therefore IP traceback can be accomplished with only a packet unlike in probabilistic markings; also it requires far less amount of memory compared to logging methods and is robust in case of DDoS.
ER  - 

TY  - CONF
JO  - Signal Processing, Communications and Networking, 2007. ICSCN '07. International Conference on
TI  - An Intelligent Agent Based Approach for Intrusion Detection and Prevention in Adhoc Networks
T2  - Signal Processing, Communications and Networking, 2007. ICSCN '07. International Conference on
IS  - 
SN  - 
VO  - 
SP  - 534
EP  - 536
AU  - Veeraraghavan, S.
AU  - Bose, S.
AU  - Anand, K.
AU  - Kannan, A.
Y1  - 22-24 Feb. 2007
PY  - 2007
KW  - ad hoc networks
KW  - correlation methods
KW  - mobile agents
KW  - mobile radio
KW  - security of data
KW  - telecommunication security
KW  - application layer
KW  - correlation theory
KW  - intelligent agent based approach
KW  - intrusion detection
KW  - log file data
KW  - mobile ad hoc network
KW  - network layer
KW  - Application software
KW  - Computational modeling
KW  - Computer networks
KW  - Computer science
KW  - Intelligent agent
KW  - Intrusion detection
KW  - Mobile ad hoc networks
KW  - Monitoring
KW  - Testing
KW  - Variable speed drives
KW  - Forecasting Adhoc Network
KW  - Intelligent Agents
KW  - Intrusion Detection System
KW  - OneR algorithm
VL  - 
JA  - Signal Processing, Communications and Networking, 2007. ICSCN '07. International Conference on
DO  - 10.1109/ICSCN.2007.350658
AB  - This paper describes about an intelligent agent based intrusion detection and prevention system for mobile ad hoc network. This system collects data from application layer and network layer and classifies them using the log file data collected from these layers and local anomalies are computed using local agents finally it is sent to a global agent for integration. The local agents monitor the two layers of the network to determine the correlation among the observed anomalous patterns, reporting such abnormal behavior to the administrator for taking possible action. Our simulation results obtained after integration shows that it is possible to obtain high intrusion-detection rates (99.2%) and low false-alarm rates
ER  - 

TY  - JOUR
JO  - Image Processing, IEEE Transactions on
TI  - Rotation, scale, and translation resilient watermarking for images
T2  - Image Processing, IEEE Transactions on
IS  - 5
SN  - 1057-7149
VO  - 10
SP  - 767
EP  - 782
AU  - Ching-Yung Lin
AU  - Min Wu
AU  - Bloom, J.A.
AU  - Cox, Ingemar J.
AU  - Miller, M.L.
AU  - Yui Man Lui
Y1  - May 2001
PY  - 2001
KW  - Fourier transforms
KW  - copy protection
KW  - data encapsulation
KW  - image coding
KW  - security of data
KW  - Fourier magnitudes
KW  - Fourier transform
KW  - JPEG compression
KW  - RST distortions
KW  - blind detection
KW  - correlation coefficient
KW  - cropping
KW  - cyclical shift
KW  - detection measure
KW  - geometric distortions
KW  - images
KW  - log-polar coordinates
KW  - log-radius axis
KW  - one-dimensional signal
KW  - public watermark
KW  - resampling
KW  - rotation
KW  - scale
KW  - search
KW  - still images
KW  - translation resilient watermarking
KW  - video
KW  - Detectors
KW  - Electrical resistance measurement
KW  - Filtering
KW  - Fourier transforms
KW  - Image coding
KW  - Image databases
KW  - Robustness
KW  - Signal processing
KW  - Signal processing algorithms
KW  - Watermarking
VL  - 10
JA  - Image Processing, IEEE Transactions on
DO  - 10.1109/83.918569
AB  - Many electronic watermarks for still images and video content are sensitive to geometric distortions. For example, simple rotation, scaling, and/or translation (RST) of an image can prevent blind detection of a public watermark. In this paper, we propose a watermarking algorithm that is robust to RST distortions. The watermark is embedded into a one-dimensional (1-D) signal obtained by taking the Fourier transform of the image, resampling the Fourier magnitudes into log-polar coordinates, and then summing a function of those magnitudes along the log-radius axis. Rotation of the image results in a cyclical shift of the extracted signal. Scaling of the image results in amplification of the extracted signal, and translation of the image has no effect on the extracted signal. We can therefore compensate for rotation with a simple search, and compensate for scaling by using the correlation coefficient as the detection measure. False positive results on a database of 10 000 images are reported. Robustness results on a database of 2000 images are described. It is shown that the watermark is robust to rotation, scale, and translation. In addition, we describe tests examining the watermarks resistance to cropping and JPEG compression
ER  - 

TY  - CONF
JO  - Information Assurance, 2005. Proceedings. Third IEEE International Workshop on
TI  - Combining static analysis and dynamic learning to build accurate intrusion detection models
T2  - Information Assurance, 2005. Proceedings. Third IEEE International Workshop on
IS  - 
SN  - 
VO  - 
SP  - 164
EP  - 177
AU  - Zhen Liu
AU  - Bridges, S.M.
AU  - Vaughn, R.B.
Y1  - 23-24 March 2005
PY  - 2005
KW  - data flow analysis
KW  - learning (artificial intelligence)
KW  - program diagnostics
KW  - pushdown automata
KW  - security of data
KW  - anomaly detection
KW  - application execution flow
KW  - attack detection
KW  - automata model
KW  - call stack information
KW  - call stack log auditing
KW  - correspondence relation
KW  - dynamic analysis
KW  - dynamic learning
KW  - hybrid push down automata
KW  - intrusion detection models
KW  - program behavior profiling
KW  - program control flow
KW  - static analysis
KW  - system call sequence monitoring
KW  - Automatic control
KW  - Bridges
KW  - Buffer overflow
KW  - Computer science
KW  - Computerized monitoring
KW  - Dynamic programming
KW  - Focusing
KW  - Intrusion detection
KW  - Learning automata
KW  - Programming environments
VL  - 
JA  - Information Assurance, 2005. Proceedings. Third IEEE International Workshop on
DO  - 10.1109/IWIA.2005.6
AB  - Anomaly detection based on monitoring of sequences of system calls has been shown to be an effective method for detection of previously unseen, potentially damaging attacks on hosts. This paper presents a new model for profiling normal program behavior for use in detection of intrusions that change application execution flow. This model is compact and efficient to operate and can be acquired using a combination of static analysis and dynamic learning. Our model (hybrid push down automata, HPDA) incorporates call stack information in the automata model and effectively captures the control flow of a program. Several important properties of the model are based on a unique correspondence relation between addresses and instructions within the model. These properties allow the HPDA to be acquired by dynamic analysis of an audit of the call stack log. Our strategy is to use static analysis to acquire a base model and then to use dynamic learning as a supplement to capture those aspects of behavior that are difficult to capture with static analysis due to techniques commonly used in modern programming environments. The model created by this combination method is shown to have a higher detection capability than models acquired by static analysis alone and a lower false positive rate than models acquired by dynamic learning alone.
ER  - 

TY  - CONF
JO  - Computer Science and Software Engineering, 2008 International Conference on
TI  - Improving Admission Control Policies in Database Management Systems, Using Data Mining Techniques
T2  - Computer Science and Software Engineering, 2008 International Conference on
IS  - 
SN  - 
VO  - 4
SP  - 247
EP  - 250
AU  - Nejad, A.F.
AU  - Kharazmi, S.
AU  - Bayati, S.
Y1  - 12-14 Dec. 2008
PY  - 2008
KW  - data mining
KW  - database management systems
KW  - security of data
KW  - admission control component
KW  - admission control policies
KW  - data mining techniques
KW  - multiuser database management system
KW  - Admission control
KW  - Communication system control
KW  - Computer science
KW  - Data mining
KW  - Database systems
KW  - Delay
KW  - Information technology
KW  - Software engineering
KW  - System performance
KW  - Throughput
KW  - Admission Control
KW  - DBMS
KW  - Data Mining
KW  - Policy
VL  - 4
JA  - Computer Science and Software Engineering, 2008 International Conference on
DO  - 10.1109/CSSE.2008.1366
AB  - Increasing workload on multi-user database management system (DBMS) can cause maximum system's throughput. With further increase of workload, throughput dramatically decreases and system performance fall down; then system starts to thrash. Existence of an admission control component for managing user's requests is necessary. Admission control component in DBMS has two main tasks. These tasks are Prevention of increasing user's communication requests to system more than a threshold and prevention, deferment or execution of received requests after optimization step and determination of request requirements. The problem is determination of allowable number of communications to DBMS. Can we predict and determine conditions to providence time and resource before reach to optimization step? In this paper we use data mining techniques to predict and describe admission control policies. According to this purpose we recommend an Admission Logging system to log existing actions before end of query processor procedures. We describe situation, structure, and changes that we need in system. Then we describe the circumstance of knowledge extraction on Admission Log.
ER  - 

TY  - JOUR
JO  - Dependable and Secure Computing, IEEE Transactions on
TI  - Ensuring Distributed Accountability for Data Sharing in the Cloud
T2  - Dependable and Secure Computing, IEEE Transactions on
IS  - 4
SN  - 1545-5971
VO  - 9
SP  - 556
EP  - 568
AU  - Sundareswaran, S.
AU  - Squicciarini, A.C.
AU  - Lin, D.
Y1  - July-Aug. 2012
PY  - 2012
KW  - authorisation
KW  - cloud computing
KW  - security of data
KW  - system monitoring
KW  - Internet
KW  - JAR programmable capabilities
KW  - cloud computing
KW  - cloud services
KW  - data sharing
KW  - decentralized information accountability framework
KW  - distributed accountability ensurance
KW  - distributed auditing mechanisms
KW  - logging mechanism
KW  - object-centered approach
KW  - Access control
KW  - Authentication
KW  - Cryptography
KW  - Distributed databases
KW  - Monitoring
KW  - Privacy
KW  - Cloud computing
KW  - accountability
KW  - data sharing.
VL  - 9
JA  - Dependable and Secure Computing, IEEE Transactions on
DO  - 10.1109/TDSC.2012.26
AB  - Cloud computing enables highly scalable services to be easily consumed over the Internet on an as-needed basis. A major feature of the cloud services is that users' data are usually processed remotely in unknown machines that users do not own or operate. While enjoying the convenience brought by this new emerging technology, users' fears of losing control of their own data (particularly, financial and health data) can become a significant barrier to the wide adoption of cloud services. To address this problem, in this paper, we propose a novel highly decentralized information accountability framework to keep track of the actual usage of the users' data in the cloud. In particular, we propose an object-centered approach that enables enclosing our logging mechanism together with users' data and policies. We leverage the JAR programmable capabilities to both create a dynamic and traveling object, and to ensure that any access to users' data will trigger authentication and automated logging local to the JARs. To strengthen user's control, we also provide distributed auditing mechanisms. We provide extensive experimental studies that demonstrate the efficiency and effectiveness of the proposed approaches.
ER  - 

TY  - CONF
JO  - Integrated Network Management (IM), 2015 IFIP/IEEE International Symposium on
TI  - Unveiling flat traffic on the Internet: An SSH attack case study
T2  - Integrated Network Management (IM), 2015 IFIP/IEEE International Symposium on
IS  - 
SN  - 
VO  - 
SP  - 270
EP  - 278
AU  - Jonker, M.
AU  - Hofstede, R.
AU  - Sperotto, A.
AU  - Pras, A.
Y1  - 11-15 May 2015
PY  - 2015
KW  - Internet
KW  - computer network security
KW  - security of data
KW  - IDSes
KW  - Internet
KW  - SSH attack
KW  - TCP retransmissions
KW  - application layer actions
KW  - backbone network
KW  - brute force attacks
KW  - campus network
KW  - flat traffic
KW  - flow exporter extension
KW  - intrusion detection
KW  - intrusion detection systems
KW  - network traffic
KW  - network traffic analysis
KW  - typical attacks
KW  - Delays
KW  - Dictionaries
KW  - Internet
KW  - Intrusion detection
KW  - Monitoring
KW  - Payloads
KW  - Probes
VL  - 
JA  - Integrated Network Management (IM), 2015 IFIP/IEEE International Symposium on
DO  - 10.1109/INM.2015.7140301
AB  - Many types of brute-force attacks are known to exhibit a characteristic `flat' behavior at the network-level, meaning that connections belonging to an attack feature a similar number of packets and bytes, and duration. Flat traffic usually results from repeating similar application-layer actions, such as login attempts in a brute-force attack. For typical attacks, hundreds of attempts span over multiple connections, with each connection containing the same, small number of attempts. The characteristic flat behavior is used by many Intrusion Detection Systems (IDSes), both for identifying the presence of attacks and - once detected - for observing deviations, pointing out potential compromises, for example. However, flatness of network traffic may become indistinct when TCP retransmissions and control information come into play. These TCP phenomena affect not only intrusion detection, but also other forms of network traffic analysis. The contribution of this work is twofold. First, we analyze the impact of retransmissions and control information on network traffic based on traffic measurements. To do so, we have developed a flow exporter extension that was deployed in both a campus and a backbone network. Second, we show that intrusion detection results improve dramatically by up to 16 percentage points once IDSes are able to `flatten' network traffic again, which we have validated by means of analyzing log files of almost 60 hosts over a period of one month.
ER  - 

TY  - CONF
JO  - INFOCOM 2005. 24th Annual Joint Conference of the IEEE Computer and Communications Societies. Proceedings IEEE
TI  - The effect of network topology on the spread of epidemics
T2  - INFOCOM 2005. 24th Annual Joint Conference of the IEEE Computer and Communications Societies. Proceedings IEEE
IS  - 
SN  - 0743-166X
VO  - 2
SP  - 1455
EP  - 1466 vol. 2
AU  - Ganesh, A.
AU  - Massoulie, L.
AU  - Towsley, D.
Y1  - 13-17 March 2005
PY  - 2005
KW  - graph theory
KW  - hypercube networks
KW  - internetworking
KW  - network servers
KW  - routing protocols
KW  - security of data
KW  - table lookup
KW  - telecommunication network topology
KW  - telecommunication security
KW  - BGP
KW  - Erdos-Renyi graph
KW  - border gateway protocol
KW  - distributed hash table
KW  - epidemic spreading
KW  - hypercube network
KW  - information dissemination
KW  - network topological property
KW  - power law graph
KW  - star topology
KW  - Computer science
KW  - Computer worms
KW  - Graph theory
KW  - Hypercubes
KW  - Impedance
KW  - Network topology
KW  - Power system faults
KW  - Stochastic processes
KW  - Sufficient conditions
KW  - Viruses (medical)
VL  - 2
JA  - INFOCOM 2005. 24th Annual Joint Conference of the IEEE Computer and Communications Societies. Proceedings IEEE
DO  - 10.1109/INFCOM.2005.1498374
AB  - Many network phenomena are well modeled as spreads of epidemics through a network. Prominent examples include the spread of worms and email viruses, and, more generally, faults. Many types of information dissemination can also be modeled as spreads of epidemics. In this paper we address the question of what makes an epidemic either weak or potent. More precisely, we identify topological properties of the graph that determine the persistence of epidemics. In particular, we show that if the ratio of cure to infection rates is larger than the spectral radius of the graph, then the mean epidemic lifetime is of order log n, where n is the number of nodes. Conversely, if this ratio is smaller than a generalization of the isoperimetric constant of the graph, then the mean epidemic lifetime is of order e<sup>na</sup>, for a positive constant a. We apply these results to several network topologies including the hypercube, which is a representative connectivity graph for a distributed hash table, the complete graph, which is an important connectivity graph for BGP, and the power law graph, of which the AS-level Internet graph is a prime example. We also study the star topology and the Erdos-Renyi graph as their epidemic spreading behaviors determine the spreading behavior of power law graphs.
ER  - 

TY  - CONF
JO  - Computing, Communication and Networking Technologies (ICCCNT), 2014 International Conference on
TI  - Webshell detection techniques in web applications
T2  - Computing, Communication and Networking Technologies (ICCCNT), 2014 International Conference on
IS  - 
SN  - 
VO  - 
SP  - 1
EP  - 7
AU  - Truong Dinh Tu
AU  - Cheng Guang
AU  - Guo Xiaojun
AU  - Pan Wubin
Y1  - 11-13 July 2014
PY  - 2014
KW  - Internet
KW  - Web sites
KW  - security of data
KW  - Internet
KW  - Web applications
KW  - Web sites security
KW  - Webshell detection technique
KW  - malicious Web sites
KW  - Computer hacking
KW  - Cryptography
KW  - Databases
KW  - Educational institutions
KW  - Web pages
KW  - Web servers
KW  - backdoor
KW  - malicious web detection
KW  - webshell
KW  - webshell detection
VL  - 
JA  - Computing, Communication and Networking Technologies (ICCCNT), 2014 International Conference on
DO  - 10.1109/ICCCNT.2014.6963152
AB  - With widely adoption of online services, malicious web sites have become a malignant tumor of the Internet. Through system vulnerabilities, attackers can upload malicious files (which are also called webshells) to web server to create a backdoor for hackers' further attacks. Therefore, finding and detecting webshell inside web application source code are crucial to secure websites. In this paper, we propose a novel method based on the optimal threshold values to identify files that contain malicious codes from web applications. Our detection system will scan and look for malicious codes inside each file of the web application, and automatically give a list of suspicious files and a detail log analysis table of each suspicious file for administrators to check further. The Experimental results show that our approach is applicable to identify webshell more efficient than some other approaches.
ER  - 

TY  - CONF
JO  - Parallel & Distributed Processing (IPDPS), 2010 IEEE International Symposium on
TI  - Attack-resistant frequency counting
T2  - Parallel & Distributed Processing (IPDPS), 2010 IEEE International Symposium on
IS  - 
SN  - 1530-2075
VO  - 
SP  - 1
EP  - 10
AU  - Bo Wu
AU  - Saia, J.
AU  - King, V.
Y1  - 19-23 April 2010
PY  - 2010
KW  - groupware
KW  - large-scale systems
KW  - peer-to-peer computing
KW  - security of data
KW  - Byzantine faults
KW  - attack resistant frequency counting
KW  - collaborative peer-to-peer algorithms
KW  - distributed data mining
KW  - large scale network
KW  - spam detection
KW  - virus detection
KW  - worm
KW  - Collaborative work
KW  - Computer networks
KW  - Computer science
KW  - Computer worms
KW  - Data mining
KW  - Fingerprint recognition
KW  - Frequency
KW  - Large-scale systems
KW  - Payloads
KW  - Peer to peer computing
KW  - Byzantine faults
KW  - butterfly
KW  - distributed
KW  - frequent items
VL  - 
JA  - Parallel & Distributed Processing (IPDPS), 2010 IEEE International Symposium on
DO  - 10.1109/IPDPS.2010.5470344
AB  - We present collaborative peer-to-peer algorithms for the problem of approximating frequency counts for popular items distributed across the peers of a large-scale network. Our algorithms are attack-resistant in the sense that they function correctly even in the case where an adaptive and computationally unbounded adversary causes up to a 1/3 fraction of the peers in the network to suffer Byzantine faults. Our algorithms are scalable in the sense that all resource costs are polylogarithmic. Specifically, latency is O(log n); the number of messages and number of bits sent and received by each peer is O(log<sup>2</sup>n) per item; and number of neighbors of each peer is O(log<sup>2</sup>n). Our motivation for addressing this problem is to provide a tool for the following three applications: worm and virus detection; spam detection; and distributed data-mining. To the best of our knowledge, our algorithms are the first attack-resistant and scalable algorithms for this problem. Moreover, surprisingly, our algorithms seem to be the first attack-resistant algorithms for any data mining problem.
ER  - 

TY  - JOUR
JO  - Information Theory, IEEE Transactions on
TI  - An analysis of the timed Z-channel
T2  - Information Theory, IEEE Transactions on
IS  - 7
SN  - 0018-9448
VO  - 44
SP  - 3162
EP  - 3168
AU  - Moskowitz, I.S.
AU  - Greenwald, S.J.
AU  - Kang, M.H.
Y1  - Nov 1998
PY  - 1998
KW  - channel capacity
KW  - computer networks
KW  - memoryless systems
KW  - processor scheduling
KW  - security of data
KW  - telecommunication security
KW  - CPU scheduling
KW  - channel capacity
KW  - computer network
KW  - covert communication channels
KW  - input symbols
KW  - mathematical analysis
KW  - memoryless channel
KW  - multilevel secure computer systems
KW  - noise
KW  - noiseless channels
KW  - output symbol transmission times
KW  - output symbols
KW  - timed Z-channel analysis
KW  - timing channel
KW  - Automation
KW  - Inspection
KW  - Laplace equations
KW  - Machine intelligence
KW  - Machine vision
KW  - Noise shaping
KW  - Notice of Violation
KW  - Robustness
KW  - Shape
KW  - Testing
VL  - 44
JA  - Information Theory, IEEE Transactions on
DO  - 10.1109/18.737549
AB  - Golomb analyzed the Z-channel, a memoryless channel with two input symbols and two output symbols, where one of the input symbols is transmitted with noise while the other is transmitted without noise, and the output symbol transmission times are equal. We generalize to the timed Z-channel, where the output symbol transmission times are not equal. The timed Z-channel appears as the basis for a large class of covert (communication) channels appearing in multilevel secure computer systems. We give a detailed mathematical analysis of the timed Z-channel and report a result expressing the capacity of the timed Z-channel as the log of the root of a characteristic equation. This generalizes Shannon's work on noiseless channels for this special case. We also report a new result bounding the timed Z-channel's capacity from below. We show how an interesting observation that Golomb reported for the Z-channel also holds for the timed Z-channel
ER  - 

TY  - CONF
JO  - Decision and Control (CDC), 2010 49th IEEE Conference on
TI  - The wireless control network: Monitoring for malicious behavior
T2  - Decision and Control (CDC), 2010 49th IEEE Conference on
IS  - 
SN  - 0743-1546
VO  - 
SP  - 5979
EP  - 5984
AU  - Sundaram, S.
AU  - Pajic, M.
AU  - Hadjicostis, C.N.
AU  - Mangharam, R.
AU  - Pappas, G.J.
Y1  - 15-17 Dec. 2010
PY  - 2010
KW  - radio networks
KW  - security of data
KW  - telecommunication control
KW  - telecommunication security
KW  - IDS
KW  - intrusion detection system
KW  - malicious behavior
KW  - resource constrained wireless node
KW  - wireless control network
KW  - Joining processes
KW  - Linear systems
KW  - Monitoring
KW  - Network topology
KW  - Sensors
KW  - Transfer functions
KW  - Wireless communication
VL  - 
JA  - Decision and Control (CDC), 2010 49th IEEE Conference on
DO  - 10.1109/CDC.2010.5717166
AB  - We consider the problem of stabilizing a plant with a network of resource constrained wireless nodes. In a companion paper, we developed a protocol where each node repeatedly transmits an appropriate (stabilizing) linear combination of the values in its neighborhood. In this paper, we design an Intrusion Detection System (IDS) for this control scheme, which observes the transmissions of certain nodes and uses that information to (a) recover the plant outputs (for data-logging and diagnostic purposes) and (b) identify malicious behavior by any of the wireless nodes in the network. We show that if the connectivity of the network is sufficiently high, the IDS only needs to observe a subset of the nodes in the network in order to achieve this objective. Our approach provides a characterization of the set of nodes that should be observed, a systematic procedure for the IDS to use to identify the malicious nodes and recover the outputs of the plant, and an upper bound on the delay required to obtain the necessary information.
ER  - 

TY  - JOUR
JO  - Dependable and Secure Computing, IEEE Transactions on
TI  - A Large-Scale Study of the Time Required to Compromise a Computer System
T2  - Dependable and Secure Computing, IEEE Transactions on
IS  - 1
SN  - 1545-5971
VO  - 11
SP  - 2
EP  - 15
AU  - Holm, H.
Y1  - Jan.-Feb. 2014
PY  - 2014
KW  - Pareto distribution
KW  - Poisson distribution
KW  - exponential distribution
KW  - log normal distribution
KW  - security of data
KW  - stochastic processes
KW  - Pareto distribution
KW  - Poisson distribution
KW  - Poisson process
KW  - TTC
KW  - computer system
KW  - cyberintrusions
KW  - cybersecurity
KW  - exponential distribution
KW  - intrusion detection
KW  - log-normal distribution
KW  - time to compromise
KW  - Computational modeling
KW  - Malware
KW  - Statistical distributions
KW  - Workstations
KW  - Invasive software (viruses
KW  - Trojan horses)
KW  - network management
KW  - risk management
KW  - worms
VL  - 11
JA  - Dependable and Secure Computing, IEEE Transactions on
DO  - 10.1109/TDSC.2013.21
AB  - A frequent assumption in the domain of cybersecurity is that cyberintrusions follow the properties of a Poisson process, i.e., that the number of intrusions is well modeled by a Poisson distribution and that the time between intrusions is exponentially distributed. This paper studies this property by analyzing all cyberintrusions that have been detected across more than 260,000 computer systems over a period of almost three years. The results show that the assumption of a Poisson process model might be unoptimal - the log-normal distribution is a significantly better fit in terms of modeling both the number of detected intrusions and the time between intrusions, and the Pareto distribution is a significantly better fit in terms of modeling the time to first intrusion. The paper also analyzes whether time to compromise (TTC) increase for each successful intrusion of a computer system. The results regarding this property suggest that time to compromise decrease along the number of intrusions of a system.
ER  - 

TY  - CONF
JO  - Communications (ICC), 2010 IEEE International Conference on
TI  - Detecting Anomalous Web Browsing via Diffusion Wavelets
T2  - Communications (ICC), 2010 IEEE International Conference on
IS  - 
SN  - 1550-3607
VO  - 
SP  - 1
EP  - 6
AU  - Ho Yan Suen
AU  - Wing Cheong Lau
AU  - OnChing Yue
Y1  - 23-27 May 2010
PY  - 2010
KW  - Internet
KW  - security of data
KW  - HTTP flooding attack
KW  - Web access logs
KW  - Webpage access sequences
KW  - anomalous Web browsing detection
KW  - denial-of-service attack
KW  - diffusion wavelets
KW  - Communications Society
KW  - Computer crime
KW  - Data mining
KW  - Detection algorithms
KW  - Floods
KW  - History
KW  - Length measurement
KW  - Navigation
KW  - Wavelet analysis
KW  - Web server
VL  - 
JA  - Communications (ICC), 2010 IEEE International Conference on
DO  - 10.1109/ICC.2010.5502089
AB  - Web access logs contain information which can be converted to represent the access history of individual users. A large number of essential attributes can be extracted from the access history. For example, the access counts of each webpage, the occurrence of different webpage access sequences and the time spent between consecutive accesses. Each of the above attributes represents a dimension in the feature space, and all the attributes together form a very high dimension space. Diffusion Wavelets can efficiently project the high dimensional data onto a low-dimensional space according to the correlations between various attributes, so that common anomaly detection algorithms can be applied. In this paper, we propose a system which leverages this technique to differentiate web-access requests generated by Denial of Service (DoS) attacks from legitimate ones. We demonstrate the effectiveness of the proposed system via simulation studies using real-world web access logs. For a simulated HTTP flooding attack which creates a 1000% overload at the web-server, the proposed scheme can reduce the ratio of the attack-to-legitimate requests admitted by the server from 200:1 to 30:1 so that more than 55% of the legitimate requests can still receive proper services under such a severe DoS attack.
ER  - 

TY  - CONF
JO  - Privacy, Security and Trust (PST), 2011 Ninth Annual International Conference on
TI  - Data preprocessing for distance-based unsupervised Intrusion Detection
T2  - Privacy, Security and Trust (PST), 2011 Ninth Annual International Conference on
IS  - 
SN  - 
VO  - 
SP  - 181
EP  - 188
AU  - Said, D.
AU  - Stirling, L.
AU  - Federolf, P.
AU  - Barker, K.
Y1  - 19-21 July 2011
PY  - 2011
KW  - principal component analysis
KW  - security of data
KW  - unsupervised learning
KW  - DBOD
KW  - Euclidean distance
KW  - IDS
KW  - Mahalanobis distance
KW  - PCA
KW  - data preprocessing
KW  - dimensionality problem
KW  - distance based outlier detection
KW  - distance based unsupervised intrusion detection
KW  - feature extraction technique
KW  - principle component analysis
KW  - Equations
KW  - Euclidean distance
KW  - Feature extraction
KW  - Intrusion detection
KW  - Principal component analysis
KW  - Training
VL  - 
JA  - Privacy, Security and Trust (PST), 2011 Ninth Annual International Conference on
DO  - 10.1109/PST.2011.5971981
AB  - Since Intrusion Detection Systems (IDSs) operate in real-time, they should be light-weighted to detect intrusions as fast as possible. Distance-based Outlier Detection (DBOD) is one of the most widely-used techniques for detecting outliers due to its simplicity and efficiency. Additionally, DBOD is an unsupervised approach which overcomes the problem of the lack of training datasets with known intrusions. However, since IDSs usually have high-dimensional datasets, using DBOD becomes subject to the curse of the dimensionality problem. Furthermore, intrusion datasets should be normalized before calculating pair-wise distance between observations. The purpose of this research is conduct a comparative study among different normalization methods in conjunction with a well-known feature extraction technique; Principle Component Analysis (PCA). Therefore, the efficiency of these methods as data preprocessing techniques can be investigated when applying DBOD to detect intrusions. Experiments were performed using two kinds of distance metrics; Euclidean distance and Mahalanobis distance. We further examined the PCA using 7 threshold values to indicate the number of Principle components to consider according to their total contribution in the variability of features. These approaches have been evaluated using the KDD Cup 1999 intrusion detection (KDD-Cup) dataset. The main purpose of this study is to find the best attribute normalization method along with the correct threshold value for PCA so that a fast unsupervised IDS can discover intrusions effectively. The results recommended using the Log normalization method combined the Euclidean distance while performing PCA.
ER  - 

TY  - CONF
JO  - Security and Privacy, 1996. Proceedings., 1996 IEEE Symposium on
TI  - An analysis of the timed Z-channel
T2  - Security and Privacy, 1996. Proceedings., 1996 IEEE Symposium on
IS  - 
SN  - 1081-6011
VO  - 
SP  - 2
EP  - 11
AU  - Moskowitz, I.S.
AU  - Greenwald, S.J.
AU  - Kang, M.H.
Y1  - 6-8 May 1996
PY  - 1996
KW  - algebra
KW  - information theory
KW  - noise
KW  - processor scheduling
KW  - security of data
KW  - time-varying channels
KW  - timing
KW  - token networks
KW  - CPU scheduler
KW  - algebraic problem
KW  - channel capacity
KW  - covert channels
KW  - input symbols
KW  - lower bound
KW  - memoryless channel
KW  - noise
KW  - noiseless channels
KW  - optimization problem
KW  - output symbol transmission times
KW  - timed Z-channel
KW  - token ring network
KW  - trinomial equation root
KW  - Equations
KW  - Information technology
KW  - Interference
KW  - Laboratories
KW  - Multilevel systems
KW  - Network topology
KW  - Postal services
KW  - Time factors
KW  - Timing
KW  - Token networks
VL  - 
JA  - Security and Privacy, 1996. Proceedings., 1996 IEEE Symposium on
DO  - 10.1109/SECPRI.1996.502664
AB  - Our timed Z-channel (a general case of the Z-channel) appears as the basis for a large class of covert channels. Golomb (1980) analyzed the Z-channel, a memoryless channel with two input symbols and two output symbols, where one of the input symbols is transmitted with noise while the other is transmitted without noise, and the output symbol transmission times are equal. We introduce the timed Z-channel, where the output symbol transmission times are different. Specifically, we show how the timed Z-channel applies to two examples of covert timing channel scenarios: a CPU scheduler and a token ring network. We then give a detailed analysis of our timed Z-channel. We report a new result expressing the capacity of the timed Z-channel as the log of the root of a trinomial equation. This changes the capacity calculation from an optimization problem into a simpler algebraic problem and illustrates the relationship between the noise and time factors. Further, it generalizes Shannon's (1948, 1949) work on noiseless channels for this special case. We also report a new result bounding the timed Z-channel's capacity from below. Finally, we show how an interesting observation that Golomb reported for the Z-channel also holds for the timed Z-channel
ER  - 

TY  - CONF
JO  - Cybernetic Intelligent Systems (CIS), 2011 IEEE 10th International Conference on
TI  - Time series analyses for forecasting network intrusions
T2  - Cybernetic Intelligent Systems (CIS), 2011 IEEE 10th International Conference on
IS  - 
SN  - 
VO  - 
SP  - 111
EP  - 116
AU  - Nehinbe, J.O.
Y1  - 1-2 Sept. 2011
PY  - 2011
KW  - forecasting theory
KW  - security of data
KW  - time series
KW  - TSA-log analyzer
KW  - computer system threats
KW  - electronic evidence monitoring
KW  - intrusion detection system
KW  - intrusion logs
KW  - network intrusion forecasting
KW  - thwart attacks
KW  - time series analysis
KW  - Computer networks
KW  - Computers
KW  - Conferences
KW  - Cybernetics
KW  - Intelligent systems
KW  - Intrusion detection
KW  - Time series analysis
KW  - Alert
KW  - intrusion
KW  - intrusion detection system
KW  - patterns of intrusions
KW  - time series analysis
KW  - timestamp
VL  - 
JA  - Cybernetic Intelligent Systems (CIS), 2011 IEEE 10th International Conference on
DO  - 10.1109/CIS.2011.6169144
AB  - Intrusion Detection Systems are fast-growing techniques for monitoring and garnering electronic evidences about suspicious activities that signify threats to computer systems. Generally, these mechanisms overwhelmingly describe and record patterns of suspicious packets as alerts in the form of intrusion logs. Thereafter, analysts must subsequently validate the content of each intrusion log to ascertain the validity of each alert. Secondly, high level of expertise is required to discern each alert. However, more time and resources are unduly spent at the expense of countermeasures that ought to be proactively initiated to thwart attacks in progress. Accordingly, TSA-Log analyzer that uses a computationally fast technique and a uniform baseline to determine patterns of intrusions is proposed in this paper. Validations that are carried out on five publicly available datasets demonstrate that propagation strategies of intrusions, efficient countermeasures and the extent of similarity of intrusions can be forecasted giving the knowledge of the patterns of alerts in intrusion logs.
ER  - 

TY  - CONF
JO  - Computer Network Defense, 2008. EC2ND 2008. European Conference on
TI  - An Architecture for Inline Anomaly Detection
T2  - Computer Network Defense, 2008. EC2ND 2008. European Conference on
IS  - 
SN  - 
VO  - 
SP  - 11
EP  - 18
AU  - Krueger, T.
AU  - Gehl, C.
AU  - Rieck, K.
AU  - Laskov, P.
Y1  - 11-12 Dec. 2008
PY  - 2008
KW  - computer network management
KW  - security of data
KW  - telecommunication security
KW  - inline anomaly detection
KW  - intrusion prevention system
KW  - packet filter
KW  - Computer architecture
KW  - Computer networks
KW  - Data analysis
KW  - Decision making
KW  - Event detection
KW  - Filters
KW  - Intelligent networks
KW  - Intrusion detection
KW  - Production systems
KW  - Protection
KW  - anomaly detection
KW  - intrusion detection
VL  - 
JA  - Computer Network Defense, 2008. EC2ND 2008. European Conference on
DO  - 10.1109/EC2ND.2008.8
AB  - In this paper we propose an intrusion prevention system (IPS) which operates inline and is capable to detect unknown attacks using anomaly detection methods. Incorporated in the framework of a packet filter each incoming packet is analyzed and -- according to an internal connection state and a computed anomaly score -- either delivered to the production system, redirected to a special hardened system or logged to a network sink for later analysis. Runtime measurements of an actual implementation prove that the performance overhead of the system is sufficient for inline processing. Accuracy measurements on real network data yield improvements especially in the number of false positives, which are reduced by a factor of five compared to a plain anomaly detector.
ER  - 

TY  - CONF
JO  - Machine Learning and Cybernetics, 2003 International Conference on
TI  - Intrusion discovery with data mining on Honeynet
T2  - Machine Learning and Cybernetics, 2003 International Conference on
IS  - 
SN  - 
VO  - 1
SP  - 41
EP  - 45 Vol.1
AU  - Jian Yin
AU  - Gang Zhang
AU  - Yi-Qun Chen
Y1  - 2-5 Nov. 2003
PY  - 2003
KW  - computer networks
KW  - data mining
KW  - security of data
KW  - Honeynet
KW  - data mining
KW  - firewall systems
KW  - log server
KW  - network attack detection system
KW  - network intrusion detection system
KW  - Abstracts
KW  - Computer science
KW  - Data analysis
KW  - Data mining
KW  - Electronic mail
KW  - Intrusion detection
KW  - Laboratories
KW  - Pattern analysis
KW  - Production
KW  - Productivity
VL  - 1
JA  - Machine Learning and Cybernetics, 2003 International Conference on
DO  - 10.1109/ICMLC.2003.1264439
AB  - In order to construct a secure network, attack and intrusion mode of intruders are analyzed for improving the capability of IDS and firewall systems. This paper presents a method for constructing a network intrusion and attack detection system on Honeynet, which is a true network that is constructed for the purpose of gathering information of intruders and attackers. Analyzing these data with data mining algorithm can work our attack modes, in this paper, we propose some method for mining patterns on various data from firewall records and log server in Honeynet, and the result can be in various forms such as associated rules of historic data or certain data structures required by IDS and firewall systems.
ER  - 

TY  - CONF
JO  - Communications, 2009. ICC '09. IEEE International Conference on
TI  - Extracting Attack Sessions from Real Traffic with Intrusion Prevention Systems
T2  - Communications, 2009. ICC '09. IEEE International Conference on
IS  - 
SN  - 1938-1883
VO  - 
SP  - 1
EP  - 5
AU  - I-Wei Chen
AU  - Po-Ching Lin
AU  - Chi-Chung Luo
AU  - Tsung-Huan Cheng
AU  - Ying-Dar Lin
AU  - Yuan-Cheng Lai
AU  - Lin, F.C.
Y1  - 14-18 June 2009
PY  - 2009
KW  - security of data
KW  - telecommunication congestion control
KW  - telecommunication security
KW  - attack session extraction
KW  - average completeness/purity
KW  - false negative
KW  - false positive
KW  - intrusion prevention system
KW  - performance index
KW  - traffic traces
KW  - Communications Society
KW  - Data mining
KW  - Databases
KW  - Information management
KW  - Paper technology
KW  - Peer to peer computing
KW  - Performance analysis
KW  - Switches
KW  - Technology management
KW  - Telecommunication traffic
VL  - 
JA  - Communications, 2009. ICC '09. IEEE International Conference on
DO  - 10.1109/ICC.2009.5199022
AB  - False Positive (FP) and False Negative (FN) happen to every Intrusion Prevention System (IPS). No one could do better judgment than others all the time. This work proposes a system of Attack Session Extraction (ASE) to create a pool of suspicious traffic traces which cause potential FNs (abbreviated as P-FNs) and potential FPs (abbreviated as P-FPs) to IPSes. Developers of IPSes can use these suspicious traffic traces to improve the accuracy of their products. Traffic traces are called suspicious since what they cause are P-FNs and P-FPs which need to be confirmed by the developers of IPSes whether P-FNs are FNs and P-FPs are FPs. First, the ASE captures real traffic and replays captured traffic traces to multiple IPSes. By comparing the logs of IPSes, we can find that some attack logs are logged or not logged only at certain IPS. The former is P-FPs, while the latter is P-FNs to that IPS. The ASE then starts to extract this suspicious traffic from replayed traffic traces. The extracted traffic traces can then be used for further analysis by IPS developers. Some of the traces may prove to be guilty, i.e. confirmed to be FNs and FPs. To completely extract a suspicious session, the ASE uses an association mechanism based on anchor packets, five-tuple and time, and similarity for the first packet, first connection, and whole session, respectively. It calculates the degree of similarity among packets to extract a suspicious session containing multiple connections. We define variation and completeness/purity as the performance indexes to evaluate ASE. The experiments demonstrate that 95% of extracted sessions have low variation, and the average completeness/purity is around 80%.
ER  - 

TY  - CONF
JO  - Control, Instrumentation, Communication and Computational Technologies (ICCICCT), 2014 International Conference on
TI  - Presenting a pattern for detection of denial of service attacks with web mining technique and fuzzy logic approach
T2  - Control, Instrumentation, Communication and Computational Technologies (ICCICCT), 2014 International Conference on
IS  - 
SN  - 
VO  - 
SP  - 156
EP  - 160
AU  - Jelodar, H.
AU  - Aramideh, J.
Y1  - 10-11 July 2014
PY  - 2014
KW  - Internet
KW  - Web sites
KW  - computer crime
KW  - computer network security
KW  - data mining
KW  - file servers
KW  - fuzzy logic
KW  - probability
KW  - DOS attacks
KW  - Web mining technique
KW  - Web site server
KW  - denial of service attack detection
KW  - fuzzy logic approach
KW  - log file analysis
KW  - preventive measures
KW  - probability
KW  - Computer crime
KW  - Computer hacking
KW  - Floods
KW  - Fuzzy systems
KW  - IP networks
KW  - Servers
KW  - Web mining
KW  - DOS attacks
KW  - Fuzzy logic
KW  - Internet hackers
KW  - Web mining
VL  - 
JA  - Control, Instrumentation, Communication and Computational Technologies (ICCICCT), 2014 International Conference on
DO  - 10.1109/ICCICCT.2014.6992947
AB  - Hackers attack a server of website collectively. In other words, DOS attacks have been sent by hackers into server. To identify DOS, an accurate approach is required. Such attacks can be detected by analyzing Log File of server. In this paper, attempt has been made to present a model by combining web mining technique with fuzzy logic approach to evaluate probability of DOS attacks using two effective factors of the number of Hits and time interval of the requests so that appropriate preventive measures can be taken under different conditions.
ER  - 

TY  - CONF
JO  - Temporal Representation and Reasoning, 2008. TIME '08. 15th International Symposium on
TI  - Time Aware Mining of Itemsets
T2  - Temporal Representation and Reasoning, 2008. TIME '08. 15th International Symposium on
IS  - 
SN  - 1530-1311
VO  - 
SP  - 93
EP  - 97
AU  - Saleh, B.
AU  - Masseglia, F.
Y1  - 16-18 June 2008
PY  - 2008
KW  - data analysis
KW  - data mining
KW  - security of data
KW  - very large databases
KW  - Web access logs
KW  - behavioural pattern mining
KW  - correlation extraction
KW  - data mining methods
KW  - intrusion detection
KW  - knowledge discovery
KW  - large databases
KW  - time aware mining
KW  - Association rules
KW  - DVD
KW  - Data mining
KW  - Intrusion detection
KW  - Itemsets
KW  - Navigation
KW  - Solids
KW  - TV
KW  - Transaction databases
KW  - Web pages
KW  - Itemsets
KW  - Optimal Frequency
KW  - Periods
VL  - 
JA  - Temporal Representation and Reasoning, 2008. TIME '08. 15th International Symposium on
DO  - 10.1109/TIME.2008.12
AB  - Frequent behavioural pattern mining is a very important topic of knowledge discovery, intended to extract correlations between items recorded in large databases or Web access logs. However, those databases are usually considered as a whole and hence, itemsets are extracted over the entire set of records. Our claim is that possible periods, hidden within the structure of the data and containing compact itemsets, may exist. These periods, as well as the itemsets they contain, might not be found by traditional data mining methods due to their very weak support. Furthermore, these periods might be lost depending on an arbitrary division of the data. The goal of our work is to find itemsets that are frequent over a specific period but would not be extracted by traditional methods since their support is very low over the whole dataset. In this paper, we introduce the definition of solid itemsets, which represent a coherent and compact behavior over a specific period, and we propose SIM, an algorithm for their extraction. This work may find many applications in sensitive domains such as fraud or intrusion detection.
ER  - 

TY  - CONF
JO  - Circuits and Systems, 2004. Proceedings. The 2004 IEEE Asia-Pacific Conference on
TI  - Seal imprint verification with rotation invariance
T2  - Circuits and Systems, 2004. Proceedings. The 2004 IEEE Asia-Pacific Conference on
IS  - 
SN  - 
VO  - 1
SP  - 597
EP  - 600 vol.1
AU  - Matsuura, T.
AU  - Yamazaki, K.
Y1  - 6-9 Dec. 2004
PY  - 2004
KW  - Fourier transforms
KW  - eigenvalues and eigenfunctions
KW  - feature extraction
KW  - image matching
KW  - image thinning
KW  - security of data
KW  - 2D Fourier series expansion coefficients
KW  - eigen vector
KW  - log-polar image
KW  - matrix eigen value
KW  - rotation invariance
KW  - rotation invariant feature
KW  - seal imprint verification method
KW  - Biometrics
KW  - Equations
KW  - Feature extraction
KW  - Fourier series
KW  - Humans
KW  - Inspection
KW  - Pixel
KW  - Seals
KW  - Skeleton
VL  - 1
JA  - Circuits and Systems, 2004. Proceedings. The 2004 IEEE Asia-Pacific Conference on
DO  - 10.1109/APCCAS.2004.1412833
AB  - This work discusses on a seal imprint verification method with rotation invariance. The rotation invariant feature is represented here by the coefficients of 2D Fourier series expansion of the log-polar image. Seal imprint can be verified by the eigen vector corresponding to the largest eigen value of a matrix defined in terms of the above Fourier coefficients. The seal imprint verification experiments are given to show the effectiveness of the proposed method.
ER  - 

TY  - CONF
JO  - Dependable Computing (PRDC), 2013 IEEE 19th Pacific Rim International Symposium on
TI  - Classification of DNS Queries for Anomaly Detection
T2  - Dependable Computing (PRDC), 2013 IEEE 19th Pacific Rim International Symposium on
IS  - 
SN  - 
VO  - 
SP  - 130
EP  - 131
AU  - Shi, H.
AU  - Iwasaki, K.
Y1  - 2-4 Dec. 2013
PY  - 2013
KW  - computer network security
KW  - pattern classification
KW  - query processing
KW  - self-organising feature maps
KW  - DNS query classification
KW  - DNS query frequency structure
KW  - DNS query log file analysis
KW  - Domain Name System
KW  - GHSOM
KW  - anomaly detection
KW  - growing hierarchical self-organizing map
KW  - healthy computers
KW  - infected computer detection
KW  - neural network
KW  - Computer crime
KW  - Computers
KW  - Internet
KW  - Malware
KW  - Time series analysis
KW  - Training
KW  - Vectors
KW  - DNS
KW  - GHSOM
KW  - classification
KW  - query interval
VL  - 
JA  - Dependable Computing (PRDC), 2013 IEEE 19th Pacific Rim International Symposium on
DO  - 10.1109/PRDC.2013.27
AB  - We propose a new method that uses a neural network, the Growing Hierarchical Self-Organizing Map (GHSOM), to analyze the DNS query log files. Due to the structure of the DNS query frequency, infected computers are easy to detect. Our experiment shows the different DNS query structure between healthy and infected computers.
ER  - 

TY  - CONF
JO  - Software Reliability Engineering Workshops (ISSREW), 2013 IEEE International Symposium on
TI  - Modeling of Failure detection and recovery in SysML
T2  - Software Reliability Engineering Workshops (ISSREW), 2013 IEEE International Symposium on
IS  - 
SN  - 
VO  - 
SP  - 85
EP  - 95
AU  - Hecht, M.
AU  - Tamaki, J.
AU  - Lo, D.
Y1  - 4-7 Nov. 2013
PY  - 2013
KW  - SysML
KW  - computational complexity
KW  - formal specification
KW  - security of data
KW  - system recovery
KW  - FMEA
KW  - SysML
KW  - artifact assessment
KW  - behavioral models
KW  - complexity
KW  - cyber attacks
KW  - failure detection
KW  - failure recovery
KW  - failure simulation
KW  - internal block diagrams
KW  - log analysis
KW  - safety criticality
KW  - structural models
KW  - ubiquity
KW  - Computer crashes
KW  - Fault trees
KW  - Payloads
KW  - Software reliability
KW  - Standards
KW  - Transforms
VL  - 
JA  - Software Reliability Engineering Workshops (ISSREW), 2013 IEEE International Symposium on
DO  - 10.1109/ISSREW.2013.6688879
AB  - A collection of slides from the author's conference presentation is given. The following topics are discussed: failure detection and recovery in SysML; ubiquity, complexity, and safety criticality of systems;FMEA; structural models; internal block diagrams; behavioral models; failure simulation; cyber attacks; log analysis; and artifact assessment.
ER  - 

TY  - CONF
JO  - Networking, Architecture, and Storage, 2009. NAS 2009. IEEE International Conference on
TI  - TH-CDP: An Efficient Block Level Continuous Data Protection System
T2  - Networking, Architecture, and Storage, 2009. NAS 2009. IEEE International Conference on
IS  - 
SN  - 
VO  - 
SP  - 395
EP  - 404
AU  - Yonghong Sheng
AU  - Dongsheng Wang
AU  - Jin-Yang He
AU  - Dapeng Ju
Y1  - 9-11 July 2009
PY  - 2009
KW  - data structures
KW  - security of data
KW  - /O request packet queue
KW  - IOzone
KW  - block level continuous data protection system
KW  - business processing
KW  - checkpointing mechanism
KW  - data writing
KW  - file system level
KW  - granular recovery point
KW  - log-structured technique
KW  - recovery assurance process
KW  - space recycling
KW  - storage systems
KW  - virtual volume image
KW  - Checkpointing
KW  - Computer errors
KW  - File systems
KW  - Helium
KW  - Image restoration
KW  - Information science
KW  - Laboratories
KW  - Production systems
KW  - Protection
KW  - Prototypes
KW  - Block level data protection
KW  - CDP
KW  - Checkpoint
KW  - Continuous Data Protection
VL  - 
JA  - Networking, Architecture, and Storage, 2009. NAS 2009. IEEE International Conference on
DO  - 10.1109/NAS.2009.69
AB  - Traditional data protection technologies, such as remote mirroring, snapshot and backup, cannot completely solve virus attack, user error problems. Continuous data protection (CDP), capturing all data writes at file or block level, is an enabling technology to storage systems against malicious attacks or user mistakes, because it allows each block data to be undoable. Most of existing data protection systems or prototypes are not real CDP ones because there is a data exposure between subsequent snapshots. Therefore they provide less granular recovery points. In addition, some products work either at file system level or at application level which lack general purpose. This paper presents the design, implementation, and performance of a new block level continuous data protection system, TH-CDP. Besides providing the basic functions of true CDP, TH-CDP provides virtual volume image at any point in time without affecting the production system. Another distinctive feature of TH-CDP is its checkpointing mechanism. By encapsulating the checkpoint information into I/O request packet queue, TH-CDP can take checkpoints without temporarily halting normal business processing or any incoming request. In addition, TH-CDP uses log-structured technique to store changed block data on raw disk, thereby speeding up both data writing and space recycling. Extensive experiments on file systems, databases using IOzone, and TPC-C benchmark show that TH-CDP can effectively improve the convenience of checkpointing and recovery assurance process. Under the circumstances of the changed block data up to 20 times of the original data size, the 100% sequence read speed of the oldest virtual volume image version is nearly 1/3 to 1/4 compared to the normal iSCSI volume.
ER  - 

TY  - CONF
JO  - DARPA Information Survivability Conference and Exposition, 2003. Proceedings
TI  - SPIE demonstration: single packet traceback
T2  - DARPA Information Survivability Conference and Exposition, 2003. Proceedings
IS  - 
SN  - 
VO  - 2
SP  - 106
EP  - 107 vol.2
AU  - Strayer, W.T.
AU  - Jones, C.E.
AU  - Snoeren, A.C.
Y1  - 22-24 April 2003
PY  - 2003
KW  - security of data
KW  - telecommunication network routing
KW  - telecommunication security
KW  - SPIE
KW  - Source Path Isolation Engine
KW  - attack graph
KW  - cache
KW  - forwarded traffic
KW  - hashes
KW  - intrusion detection system
KW  - memory requirement
KW  - packet digests
KW  - query
KW  - routers
KW  - simulated reverse-path flooding algorithm
KW  - single IP packet traceback
KW  - tap boxes
KW  - Computer crime
KW  - Data structures
KW  - Dissolved gas analysis
KW  - Engines
KW  - Floods
KW  - History
KW  - Intrusion detection
KW  - Isolation technology
KW  - TCPIP
KW  - Technological innovation
VL  - 2
JA  - DARPA Information Survivability Conference and Exposition, 2003. Proceedings
DO  - 10.1109/DISCEX.2003.1194937
AB  - SPIE, the Source Path Isolation Engine, is a DARPA-funded system for tracing single IP packets back through a network of instrumented routers or tap boxes that are associated with the routers. Historically, tracing individual packets by keeping packet logs at each router has required prohibitive amounts of memory; one of SPIE's key innovations is to reduce the memory requirement (down to 0.5% of link capacity) by storing only packet digests, that is, hashes of the packets rather than the packet itself. SPIE-enhanced routers maintain a cache of packet digests for recently forwarded traffic. If a packet is determined to be offensive by an intrusion detection system (or judged interesting by some other metric), a query is dispatched to the SPIE system that, in turn, queries routers for packet digests of the relevant time periods. ne results of this query are used in a simulated reverse-path flooding algorithm to build a highly reliable and accurate attack graph that identifies the packet's source or sources.
ER  - 

TY  - CONF
JO  - Dependable Systems and Networks, 2007. DSN '07. 37th Annual IEEE/IFIP International Conference on
TI  - Web Services Wind Tunnel: On Performance Testing Large-Scale Stateful Web Services
T2  - Dependable Systems and Networks, 2007. DSN '07. 37th Annual IEEE/IFIP International Conference on
IS  - 
SN  - 
VO  - 
SP  - 612
EP  - 617
AU  - De Barros, M.
AU  - Shiau, J.
AU  - Chen Shang
AU  - Gidewall, K.
AU  - Hui Shi
AU  - Forsmann, J.
Y1  - 25-28 June 2007
PY  - 2007
KW  - Web services
KW  - program testing
KW  - security of data
KW  - software performance evaluation
KW  - Markov chains
KW  - Web services wind tunnel
KW  - cache-based load simulation tools
KW  - data integrity
KW  - data sanitization
KW  - log analysis
KW  - performance testing
KW  - Availability
KW  - Benchmark testing
KW  - Environmental management
KW  - Large-scale systems
KW  - Pattern analysis
KW  - Production systems
KW  - Reproducibility of results
KW  - Scalability
KW  - System testing
KW  - Web services
VL  - 
JA  - Dependable Systems and Networks, 2007. DSN '07. 37th Annual IEEE/IFIP International Conference on
DO  - 10.1109/DSN.2007.102
AB  - New versions of existing large-scale web services such as Passport.comcopy have to go through rigorous performance evaluations in order to ensure a high degree of availability. Performance testing (such as benchmarking, scalability, and capacity tests) of large-scale stateful systems in managed test environments has many different challenges, mainly related to the reproducibility of production conditions in live data centers. One of these challenges is creating a dataset in a test environment that mimics the actual dataset in production. Other challenges involve the characterization of load patterns in production based on log analysis and proper load simulation via reutilization of data from the existing dataset. The intent of this paper is to describe practical approaches to address some of the aforementioned challenges through the use of various novel techniques. For example, this paper discusses data sanitization, which is the alteration of large datasets in a controlled manner to obfuscate sensitive information, preserving data integrity, relationships, and data equivalence classes. This paper also provides techniques for load pattern characterization via the application of Markov chains to custom and generic logs, as well as general guidelines for the development of cache-based load simulation tools tailored for the performance evaluation of stateful systems.
ER  - 

TY  - CONF
JO  - Computational Intelligence and Informatics (CINTI), 2010 11th International Symposium on
TI  - [Title page]
T2  - Computational Intelligence and Informatics (CINTI), 2010 11th International Symposium on
IS  - 
SN  - 
VO  - 
SP  - 1
EP  - 1
Y1  - 18-20 Nov. 2010
PY  - 2010
KW  - character recognition
KW  - computer aided instruction
KW  - computer networks
KW  - consumer behaviour
KW  - data mining
KW  - embedded systems
KW  - fuzzy neural nets
KW  - fuzzy set theory
KW  - grammars
KW  - graph colouring
KW  - greenhouses
KW  - mobile computing
KW  - ontologies (artificial intelligence)
KW  - security of data
KW  - social networking (online)
KW  - support vector machines
KW  - user interfaces
KW  - Aumann equilibrium
KW  - Nash equilibria
KW  - artificial neural network
KW  - character recognition
KW  - computer network
KW  - consumer behaviour
KW  - embedded system
KW  - grammar system
KW  - graph coloring
KW  - intelligent greenhouse
KW  - intrusion detection
KW  - log mining
KW  - mobile learning
KW  - neural fuzzy system
KW  - ontologies
KW  - social network
KW  - support vector machine
KW  - triangular fuzzy number
KW  - user access
VL  - 
JA  - Computational Intelligence and Informatics (CINTI), 2010 11th International Symposium on
DO  - 10.1109/CINTI.2010.5672217
AB  - The following topics are dealt with: ontologies; grammar systems; Aumann equilibrium; intrusion detection; support vector machine; Nash equilibria; intelligent greenhouse; neural fuzzy system; computer network; triangular fuzzy number; graph coloring; social network; embedded system; artificial neural network; user access; log mining; character recognition; consumer behaviour; and mobile learning.
ER  - 

TY  - CONF
JO  - Parallel, Distributed and Network-Based Processing (PDP), 2010 18th Euromicro International Conference on
TI  - Event Correlation on the Basis of Activation Patterns
T2  - Parallel, Distributed and Network-Based Processing (PDP), 2010 18th Euromicro International Conference on
IS  - 
SN  - 1066-6192
VO  - 
SP  - 631
EP  - 640
AU  - Teufl, P.
AU  - Payer, U.
AU  - Fellner, R.
Y1  - 17-19 Feb. 2010
PY  - 2010
KW  - security of data
KW  - activation patterns
KW  - artificial intelligence
KW  - connection statistics
KW  - event correlation system
KW  - intrusion detection systems
KW  - log entries
KW  - machine learning
KW  - protocol identifiers
KW  - superordinate incidences
KW  - Artificial intelligence
KW  - Data mining
KW  - Event detection
KW  - Expert systems
KW  - Humans
KW  - Information processing
KW  - Intrusion detection
KW  - Libraries
KW  - Machine learning
KW  - Sensor systems
KW  - activation patterns
KW  - event correlation
KW  - kdd
KW  - machine learning
KW  - semantic networks
KW  - sensor fusion
KW  - spreading activation
KW  - unsupervised clustering
VL  - 
JA  - Parallel, Distributed and Network-Based Processing (PDP), 2010 18th Euromicro International Conference on
DO  - 10.1109/PDP.2010.80
AB  - Intrusion Detection Systems (IDS) deploy various sensors that collect data, process this data and report events. The process of combining these events or superordinate incidences is known as event correlation. The key issues of this process are (1) to find a way how to combine events based on different data types (e. g. log entries, connection statistics or protocol identifiers), (2) to build a model representing the relations between the events and (3) to apply subsequent analysis that allow us to extract meaningful information from the trained model. In order to address these key issues, we introduce the concept of Activation Patterns. These patterns are generated by applying various techniques from machine learning and artificial intelligence to the raw event data. The presented technique is then integrated into an event correlation system. We describe the system and evaluate it by analyzing a popular intrusion detection data set consisting of a wide range of different features.
ER  - 

TY  - CONF
JO  - Electronic and Mechanical Engineering and Information Technology (EMEIT), 2011 International Conference on
TI  - Research on incremental decision tree algorithm
T2  - Electronic and Mechanical Engineering and Information Technology (EMEIT), 2011 International Conference on
IS  - 
SN  - 
VO  - 1
SP  - 303
EP  - 306
AU  - Chi Qingyun
Y1  - 12-14 Aug. 2011
PY  - 2011
KW  - Internet
KW  - consumer behaviour
KW  - data analysis
KW  - decision trees
KW  - learning (artificial intelligence)
KW  - pattern classification
KW  - security of data
KW  - RAINFOREST structure
KW  - SPRINT decision tree algorithm
KW  - Web log analysis
KW  - customer behavior
KW  - data analysis
KW  - incremental decision tree algorithm
KW  - network intrusion detection system
KW  - online classification system
KW  - sustainable operation
KW  - Algorithm design and analysis
KW  - Classification algorithms
KW  - Data mining
KW  - Decision trees
KW  - Indexes
KW  - Remuneration
KW  - Training
KW  - Data mining
KW  - Gini-index
KW  - Incremental learning
KW  - decision tree
VL  - 1
JA  - Electronic and Mechanical Engineering and Information Technology (EMEIT), 2011 International Conference on
DO  - 10.1109/EMEIT.2011.6022930
AB  - For data analysis of increase rapidly customer behavior, Web log analysis, network intrusion detection systems and other online classification system, how to quickly adapt to new samples is the key to ensure proper classification and sustainable operation. This paper presents a new adaptation data incremental decision tree algorithm, which combines RAINFOREST structure. It combines with the traditional SPRINT decision tree algorithm, and uses new samples quickly train a new decision tree based on the original decision tree. The improved algorithm deal with new samples at any time to produce a decision tree related, and the tree has been optimized with real-time.
ER  - 

TY  - CONF
JO  - Web Services (ICWS), 2011 IEEE International Conference on
TI  - Reputation-Driven Web Service Selection Based on Collaboration Network
T2  - Web Services (ICWS), 2011 IEEE International Conference on
IS  - 
SN  - 
VO  - 
SP  - 704
EP  - 705
AU  - Xilu Zhu
AU  - Bai Wang
AU  - Shangguang Wang
Y1  - 4-9 July 2011
PY  - 2011
KW  - Web services
KW  - security of data
KW  - Web service collaboration network
KW  - collaboration reputation
KW  - collaboration trust
KW  - composite service execution log
KW  - reputation model
KW  - reputation-driven Web service selection
KW  - trustworthy Web service selection
KW  - Collaboration
KW  - Communities
KW  - Compounds
KW  - Equations
KW  - Quality of service
KW  - Web services
KW  - Wireless sensor networks
KW  - Collaboration Reputation
KW  - Community
KW  - Trust
KW  - Web Service Collaboration Network
KW  - Web Service Selection
VL  - 
JA  - Web Services (ICWS), 2011 IEEE International Conference on
DO  - 10.1109/ICWS.2011.23
AB  - Most of trustworthy web service selection simply focus on individual reputation and ignore the collaboration reputation between services. To enhance the collaboration trust during web service selection, a reputation model called collaboration reputation is proposed. The reputation model is built on web service collaboration network(WSCN), which is constructed in terms of the composite service execution log. Thus, the WSCN aims to maintain the trustworthy collaboration alliance among web services, In WSCN, the collaboration reputation can be assessed by two metrics, one called invoking reputation is computed by recommendation, which is selected from the community structure hiding in WSCN, the other is assessed by the invoked web service. In addition, the web service selection based on WSCN is designed.
ER  - 

TY  - CONF
JO  - Power Engineering Society Winter Meeting, 2000. IEEE
TI  - Practical aspects of running Netscape server in deregulated market operations
T2  - Power Engineering Society Winter Meeting, 2000. IEEE
IS  - 
SN  - 
VO  - 3
SP  - 1662
EP  - 1667 vol.3
AU  - Lee, W.
AU  - Alaywan, Z.
Y1  - 23-27 Jan 2000
PY  - 2000
KW  - electricity supply industry
KW  - electronic commerce
KW  - information resources
KW  - network servers
KW  - power generation scheduling
KW  - security of data
KW  - California ISO
KW  - Netscape Enterprise Server
KW  - Netscape server
KW  - SNMP monitoring
KW  - bidding results retrieval
KW  - bids submission
KW  - certificates
KW  - deregulated market operations
KW  - e-commerce
KW  - electricity market
KW  - keys
KW  - logging
KW  - object signing
KW  - online trading
KW  - scheduling coordinators
KW  - secure socket layer
KW  - secure web site
KW  - Availability
KW  - Electricity supply industry
KW  - Electricity supply industry deregulation
KW  - ISO
KW  - Job shop scheduling
KW  - Maintenance
KW  - Monitoring
KW  - Power system reliability
KW  - Processor scheduling
KW  - Sockets
VL  - 3
JA  - Power Engineering Society Winter Meeting, 2000. IEEE
DO  - 10.1109/PESW.2000.847594
AB  - California ISO uses a secure web site to conduct the electricity market place. Scheduling coordinators submit bids and schedules into this web site and retrieve bidding results and final schedules/prices from it. This paper discusses the experience and practical aspects of running and administering this web site using Netscape Enterprise Server. It covers the usage of secure socket layer, keys and certificates, users and groups, logging, SNMP monitoring and object signing. This also describes how to maintain high availability of this important web site
ER  - 

TY  - CONF
JO  - Management of Replicated Data, 1990. Proceedings., Workshop on the
TI  - Implicit replication in a network file server
T2  - Management of Replicated Data, 1990. Proceedings., Workshop on the
IS  - 
SN  - 
VO  - 
SP  - 85
EP  - 90
AU  - Bhide, A.
AU  - Elnozahy, E.N.
AU  - Morgan, S.P.
Y1  - 8-9 Nov 1990
PY  - 1990
KW  - file organisation
KW  - file servers
KW  - network servers
KW  - security of data
KW  - HA-NFS
KW  - IBM RISC
KW  - disk log
KW  - file replication
KW  - network file server
KW  - server reliability
KW  - storage reliability
KW  - Application software
KW  - Computer science
KW  - Fault tolerance
KW  - File servers
KW  - File systems
KW  - Guidelines
KW  - Intelligent networks
KW  - Network servers
KW  - Protocols
KW  - Reduced instruction set computing
VL  - 
JA  - Management of Replicated Data, 1990. Proceedings., Workshop on the
DO  - 10.1109/MRD.1990.138251
AB  - The design and implementation of a highly available network file server (HA-NFS) is reported. It is implemented on a network of workstations from the IBM RISC System/6000 family. HA-NFS servers preserve the semantics of the NFS protocol and can be used by existing NFS clients without modification. Therefore, existing application programs can benefit from highly availability without alteration. HA-NFS achieves storage reliability by (optionally) replicating files on different disks. However, all copies of the same file are controlled by a single server, reducing the cost of ensuring consistency. To achieve server reliability, each server is implicitly replicated by a backup that can access the server's disks if the server fails. During normal operation, the backup monitors the liveness of the server but does not maintain information about the server's internal state. Each server maintains a disk log that records state information normally kept in memory
ER  - 

TY  - CONF
JO  - Trust, Security and Privacy in Computing and Communications (TrustCom), 2011 IEEE 10th International Conference on
TI  - Abnormal Process Instances Identification Method in Healthcare Environment
T2  - Trust, Security and Privacy in Computing and Communications (TrustCom), 2011 IEEE 10th International Conference on
IS  - 
SN  - 
VO  - 
SP  - 1387
EP  - 1392
AU  - Bingning Han
AU  - Lihong Jiang
AU  - Hongming Cai
Y1  - 16-18 Nov. 2011
PY  - 2011
KW  - data mining
KW  - diseases
KW  - health care
KW  - hospitals
KW  - pattern classification
KW  - security of data
KW  - APIIM
KW  - abnormal process instances identification method
KW  - classification attributes
KW  - disease
KW  - event logs
KW  - healthcare environment
KW  - hospitals
KW  - medical test items
KW  - outlier detection technology
KW  - process analysis
KW  - process mining
KW  - standard clinical pathway optimization
KW  - Data mining
KW  - Detection algorithms
KW  - Diabetes
KW  - Educational institutions
KW  - Hospitals
KW  - Process control
KW  - business process mining
KW  - clinical pathway
KW  - outlier detection
KW  - process analysis
VL  - 
JA  - Trust, Security and Privacy in Computing and Communications (TrustCom), 2011 IEEE 10th International Conference on
DO  - 10.1109/TrustCom.2011.189
AB  - In order to gain the competitive advantage, more and more hospitals put their attention on determining and optimizing the standard clinical pathway. However, there are many abnormal instances in the event logs which disturb the effect of the process mining and the process analysis. Also, the prescription and the medical test items may have a large difference in the specific disease, where has the similar clinical pathways due to the multi-factor in the clinical pathways. In this paper, an abnormal process instances identification method (APIIM) is proposed. Given the event logs and the standard clinical pathway, the method classifies the instances based on the classification attributes and identifies the abnormal instances by the outlier detection technology. Moreover, a case study using the real data in one hospital is implemented and the result shows that the method is effective and efficient in discovering the abnormal process instances in the healthcare environment.
ER  - 

TY  - CONF
JO  - Computer Engineering and Technology (ICCET), 2010 2nd International Conference on
TI  - Detecting capability evaluate of spider detection techniques
T2  - Computer Engineering and Technology (ICCET), 2010 2nd International Conference on
IS  - 
SN  - 
VO  - 7
SP  - V7-268
EP  - V7-271
AU  - Fan Chunlong
AU  - Yu Zhouhua
AU  - Xu Lei
Y1  - 16-18 April 2010
PY  - 2010
KW  - Internet
KW  - belief networks
KW  - binomial distribution
KW  - decision trees
KW  - security of data
KW  - statistical analysis
KW  - Bayesian network
KW  - EMT
KW  - SDT
KW  - Web log data
KW  - binomial distribution theory
KW  - capability evaluate
KW  - decision tree
KW  - detecting capability
KW  - evaluation method
KW  - harvesting Internet resources
KW  - spider detection techniques
KW  - statistical hypothesis
KW  - trap technique
KW  - traps layout information
KW  - Aerospace engineering
KW  - Bayesian methods
KW  - Computer science
KW  - Computerized monitoring
KW  - Decision trees
KW  - Internet
KW  - Manuals
KW  - Robots
KW  - Tin
KW  - Uniform resource locators
KW  - Spider
KW  - binomial distribution
KW  - evaluation method
KW  - spider detection techniques
VL  - 7
JA  - Computer Engineering and Technology (ICCET), 2010 2nd International Conference on
DO  - 10.1109/ICCET.2010.5485230
AB  - Spider is a program for harvesting internet resources. In order to regulate and monitor accessing behavior of spiders, many Spiders Detection Techniques (SDT) are proposed based on Decision Tree and Bayesian Network etc. At present, the evaluation of these detection techniques mainly rely on manual analysis of web log data. In order to objectively and accurately evaluate the detecting capability of SDT, an Evaluation Method based on Trap technique (EMT) is proposed in this paper. The principles of EMT base on the statistical hypothesis that the users captured by trap obeying binomial distribution theory. The traps layout information and users accessing information are used to calculate evaluating indicators. The evaluating result of experiment indicates that EMT has an accurate and scientific conclusion.
ER  - 

TY  - CONF
JO  - Information Science and Applications (ICISA), 2013 International Conference on
TI  - Anti-SIFT Images Based CAPTCHA Using Versatile Characters
T2  - Information Science and Applications (ICISA), 2013 International Conference on
IS  - 
SN  - 
VO  - 
SP  - 1
EP  - 4
AU  - Chen-Chiung Hsieh
AU  - Zong-Yu Wu
Y1  - 24-26 June 2013
PY  - 2013
KW  - feature extraction
KW  - image coding
KW  - security of data
KW  - transforms
KW  - HELLO CAPTCHA
KW  - Website account
KW  - antiSIFT image based CAPTCHA
KW  - automated image analysis technologies
KW  - automated program attacks
KW  - automation processes
KW  - average logging time
KW  - human form filling tasks
KW  - illegal behavior
KW  - innovative image-based CAPTCHA
KW  - pattern recognition
KW  - reCAPTCHA
KW  - scale-invariant feature transform
KW  - spam e-mail
KW  - text boxes
KW  - versatile characters
KW  - CAPTCHAs
KW  - Computers
KW  - Feature extraction
KW  - Image color analysis
KW  - Man machine systems
KW  - Switches
KW  - Transforms
VL  - 
JA  - Information Science and Applications (ICISA), 2013 International Conference on
DO  - 10.1109/ICISA.2013.6579426
AB  - Due to vigorous development of pattern recognition, traditional human form filling tasks would be replaced by automated processes. However, these automation processes are often misused for illegal behavior such as spam e-mail or application for website account. In order to prevent website owner from suffering the attacks of automated program, this paper proposed an innovative image-based CAPTCHA for distinguishing human and computer by embedding versatile characters in the images. The proposed method makes the characters indiscernible by automated image analysis technologies like scale-invariant feature transform while human can easily distinguish the location of the embedded characters. Our designed mechanism was capable to elude such kind of attacks. In experiments, 15 users were invited to test the system and the success rate is 86%. If wrong operations like clicking out of text boxes were excluded, the success rate reached 95%. Compare the average logging time with reCAPTCHA and HELLO CAPTCHA, the proposed method is faster than these two methods by 32 seconds and 115 seconds, respectively.
ER  - 

TY  - CONF
JO  - Dependability of Computer Systems, 2009. DepCos-RELCOMEX '09. Fourth International Conference on
TI  - Detecting Botnets in Computer Networks Using Multi-agent Technology
T2  - Dependability of Computer Systems, 2009. DepCos-RELCOMEX '09. Fourth International Conference on
IS  - 
SN  - 
VO  - 
SP  - 192
EP  - 201
AU  - Szymczyk, M.
Y1  - June 30 2009-July 2 2009
PY  - 2009
KW  - multi-agent systems
KW  - operating systems (computers)
KW  - security of data
KW  - botnets detection
KW  - bots detector
KW  - computer networks
KW  - illegal activities
KW  - intrusion detection system
KW  - malicious software
KW  - multiagent technology
KW  - operating system event log analyzer
KW  - Computer networks
VL  - 
JA  - Dependability of Computer Systems, 2009. DepCos-RELCOMEX '09. Fourth International Conference on
DO  - 10.1109/DepCoS-RELCOMEX.2009.46
AB  - The paper presents a hybrid model of the bots detector which is a combination of host intrusion detection system and the operating system event log analyzer. Bot can be defined as a computer that have been attacked by a hacker or infected with malicious software and is used for illegal activities. Collections of infected computers form a botnet. The proposed system is used to detect bots based on the evaluation of events occurring in the operating system and network environment. Detection algorithms based on the signatures derived from the analysis of the various types of malicious software that creates bots. The model has been implemented using multi-agent technology.
ER  - 

TY  - CONF
JO  - Systematic Approaches to Digital Forensic Engineering, 2008. SADFE '08. Third International Workshop on
TI  - [Front cover]
T2  - Systematic Approaches to Digital Forensic Engineering, 2008. SADFE '08. Third International Workshop on
IS  - 
SN  - 
VO  - 
SP  - C1
EP  - C1
Y1  - 22-22 May 2008
PY  - 2008
KW  - security of data
KW  - digital forensics
KW  - legal views
KW  - residual data
KW  - workstation logs
VL  - 
JA  - Systematic Approaches to Digital Forensic Engineering, 2008. SADFE '08. Third International Workshop on
DO  - 10.1109/SADFE.2008.26
AB  - The following topics are dealt with: digital forensics; residual data; workstation logs; and legal views.
ER  - 

TY  - CONF
JO  - Information Security South Africa (ISSA), 2011
TI  - An evaluation of lightweight classification methods for identifying malicious URLs
T2  - Information Security South Africa (ISSA), 2011
IS  - 
SN  - 
VO  - 
SP  - 1
EP  - 6
AU  - Egan, S.
AU  - Irwin, B.
Y1  - 15-17 Aug. 2011
PY  - 2011
KW  - Internet
KW  - information analysis
KW  - pattern classification
KW  - security of data
KW  - CW classifier update methods
KW  - Phishtank
KW  - URL length
KW  - WHOIS lookups
KW  - blacklist lookups
KW  - content analysis
KW  - directory structure
KW  - incident response analysis
KW  - information gathering
KW  - lexical analysis
KW  - lightweight classification algorithms
KW  - link filtering
KW  - malicious URL identification
KW  - proxy traffic logs
KW  - Algorithm design and analysis
KW  - Biological neural networks
KW  - Browsers
KW  - Electronic mail
KW  - IP networks
KW  - Neurons
KW  - Training data
KW  - Content filtering
KW  - Heuristics
KW  - Malware
KW  - Phishing
KW  - URL classification
VL  - 
JA  - Information Security South Africa (ISSA), 2011
DO  - 10.1109/ISSA.2011.6027532
AB  - Recent research has shown that it is possible to identify malicious URLs through lexical analysis of their URL structures alone. This paper intends to explore the effectiveness of these lightweight classification algorithms when working with large real world datasets including lists of malicious URLs obtained from Phishtank as well as largely filtered benign URLs obtained from proxy traffic logs. Lightweight algorithms are defined as methods by which URLs are analysed that do not use external sources of information such as WHOIS lookups, blacklist lookups and content analysis. These parameters include URL length, number of delimiters as well as the number of traversals through the directory structure and are used throughout much of the research in the paradigm of lightweight classification. Methods which include external sources of information are often called fully featured classifications and have been shown to be only slightly more effective than a purely lexical analysis when considering both false-positives and false-negatives. This distinction allows these algorithms to be run client side without the introduction of additional latency, but still providing a high level of accuracy through the use of modern techniques in training classifiers. Analysis of this type will also be useful in an incident response analysis where large numbers of URLs need to be filtered for potentially malicious URLs as an initial step in information gathering as well as end user implementations such as browser extensions which could help protect the user from following potentially malicious links. Both AROW and CW classifier update methods will be used as prototype implementations and their effectiveness will be compared to fully featured analysis results. These methods are interesting because they are able to train on any labelled data, including instances in which their prediction is correct, allowing them to build a confidence in specific lexical features. This makes it possible fo- - r them to be trained using noisy input data, making them ideal for real world applications such as link filtering and information gathering.
ER  - 

TY  - CONF
JO  - Signals, Systems and Computers, 2006. ACSSC '06. Fortieth Asilomar Conference on
TI  - Colluder Detection for Nonlinear Collusion Attacks
T2  - Signals, Systems and Computers, 2006. ACSSC '06. Fortieth Asilomar Conference on
IS  - 
SN  - 1058-6393
VO  - 
SP  - 603
EP  - 607
AU  - Yingwei Yao
Y1  - Oct. 29 2006-Nov. 1 2006
PY  - 2006
KW  - fingerprint identification
KW  - security of data
KW  - colluder detection
KW  - colluder identification
KW  - computational complexity
KW  - correlation-based detectors
KW  - digital fingerprinting systems
KW  - log-likelihood ratio tests
KW  - nonlinear collusion attacks
KW  - Computational complexity
KW  - Cryptography
KW  - Degradation
KW  - Detectors
KW  - Fingerprint recognition
KW  - Statistical distributions
KW  - Statistics
KW  - Technology management
KW  - Testing
KW  - Watermarking
VL  - 
JA  - Signals, Systems and Computers, 2006. ACSSC '06. Fortieth Asilomar Conference on
DO  - 10.1109/ACSSC.2006.354819
AB  - We investigate the problem of colluder identification for digital fingerprinting systems under nonlinear collusion attacks. Formulating colluder detection as a binary hypothesis testing problem, we derive the log-likelihood ratio tests for various nonlinear collusion attacks. Utilizing the approximate distribution of the order statistics, we obtain suboptimal detection statistics with low complexity. Compared with the existing correlation-based detectors, these detectors provide substantial improvement in both detection performance and computational complexity.
ER  - 

TY  - CONF
JO  - Data Mining, 2004. ICDM '04. Fourth IEEE International Conference on
TI  - Estimation of false negatives in classification
T2  - Data Mining, 2004. ICDM '04. Fourth IEEE International Conference on
IS  - 
SN  - 
VO  - 
SP  - 475
EP  - 478
AU  - Mane, S.
AU  - Srivastava, J.
AU  - San-Yih Hwang
AU  - Vayghan, J.
Y1  - 1-4 Nov. 2004
PY  - 2004
KW  - classification
KW  - security of data
KW  - unsolicited e-mail
KW  - capture-recapture based method
KW  - classification problem
KW  - false negative estimation
KW  - log-linear model
KW  - network intrusion
KW  - spam detection
KW  - Computer science
KW  - Costs
KW  - Humans
KW  - Information management
KW  - Intelligent networks
KW  - Intrusion detection
KW  - Machine learning
KW  - Marketing and sales
KW  - Testing
KW  - Time factors
VL  - 
JA  - Data Mining, 2004. ICDM '04. Fourth IEEE International Conference on
DO  - 10.1109/ICDM.2004.10048
AB  - In many classification problems such as spam detection and network intrusion, a large number of unlabeled test instances are predicted negative by the classifier However, the high costs as well as time constraints on an expert's time prevent further analysis of the "predicted false" class instances in order to segregate the false negatives from the true negatives. A systematic method is thus required to obtain an estimate of the number of false negatives. A capture-recapture based method can be used to obtain an ML-estimate of false negatives when two or more independent classifiers are available. In the case for which independence does not hold, we can apply log-linear models to obtain an estimate of false negatives. However, as shown in this paper, lesser the dependencies among the classifiers, better is the estimate obtained for false negatives. Thus, ideally independent classifiers should be used to estimate the false negatives in an unlabeled dataset. Experimental results on the spam dataset from the UCI machine learning repository are presented.
ER  - 

TY  - CONF
JO  - Computer Security Applications Conference, 2004. 20th Annual
TI  - Design, implementation, and evaluation of a repairable database management system
T2  - Computer Security Applications Conference, 2004. 20th Annual
IS  - 
SN  - 1063-9527
VO  - 
SP  - 179
EP  - 188
AU  - Tzi-cker Chiueh
AU  - Pilania, D.
Y1  - 6-10 Dec. 2004
PY  - 2004
KW  - SQL
KW  - back-up procedures
KW  - database management systems
KW  - fault tolerance
KW  - security of data
KW  - transaction processing
KW  - Phoenix system
KW  - PostgreSQL DBMS
KW  - database damage repair process
KW  - database management systems
KW  - intertransaction dependency information
KW  - software errors
KW  - Availability
KW  - Database systems
KW  - Delay
KW  - Error correction
KW  - Hardware
KW  - Measurement
KW  - Protection
KW  - Prototypes
KW  - Throughput
KW  - Transaction databases
VL  - 
JA  - Computer Security Applications Conference, 2004. 20th Annual
DO  - 10.1109/CSAC.2004.15
AB  - Although conventional database management systems are designed to tolerate hardware and to a lesser extent even software errors, they cannot protect themselves against syntactically correct and semantically damaging transactions, which could arise because of malicious attacks or honest mistakes. The lack of fast post-intrusion or post-error damage repair in modern DBMSs results in a longer mean time to repair (MTTR) and sometimes permanent data loss that could have been saved by more intelligent repair mechanisms. In this paper, we describe the design and implementation of Phoenix - a system that significantly improves the efficiency and precision of a database damage repair process after an intrusion or operator error and thus, increases the overall database system availability. The two key ideas underlying Phoenix are (1) maintaining persistent intertransaction dependency information at run time to allow selective undo of database transactions that are considered "infected" by the intrusion or error in question and (2) exploiting information present in standard database logs for fast selective undo. Performance measurements on a fully operational Phoenix prototype, which is based on the PostgreSQL DBMS, demonstrate that Phoenix incurs a response time and a throughput penalty of less than 5% and 8%, respectively, under the TPC-C benchmark, but it can speed up the post-intrusion database repair process by at least an order of magnitude when compared with a manual repair process.
ER  - 

TY  - CONF
JO  - Big Data (BigData Congress), 2014 IEEE International Congress on
TI  - Contextual Anomaly Detection in Big Sensor Data
T2  - Big Data (BigData Congress), 2014 IEEE International Congress on
IS  - 
SN  - 
VO  - 
SP  - 64
EP  - 71
AU  - Hayes, M.A.
AU  - Capretz, M.A.M.
Y1  - June 27 2014-July 2 2014
PY  - 2014
KW  - Big Data
KW  - Internet of Things
KW  - security of data
KW  - Internet of Things
KW  - big sensor data
KW  - context-aware anomaly detection algorithm
KW  - contextual anomaly detection
KW  - environmental sensors
KW  - multivariate clustering algorithm
KW  - real-time point anomaly detection
KW  - sensor profiles
KW  - streaming sensor networks
KW  - Big data
KW  - Clustering algorithms
KW  - Context
KW  - Detection algorithms
KW  - Detectors
KW  - Mathematical model
KW  - Real-time systems
KW  - Big Data Analytics
KW  - Contextual Anomaly Detection
KW  - Multivariate Clustering
KW  - Predictive Modelling
VL  - 
JA  - Big Data (BigData Congress), 2014 IEEE International Congress on
DO  - 10.1109/BigData.Congress.2014.19
AB  - Performing predictive modelling, such as anomaly detection, in Big Data is a difficult task. This problem is compounded as more and more sources of Big Data are generated from environmental sensors, logging applications, and the Internet of Things. Further, most current techniques for anomaly detection only consider the content of the data source, i.e. the data itself, without concern for the context of the data. As data becomes more complex it is increasingly important to bias anomaly detection techniques for the context, whether it is spatial, temporal, or semantic. The work proposed in this paper outlines a contextual anomaly detection technique for use in streaming sensor networks. The technique uses a well-defined content anomaly detection algorithm for real-time point anomaly detection. Additionally, we present a post-processing context-aware anomaly detection algorithm based on sensor profiles, which are groups of contextually similar sensors generated by a multivariate clustering algorithm. Our proposed research has been implemented and evaluated with real-world data provided by Powersmiths, located in Brampton, Ontario, Canada.
ER  - 

TY  - CONF
JO  - Interaction Sciences (ICIS), 2011 4th International Conference on
TI  - Design of remote control system for data protection and backup in mobile devices
T2  - Interaction Sciences (ICIS), 2011 4th International Conference on
IS  - 
SN  - 
VO  - 
SP  - 189
EP  - 193
AU  - Inwhee Joe
AU  - Yoonsang Lee
Y1  - 16-18 Aug. 2011
PY  - 2011
KW  - Web sites
KW  - mobile computing
KW  - mobile handsets
KW  - security of data
KW  - telecontrol
KW  - Website provision unit
KW  - access management unit
KW  - data backup
KW  - data protection
KW  - joiner confirmation information
KW  - mobile communication terminals
KW  - mobile devices
KW  - personal information leakage
KW  - remote control service code
KW  - remote control system
KW  - service completion code
KW  - service execution unit
KW  - smartphone
KW  - Control systems
KW  - Mobile communication
KW  - Servers
KW  - Smart phones
VL  - 
JA  - Interaction Sciences (ICIS), 2011 4th International Conference on
DO  - 
AB  - Performance upgrades for mobile devices took place due to jumping development of technology, so high-performance terminals that anyone can directly search and amend desired information anywhere and anytime, namely, mobile communication terminals called Smartphone were released to the market. Such terminals can store information that an individual saved, for example, call log, where to make contact and address of acquaintances, transmit/receive message and mail, photographs and videos, etc., but there are worries on surreptitious use of other people due to leaking of personal information through loss or theft of these terminals, so our paper aims to realize a system that remotely controls information inside the terminals in this case. The proposed system consists of four functional units: a website provision unit playing a role of interface so that users can remotely control a terminal, a user confirmation unit that performs joiner confirmation information from a system of a mobile communication company joined after a user confirm oneself as the person oneself, an access management unit that sets access by transmitting an access code for initiating a remote control program if a person is confirmed as a real owner of the terminal, and finally a service execution unit that transmits a remote control service code and an environment setting value for remotely controlling a terminal to the terminal and receives a service completion code from the terminal.
ER  - 

TY  - JOUR
JO  - Systems, Man and Cybernetics, Part A: Systems and Humans, IEEE Transactions on
TI  - Evaluating damage from cyber attacks: a model and analysis
T2  - Systems, Man and Cybernetics, Part A: Systems and Humans, IEEE Transactions on
IS  - 4
SN  - 1083-4427
VO  - 31
SP  - 300
EP  - 310
AU  - Lala, C.
AU  - Panda, B.
Y1  - Jul 2001
PY  - 2001
KW  - data structures
KW  - database management systems
KW  - security of data
KW  - system recovery
KW  - transaction processing
KW  - cyber attacks
KW  - damage appraisal
KW  - damage evaluation
KW  - data structures
KW  - database update
KW  - dependency relationships
KW  - recovery
KW  - simulation
KW  - Acceleration
KW  - Analytical models
KW  - Appraisal
KW  - Computational modeling
KW  - Data structures
KW  - History
KW  - Intrusion detection
KW  - Performance analysis
KW  - Protection
KW  - Transaction databases
VL  - 31
JA  - Systems, Man and Cybernetics, Part A: Systems and Humans, IEEE Transactions on
DO  - 10.1109/3468.935047
AB  - Accurate recovery from a cyber attack depends on fast and perfect damage assessment. For damage assessment, traditional recovery methods require that the log of an affected database must be scanned starting from the attacking transaction until the end. This is a time-consuming task. Our objective in this research is to provide techniques that can be used to accelerate the damage appraisal process and produce a correct result. We have presented a damage assessment model and four data structures associated with the model. Each of these structures uses dependency relationships among transactions, which update the database. These relationships are later used to determine exactly which transactions and exactly which data items are affected by the attacker. A performance comparison analysis obtained using simulation is provided to demonstrate the benefit of our model
ER  - 

TY  - JOUR
JO  - Electronics Letters
TI  - kth least significant bit of a plaintext in reciprocal number cryptosystem
T2  - Electronics Letters
IS  - 15
SN  - 0013-5194
VO  - 23
SP  - 810
EP  - 811
AU  - Kurosawa, K.
Y1  - July 16 1987
PY  - 1987
KW  - cryptography
KW  - security of data
KW  - factoring
KW  - factorisation
KW  - kth least significant bit
KW  - plaintext
KW  - public key cryptosystem
KW  - reciprocal number cryptosystem
VL  - 23
JA  - Electronics Letters
DO  - 10.1049/el:19870574
AB  - Breaking the public key cryptosystem proposed by the author has been proven to be as hard as factoring a large number. The letter shows that guessing the kth least significant bit of a plaintext is also as difficult as factorisation (k  log n, where n is the bit length of a plaintext).
ER  - 

TY  - CONF
JO  - Machine Learning for Signal Processing (MLSP), 2014 IEEE International Workshop on
TI  - A kernel-based nonparametric test for anomaly detection over line networks
T2  - Machine Learning for Signal Processing (MLSP), 2014 IEEE International Workshop on
IS  - 
SN  - 
VO  - 
SP  - 1
EP  - 6
AU  - Shaofeng Zou
AU  - Yingbin Liang
AU  - Poor, H.V.
Y1  - 21-24 Sept. 2014
PY  - 2014
KW  - computational complexity
KW  - security of data
KW  - 1D line network
KW  - anomalous interval
KW  - anomaly detection
KW  - computational complexity reduction
KW  - kernel-based nonparametric test
KW  - line networks
KW  - maximum mean discrepancy
KW  - nonparametric problem
KW  - reproducing kernel Hilbert space
KW  - Hilbert space
KW  - Kernel
KW  - Laplace equations
KW  - Parametric statistics
KW  - Probability distribution
KW  - Random variables
VL  - 
JA  - Machine Learning for Signal Processing (MLSP), 2014 IEEE International Workshop on
DO  - 10.1109/MLSP.2014.6958913
AB  - The nonparametric problem of detecting existence of an anomalous interval over a one-dimensional line network is studied. Nodes corresponding to an anomalous interval (if one exists) receive samples generated by a distribution q, which is different from the distribution p that generates samples for other nodes. If an anomalous interval does not exist, then all nodes receive samples generated by p. It is assumed that the distributions p and q are arbitrary, and are unknown. In order to detect whether an anomalous interval exists, a test is built based on mean embeddings of distributions into a reproducing kernel Hilbert space (RKHS) and the metric of maximum mean discrepancy (MMD). It is shown that as the network size n goes to infinity, if the minimum length of candidate anomalous intervals is larger than a threshold which has the order O(log n), the proposed test is asymptotically successful. An efficient algorithm to perform the test with substantial computational complexity reduction is proposed, and is shown to be asymptotically successful if the condition on the minimum length of candidate anomalous interval is satisfied. Numerical results are provided, which are consistent with the theoretical results.
ER  - 

TY  - CONF
JO  - Computational and Information Sciences (ICCIS), 2010 International Conference on
TI  - Research and Implementation the Integrated Support Platform for Water Information
T2  - Computational and Information Sciences (ICCIS), 2010 International Conference on
IS  - 
SN  - 
VO  - 
SP  - 487
EP  - 490
AU  - Chen Tian-Qing
AU  - Xie Jian-cang
AU  - Yue Xin-Li
AU  - Zhang Gang
Y1  - 17-19 Dec. 2010
PY  - 2010
KW  - information systems
KW  - security of data
KW  - user interfaces
KW  - water supply
KW  - authentication mechanisms
KW  - business application in-depth
KW  - decentralized management model
KW  - decentralized registry
KW  - unified access entrance
KW  - unified user management
KW  - water business system
KW  - water information system
KW  - water informatization construction
KW  - Access control
KW  - Authentication
KW  - Business
KW  - Water resources
KW  - Web services
KW  - XML
KW  - Component
KW  - Meta-synthesis
KW  - Water Information
KW  - the Integrated Support Platform for Water Information
VL  - 
JA  - Computational and Information Sciences (ICCIS), 2010 International Conference on
DO  - 10.1109/ICCIS.2010.126
AB  - After the few years' development, the informatization construction of water has made great achievements. With the expansion of business application in-depth, it exposed some After the few years' development, the informatization construction of water has made great achievements. With the expansion of business application in-depth, it exposed some problems such as system single-function, single application mode, poor scalability issues and so on. Each application has its own separate set of authentication mechanisms, adopted a decentralized registry, and decentralized management model. However, the use of these systems function, it means that required several authentication to log on different systems. In this paper, research and practice, the author proposed an integrated support platform for water information system. The system through established a unified user management and unified access entrance, to enable the original water business system centralized access to.
ER  - 

TY  - CONF
JO  - Wireless Communications, Networking and Mobile Computing (WiCOM 2014), 10th International Conference on
TI  - A detection method for malicious codes in Android apps
T2  - Wireless Communications, Networking and Mobile Computing (WiCOM 2014), 10th International Conference on
IS  - 
SN  - 
VO  - 
SP  - 514
EP  - 519
AU  - Jinxin Liu
AU  - Hao Wu
AU  - Huabin Wang
Y1  - 26-28 Sept. 2014
PY  - 2014
KW  - Android (operating system)
KW  - application program interfaces
KW  - mobile computing
KW  - program diagnostics
KW  - security of data
KW  - API functions
KW  - Android apps
KW  - Android operating system
KW  - information leakage
KW  - malicious chargeback
KW  - malicious codes
KW  - mobile terminals
KW  - reverse technology
KW  - static analysis
KW  - Android
KW  - Dynamic Analysis
KW  - Static Analysis
VL  - 
JA  - Wireless Communications, Networking and Mobile Computing (WiCOM 2014), 10th International Conference on
DO  - 10.1049/ic.2014.0154
AB  - In recent years, the Android operating system for mobile terminals has developed very quickly. A variety of mobile devices which are using Android operating system are more than 60% in the domestic market share. With the number of Android application raising fast, a variety of information leakage, malicious chargeback, failure of operating system events occurred frequently; the safety of Android system also attracts wide attention of researchers. In this paper, combining static analysis and dynamic analysis, we present a malicious code detection method and implementation. Through the statistics of the sensitive API functions and tracking the flow of sensitive information, static analysis module uses the static analysis reverse technology to achieve the detection of malicious behaviors. And dynamic analysis module mainly uses system log analysis and records a variety of sensitive behaviors generated during the operation to discover intrusions. Furthermore, the ultimate combination of static analysis and dynamic analysis will determine whether the target software contains malicious codes.
ER  - 

TY  - CONF
JO  - Computer and Information Technology (ICCIT), 2013 16th International Conference on
TI  - Intrusion detection learning algorithm through network mining
T2  - Computer and Information Technology (ICCIT), 2013 16th International Conference on
IS  - 
SN  - 
VO  - 
SP  - 490
EP  - 495
AU  - Abu Afza, A.J.M.
AU  - Uddin, M.S.
Y1  - 8-10 March 2014
PY  - 2014
KW  - data mining
KW  - learning (artificial intelligence)
KW  - pattern classification
KW  - pattern clustering
KW  - security of data
KW  - KDD99 dataset
KW  - adaptive network intrusion detection
KW  - conditional probability
KW  - false positive reduction
KW  - intrusion detection learning algorithm
KW  - naive Bayesian classifier
KW  - network logs similarity
KW  - network mining
KW  - pattern clustering
KW  - supervised learning process
KW  - unsupervised learning process
KW  - Bayes methods
KW  - Classification algorithms
KW  - Clustering algorithms
KW  - Computers
KW  - Intrusion detection
KW  - Niobium
KW  - boosting
KW  - intrusion detection
KW  - na&#x00EF;ve Bayesian classifier
VL  - 
JA  - Computer and Information Technology (ICCIT), 2013 16th International Conference on
DO  - 10.1109/ICCITechn.2014.6997324
AB  - This paper presents a learning algorithm for adaptive network intrusion detection based on clustering and nai&#x0308;ve Bayesian classifier, which induces a hybridization of unsupervised and supervised learning processes. The proposed approach scales up the balance detection rates for different types of network intrusions, and keeps the false positives at acceptable level in network intrusion detection. The algorithm first clusters the network logs into several groups based on similarity of network logs, and then calculates the prior and class conditional probabilities for each cluster. In classifying a new network log, the algorithm calculates the similarity of attribute values of network data with each cluster and initialize a weight value for each cluster. Then each cluster classifies the network data with its priori and conditional probabilities that multiply with respective cluster's weight value. Finally, voting techniques applied for classifying the new network data based on each cluster's classification result. The performance of the proposed algorithm tested by employing KDD99 benchmark network intrusion detection dataset, and the experimental results proved that it improves the detection rates as well as reduces the false positives for different types of network intrusions.
ER  - 

TY  - CONF
JO  - Applications of Digital Information and Web Technologies, 2008. ICADIWT 2008. First International Conference on the
TI  - A splog filtering method based on string copy detection
T2  - Applications of Digital Information and Web Technologies, 2008. ICADIWT 2008. First International Conference on the
IS  - 
SN  - 
VO  - 
SP  - 543
EP  - 548
AU  - Takeda, T.
AU  - Takasu, A.
Y1  - 4-6 Aug. 2008
PY  - 2008
KW  - Web sites
KW  - computational complexity
KW  - electronic mail
KW  - information filtering
KW  - security of data
KW  - Web links
KW  - blogosphere
KW  - document length
KW  - spam like e-mail
KW  - splog filtering method
KW  - string copy detection
KW  - time complexity
KW  - Blogs
KW  - Character generation
KW  - Electronic mail
KW  - Frequency
KW  - Informatics
KW  - Information analysis
KW  - Information filtering
KW  - Information filters
KW  - Search engines
KW  - Text categorization
VL  - 
JA  - Applications of Digital Information and Web Technologies, 2008. ICADIWT 2008. First International Conference on the
DO  - 10.1109/ICADIWT.2008.4664407
AB  - Recently many people publicize their blogs and the blogosphere becomes an important information source. It is used for various purposes such as analyzing trends and reputations, marketing, etc. One problem of blogosphere is spam like e-mails and web links. There are many spam blogs (splogs) that are generated to make users to access specific sites. This paper proposes a splog filtering method. Splog is usually generated automatically by copying words and phrases from other documents. Therefore, the proposed method detects strings appearing in multiple blogs and uses a copy rate of strings as a key feature for splog filtering. To evaluate the proposed method, we constructed an evaluation corpus by gathering blogs randomly during a certain period of time and manually judged whether each blog is splog or not. The experiment using this corpus reveals several features of splog filtering by copy string detection. The proposed method uses the suffix array for copied substring detection and it can judge each blog with time complexity of (m<sup>2</sup> log n) where n and m denote total length of documents used for copy detection and the lengths of the blog to be judged, respectively.
ER  - 

TY  - CONF
JO  - Computing Communication and Networking Technologies (ICCCNT), 2010 International Conference on
TI  - A cluster based cost effective contributory key agreement protocol for secure group communication
T2  - Computing Communication and Networking Technologies (ICCCNT), 2010 International Conference on
IS  - 
SN  - 
VO  - 
SP  - 1
EP  - 12
AU  - Jabeenbegum, S.
AU  - Purusothaman, T.
AU  - Karthi, M.
AU  - Balachandar, N.
AU  - Arunkumar, N.
Y1  - 29-31 July 2010
PY  - 2010
KW  - cryptographic protocols
KW  - distributed processing
KW  - security of data
KW  - workstation clusters
KW  - cluster based hierarchical key distribution protocol
KW  - cluster based key agreement protocol
KW  - cost effective contributory key agreement protocol
KW  - dummy nodes
KW  - heterogeneous distributed environment
KW  - key computation cost
KW  - key management
KW  - key server storage cost
KW  - secure group communication
KW  - wired environment
KW  - wireless environment
KW  - Computer architecture
KW  - Educational institutions
KW  - Internet
KW  - Protocols
KW  - Scalability
KW  - Servers
KW  - Wireless communication
KW  - Clustering
KW  - Complexity Analysis
KW  - Elliptic Curve Cryptography
KW  - Group key formation
KW  - Secure Group Communication
VL  - 
JA  - Computing Communication and Networking Technologies (ICCCNT), 2010 International Conference on
DO  - 10.1109/ICCCNT.2010.5591583
AB  - In a Heterogeneous distributed Environment, communication between the group members must be secured and authenticated. In a group, the member contribution in forming the group key is important. But forming a group key in a efficient manner is and to minimize the complexity in forming the group key, we are proposing a new protocol which consumes minimum number of key computations, less communication costs and number of keys stored in a server. Group members are in dynamic nature even one join/ leave the group the group key must be revoked and the new must be generated to maintain the forward and backward secrecy. If group members are large then the scalability problem also arises. In particular, cost for key establishment and key renewing is usually relevant to the group size and subsequently becomes a performance bottleneck in achieving scalability. To address this problem, for wired / wireless communication our new approach that features decoupling of group size and computation cost for group key management. By using a Cluster based Hierarchical Key Distribution Protocol, the load of key management can be shared by a cluster of dummy nodes without revealing group messages to them. The proposed scheme provides better scalability because the Key Computation cost is log<sub>8</sub> [n|m] and Communication cost is 0(1), the key server storage cost is [d/d -1]m+n, n &#x226B;m. Our proposed protocol provides the best performance in communication, key computations and the key server storage cost than existing approaches and it is applicable for wired / wireless environments also.
ER  - 

TY  - CONF
JO  - Advances in Social Networks Analysis and Mining (ASONAM), 2010 International Conference on
TI  - A Study on Social Network Metrics and Their Application in Trust Networks
T2  - Advances in Social Networks Analysis and Mining (ASONAM), 2010 International Conference on
IS  - 
SN  - 
VO  - 
SP  - 168
EP  - 175
AU  - Varlamis, I.
AU  - Eirinaki, M.
AU  - Louta, M.
Y1  - 9-11 Aug. 2010
PY  - 2010
KW  - human computer interaction
KW  - information dissemination
KW  - promotion (marketing)
KW  - security of data
KW  - social networking (online)
KW  - customer review site
KW  - information dissemination
KW  - information propagation
KW  - product placement
KW  - product promotion
KW  - social media
KW  - social network analysis
KW  - social network metrix
KW  - Blogs
KW  - Book reviews
KW  - Collaboration
KW  - Mathematical model
KW  - Measurement
KW  - Media
KW  - Social network services
KW  - Web 2.0
KW  - graph metrics
KW  - influence
KW  - social network analysis
KW  - trust
VL  - 
JA  - Advances in Social Networks Analysis and Mining (ASONAM), 2010 International Conference on
DO  - 10.1109/ASONAM.2010.40
AB  - Social network analysis has recently gained a lot of interest because of the advent and the increasing popularity of social media, such as blogs, social networks, micro logging, or customer review sites. Such media often serve as platforms for information dissemination and product placement or promotion. In this environment, influence and trust are becoming essential qualities among user interactions. In this work, we perform an extensive study of various metrics related to the aforementioned elements, and their effect in the process of information propagation in the virtual world. In order to better understand the properties of links and the dynamics of social networks, we distinguish between permanent and transient links and in the latter case, we consider the link freshness. Moreover, we distinguish between local and global influence and compare suggestions provided by locally or globally trusted users.
ER  - 

TY  - CONF
JO  - Information Technology, 2007. ITNG '07. Fourth International Conference on
TI  - Data Fusion Support for Intrusion Detection and Prevention
T2  - Information Technology, 2007. ITNG '07. Fourth International Conference on
IS  - 
SN  - 
VO  - 
SP  - 966
EP  - 966
AU  - Beheshti, M.
AU  - Wasniowski, R.A.
Y1  - 2-4 April 2007
PY  - 2007
KW  - security of data
KW  - sensors
KW  - data fusion
KW  - intrusion detection system
KW  - multisensor
KW  - traffic analysis
KW  - Computer network management
KW  - Computer science
KW  - Computerized monitoring
KW  - Databases
KW  - Intrusion detection
KW  - Protection
KW  - Prototypes
KW  - Sensor systems
KW  - Traffic control
KW  - Web server
VL  - 
JA  - Information Technology, 2007. ITNG '07. Fourth International Conference on
DO  - 10.1109/ITNG.2007.62
AB  - Our main purpose for this work is to examine how to integrate multiple intrusion detection sensors in the order to minimize the number of incorrect-alarms The first problem is how to integrate data from multiple sensors, and the second how to identify most important data provided by multiple sensors. We are currently developing series of analytical models to use potential benefits of multiple sensors for reducing false alarms. The purpose of this presentation is to discuss implementation of prototype multisensor based intrusion detection system. We are especially interested in analyzing traffic that has an abnormal or malicious character and should prompt a closer look. A specific feature of the model is that the systems use multiple sensors to process log files
ER  - 

TY  - CONF
JO  - Networking and Distributed Computing (ICNDC), 2010 First International Conference on
TI  - UPDS: Reliable Storage for Personal Data in Online Services
T2  - Networking and Distributed Computing (ICNDC), 2010 First International Conference on
IS  - 
SN  - 
VO  - 
SP  - 357
EP  - 361
AU  - Hong Liu
AU  - Jiangning Cui
AU  - Taoying Liu
AU  - Wei Li
Y1  - 21-24 Oct. 2010
PY  - 2010
KW  - content-based retrieval
KW  - distributed processing
KW  - information services
KW  - personal computing
KW  - reliability
KW  - security of data
KW  - storage management
KW  - UPDS
KW  - content based block striping technology
KW  - data access model
KW  - data availability
KW  - data durability
KW  - erasure coding
KW  - online service
KW  - parity scheme
KW  - personal data storage system
KW  - replication scheme
KW  - Availability
KW  - Bandwidth
KW  - Biological system modeling
KW  - Data models
KW  - Encoding
KW  - Memory
VL  - 
JA  - Networking and Distributed Computing (ICNDC), 2010 First International Conference on
DO  - 10.1109/ICNDC.2010.75
AB  - We all have a large amount of data stored in Gmail, Facebook, Twitter, Google Docs and many other online services. With the increasingly importance of these online data, we need an effective way to protect it. This paper proposes a personal data storage system, UPDS, to improve personal data's availability and durability. Two common approaches to improve availability and durability are complete replication and parity scheme such as erasure coding. Although the erasure coding scheme can reduce storage cost, the self-contained feature of online service data source increases its bandwidth cost and decreases its durability. Leveraging the temporal locality of personal data's access model, we propose a lazy-committed log-structure storage architecture and content-based block striping technology to solve the problem. This paper explores the distributed storage system on self-contained data source for the first time and analyzes it system model. The system analysis results show that UPDS can improve availability and durability of personal data at low cost.
ER  - 

TY  - CONF
JO  - Computer Science and Information Technology (CSIT), 2013 5th International Conference on
TI  - An optimistic approach in distributed database concurrency control
T2  - Computer Science and Information Technology (CSIT), 2013 5th International Conference on
IS  - 
SN  - 
VO  - 
SP  - 71
EP  - 75
AU  - Rawashdeh, O.A.
AU  - Muhareb, H.A.
AU  - Al-Sayid, N.A.
Y1  - 27-28 March 2013
PY  - 2013
KW  - concurrency control
KW  - distributed databases
KW  - security of data
KW  - commit timestamps
KW  - concurrency control anomaly
KW  - data log tracking
KW  - data reliability
KW  - distributed database concurrency control
KW  - integer flag
KW  - optimistic approach
KW  - optimistic concurrency control solution
KW  - read-write transactions
KW  - roll back process tracking
KW  - Computer science
KW  - Concurrency control
KW  - Database systems
KW  - Distributed databases
KW  - Protocols
KW  - Real-time systems
KW  - Concurrency Control
KW  - Distributed database
KW  - Optimistic
VL  - 
JA  - Computer Science and Information Technology (CSIT), 2013 5th International Conference on
DO  - 10.1109/CSIT.2013.6588761
AB  - In this paper, we present an optimistic concurrency control solution. The proposed solution represents an excellent blossom in the concurrency control field. It deals with the concurrency control anomalies, and, simultaneously, assures the reliability of the data before read-write transactions and after successfully committed. It can be used within the distributed database to track data logs and roll back processes to overcome distributed database anomalies. The method is based on commit timestamps for validation and an integer flag that is incremented each time a successful update on the record is committed.
ER  - 

TY  - CONF
JO  - Applications and the Internet, 2002. (SAINT 2002). Proceedings. 2002 Symposium on
TI  - Remote attack detection method in IDA: MLSI-based intrusion detection using discriminant analysis
T2  - Applications and the Internet, 2002. (SAINT 2002). Proceedings. 2002 Symposium on
IS  - 
SN  - 
VO  - 
SP  - 64
EP  - 73
AU  - Asaka, M.
AU  - Onabura, T.
AU  - Inoue, T.
AU  - Goto, S.
Y1  - 2002
PY  - 2002
KW  - security of data
KW  - system monitoring
KW  - IDA
KW  - Intrusion Detection Agent system
KW  - MLSI-based intrusion detection
KW  - certain event
KW  - classification function
KW  - discriminant analysis
KW  - host machine
KW  - remote attack detection method
KW  - system calls
KW  - system log monitoring
KW  - Internet
KW  - Intrusion detection
VL  - 
JA  - Applications and the Internet, 2002. (SAINT 2002). Proceedings. 2002 Symposium on
DO  - 10.1109/SAINT.2002.994451
AB  - In order to detect intrusions, IDA (Intrusion Detection Agent system) initially monitors system logs in order to discover an MLSI-which is an certain event which in many cases occurs during an intrusion. If an MLSI is found, then IDA judges whether the MLSI is accompanied by an intrusion. We adopt discriminant analysis to analyze information after IDA detects an MLSI in a remote attack. Discriminant analysis provides a classification function that allows IDA to separate intrusive activities from non-intrusive activities. Using discriminant analysis, we can detect intrusions by analyzing only a part of system calls occurring on a host machine, and we can determine whether an unknown sample is an intrusion. In this paper, we explain in detail how we perform discriminant analysis to detect intrusions, and evaluate the classification function. We also describe how to extract a sample from system logs, which is necessary to implement the discriminant analysis function in IDA
ER  - 

TY  - CONF
JO  - Computer Communications and Networks, 2009. ICCCN 2009. Proceedings of 18th Internatonal Conference on
TI  - Certified Internet Coordinates
T2  - Computer Communications and Networks, 2009. ICCCN 2009. Proceedings of 18th Internatonal Conference on
IS  - 
SN  - 1095-2055
VO  - 
SP  - 1
EP  - 8
AU  - Kaafar, M.A.
AU  - Mathy, L.
AU  - Barakat, C.
AU  - Salamatian, K.
AU  - Turletti, T.
AU  - Dabbous, W.
Y1  - 3-6 Aug. 2009
PY  - 2009
KW  - Internet
KW  - Weibull distribution
KW  - log normal distribution
KW  - security of data
KW  - Internet coordinate systems
KW  - PlanetLab
KW  - Weibull distribution
KW  - coordinate inter-shift times
KW  - distance estimations
KW  - lognormal distribution
KW  - long-tail distribution
KW  - node coordinate
KW  - time limited validity certificate
KW  - Coordinate measuring machines
KW  - Delay estimation
KW  - Distance measurement
KW  - Distributed computing
KW  - Embedded computing
KW  - Extraterrestrial measurements
KW  - Internet
KW  - Network servers
KW  - Testing
KW  - Web server
VL  - 
JA  - Computer Communications and Networks, 2009. ICCCN 2009. Proceedings of 18th Internatonal Conference on
DO  - 10.1109/ICCCN.2009.5235308
AB  - We address the issue of asserting the accuracy of coordinates advertised by nodes of Internet coordinate systems during distance estimations. Indeed, some nodes may lie deliberately about their coordinates to mount various attacks against applications and overlays. Our proposed method consists in two steps: 1) establish the correctness of a node's claimed coordinate (which leverages our previous work on securing the coordinates embedding phase using a Surveyor infrastructure); and 2) issue a time limited validity certificate for each verified coordinate. Validity periods are computed based on an analysis of coordinate inter-shift times observed on PlanetLab, and shown to follow a long-tail distribution (lognormal distribution in most cases, or Weibull distribution otherwise). The effectiveness of the coordinate certification method is validated by measuring the impact of a variety of attacks on distance estimates.
ER  - 

TY  - CONF
JO  - Integrated Intelligent Computing (ICIIC), 2010 First International Conference on
TI  - Biometric Authentication for Intrusion Detection Systems
T2  - Integrated Intelligent Computing (ICIIC), 2010 First International Conference on
IS  - 
SN  - 
VO  - 
SP  - 195
EP  - 199
AU  - Challita, K.
AU  - Farhat, H.
AU  - Khaldi, K.
Y1  - 5-7 Aug. 2010
PY  - 2010
KW  - biometrics (access control)
KW  - security of data
KW  - authentication function
KW  - biometric authentication
KW  - computer system access
KW  - intrusion detection systems
KW  - Authentication
KW  - Computers
KW  - Databases
KW  - Fingerprint recognition
KW  - Intrusion detection
KW  - Mice
VL  - 
JA  - Integrated Intelligent Computing (ICIIC), 2010 First International Conference on
DO  - 10.1109/ICIIC.2010.15
AB  - Most of the currently available intrusion detection systems do not provide any authentication functionality in order to identify the users who access a computer system. In particular, insiders are able to misuse their privileges without being detected. Some intrusion detection systems aim to authenticate legitimate users once they log in to the system, as well as to prevent access to unauthorized people who try to impersonate other users. The main goal of this paper is to introduce and implement a novel approach that uses the fingerprint technique to complement a host-based intrusion detection system in order to improve its level of authentication, which would allow us to detect more effectively any misuse of the computer system that is running it.
ER  - 

TY  - CONF
JO  - Information Assurance Workshop, 2004. Proceedings from the Fifth Annual IEEE SMC
TI  - Mining data relationships for database damage assessment in a post information warfare scenario
T2  - Information Assurance Workshop, 2004. Proceedings from the Fifth Annual IEEE SMC
IS  - 
SN  - 
VO  - 
SP  - 401
EP  - 409
AU  - Yi Hu
AU  - Panda, B.
Y1  - 10-11 June 2004
PY  - 2004
KW  - data mining
KW  - data structures
KW  - database management systems
KW  - security of data
KW  - cyber attack detection
KW  - data mining
KW  - data structure
KW  - database damage assessment
KW  - database operation phase
KW  - intrusion response team
KW  - post information warfare scenario
KW  - Data mining
KW  - Data structures
KW  - Database systems
KW  - Intrusion detection
KW  - Operating systems
KW  - Transaction databases
KW  - USA Councils
VL  - 
JA  - Information Assurance Workshop, 2004. Proceedings from the Fifth Annual IEEE SMC
DO  - 10.1109/IAW.2004.1437845
AB  - After the detection of a cyber attack on a database system, the intrusion response team of any organization needs to know the damage profile immediately in order to design an appropriate response strategy. Unfortunately obtaining the precise damage status can take up to hours even days. This is because existing approaches to database damage assessment involve significant amount of work including scanning the log file or other auxiliary data structures. Our approach concentrates on making an estimated damage profile as soon as possible. This model is based exclusively on a priori knowledge of data relationships mined during normal database operation phase. This knowledge can be used during damage assessment phase for faster damage assessment.
ER  - 

TY  - CONF
JO  - Dependable Systems and Networks, 2007. DSN '07. 37th Annual IEEE/IFIP International Conference on
TI  - Portable and Efficient Continuous Data Protection for Network File Servers
T2  - Dependable Systems and Networks, 2007. DSN '07. 37th Annual IEEE/IFIP International Conference on
IS  - 
SN  - 
VO  - 
SP  - 687
EP  - 697
AU  - Ningning Zhu
AU  - Tzi-cker Chiueh
Y1  - 25-28 June 2007
PY  - 2007
KW  - file organisation
KW  - security of data
KW  - continuous data protection
KW  - disk access interface
KW  - file system
KW  - network file servers
KW  - operating systems
KW  - user-level continuous data protection
KW  - Computer science
KW  - Delay
KW  - File servers
KW  - File systems
KW  - Hardware
KW  - Humans
KW  - Loss measurement
KW  - Operating systems
KW  - Protection
KW  - Protocols
VL  - 
JA  - Dependable Systems and Networks, 2007. DSN '07. 37th Annual IEEE/IFIP International Conference on
DO  - 10.1109/DSN.2007.74
AB  - Continuous data protection, which logs every update to a file system, is an enabling technology to protect file systems against malicious attacks and/or user mistakes, because it allows each file update to be undoable. Existing implementations of continuous data protection work either at disk access interface or within the file system. Despite the implementation complexity, their performance overhead is significant when compared with file systems that do not support continuous data protection. Moreover, such kernel-level file update logging implementation is complex and cannot be easily ported to other operating systems. This paper describes the design and implementation of four user-level continuous data protection implementations for NFS servers, all of which work on top of the NFS protocol and thus can be easily ported to any operating systems that support NFS. Measurements obtained from running standard benchmarks and real-world NFS traces on these user-level continuous data protection systems demonstrate a surprising result: Performance of NFS servers protected by pure user-level continuous data protection schemes is comparable to that of unprotected vanilla NFS servers.
ER  - 

TY  - CONF
JO  - Information Assurance Workshop, 2005. IAW '05. Proceedings from the Sixth Annual IEEE SMC
TI  - Knowledge sharing honeynets
T2  - Information Assurance Workshop, 2005. IAW '05. Proceedings from the Sixth Annual IEEE SMC
IS  - 
SN  - 
VO  - 
SP  - 240
EP  - 243
AU  - Sudaharan, S.
AU  - Dhammalapathi, S.
AU  - Rai, S.
AU  - Wijesekera, D.
Y1  - 15-17 June 2005
PY  - 2005
KW  - computer networks
KW  - real-time systems
KW  - security of data
KW  - active network monitoring
KW  - attack strategy
KW  - cooperating honeynets
KW  - coordinated Internet attacks
KW  - distributed Internet attacks
KW  - event log analysis
KW  - honeynet clusters
KW  - intrusion detection systems
KW  - knowledge sharing honeynets
KW  - malicious traffic
KW  - network administration
KW  - production networks
KW  - real-time warning systems
KW  - Hardware
KW  - Intrusion detection
KW  - Military computing
KW  - Monitoring
KW  - Network servers
KW  - Production systems
KW  - Real time systems
KW  - Switches
KW  - Telecommunication traffic
KW  - Web server
VL  - 
JA  - Information Assurance Workshop, 2005. IAW '05. Proceedings from the Sixth Annual IEEE SMC
DO  - 10.1109/IAW.2005.1495958
AB  - Due to the prevalence of distributed and coordinated Internet attacks, many researchers and network administrators study the nature and strategies of attackers. To analyze event logs, using intrusion detection systems and active network monitoring, honeynets are being deployed to attract potential attackers in order to investigate their modus operandi. The goal is to use honeynet clusters as real-time warning systems in production networks. Towards satisfying this objective, we have built a honeynet cluster and have run experiments to determine its effectiveness. Majority of the honeynets function in isolation, not sharing information in real time. In order to rectify this deficiency, the authors built a federation of cooperating honeynets (referred to as knowledge sharing honeynets) that shares knowledge of malicious traffic. This paper describes the methods in building a hardware assisted honeynet cluster and testing its effectiveness.
ER  - 

TY  - CONF
JO  - Parallel and Distributed Processing Symposium, 2007. IPDPS 2007. IEEE International
TI  - Scattered Black Hole Search in an Oriented Ring using Tokens
T2  - Parallel and Distributed Processing Symposium, 2007. IPDPS 2007. IEEE International
IS  - 
SN  - 
VO  - 
SP  - 1
EP  - 8
AU  - Dobrev, S.
AU  - Santoro, N.
AU  - Shi, W.
Y1  - 26-30 March 2007
PY  - 2007
KW  - computational complexity
KW  - mobile agents
KW  - security of data
KW  - token networks
KW  - asynchronous ring network
KW  - leader election problem
KW  - mobile agents
KW  - rendezvous problem
KW  - scattered black hole search
KW  - Computer networks
KW  - Computer science
KW  - Costs
KW  - Decontamination
KW  - Information technology
KW  - Mobile agents
KW  - Nominations and elections
KW  - Pressing
KW  - Scattering
KW  - Search problems
KW  - Anonymous
KW  - Asynchronous
KW  - Election
KW  - Mobile Agents
KW  - Rendezvous
KW  - Ring
KW  - Scattered
KW  - Token
VL  - 
JA  - Parallel and Distributed Processing Symposium, 2007. IPDPS 2007. IEEE International
DO  - 10.1109/IPDPS.2007.370460
AB  - A black hole is a highly harmful host that disposes of visiting agents upon their arrival without any observable trace of the destruction. The problem of locating the black hole in asynchronous ring network is known to be solvable by a team of mobile agents if each node is equipped with a whiteboard. A simpler and less expensive inter-communication and synchronization mechanism is provided by tokens: each agent has available a bounded number of tokens that can be carried, placed in a node or/and on a port of the node, or removed. All tokens are identical and no other form of communication or coordination is available to the agents. It is known that locating the black hole in an anonymous ring network using tokens is feasible when the team of agents is initially collocated (i.e. they all start from the same host). Recently, the more difficult case when the agents are scattered (i.e., when the agents do not start from the same host) has also been examined and solutions requiring only O(1) tokens per agent but using a total of O(n<sup>2</sup>) moves have been presented. The number of moves can be reduced to O(kn + n log n) if the number k of agents is known. In this paper, we study the impact of orientation and knowledge of team size on the cost of black hole location by scattered agents with tokens. We prove that, in oriented rings, the number of moves can be reduced from O(n<sup>2</sup>) to the optimal Theta(nlogn) using only O(1) tokens per agent, without any knowledge of the team size. This result holds even if both agents and nodes are anonymous. Interestingly, the proposed algorithm solves, with the same cost, also the leader election problem and the rendezvous problem for the scattered agents despite the presence of a BH.
ER  - 

TY  - CONF
JO  - Cyberworlds (CW), 2010 International Conference on
TI  - A New Graphical Password Scheme Resistant to Shoulder-Surfing
T2  - Cyberworlds (CW), 2010 International Conference on
IS  - 
SN  - 
VO  - 
SP  - 194
EP  - 199
AU  - Haichang Gao
AU  - Zhongjie Ren
AU  - Xiuling Chang
AU  - Xiyang Liu
AU  - Aickelin, U.
Y1  - 20-22 Oct. 2010
PY  - 2010
KW  - computer graphics
KW  - security of data
KW  - user interfaces
KW  - DAS
KW  - PDA
KW  - association mnemonics
KW  - authentication session
KW  - graphical password scheme
KW  - password
KW  - password images
KW  - sequence retrieval
KW  - shoulder surfing
KW  - visual interface
KW  - Authentication
KW  - Head
KW  - Personal digital assistants
KW  - Prototypes
KW  - Resistance
KW  - Usability
KW  - PDA
KW  - authentication
KW  - graphical password
KW  - shoulder-surfing
VL  - 
JA  - Cyberworlds (CW), 2010 International Conference on
DO  - 10.1109/CW.2010.34
AB  - Shoulder-surfing is a known risk where an attacker can capture a password by direct observation or by recording the authentication session. Due to the visual interface, this problem has become exacerbated in graphical passwords. There have been some graphical schemes resistant or immune to shoulder-surfing, but they have significant usability drawbacks, usually in the time and effort to log in. In this paper, we propose and evaluate a new shoulder-surfing resistant scheme which has a desirable usability for PDAs. Our inspiration comes from the drawing input method in DAS and the association mnemonics in Story for sequence retrieval. The new scheme requires users to draw a curve across their password images orderly rather than click directly on them. The drawing input trick along with the complementary measures, such as erasing the drawing trace, displaying degraded images, and starting and ending with randomly designated images provide a good resistance to shoulder-surfing. A preliminary user study showed that users were able to enter their passwords accurately and to remember them over time.
ER  - 

TY  - CONF
JO  - Computational Intelligence in Games (CIG), 2013 IEEE Conference on
TI  - Behavioral-based cheating detection in online first person shooters using machine learning techniques
T2  - Computational Intelligence in Games (CIG), 2013 IEEE Conference on
IS  - 
SN  - 2325-4270
VO  - 
SP  - 1
EP  - 8
AU  - Alayed, H.
AU  - Frangoudes, F.
AU  - Neuman, C.
Y1  - 11-13 Aug. 2013
PY  - 2013
KW  - computer games
KW  - data privacy
KW  - learning (artificial intelligence)
KW  - pattern classification
KW  - security of data
KW  - behavioral-based cheating detection
KW  - cheating prevention
KW  - commercial online game
KW  - game logs
KW  - gaming companies
KW  - machine learning classifiers
KW  - machine learning techniques
KW  - online first person shooters
KW  - server-side anticheating solution
KW  - user privacy
KW  - Accuracy
KW  - Feature extraction
KW  - Games
KW  - Logistics
KW  - Servers
KW  - Support vector machines
KW  - Weapons
KW  - Cheating Detection
KW  - Machine Learning
KW  - Online Games
VL  - 
JA  - Computational Intelligence in Games (CIG), 2013 IEEE Conference on
DO  - 10.1109/CIG.2013.6633617
AB  - Cheating in online games comes with many consequences for both players and companies. Therefore, cheating detection and prevention is an important part of developing a commercial online game. Several anti-cheating solutions have been developed by gaming companies. However, most of these companies use cheating detection measures that may involve breaches to users' privacy. In our paper, we provide a server-side anti-cheating solution that uses only game logs. Our method is based on defining an honest player's behavior and cheaters' behavior first. After that, using machine learning classifiers to train cheating models, then detect cheaters. We presented our results in different organizations to show different options for developers, and our methods' results gave a very high accuracy in most of the cases. Finally, we provided a detailed analysis of our results with some useful suggestions for online games developers.
ER  - 

TY  - CONF
JO  - Multitopic Conference (INMIC), 2011 IEEE 14th International
TI  - Application of statistical sampling to predict faults from real time alarm data
T2  - Multitopic Conference (INMIC), 2011 IEEE 14th International
IS  - 
SN  - 
VO  - 
SP  - 290
EP  - 295
AU  - Kazmi, A.S.
Y1  - 22-24 Dec. 2011
PY  - 2011
KW  - alarm systems
KW  - radio networks
KW  - security of data
KW  - statistical analysis
KW  - denial of service
KW  - historical alarm logs
KW  - overall operational complexity
KW  - predict faults
KW  - real time alarm data
KW  - statistical sampling
KW  - switching circuit
KW  - telecom alarm sequence analyzer
KW  - telecommunication systems
KW  - wireless networks
KW  - Analytical models
KW  - Monitoring
KW  - Multiaccess communication
KW  - Predictive models
KW  - Training
KW  - Wireless communication
VL  - 
JA  - Multitopic Conference (INMIC), 2011 IEEE 14th International
DO  - 10.1109/INMIC.2011.6151490
AB  - The faults in today's telecommunication systems happen frequently. The reason is simply the complexity of the telecommunication networks and other supported hardware. Various parts of the network may be supplied by various vendors, thus adding to the overall operational complexity. Furthermore these networks may consist of various types of networks, e.g. computer, switching, circuit and wireless networks, inter-operating. The result of such complexities is that a number of faults will happen over unit time intervals. Many of these faults may result in denial of service to the end users, consequently causing revenue losses to the telecommunication companies. Therefore various fault prediction and correction techniques have been proposed. Most, if not all, of these techniques are based on analysis of the historical alarm logs. The Telecom Alarm Sequence Analyzer (TASA) project has proposed associate and episodal rules for historical alarms. We have practically applied TASA episodal rules to identify sequence of alarms and then used the probabilities of these alarm sequences to predict future sequences of alarms. We have used the proposed alarm prediction technique on the real time alarm data of a telecommunication company and predicted future alarms sequences. Furthermore we have compared the predicted alarms against the actual alarm sequences to check the accuracy of our technique. The proposed technique does not make any assumption about the alarm data. We have concluded that the proposed technique has practical usage for alarm prediction in telecommunication networks.
ER  - 

TY  - CONF
JO  - Dependable Systems and Networks (DSN), 2010 IEEE/IFIP International Conference on
TI  - Data recovery for web applications
T2  - Dependable Systems and Networks (DSN), 2010 IEEE/IFIP International Conference on
IS  - 
SN  - 
VO  - 
SP  - 81
EP  - 90
AU  - Akkus&#x0327;, I.E.
AU  - Goel, A.
Y1  - June 28 2010-July 1 2010
PY  - 2010
KW  - Web services
KW  - file servers
KW  - security of data
KW  - Drupal
KW  - Gallery2
KW  - Web application
KW  - Wordpress
KW  - data corruption
KW  - data recovery
KW  - Blogs
KW  - Computer bugs
KW  - Content management
KW  - Databases
KW  - Manuals
KW  - Network servers
KW  - Pricing
KW  - Runtime
KW  - Testing
VL  - 
JA  - Dependable Systems and Networks (DSN), 2010 IEEE/IFIP International Conference on
DO  - 10.1109/DSN.2010.5544951
AB  - Web-based applications store their data at the server side. This design has several benefits, but it can also cause a serious problem because a misconfiguration, bug or vulnerability leading to data loss or corruption can affect many users. While data backup solutions can help resolve some of these issues, they do not help diagnose the events that led to the corruption or the precise set of changes caused by these events. In this paper, we describe the design of a recovery system that helps administrators recover from data corruption caused by bugs in web applications. Our system tracks application requests, helping identify requests that cause data corruption, and reuses undo logs already kept by databases to selectively recover from the effects of these requests. The main challenge is to correlate requests across the multiple tiers of the application to determine the correct recovery actions. We explore using dependencies both within and across requests at three layers (database, application, and client) to help identify data corruption accurately. We evaluate our system using known bugs in popular web applications, including Wordpress, Drupal and Gallery2. Our results show that our system enables recovery from data corruption without loss of critical data and incurs small runtime overhead.
ER  - 

TY  - CONF
JO  - Communications, 2009. ICC '09. IEEE International Conference on
TI  - Modeling Human Behavior for Defense Against Flash-Crowd Attacks
T2  - Communications, 2009. ICC '09. IEEE International Conference on
IS  - 
SN  - 1938-1883
VO  - 
SP  - 1
EP  - 6
AU  - Oikonomou, G.
AU  - Mirkovic, J.
Y1  - 14-18 June 2009
PY  - 2009
KW  - security of data
KW  - DDoS defense
KW  - Web traffic logs
KW  - distributed denial of service
KW  - flash-crowd attacks
KW  - graphical puzzles
KW  - human behavior modeling
KW  - human interaction dynamics
KW  - human-vs-bot differentiation
KW  - request dynamics
KW  - request semantics
KW  - service requests
KW  - Communications Society
KW  - Computer crime
KW  - Distributed computing
KW  - Event detection
KW  - Face detection
KW  - Floods
KW  - Humans
KW  - Object detection
KW  - Peer to peer computing
KW  - Telecommunication traffic
VL  - 
JA  - Communications, 2009. ICC '09. IEEE International Conference on
DO  - 10.1109/ICC.2009.5199191
AB  - Flash-crowd attacks are the most vicious form of distributed denial of service (DDoS). They flood the victim with service requests generated from numerous bots. Attack requests are identical in content to those generated by legitimate, human users, and bots send at a low rate to appear non-aggressive - these features defeat many existing DDoS defenses. We propose defenses against flash-crowd attacks via human behavior modeling, which differentiate DDoS bots from human users. Current approaches to human-vs-bot differentiation, such as graphical puzzles, are insufficient and annoying to humans, whereas our defenses are highly transparent. We model three aspects of human behavior: a) request dynamics, by learning several chosen features of human interaction dynamics, and detecting bots that exhibit higher aggressiveness in one or more of these features, b) request semantics, by learning transitional probabilities of user requests, and detecting bots that generate valid but low-probability sequences, and c) ability to process visual cues, by embedding into server replies human-invisible objects, which cannot be detected by automated analysis, and flagging users that visit them as bots. We evaluate our defenses' performance on a series of Web traffic logs, interlaced with synthetically generated attacks, and conclude that they raise the bar for a successful, sustained attack to botnets whose size is larger than the size observed in 1-5% of DDoS attacks today.
ER  - 

TY  - CONF
JO  - Integrated Network Management (IM), 2015 IFIP/IEEE International Symposium on
TI  - ADAMANT &#x2014; An Anomaly Detection Algorithm for MAintenance and Network Troubleshooting
T2  - Integrated Network Management (IM), 2015 IFIP/IEEE International Symposium on
IS  - 
SN  - 
VO  - 
SP  - 1292
EP  - 1297
AU  - Martinez, E.
AU  - Fallon, E.
AU  - Fallon, S.
AU  - MingXue Wang
Y1  - 11-15 May 2015
PY  - 2015
KW  - performance evaluation
KW  - search engines
KW  - security of data
KW  - text analysis
KW  - ADAMANT
KW  - CEP
KW  - SQL
KW  - Twitter feeds
KW  - abnormal terms detection
KW  - additional_info
KW  - anomaly detection algorithm for maintenance and network troubleshooting
KW  - complex event processing
KW  - configurable time intervals
KW  - data processing
KW  - distance based outlier detection
KW  - network analysis
KW  - network operators
KW  - network performance
KW  - network state
KW  - problem_text
KW  - search engine
KW  - sliding windows
KW  - telecommunications analytical applications
KW  - telecommunications data records
KW  - telecommunications data structures
KW  - telecommunications domain
KW  - text analytic based network anomaly detection approach
KW  - text documents
KW  - user_text
KW  - Algorithm design and analysis
KW  - Big data
KW  - Conferences
KW  - Detection algorithms
KW  - Indexes
KW  - Search engines
KW  - Telecommunications
KW  - distance based
KW  - outlier
KW  - search Engine
KW  - sliding windows
KW  - text anomaly
VL  - 
JA  - Integrated Network Management (IM), 2015 IFIP/IEEE International Symposium on
DO  - 10.1109/INM.2015.7140484
AB  - Network operators are increasingly using analytic applications to improve the performance of their networks. Telecommunications analytical applications typically use SQL and Complex Event Processing (CEP) for data processing, network analysis and troubleshooting. Such approaches are hindered as they require an in-depth knowledge of both the telecommunications domain and telecommunications data structures in order to create the required queries. Valuable information contained in free form text data fields such as &#x201C;additional_info&#x201D;, &#x201C;user_text&#x201D; or &#x201C;problem_text&#x201D; can also be ignored. This work proposes An Anomaly Detection Algorithm for MAintenance and Network Troubleshooting (ADAMANT), a text analytic based network anomaly detection approach. Once telecommunications data records have been indexed, ADAMANT uses distance based outlier detection within sliding windows to detect abnormal terms at configurable time intervals. Traditional approaches focus on a specific type of record and create specific cause and effect rules. With the ADAMANT approach all free form text fields of alarms, logs, etc. are treated as text documents similar to Twitter feeds. All documents within a window represent a snapshot of the network state that is processed by ADAMANT. The ADAMANT approach focuses on text analytics to provide automated analysis without the requirement for SQL/CEP queries. Such an approach provides distinct network insights in comparison to traditional approaches.
ER  - 

TY  - CONF
JO  - Electronics, Robotics and Automotive Mechanics Conference, 2008. CERMA '08
TI  - Types of Hosts on a Remote File Inclusion (RFI) Botnet
T2  - Electronics, Robotics and Automotive Mechanics Conference, 2008. CERMA '08
IS  - 
SN  - 
VO  - 
SP  - 105
EP  - 109
AU  - Robledo, H.F.G.
Y1  - Sept. 30 2008-Oct. 3 2008
PY  - 2008
KW  - Web services
KW  - file servers
KW  - security of data
KW  - Web server attacks
KW  - dynamic IP addresses
KW  - historical data
KW  - remote file inclusion botnet
KW  - Automotive engineering
KW  - Computer crime
KW  - Computer languages
KW  - Databases
KW  - Internet
KW  - Programming profession
KW  - Radiofrequency interference
KW  - Robots
KW  - Vehicle dynamics
KW  - Web server
KW  - rfi botnets
KW  - tracking botnets
VL  - 
JA  - Electronics, Robotics and Automotive Mechanics Conference, 2008. CERMA '08
DO  - 10.1109/CERMA.2008.60
AB  - Web server attacks are increasingly in short time for different purposes, one of the principal vectors of this attacks are RFI and even the automatic way to do this. We suppose that in a botnet involved in RFI attacks, the attackers (host that launch the attack) are web servers compromised since the natural format of the attack and the tool (remote file to include). So we go deeper identified the type of host that is the attacker through a remote analysis based on domain name, content, and dynamic ip addresses.A large botnet involved in RFI attacks was tracked by almost a year and we figure out the behavior and the kind of host are the attackers and the hosters. This track were made by one University web server logs, compared with other sources. The interesting facts founded here are related to the botnet selected to study. This botnet is formed by other kind of hosts, not web servers at all. And the tool used to compromise web server is a very general shell. Other contribution of this work is a methodology for tracking RFI botnets, that could be used in real time or for historical data.
ER  - 

TY  - CONF
JO  - Mobile Data Management (MDM), 2014 IEEE 15th International Conference on
TI  - A Location-Based Authentication System Leveraging Smartphones
T2  - Mobile Data Management (MDM), 2014 IEEE 15th International Conference on
IS  - 
SN  - 
VO  - 1
SP  - 83
EP  - 88
AU  - Albayram, Y.
AU  - Khan, M.M.H.
AU  - Bamis, A.
AU  - Kentros, S.
AU  - Nguyen, N.
AU  - Ruhua Jiang
Y1  - 14-18 July 2014
PY  - 2014
KW  - mobile computing
KW  - security of data
KW  - smart phones
KW  - Bayesian classifier based authentication algorithm
KW  - Wi-Fi access point
KW  - location profile
KW  - location-based authentication system
KW  - smart phones
KW  - user friend data
KW  - Accuracy
KW  - Authentication
KW  - Bayes methods
KW  - Clustering algorithms
KW  - Google
KW  - IEEE 802.11 Standards
KW  - Smart phones
KW  - Android
KW  - Location based Authentication
KW  - Smartphones
KW  - Usability
KW  - User Authentication
VL  - 1
JA  - Mobile Data Management (MDM), 2014 IEEE 15th International Conference on
DO  - 10.1109/MDM.2014.16
AB  - This paper investigates a location-based authentication system where authentication questions are generated based on users' locations tracked by smartphones. More specifically, the system builds a location profile for a user based on periodically logged Wi-Fi access point beacons over time, and leverages this location profile to generate authentication questions. To evaluate the various aspects of this location-based authentication approach, we deployed the application on users' smartphones and conducted a real-life study for one month with 14 users. To simulate various kinds of adversaries (e.g., Naive vs. Knowledgeable), in our study, we recruited volunteers in pairs (e.g., Friends), in addition to single participants. Over the course of the experiment, each user is periodically presented with two sets of authentication questions. The first set is generated based on a user's own data. The second set is generated based on a randomly selected user's data. Additionally, in cases of paired participants, each user is presented with a third set of questions which is generated based on the user's friend's data. In each case, three different kinds of questions of varying difficulty levels are generated and presented to the user. Finally, we present a Bayesian classifier based authentication algorithm that can authenticate legitimate users with high accuracy by leveraging individual response patterns. We also discuss various aspects of location-based authentication mechanisms based on our findings in this paper.
ER  - 

TY  - CONF
JO  - Recent Trends in Information Systems (ReTIS), 2011 International Conference on
TI  - Revolution in authentication process by using biometrics
T2  - Recent Trends in Information Systems (ReTIS), 2011 International Conference on
IS  - 
SN  - 
VO  - 
SP  - 36
EP  - 41
AU  - Chowdhury, A.
Y1  - 21-23 Dec. 2011
PY  - 2011
KW  - biometrics (access control)
KW  - security of data
KW  - Biometrics systems
KW  - Tsunami
KW  - authentication process revolution
KW  - biometric logins
KW  - natural disasters
KW  - personal identification
KW  - terrorist attacks
KW  - Ear
KW  - Face
KW  - Fingerprint recognition
KW  - Iris recognition
KW  - Skin
KW  - Speech recognition
KW  - DNA
KW  - Fingernail bed
KW  - Fusion
KW  - Gait
KW  - Multi biometrics
KW  - UID
KW  - VNTR
KW  - identification
KW  - proof positive
KW  - retina
KW  - thermograms
KW  - verification
VL  - 
JA  - Recent Trends in Information Systems (ReTIS), 2011 International Conference on
DO  - 10.1109/ReTIS.2011.6146836
AB  - Terrorist attacks and natural disasters like Tsunami made us realize the need for rapid and precise personal identification. Biometrics systems and mainly multi-biometric systems provide tools to enforce reliable logs of system transactions and protect an individual's right to privacy. Passwords and user ids are often passed, written down for convenience or reused multiple times in different networks. Biometric logins would make it unfeasible for anyone, other than the intended person to login to the network or system but at the same time no need to remember passwords and user ids. Governments, law enforcement agencies, military and industrial companies are already making partial use of this technology. Researchers and practitioners are integrating identifying technologies into robotics, human computer interfaces, gaming systems, mobile devices, door locks, automobiles, and so on. Government of India is introducing biometric based authentication system for all Indian citizens. This paper describes different types of biometric practices, recent trend of using biometrics, biometric fusion and its uses in India.
ER  - 

TY  - CONF
JO  - Intelligent Networks and Intelligent Systems, 2008. ICINIS '08. First International Conference on
TI  - A Multiple Keyword Fusion Scheme for P2P IDS Alert
T2  - Intelligent Networks and Intelligent Systems, 2008. ICINIS '08. First International Conference on
IS  - 
SN  - 
VO  - 
SP  - 317
EP  - 320
AU  - Ming Xu
AU  - Chaochi Lin
AU  - Qin Chen
Y1  - 1-3 Nov. 2008
PY  - 2008
KW  - distributed processing
KW  - peer-to-peer computing
KW  - security of data
KW  - DHT
KW  - DShield Dataset
KW  - P2P IDS alert
KW  - distributed hash table
KW  - distributed intrusion alert fusion scheme
KW  - distributed intrusion detection system
KW  - load balancing
KW  - multiple keyword fusion scheme
KW  - routing infrastructure
KW  - Chaotic communication
KW  - Computer applications
KW  - Computer worms
KW  - Intelligent networks
KW  - Intelligent systems
KW  - Intrusion detection
KW  - Peer to peer computing
KW  - Routing
KW  - Sensor phenomena and characterization
KW  - Sensor systems
VL  - 
JA  - Intelligent Networks and Intelligent Systems, 2008. ICINIS '08. First International Conference on
DO  - 10.1109/ICINIS.2008.43
AB  - Alert fusion is a key problem in distributed intrusion detection system (DIDS). The paper proposes a distributed intrusion alert fusion scheme based on multiple keywords and routing infrastructure: distributed hash table (DHT). All the related alerts produced by local sensor can be routed and fused to their corresponding peers by multiple keywords, while evenly distributing unrelated alerts to different peer. We evaluation our scheme with a real-world intrusion detection dataset (DShield Dataset), which has been collected firewall and NIDS logs from over 1600 administrators across the world. Experimental results show that our scheme has well scalable, and can achieve significant improvement in load balancing.
ER  - 

TY  - CONF
JO  - Data Engineering (ICDE), 2015 IEEE 31st International Conference on
TI  - HaTen2: Billion-scale tensor decompositions
T2  - Data Engineering (ICDE), 2015 IEEE 31st International Conference on
IS  - 
SN  - 
VO  - 
SP  - 1047
EP  - 1058
AU  - Jeon, I.
AU  - Papalexakis, E.E.
AU  - Kang, U.
AU  - Faloutsos, C.
Y1  - 13-17 April 2015
PY  - 2015
KW  - Internet
KW  - security of data
KW  - tensors
KW  - HaTen2
KW  - MapReduce platform
KW  - Web data
KW  - billion-scale tensor decompositions
KW  - columns
KW  - fibers
KW  - large scale real-world data
KW  - modeling
KW  - multidimensional data
KW  - network intrusion logs
KW  - network traffic
KW  - rows
KW  - scalable distributed suite
KW  - social networks
KW  - tensor decomposition algorithms
KW  - tensor decomposition methods
KW  - Algorithm design and analysis
KW  - Computer science
KW  - Data models
KW  - Matrix converters
KW  - Matrix decomposition
KW  - Scalability
KW  - Tensile stress
VL  - 
JA  - Data Engineering (ICDE), 2015 IEEE 31st International Conference on
DO  - 10.1109/ICDE.2015.7113355
AB  - How can we find useful patterns and anomalies in large scale real-world data with multiple attributes? For example, network intrusion logs, with (source-ip, target-ip, port-number, timestamp)? Tensors are suitable for modeling these multi-dimensional data, and widely used for the analysis of social networks, web data, network traffic, and in many other settings. However, current tensor decomposition methods do not scale for tensors with millions and billions of rows, columns and `fibers', that often appear in real datasets. In this paper, we propose HaTen2, a scalable distributed suite of tensor decomposition algorithms running on the MapReduce platform. By carefully reordering the operations, and exploiting the sparsity of real world tensors, HaTen2 dramatically reduces the intermediate data, and the number of jobs. As a result, using HaTen2, we analyze big real-world tensors that can not be handled by the current state of the art, and discover hidden concepts.
ER  - 

TY  - CONF
JO  - Semantic Media Adaptation and Personalization, 2008. SMAP '08. Third International Workshop on
TI  - Enhancing User Privacy in Adaptive Web Sites with Client-Side User Profiles
T2  - Semantic Media Adaptation and Personalization, 2008. SMAP '08. Third International Workshop on
IS  - 
SN  - 
VO  - 
SP  - 170
EP  - 176
AU  - Kolias, C.
AU  - Kolias, V.
AU  - Anagnostopoulos, I.
AU  - Kambourakis, G.
AU  - Kayafas, E.
Y1  - 15-16 Dec. 2008
PY  - 2008
KW  - Web sites
KW  - data privacy
KW  - security of data
KW  - user interfaces
KW  - Web personalization
KW  - abstract architecture
KW  - adaptive Web sites
KW  - client-side user profiles
KW  - personal privacy preferences
KW  - user privacy
KW  - Adaptive control
KW  - Condition monitoring
KW  - Electronic learning
KW  - Management information systems
KW  - Privacy
KW  - Programmable control
KW  - Service oriented architecture
KW  - Systems engineering and theory
KW  - Web pages
KW  - Web sites
KW  - Adaptive Hypermedia
KW  - Client-Side Personalization
KW  - Privacy
KW  - XML
VL  - 
JA  - Semantic Media Adaptation and Personalization, 2008. SMAP '08. Third International Workshop on
DO  - 10.1109/SMAP.2008.32
AB  - Web personalization is an elegant and flexible process of making a web site responsive to the unique needs of each individual user. Data that reflects user preferences and likings, comprising therefore a user profile, are gathered to an adaptive web site in a non transparent manner. This situation however raises serious privacy concerns to the end user. When browsing aweb site, users are not aware of several important privacy parameters i.e., which behavior will be monitored and logged, how it will be processed, how long it will be kept, and with whom it will be shared in the long run. In this paper we propose an abstract architecture that enhances user privacy during interaction with adaptive web sites. This architecture enables users to create and update their personal privacy preferences for the adaptive web sites they visit by holding their (user) profiles in the client side instead of the server side. By doing so users will be able to self-confine the personalization experience the adaptive sites offer, thus enhancing privacy.
ER  - 

TY  - CONF
JO  - Distributed Computing Systems, 2003. Proceedings. 23rd International Conference on
TI  - Protecting BGP routes to top level DNS servers
T2  - Distributed Computing Systems, 2003. Proceedings. 23rd International Conference on
IS  - 
SN  - 1063-6927
VO  - 
SP  - 322
EP  - 331
AU  - Lan Wang
AU  - Xiaoliang Zhao
AU  - Dan Pei
AU  - Bush, R.
AU  - Massey, D.
AU  - Mankin, A.
AU  - Wu, S.F.
AU  - Lixia Zhang
Y1  - 19-22 May 2003
PY  - 2003
KW  - IP networks
KW  - Internet
KW  - fault tolerant computing
KW  - network routing
KW  - security of data
KW  - BGP path filtering
KW  - DNS infrastructure protection
KW  - IP address
KW  - Internet
KW  - domain name system
KW  - fault-tolerance
KW  - route hijacking
KW  - Current measurement
KW  - Domain Name System
KW  - IEEE news
KW  - Network servers
KW  - Protection
KW  - Redundancy
KW  - Routing
KW  - Testing
KW  - Web and internet services
KW  - Web server
VL  - 
JA  - Distributed Computing Systems, 2003. Proceedings. 23rd International Conference on
DO  - 10.1109/ICDCS.2003.1203481
AB  - The Domain Name System (DNS) is an essential part of the Internet infrastructure and provides fundamental services, such as translating host names into IP addresses for Internet communication. The DNS is vulnerable to a number of potential faults and attacks. In particular, false routing announcements can deny access to the DNS service or redirect DNS queries to a malicious impostor Due to the hierarchical DNS design, a single fault or attack against the routes to any of the top level DNS servers can disrupt Internet services to millions of users. In this paper we propose a path-filtering approach to protect the routes to the critical top level DNS servers. Our approach exploits the high degree of redundancy in top level DNS servers and also exploits the observation that popular destinations, including top level DNS servers, are well connected via stable routes. Our path-filter restricts the potential top level DNS server route changes to be within a set of established paths. Heuristics derived from routing operations are used to adjust the potential routes overtime. We tested our path-filtering design against BGP routing logs and the results show that the design can effectively ensure correct routes to top level DNS servers without impacting DNS service availability.
ER  - 

TY  - CONF
JO  - Dependable, Autonomic and Secure Computing, 2nd IEEE International Symposium on
TI  - On Recognizing Virtual Honeypots and Countermeasures
T2  - Dependable, Autonomic and Secure Computing, 2nd IEEE International Symposium on
IS  - 
SN  - 
VO  - 
SP  - 211
EP  - 218
AU  - Xinwen Fu
AU  - Wei Yu
AU  - Dan Cheng
AU  - Xuejun Tan
AU  - Streff, K.
AU  - Graham, S.
Y1  - Sept. 29 2006-Oct. 1 2006
PY  - 2006
KW  - security of data
KW  - Honeyd
KW  - Neyman-Pearson decision theory
KW  - attacker behavior analysis
KW  - fingerprint attack
KW  - virtual honeypots
KW  - Computer networks
KW  - Counting circuits
KW  - Decision theory
KW  - Delay
KW  - Fingerprint recognition
KW  - IP networks
KW  - Intrusion detection
KW  - Law
KW  - Legal factors
KW  - Operating systems
VL  - 
JA  - Dependable, Autonomic and Secure Computing, 2nd IEEE International Symposium on
DO  - 10.1109/DASC.2006.36
AB  - Honeypots are decoys designed to trap, delay, and gather information about attackers. We can use honeypot logs to analyze attackers' behaviors and design new defenses. A virtual honeypot can emulate multiple honeypots on one physical machine and provide great flexibility in representing one or more networks of machines. But when attackers recognize a honeypot, it becomes useless. In this paper, we address issues related to detecting and "camouflaging" virtual honeypots, in particular Honeyd, which can emulate any size of network on physical machines. We find that an attacker may remotely fingerprint Honeyd by measuring the latency of the network links emulated by Honeyd. We analyze the threat from this fingerprint attack based on the Neyman-Pearson decision theory and find that this class of attack can achieve a high detection rate and low false alarm rate. In order to counter this fingerprint attack, we make virtual honeypots behave like their surrounding networks and blend in with their surroundings. We design a camouflaged Honeyd by revising a small part of the Honeyd toolkit code and by appropriately patching the operating system. Our experiments demonstrate the effectiveness of our approach to camouflaging Honeyd
ER  - 

TY  - CONF
JO  - Networking, Architecture and Storage (NAS), 2010 IEEE Fifth International Conference on
TI  - A High Effective Indexing and Retrieval Method Providing Block-Level Timely Recovery to Any Point-in-Time
T2  - Networking, Architecture and Storage (NAS), 2010 IEEE Fifth International Conference on
IS  - 
SN  - 
VO  - 
SP  - 41
EP  - 50
AU  - Yonghong Sheng
AU  - Dan Xu
AU  - Dongsheng Wang
Y1  - 15-17 July 2010
PY  - 2010
KW  - indexing
KW  - information retrieval
KW  - security of data
KW  - B+ tree
KW  - CDP logs
KW  - block-level continuous data protection
KW  - disk write operation
KW  - hierarchical spatial-temporal indexing method
KW  - incremental retrieval method
KW  - index snapshot
KW  - indexing method
KW  - multiversion B-tree
KW  - point-in-time recovery
KW  - production storage metadata
KW  - retrieval method
KW  - time window
KW  - History
KW  - Image restoration
KW  - Indexing
KW  - Partitioning algorithms
KW  - Production
KW  - Time domain analysis
KW  - CDP
KW  - Recovery
KW  - data protection
KW  - index
KW  - storage
VL  - 
JA  - Networking, Architecture and Storage (NAS), 2010 IEEE Fifth International Conference on
DO  - 10.1109/NAS.2010.63
AB  - Block-level continuous data protection (CDP) logs every disk write operation so that the disk can be rolled back to any arbitrary point-in-time within a time window. For each update operation is time stamped and logged, the indexing for such huge amounts of records is an important and challenging problem. Unfortunately, the conventional indexing methods can not efficiently record large numbers of versions and support instant &#x201C;time-travel&#x201D; types of queries in CDP. In this paper, we present an effective indexing method providing timely recovery to any point-in-time in comprehensive versioning systems, called the Hierarchical Spatial-Temporal Indexing Method (HSTIM). The basic principle of HSTIM is to partition the time domain and the production storage LBAs into time slice and segments respectively according to update frequency of disk IOs, and build separate index file for each segment. In order to meet the demands of instant view of history data, the metadata of production storage is independently indexed. For long-time history data retrieval requirements, index snapshot is introduced in HSTIM to reduce the retrieval time. Another distinctive feature of HSTIM is its incremental retrieval method, which achieves high query performance at time point t + t if neighboring time point t is queried previously. The paper compares HSTIM with traditional B+-tree and multi-version B-tree (MVBT) index in many aspects. Experiments with real workload IO trace files show that HSTIM can locate history data within 8.05 seconds for recovery point of 48 hours, while B+-tree consumes 24.04 seconds. If the index snapshot is applied, HSTIM can reduce such retrieval time within 3 seconds.
ER  - 

TY  - CONF
JO  - Advanced Information Networking and Applications (AINA), 2010 24th IEEE International Conference on
TI  - Minimizing Execution Costs when Using Globally Distributed Cloud Services
T2  - Advanced Information Networking and Applications (AINA), 2010 24th IEEE International Conference on
IS  - 
SN  - 1550-445X
VO  - 
SP  - 222
EP  - 229
AU  - Pandey, S.
AU  - Barker, A.
AU  - Gupta, K.K.
AU  - Buyya, R.
Y1  - 20-23 April 2010
PY  - 2010
KW  - distributed processing
KW  - minimisation
KW  - nonlinear programming
KW  - security of data
KW  - cloud computing
KW  - data retrieval minimization
KW  - execution cost minimization
KW  - globally distributed cloud services
KW  - intrusion detection application workflow
KW  - nonlinear programming model
KW  - Application software
KW  - Cloud computing
KW  - Computer science
KW  - Costs
KW  - Data mining
KW  - Distributed computing
KW  - Information retrieval
KW  - Intrusion detection
KW  - Laboratories
KW  - Software engineering
KW  - Cloud computing
KW  - Workflow scheduling
KW  - multi-source data retrieval
KW  - non-linear programming
VL  - 
JA  - Advanced Information Networking and Applications (AINA), 2010 24th IEEE International Conference on
DO  - 10.1109/AINA.2010.30
AB  - Cloud computing is an emerging technology that allows users to utilize on-demand computation, storage, data and services from around the world. However, Cloud service providers charge users for these services. Specifically, to access data from their globally distributed storage edge servers, providers charge users depending on the user's location and the amount of data transferred. When deploying data-intensive applications in a Cloud computing environment, optimizing the cost of transferring data to and from these edge servers is a priority, as data play the dominant role in the application's execution. In this paper, we formulate a non-linear programming model to minimize the data retrieval and execution cost of data-intensive workflows in Clouds. Our model retrieves data from Cloud storage resources such that the amount of data transferred is inversely proportional to the communication cost. We take an example of an `intrusion detection' application workflow, where the data logs are made available from globally distributed Cloud storage servers. We construct the application as a workflow and experiment with Cloud based storage and compute resources. We compare the cost of multiple executions of the workflow given by a solution of our non-linear program against that given by Amazon CloudFront's `nearest' single data source selection. Our results show a savings of three-quarters of total cost using our model.
ER  - 

TY  - CONF
JO  - Reliable Distributed Systems, 2008. SRDS '08. IEEE Symposium on
TI  - An Incremental File System Consistency Checker for Block-Level CDP Systems
T2  - Reliable Distributed Systems, 2008. SRDS '08. IEEE Symposium on
IS  - 
SN  - 1060-9857
VO  - 
SP  - 157
EP  - 162
AU  - Maohua Lu
AU  - Tzi-cker Chiueh
AU  - Shibiao Lin
Y1  - 6-8 Oct. 2008
PY  - 2008
KW  - file organisation
KW  - incremental compilers
KW  - security of data
KW  - block-level continuous data protection system
KW  - consistency checker
KW  - incremental file system check mechanism
KW  - metadata consistency
KW  - recovery point objective
KW  - storage system
KW  - Access protocols
KW  - Application software
KW  - Computer network reliability
KW  - Computer science
KW  - File servers
KW  - File systems
KW  - Image storage
KW  - Measurement
KW  - Network servers
KW  - Protection
KW  - CDP
KW  - FSCK
VL  - 
JA  - Reliable Distributed Systems, 2008. SRDS '08. IEEE Symposium on
DO  - 10.1109/SRDS.2008.20
AB  - A block-level continuous data protection (CDP) system logs every disk block update from an application server (e.g., a file or DBMS server) to a storage system so that any disk updates within a time window are undoable, and thus is able to provide a more flexible and efficient data protection service than conventional periodic data backup systems. Unfortunately, no existing block-level CDP systems can support arbitrary point-in-time snapshots that are guaranteed to be consistent with respect to the metadata of the application server. This deficiency seriously limits the flexibility in recovery point objective (RTO) of block-level CDP systems from the standpoint of the application servers whose data they protect. This paper describes an incremental file system check mechanism (iFSCK) that is designed to address this deficiency for file servers, and exploits file system-specific knowledge to quickly fix an arbitrary point-in-time block-level snapshot so that it is consistent with respect to file system metadata. Performance measurements taken from a fully operational iFSCK prototype show that iFSCK can turn a 10 GB point-in-time block-level snapshot to be file-system consistent in less than 1 second, and takes less than 25% of the time required by the Fsck utility for vanilla ext3 under relaxed metadata consistency requirements.
ER  - 

TY  - CONF
JO  - E-Science Workshops, 2009 5th IEEE International Conference on
TI  - Nonparametric multivariate anomaly analysis in support of HPC resilience
T2  - E-Science Workshops, 2009 5th IEEE International Conference on
IS  - 
SN  - 
VO  - 
SP  - 80
EP  - 85
AU  - Ostrouchov, G.
AU  - Naughton, T.
AU  - Engelmann, C.
AU  - Vallee, G.
AU  - Scott, S.L.
Y1  - 9-11 Dec. 2009
PY  - 2009
KW  - security of data
KW  - software fault tolerance
KW  - statistical analysis
KW  - HPC resilience
KW  - data anomaly monitoring identification
KW  - high-performance computing systems
KW  - large-scale computing systems
KW  - nonparametric multivariate anomaly analysis
KW  - root-cause diagnosis
KW  - size failure detection
KW  - statistical analysis techniques
KW  - system logs
KW  - Computer science
KW  - Condition monitoring
KW  - Failure analysis
KW  - Laboratories
KW  - Large-scale systems
KW  - Mathematics
KW  - Prototypes
KW  - Resilience
KW  - Runtime
KW  - Scientific computing
VL  - 
JA  - E-Science Workshops, 2009 5th IEEE International Conference on
DO  - 10.1109/ESCIW.2009.5407992
AB  - Large-scale computing systems provide great potential for scientific exploration. However, the complexity that accompanies these enormous machines raises challenges for both, users and operators. The effective use of such systems is often hampered by failures encountered when running applications on systems containing tens-of-thousands of nodes and hundreds-of-thousands of compute cores capable of yielding petaflops of performance. In systems of this size failure detection is complicated and root-cause diagnosis difficult. This paper describes our recent work in the identification of anomalies in monitoring data and system logs to provide further insights into machine status, runtime behavior, failure modes and failure root causes. It discusses the details of an initial prototype that gathers the data and uses statistical techniques for analysis.
ER  - 

TY  - CONF
JO  - Data Mining Workshops (ICDMW), 2013 IEEE 13th International Conference on
TI  - Evaluation and Comparison of Classification Techniques for Network Intrusion Detection
T2  - Data Mining Workshops (ICDMW), 2013 IEEE 13th International Conference on
IS  - 
SN  - 
VO  - 
SP  - 335
EP  - 342
AU  - Giray, S.M.
AU  - Polat, A.G.
Y1  - 7-10 Dec. 2013
PY  - 2013
KW  - data mining
KW  - pattern classification
KW  - security of data
KW  - KDD 99 dataset
KW  - NSL KDD dataset
KW  - classification technique comparison
KW  - classification technique evaluation
KW  - data mining
KW  - network IDS domain
KW  - network intrusion detection system logs
KW  - noisy network environment
KW  - threat classification
KW  - threat detection
KW  - Algorithm design and analysis
KW  - Classification algorithms
KW  - Intrusion detection
KW  - Noise
KW  - Noise measurement
KW  - Prediction algorithms
KW  - Training
KW  - Intrusion detection
KW  - anomaly identification
KW  - classifier
KW  - false alarm
KW  - ignored attack
KW  - prediction
VL  - 
JA  - Data Mining Workshops (ICDMW), 2013 IEEE 13th International Conference on
DO  - 10.1109/ICDMW.2013.83
AB  - Data mining provides a useful environment and set of tools for processing large datasets such as Intrusion Detection Systems (IDS) logs. Researchers improve existing IDS models by comparing the performance of various algorithms on these datasets. It is very important to keep in mind that an IDS often has to work in a noisy network environment. Network noise is one of the most challenging issues for efficient threat detection and classification. In this study, normal and noisy datasets for network IDS domain are used and various classification algorithms are evaluated. The results show that an evaluation of algorithms without noise is misleading for IDSs since algorithms that perform best without noise do not necessarily achieve the same in a realistic noisy environment. Moreover refined NSL KDD dataset allows a more realistic evaluation of various algorithms than the original KDD 99 dataset.
ER  - 

TY  - CONF
JO  - Web Intelligence and Intelligent Agent Technology, 2008. WI-IAT '08. IEEE/WIC/ACM International Conference on
TI  - Intent-Driven Insider Threat Detection in Intelligence Analyses
T2  - Web Intelligence and Intelligent Agent Technology, 2008. WI-IAT '08. IEEE/WIC/ACM International Conference on
IS  - 
SN  - 
VO  - 2
SP  - 345
EP  - 349
AU  - Santos, E.
AU  - Hien Nguyen
AU  - Fei Yu
AU  - Keumjoo Kim
AU  - Deqing Li
AU  - Wilkinson, J.T.
AU  - Olson, A.
AU  - Jacob, R.
Y1  - 9-12 Dec. 2008
PY  - 2008
KW  - security of data
KW  - user modelling
KW  - intelligence analyses
KW  - intent-driven insider threat detection
KW  - logged information
KW  - user modeling technique
KW  - Computational intelligence
KW  - Computational modeling
KW  - Decision making
KW  - Educational institutions
KW  - Government
KW  - Information analysis
KW  - Intelligent agent
KW  - Interference constraints
KW  - Jacobian matrices
KW  - Time factors
KW  - insider threats
KW  - intelligence analyses
KW  - intelligence community
KW  - user modeling
VL  - 2
JA  - Web Intelligence and Intelligent Agent Technology, 2008. WI-IAT '08. IEEE/WIC/ACM International Conference on
DO  - 10.1109/WIIAT.2008.376
AB  - When decisions need to be made in government, the intelligence community (IC) is tasked with analyzing the situation. This analysis is based on a huge amount of information and usually under severe time constraints. As such, it is particularly vulnerable to attacks from insiders with malicious intent. A malicious insider may alter, fabricate, or hide critical information in their analytical products, such as reports, in order to interfere with the decision making process. In this paper, we focus on detecting such malicious insiders. Malicious actions such as disinformation tend to be very subtle and thus difficult to detect. Therefore, we employ a user modeling technique to model an insider based on logged information and documents accessed while accomplishing an intelligence analysis task. We create a computational model for each insider and apply several detection metrics to analyze this model as it changes over time. If any deviation of behavior is detected, alerts can be issued. A pilot test revealed that the computed deviations had a high correlation with insiderspsila cognitive styles. Based on this finding, we designed a framework that minimized the impact of differences in cognitive styles. In our evaluation, we used data collected from intelligence analysts, and simulated malicious insiders based on this data. A high percentage of the simulated malicious insiders were successfully detected.
ER  - 

TY  - JOUR
JO  - Communications Magazine, IEEE
TI  - Using whitelisting to mitigate DDoS attacks on critical Internet sites
T2  - Communications Magazine, IEEE
IS  - 7
SN  - 0163-6804
VO  - 48
SP  - 110
EP  - 115
AU  - Myungkeun Yoon
Y1  - July 2010
PY  - 2010
KW  - Internet
KW  - security of data
KW  - DDoS attackers
KW  - VIP list
KW  - authentication processes
KW  - botnet size
KW  - critical internet sites
KW  - login logs
KW  - monetary profit
KW  - source addresses
KW  - whitelist
KW  - Bandwidth
KW  - Computer crime
KW  - Computer vision
KW  - Floods
KW  - Internet
KW  - Network servers
KW  - Personal communication networks
KW  - System testing
KW  - Telecommunication traffic
KW  - Web server
VL  - 48
JA  - Communications Magazine, IEEE
DO  - 10.1109/MCOM.2010.5496886
AB  - As DDoS attackers pursue monetary profit, critical Internet sites (CISs) become a good target. These attacks will be more difficult to defend because the botnet size continuously increases, and the attackers spare no pains in preparing the attacks. Under this new paradigm, current anti-DDoS systems may be fooled; we need a new survival strategy. We propose a novel DDoS mitigation scheme for CISs. We observe that CISs can continue their main businesses if most important clients can access the services. This motivates us to build a whitelist, called a VIP list in this article, and the source addresses in the list are given higher priority when the CIS is under attack. The VIP list is built from the previous login logs of authentication processes at the application layer. The experimental results show that the proposed scheme effectively mitigates DDoS attacks.
ER  - 

TY  - CONF
JO  - Web Intelligence and Intelligent Agent Technology, 2008. WI-IAT '08. IEEE/WIC/ACM International Conference on
TI  - Privacy with Web Serivces: Intelligence Gathering and Enforcement
T2  - Web Intelligence and Intelligent Agent Technology, 2008. WI-IAT '08. IEEE/WIC/ACM International Conference on
IS  - 
SN  - 
VO  - 3
SP  - 546
EP  - 549
AU  - Bodorik, P.
AU  - Jutla, D.
Y1  - 9-12 Dec. 2008
PY  - 2008
KW  - Web services
KW  - business data processing
KW  - knowledge based systems
KW  - security of data
KW  - software architecture
KW  - Web services architecture
KW  - Web services platform
KW  - agent-based enterprise information technology infrastructure
KW  - intelligence enforcement
KW  - intelligence gathering
KW  - privacy knowledge base
KW  - private information
KW  - Application software
KW  - Computer architecture
KW  - Data privacy
KW  - Information systems
KW  - Intelligent agent
KW  - Law
KW  - Middleware
KW  - Service oriented architecture
KW  - Software maintenance
KW  - Web services
KW  - P3P
KW  - Privacy Agent
KW  - Privacy Context
KW  - Privacy Enforcement
KW  - Privacy Information (PI)
KW  - Privacy Knowledge Base
KW  - WSA
KW  - Web Services (WS)
KW  - Web Services Architecture
VL  - 3
JA  - Web Intelligence and Intelligent Agent Technology, 2008. WI-IAT '08. IEEE/WIC/ACM International Conference on
DO  - 10.1109/WIIAT.2008.383
AB  - This paper focuses on agent-based enterprise information technology infrastructure support for privacy in the Web services architecture (WSA) in order to enforce privacy policies on private information (PI) used by applications. We provision the Web services platform (WSP) with mechanisms to not only enforce privacy policies on PI used by a Web service, but also gather intelligence about PI that is exchanged in invoking and executing Web services. Gathered information written in logs is then analyzed by a privacy agent to update a privacy knowledge base (KB) that captures information on applications, Web services they invoke, context of invocation, and PI stored, managed, and used by the enterprise. This loop-observe execution of Web services, update the knowledge base with observations, and then update rules governing privacy enforcement, provides for adaptive learning about the use of PI in the organization and also enforcement of privacy policies in Web services executions.
ER  - 

TY  - CONF
JO  - Big Data (BigData Congress), 2014 IEEE International Congress on
TI  - Knowledge Discovery from Big Data for Intrusion Detection Using LDA
T2  - Big Data (BigData Congress), 2014 IEEE International Congress on
IS  - 
SN  - 
VO  - 
SP  - 760
EP  - 761
AU  - Jingwei Huang
AU  - Kalbarczyk, Z.
AU  - Nicol, D.M.
Y1  - June 27 2014-July 2 2014
PY  - 2014
KW  - Big Data
KW  - data mining
KW  - learning (artificial intelligence)
KW  - security of data
KW  - Big Data
KW  - LDA
KW  - LDA models
KW  - anomaly-based methods
KW  - intrusion detection
KW  - knowledge discovery
KW  - latent Dirichlet allocation
KW  - learning algorithms
KW  - network logs
KW  - signature-based methods
KW  - system logs
KW  - topic modeling problem
KW  - Big data
KW  - Data models
KW  - Intrusion detection
KW  - Knowledge discovery
KW  - Monitoring
KW  - Vocabulary
KW  - LDA
KW  - big data
KW  - data mining
KW  - intrusion detection
VL  - 
JA  - Big Data (BigData Congress), 2014 IEEE International Congress on
DO  - 10.1109/BigData.Congress.2014.111
AB  - This paper explores a hybrid approach of intrusion detection through knowledge discovery from big data using Latent Dirichlet Allocation (LDA). We identify the "hidden" patterns of operations conducted by both normal users and malicious users from a large volume of network/systems logs, by mapping this problem to the topic modeling problem and leveraging the well established LDA models and learning algorithms. This new approach potentially completes the strength of signature-based and anomaly-based methods.
ER  - 

TY  - CONF
JO  - Data Engineering, 2005. ICDE 2005. Proceedings. 21st International Conference on
TI  - Design, Implementation, and Evaluation of a Repairable Database Management System
T2  - Data Engineering, 2005. ICDE 2005. Proceedings. 21st International Conference on
IS  - 
SN  - 1084-4627
VO  - 
SP  - 1024
EP  - 1035
AU  - Tzi-cker Chiueh
AU  - Pilania, D.
Y1  - 05-08 April 2005
PY  - 2005
KW  - SQL
KW  - database management systems
KW  - security of data
KW  - system recovery
KW  - Phoenix prototype
KW  - PostgreSQL DBMS
KW  - database transactions
KW  - mean time to failure
KW  - mean time to repair
KW  - post-intrusion database repair process
KW  - repairable database management system
KW  - system availability
KW  - Availability
KW  - Database systems
KW  - Delay
KW  - Error correction
KW  - Hardware
KW  - Measurement
KW  - Protection
KW  - Prototypes
KW  - Throughput
KW  - Transaction databases
VL  - 
JA  - Data Engineering, 2005. ICDE 2005. Proceedings. 21st International Conference on
DO  - 10.1109/ICDE.2005.49
AB  - Although conventional database management systems are designed to tolerate hardware and to a lesser extent even software errors, they cannot protect themselves against syntactically correct and semantically damaging transactions, which could arise because of malicious attacks or honest mistakes. The lack of fast post-intrusion or post-error damage repair in modern DBMSs results in a longer Mean Time to Repair (MTTR) and sometimes permanent data loss that could have been saved by more intelligent repair mechanisms. In this paper, we describe the design and implementation of Phoenix - a system that significantly improves the efficiency and precision of a database damage repair process after an intrusion or operator error and thus, increases the overall database system availability. The two key ideas underlying Phoenix are (1) maintaining persistent inter-transaction dependency information at run time to allow selective undo of database transactions that are considered "infected" by the intrusion or error in question and (2) exploiting information present in standard database logs for fast selective undo. Performance measurements on a fully operational Phoenix prototype, which is based on the PostgreSQL DBMS, demonstrate that Phoenix incurs a response time and a throughput penalty of less than 5% and 8%, respectively, under the TPC-C benchmark, but it can speed up the post-intrusion database repair process by at least an order of magnitude when compared with a manual repair process.
ER  - 

TY  - CONF
JO  - Program Comprehension (ICPC), 2011 IEEE 19th International Conference on
TI  - Trust-Based Requirements Traceability
T2  - Program Comprehension (ICPC), 2011 IEEE 19th International Conference on
IS  - 
SN  - 1092-8138
VO  - 
SP  - 111
EP  - 120
AU  - Ali, N.
AU  - Gueheneuc, Y.
AU  - Antoniol, G.
Y1  - 22-24 June 2011
PY  - 2011
KW  - information retrieval
KW  - security of data
KW  - IR-based traceability recovery approach
KW  - Trustrace approach
KW  - information retrieval approach
KW  - traceability link
KW  - trust-based requirements traceability
KW  - vector space model
KW  - Documentation
KW  - Electronic mail
KW  - Indexing
KW  - Probabilistic logic
KW  - Semantics
KW  - Software
KW  - Web sites
KW  - Traceability
KW  - experts
KW  - requirements
KW  - source code
KW  - trust-based model
VL  - 
JA  - Program Comprehension (ICPC), 2011 IEEE 19th International Conference on
DO  - 10.1109/ICPC.2011.42
AB  - Information retrieval (IR) approaches have proven useful in recovering traceability links between free text documentation and source code. IR-based traceability recovery approaches produce ranked lists of traceability links between pieces of documentation and source code. These traceability links are then pruned using various strategies and, finally, validated by human experts. In this paper we propose two contributions to improve the precision and recall of traceability links and, thus, reduces the required human experts' manual validation effort. First, we propose a novel approach, Trustrace, inspired by Web trust models to improve the precision and recall of traceability links: Trustrace uses any traceability recovery approach to obtain a set of traceability links, which rankings are then re-evaluated using a set of other traceability recovery approaches. Second, we propose a novel traceability recovery approach, Histrace, to identify traceability links between requirements and source code through CVS/SVN change logs using a Vector Space Model (VSM). We combine a traditional recovery traceability approach with Histrace to build Trustrace<sup>VSM, Histrace</sup> in which we use Histrace as one expert adding knowledge to the traceability links extracted from CVS/SVN change logs. We apply Trustrace<sup>VSM, Histrace</sup> on two case studies to compare its traceability links with those recovered using only the VSM-based approach, in terms of precision and recall. We show that Trustrace<sup>VSM, Histrace</sup> improves with statistical significance the precision of the traceability links while also improving recall but without statistical significance.
ER  - 

TY  - CONF
JO  - Services Computing (SCC), 2011 IEEE International Conference on
TI  - Magnifier: Online Detection of Performance Problems in Large-Scale Cloud Computing Systems
T2  - Services Computing (SCC), 2011 IEEE International Conference on
IS  - 
SN  - 
VO  - 
SP  - 418
EP  - 425
AU  - Haibo Mi
AU  - Huaimin Wang
AU  - Gang Yin
AU  - Hua Cai
AU  - Qi Zhou
AU  - Tingtao Sun
AU  - Yangfan Zhou
Y1  - 4-9 July 2011
PY  - 2011
KW  - cloud computing
KW  - graph theory
KW  - large-scale systems
KW  - online operation
KW  - real-time systems
KW  - security of data
KW  - Magnifier
KW  - cloud computing
KW  - execution path graph
KW  - hierarchical structure
KW  - large-scale systems
KW  - online anomaly detection
KW  - performance degradation
KW  - real-world enterprise system
KW  - Cloud computing
KW  - Degradation
KW  - Fluctuations
KW  - Indexes
KW  - Instruments
KW  - Principal component analysis
KW  - Servers
VL  - 
JA  - Services Computing (SCC), 2011 IEEE International Conference on
DO  - 10.1109/SCC.2011.25
AB  - In large-scale cloud computing systems, even a simple user request may go through numerous of services that are deployed on different physical machines. As a result, it is a great challenge to online localize the prime causes of performance degradation in such systems. Existing end-to-end request tracing approaches are not suitable for online anomaly detection because their time complexity is exponential in the size of the trace logs. In this paper, we propose an approach, namely Magnifier, to rapidly diagnose the source of performance degradation in large-scale non-stop cloud systems. In Magnifier, the execution path graph of a user request is modeled by a hierarchical structure including component layer, module layer and function layer, and anomalies are detected from higher layer to lower layer separately. In each layer every node is assigned a newly created identifier in addition to the global identifier of the request, which significantly decreases the size of parsing trace logs and accelerates the anomaly detection process. We conduct extensive experiments over a real-world enterprise system (the Alibaba cloud computing platform) providing services for the public. The results show that Magnifier can locate the prime causes of performance degradation more accurately and efficiently.
ER  - 

TY  - CONF
JO  - Integrated Network Management, 2009. IM '09. IFIP/IEEE International Symposium on
TI  - Problem classification method to enhance the ITIL incident and problem
T2  - Integrated Network Management, 2009. IM '09. IFIP/IEEE International Symposium on
IS  - 
SN  - 
VO  - 
SP  - 295
EP  - 298
AU  - Song, Y.
AU  - Sailer, A.
AU  - Shaikh, H.
Y1  - 1-5 June 2009
PY  - 2009
KW  - computational complexity
KW  - distributed processing
KW  - learning (artificial intelligence)
KW  - pattern classification
KW  - security of data
KW  - system monitoring
KW  - ITIL incident
KW  - batch learning algorithms
KW  - computational complexity
KW  - detecting anomaly
KW  - distributed Web applications
KW  - hierarchical incremental learning algorithm
KW  - internal local classifier parameters
KW  - machine learning methods
KW  - monitored system
KW  - online learning framework
KW  - operational costs
KW  - problem classification method
KW  - problem determination
KW  - substantial reduction
KW  - system administration costs
KW  - Computer science
KW  - Computerized monitoring
KW  - Costs
KW  - Databases
KW  - Decision trees
KW  - Learning systems
KW  - Machine learning
KW  - System performance
KW  - Taxonomy
KW  - Training data
KW  - hierarchical classification
KW  - machine learningP
KW  - machine learningroblem determination
KW  - online learning
KW  - roblem determination
VL  - 
JA  - Integrated Network Management, 2009. IM '09. IFIP/IEEE International Symposium on
DO  - 10.1109/INM.2009.5188825
AB  - Problem determination and resolution PDR is the process of detecting anomalies in a monitored system, locating the problems responsible for the issue, determining the root cause and fixing the cause of the problem. The cost of PDR represents a substantial part of operational costs, and faster, more effective PDR can contribute to a substantial reduction in system administration costs. In this paper, we propose to automate the process of PDR by leveraging machine learning methods. The main focus is to effectively categorize the problem a user experiences by recognizing the problem specificity leveraging all available training data such like the performance data and the logs data. Specifically, we transform the structure of the problem into a hierarchy which can be determined by existing taxonomy in advance. We then propose an efficient hierarchical incremental learning algorithm which is capable of adjusting its internal local classifier parameters in real-time. Comparing to the traditional batch learning algorithms, this online learning framework can significantly decrease the computational complexity of the training process by learning from new instances on an incremental fashion. In the same time this reduces the amount of memory required to store the training instances. We demonstrate the efficiency of our approach by learning hierarchical problem patterns for several issues occurring in distributed web applications. Experimental results show that our approach substantially outperforms previous methods.
ER  - 

TY  - CONF
JO  - Information Assurance Workshop, 2005. IAW '05. Proceedings from the Sixth Annual IEEE SMC
TI  - A visualization paradigm for network intrusion detection
T2  - Information Assurance Workshop, 2005. IAW '05. Proceedings from the Sixth Annual IEEE SMC
IS  - 
SN  - 
VO  - 
SP  - 92
EP  - 99
AU  - Livnat, Y.
AU  - Agutter, J.
AU  - Moon, S.
AU  - Erbacher, R.F.
AU  - Foresti, S.
Y1  - 15-17 June 2005
PY  - 2005
KW  - computer networks
KW  - data visualisation
KW  - security of data
KW  - abnormal network activity detection
KW  - alert temporal distribution
KW  - attack trend
KW  - attribute representation
KW  - data visualization
KW  - disparate logs
KW  - many-to-one correlation
KW  - network alert visual correlation
KW  - network intrusion detection
KW  - user situational awareness
KW  - visual metaphor
KW  - Complex networks
KW  - Data visualization
KW  - Decision making
KW  - Displays
KW  - Humans
KW  - Intrusion detection
KW  - Moon
KW  - Network topology
KW  - Telecommunication traffic
KW  - Vehicles
VL  - 
JA  - Information Assurance Workshop, 2005. IAW '05. Proceedings from the Sixth Annual IEEE SMC
DO  - 10.1109/IAW.2005.1495939
AB  - We present a novel paradigm for visual correlation of network alerts from disparate logs. This paradigm facilitates and promotes situational awareness in complex network environments. Our approach is based on the notion that, by definition, an alert must possess three attributes, namely: what, when, and where. This fundamental premise, which we term w<sup>3</sup>, provides a vehicle for comparing between seemingly disparate events. We propose a concise and scalable representation of these three attributes, that leads to a flexible visualization tool that is also clear and intuitive to use. Within our system, alerts can be grouped and viewed hierarchically with respect to both their type, i.e., the what, and to their where attributes. Further understanding is gained by displaying the temporal distribution of alerts to reveal complex attack trends. Finally, we propose a set of visual metaphor extensions that augment the proposed paradigm and enhance users' situational awareness. These metaphors direct the attention of users to many-to-one correlations within the current display helping them detect abnormal network activity.
ER  - 

TY  - CONF
JO  - Distributed Computing Systems, 2009. ICDCS '09. 29th IEEE International Conference on
TI  - File Versioning for Block-Level Continuous Data Protection
T2  - Distributed Computing Systems, 2009. ICDCS '09. 29th IEEE International Conference on
IS  - 
SN  - 1063-6927
VO  - 
SP  - 327
EP  - 334
AU  - Maohua Lu
AU  - Tzi-cker Chiueh
Y1  - 22-26 June 2009
PY  - 2009
KW  - file organisation
KW  - file servers
KW  - security of data
KW  - DBMS servers
KW  - NFS client
KW  - NFS server
KW  - block-level continuous data protection
KW  - disk block update
KW  - disk block versions
KW  - file metadata
KW  - host file system
KW  - incremental consistency check mechanism
KW  - name-based user-level file versioning system
KW  - standard file servers
KW  - Distributed computing
KW  - File servers
KW  - File systems
KW  - Image converters
KW  - Image restoration
KW  - Linux
KW  - Measurement
KW  - Operating systems
KW  - Protection
KW  - Prototypes
KW  - CDP
KW  - File Versioning
VL  - 
JA  - Distributed Computing Systems, 2009. ICDCS '09. 29th IEEE International Conference on
DO  - 10.1109/ICDCS.2009.48
AB  - Block-level continuous data protection (CDP) logs every disk block update so that disk updates within a time window are undoable. Standard file servers and DBMS servers can enjoy the data protection service offered by block-level CDP without any modification. Unfortunately, no existing block-level CDP systems can provide users a file versioning view on top of the block versions they maintain. As a result, the data they maintain cannot be used as an extension to the on-line system with which users routinely interact. This paper describes a name-based user-level file versioning system called UVFS that is designed to reconstruct file versions from disk block versions maintained by a block-level CDP. UVFS reconstructs file versions by following the last modified time of files and directories, a common file metadata supported by almost all modern file systems, and therefore does not require any modification to the host file system that a block-level CDP system protects. In addition, UVFS incorporates a file system-specific incremental consistency check mechanism to quickly convert an arbitrary point-in-time block-level snapshot to a file system-consistent one. Performance measurements taken from a fully operational UVFS prototype show that the average end-to-end elapsed time required to discover a file version is under 50 msec from the perspective of an NFS client serviced by an NFS server backed by a block-level CDP system.
ER  - 

TY  - CONF
JO  - Intelligent Agent & Multi-Agent Systems, 2009. IAMA 2009. International Conference on
TI  - Terror tracking using advanced web mining perspective
T2  - Intelligent Agent & Multi-Agent Systems, 2009. IAMA 2009. International Conference on
IS  - 
SN  - 
VO  - 
SP  - 1
EP  - 4
AU  - Anand, T.
AU  - Padmapriya, S.
AU  - Kirubakaran, E.
Y1  - 22-24 July 2009
PY  - 2009
KW  - Internet
KW  - data mining
KW  - security of data
KW  - Web content mining
KW  - Web page content
KW  - Web structure mining
KW  - Web usage logs
KW  - Web usage mining
KW  - advanced Web mining
KW  - terror tracking
KW  - user access pattern discovery
KW  - Blogs
KW  - Computer science
KW  - Counting circuits
KW  - Data mining
KW  - Information technology
KW  - Machine learning
KW  - Terrorism
KW  - Text mining
KW  - Web mining
KW  - Web pages
KW  - content mining and patterns
KW  - structure mining
KW  - usage mining
VL  - 
JA  - Intelligent Agent & Multi-Agent Systems, 2009. IAMA 2009. International Conference on
DO  - 10.1109/IAMA.2009.5228034
AB  - Web mining is a rapidly growing research area. It consists of Web usage mining, Web structure mining, and Web content mining. Web usage mining refers to the discovery of user access patterns from Web usage logs. Web structure mining tries to discover useful knowledge from the structure of hyperlinks. Web content mining aims to extract/mine useful information or knowledge from Web page contents. Web mining techniques can be used for detecting and avoiding terror threats caused by terrorists all over the world.
ER  - 

TY  - CONF
JO  - Emerging Technologies (ICET), 2010 6th International Conference on
TI  - Hierarchical sessionization at preprocessing level of WUM based on swarm intelligence
T2  - Emerging Technologies (ICET), 2010 6th International Conference on
IS  - 
SN  - 
VO  - 
SP  - 21
EP  - 26
AU  - Hussain, T.
AU  - Asghar, S.
AU  - Masood, N.
Y1  - 18-19 Oct. 2010
PY  - 2010
KW  - Internet
KW  - Web sites
KW  - data mining
KW  - information filtering
KW  - pattern clustering
KW  - Web based application
KW  - Web log file analysis
KW  - Web log preprocessing
KW  - Web server log file
KW  - Web usage mining
KW  - Website management
KW  - data cleaning
KW  - data filtering
KW  - evolutionary change
KW  - hierarchical sessionization
KW  - pattern discovery
KW  - session clustering
KW  - session identification
KW  - swarm intelligence
KW  - user identification
KW  - user pattern extraction
KW  - Cleaning
KW  - Clustering algorithms
KW  - Data mining
KW  - Euclidean distance
KW  - Filtering
KW  - Filtering algorithms
KW  - IP networks
KW  - Hierarchical Sessionization
KW  - Particle Swarm
KW  - Preprocessing
KW  - Structured Information
KW  - Web Usage Mining
VL  - 
JA  - Emerging Technologies (ICET), 2010 6th International Conference on
DO  - 10.1109/ICET.2010.5638388
AB  - Web based applications are increasing at an enormous speed and consequently its users are also increasing at an exponential speed. The evolutionary changes in technology have made it possible to capture the users' essence and interactions with web applications through web server log file as web usage. The web usage Mining (WUM) is the process of discovering hidden patterns from the web usage. Due to large amount of &#x201C;irrelevant information&#x201D; in the web log, the original log file cannot be directly used in the WUM process. Therefore, the preprocessing of web log file becomes imperative. The proper analysis of web log file is beneficial to manage the websites effectively for administrative and users' prospective. Web log preprocessing is an initial necessary step to improve the quality and efficiency of the later steps of WUM. There are number of techniques available at preprocessing level of WUM such as data cleaning; data filtering; user identification; session identification and session clustering. In this research paper, a complete preprocessing technique is being proposed to preprocess the web log for extraction of user patterns. Data cleaning algorithm removes the irrelevant entries from web log and filtering algorithm discards the uninterested attributes from log file. User and sessions are identified. Proposed hierarchical sessionization algorithm generates the hierarchy of sessions. We obtain unbiased hierarchical clusters from the web log file.
ER  - 

TY  - CONF
JO  - Program Comprehension, 2003. 11th IEEE International Workshop on
TI  - Verification of recovered software architectures
T2  - Program Comprehension, 2003. 11th IEEE International Workshop on
IS  - 
SN  - 1092-8138
VO  - 
SP  - 258
EP  - 265
AU  - Gannod, G.C.
AU  - Murthy, S.
Y1  - 10-11 May 2003
PY  - 2003
KW  - finite state machines
KW  - reverse engineering
KW  - software architecture
KW  - finite-state model
KW  - liveness conditions
KW  - log file analysis
KW  - log files
KW  - reverse engineering
KW  - safety
KW  - software architectures
KW  - software behavior
KW  - software developers
KW  - Computer architecture
KW  - Computer science
KW  - Engineering profession
KW  - Information resources
KW  - Instruments
KW  - NASA
KW  - Reverse engineering
KW  - Safety
KW  - Sampling methods
KW  - State-space methods
VL  - 
JA  - Program Comprehension, 2003. 11th IEEE International Workshop on
DO  - 10.1109/WPC.2003.1199210
AB  - A common technique employed by software developers is the use of log files to generate traces of observed software behavior. As a resource for reverse engineering, a log file has the advantage of being an accurate account of software behavior. Model checking approaches work by using exploration to determine whether certain safety and liveness conditions are satisfied by a finite-state model. In this paper we describe an approach that combines the use of model checking and log file analysis to facilitate verification of recovered models.
ER  - 

TY  - CONF
JO  - Computer Software and Applications Conference (COMPSAC), 2012 IEEE 36th Annual
TI  - Event Indexing and Searching for High Volumes of Event Streams in the Cloud
T2  - Computer Software and Applications Conference (COMPSAC), 2012 IEEE 36th Annual
IS  - 
SN  - 0730-3157
VO  - 
SP  - 405
EP  - 415
AU  - Miao Wang
AU  - Holub, V.
AU  - Murphy, J.
AU  - O'Sullivan, P.
Y1  - 16-20 July 2012
PY  - 2012
KW  - cloud computing
KW  - data analysis
KW  - data mining
KW  - indexing
KW  - information filters
KW  - information retrieval
KW  - pattern clustering
KW  - search engines
KW  - bloom filter technique
KW  - cloud computing
KW  - data retrieval
KW  - dynamic stream data handling
KW  - enterprise system
KW  - event indexing
KW  - event searching
KW  - event stream
KW  - historical event
KW  - indexing engine
KW  - inverted indexing technique
KW  - log file analysis
KW  - pattern searching
KW  - run time correlation engine logging framework
KW  - software application deployement
KW  - Arrays
KW  - Data analysis
KW  - Engines
KW  - Indexing
KW  - Silicon
KW  - Event Indexing
KW  - Event Searching
KW  - Event Stream
VL  - 
JA  - Computer Software and Applications Conference (COMPSAC), 2012 IEEE 36th Annual
DO  - 10.1109/COMPSAC.2012.60
AB  - Deployed software applications use log files to keep a record of system events. Log analysis provides support for system administrators to gain the knowledge of system health and behavior. As a result, the ability to efficiently search for patterns in historical events has become a major requirement for timely analysis. Enterprise systems today produce high volumes of log data, regularly in the order of thousands of events per second, which requires to build inverted indexes for quick data retrieval. However, current inverted indexing techniques are rarely designed to handle high volumes of dynamic stream data and often resource consuming. We propose an efficient indexing solution, which reduces the necessary resources by employing bloom filter techniques. The solution builds a generic indexing engine for the Run Time Correlation Engine logging framework to achieve efficient monitoring in the Cloud. In particular, our solution is able to deliver significant performance improvement over existing indexing engines.
ER  - 

TY  - CONF
JO  - Machine Learning and Cybernetics, 2004. Proceedings of 2004 International Conference on
TI  - Services of prediction for visiting path based on improved matrix clustering
T2  - Machine Learning and Cybernetics, 2004. Proceedings of 2004 International Conference on
IS  - 
SN  - 
VO  - 3
SP  - 1723
EP  - 1726 vol.3
AU  - Yu-Qing Peng
AU  - Tie-Jun Li
AU  - Mei-Na Chen
AU  - Tao Lin
Y1  - 26-29 Aug. 2004
PY  - 2004
KW  - Web sites
KW  - data mining
KW  - matrix algebra
KW  - optimisation
KW  - pattern classification
KW  - pattern clustering
KW  - prediction theory
KW  - user interfaces
KW  - Web log file analysis
KW  - Web log mining algorithms
KW  - appliance phase
KW  - data preprocessing phase
KW  - graph theory
KW  - information services
KW  - optimized matrix clustering algorithm
KW  - pattern analysis
KW  - user classification
KW  - user visiting path prediction algorithm
KW  - Algorithm design and analysis
KW  - Clustering algorithms
KW  - Computer science
KW  - Counting circuits
KW  - Data mining
KW  - Electronic mail
KW  - Home appliances
KW  - Robotics and automation
KW  - Topology
KW  - Web mining
VL  - 3
JA  - Machine Learning and Cybernetics, 2004. Proceedings of 2004 International Conference on
DO  - 10.1109/ICMLC.2004.1382053
AB  - According to the analysis of Web log files, matrix clustering algorithm not only can be optimized, but also mining results can be applied to predicting users' visiting path and classify new users. The data-preprocessing phase has been discussed in detail. Then, Web log mining algorithms and pattern analysis and appliance phase are presented, including the optimized matrix clustering algorithm and the basic idea and realization of the prediction algorithm. Experiments show that the algorithm is effective and feasible.
ER  - 

TY  - CONF
JO  - eHealth, Telemedicine, and Social Medicine, 2009. eTELEMED '09. International Conference on
TI  - Evaluation of an Interactive Web-Based Application to Promote Healthy Behavior in Order to Maintain a Healthy Weight &#150; Preliminary Findings
T2  - eHealth, Telemedicine, and Social Medicine, 2009. eTELEMED '09. International Conference on
IS  - 
SN  - 
VO  - 
SP  - 275
EP  - 279
AU  - Kelders, S.M.
AU  - Kerkhof, S.
AU  - van Gemert-Pijnen, J.E.W.
AU  - Seydel, E.R.
AU  - Markus, F.
AU  - Werkman, A.
Y1  - 1-7 Feb. 2009
PY  - 2009
KW  - Internet
KW  - interactive systems
KW  - medical computing
KW  - psychology
KW  - Healthy Weight Assistant
KW  - Netherlands Nutrition Centre
KW  - healthy behavior change intervention
KW  - interactive Web-based application
KW  - log-file analysis
KW  - qualitative analysis
KW  - real-time usability-test
KW  - Computerized monitoring
KW  - Feedback
KW  - Instruments
KW  - Internet
KW  - Organizing
KW  - TV
KW  - Telemedicine
KW  - exercise
KW  - healthy living
KW  - interactive
KW  - internet
KW  - nutrition
KW  - prevention
VL  - 
JA  - eHealth, Telemedicine, and Social Medicine, 2009. eTELEMED '09. International Conference on
DO  - 10.1109/eTELEMED.2009.34
AB  - Web-based interactive applications may combine the interactive and tailored nature of successful behavior change interventions with the wide reach needed to target the general population. There is a lack of insight in the requirements for successful interactive Web-based applications in prevention. The objective of this research is to evaluate the Healthy Weight Assistant (HWA) of the Netherlands Nutrition Centre and give recommendations for optimization of this application. This study consists of questionnaires (n=703, follow-up n=431), real-time usability-tests, log-file analysis and qualitative analysis. From the preliminary results we see that improvement with maximum effect and minimal change of the HWA can be found in motivation to keep using the application and motivation to change behavior. This can be achieved by sending automatic (tailored) reminders, restructuring the second stage in the application (motivation and goal setting) and by adding a tab 'my goals' to the application.
ER  - 

TY  - CONF
JO  - Software Engineering, 2001. ICSE 2001. Proceedings of the 23rd International Conference on
TI  - Evaluating the reverse engineering capabilities of Web tools for understanding site content and structure: a case study
T2  - Software Engineering, 2001. ICSE 2001. Proceedings of the 23rd International Conference on
IS  - 
SN  - 0270-5257
VO  - 
SP  - 514
EP  - 523
AU  - Tilley, S.
AU  - Shihong Huang
Y1  - 12-19 May 2001
PY  - 2001
KW  - Internet
KW  - information resources
KW  - reverse engineering
KW  - Internet
KW  - REEF
KW  - Web server log file analysis
KW  - Web site content
KW  - Web site reverse engineering
KW  - Web tools
KW  - case study
KW  - data gathering
KW  - reverse engineering environment framework
KW  - Application software
KW  - Companies
KW  - Computer architecture
KW  - Computer science
KW  - Information analysis
KW  - Large-scale systems
KW  - Libraries
KW  - Reverse engineering
KW  - Web page design
KW  - Web server
VL  - 
JA  - Software Engineering, 2001. ICSE 2001. Proceedings of the 23rd International Conference on
DO  - 10.1109/ICSE.2001.919126
AB  - This paper describes an evaluation of the reverse engineering capabilities of three Web tools for understanding site content and structure. The evaluation is based on partitioning Web sites into three classes (static, interactive, and dynamic), and is structured using an existing reverse engineering environment framework (REEF). This case study also represents an initial evaluation of the applicability of the REEF in the related but qualitatively different domain of Web sites. The case study highlights several shortcomings of current Web tools in the context of aiding understanding to support evolution. For example, most Web tools are geared towards new page design and development, not to understanding detailed page content or overall site structure. The evaluation also identified some aspects of the REEF that might benefit from refinement to better reflect Web tool capabilities that support common evolution tasks. For example, Web server log file analysis as a specialized form of data gathering and subsequence information presentation.
ER  - 

TY  - CONF
JO  - Dynamic Analysis, 2007. WODA '07. Fifth International Workshop on
TI  - Fifth International Workshop on Dynamic Analysis - Cover
T2  - Dynamic Analysis, 2007. WODA '07. Fifth International Workshop on
IS  - 
SN  - 
VO  - 
SP  - c1
EP  - c1
Y1  - 20-26 May 2007
PY  - 2007
KW  - data analysis
KW  - data structures
KW  - class loading
KW  - constraint detection
KW  - data structure
KW  - dynamic analysis
KW  - log file analysis
VL  - 
JA  - Dynamic Analysis, 2007. WODA '07. Fifth International Workshop on
DO  - 10.1109/WODA.2007.4
AB  - The following topics are discussed: dynamic analysis; data structure; class loading; constraint detection; log file analysis.
ER  - 

TY  - CONF
JO  - Simulation Conference, 2001. Proceedings of the Winter
TI  - A practical bottleneck detection method
T2  - Simulation Conference, 2001. Proceedings of the Winter
IS  - 
SN  - 
VO  - 2
SP  - 949
EP  - 953 vol.2
AU  - Roser, C.
AU  - Nakano, M.
AU  - Tanaka, M.
Y1  - 2001
PY  - 2001
KW  - discrete event simulation
KW  - discrete event systems
KW  - average machine activity duration
KW  - bottleneck detection method
KW  - bottleneck machines
KW  - discrete event system
KW  - log file analysis
KW  - longest average uninterrupted active period
KW  - nonbottleneck machines
KW  - simulation tools
KW  - usability
KW  - Analytical models
KW  - Computer aided manufacturing
KW  - Discrete event systems
KW  - Event detection
KW  - Laboratories
KW  - Research and development
KW  - Throughput
KW  - Time measurement
KW  - Usability
KW  - Vehicles
VL  - 2
JA  - Simulation Conference, 2001. Proceedings of the Winter
DO  - 10.1109/WSC.2001.977398
AB  - This paper describes a novel method for detecting the bottleneck in a discrete event system by examining the average duration of a machine being active for all machines. The machine with the longest average uninterrupted active period is considered the bottleneck. The method is widely applicable and also capable of analyzing complex and sophisticated systems. The results are highly accurate, distinguishing between bottleneck machines and non-bottleneck machines with a high level of confidence. This approach is very easy to use and can be implemented into existing simulation tools with little effort, requiring only an analysis of the log file which is readily available by almost all simulation tools. This method satisfies not only academic requirements with respect to accuracy but also industry requirements with respect to usability
ER  - 

TY  - CONF
JO  - EAEEIE Annual Conference (EAEEIE), 2011 Proceedings of the 22nd
TI  - Analysis of students' behaviour in e-learning system
T2  - EAEEIE Annual Conference (EAEEIE), 2011 Proceedings of the 22nd
IS  - 
SN  - 
VO  - 
SP  - 1
EP  - 6
AU  - Capay, M.
AU  - Mesarosova, M.
AU  - Balogh, Z.
Y1  - 13-15 June 2011
PY  - 2011
KW  - computer aided instruction
KW  - health care
KW  - medical administrative data processing
KW  - E-learning in Community Care project
KW  - e-course
KW  - e-learning system
KW  - electronic course
KW  - electronic learning
KW  - log file analysis
KW  - nonanonymous questionnaire
KW  - role of a nurse in community care module
KW  - student behaviour analysis
KW  - usage analysis
KW  - Communities
KW  - Educational institutions
KW  - Electronic learning
KW  - Least squares approximation
KW  - Materials
KW  - Medical services
KW  - E-course
KW  - E-learning
KW  - Life Long Learning
KW  - Questionnaire
KW  - Usage Analysis
VL  - 
JA  - EAEEIE Annual Conference (EAEEIE), 2011 Proceedings of the 22nd
DO  - 
AB  - In certain stages of the educational process it is necessary to verify the quality of the study program that is realized via e-learning. The aim of the research presented in the paper was to get relevant feedback on the quality of one module of created study program for nurses, specifically the module &#x201C;Role of a nurse in community care&#x201D; which was one of the outcomes of an international project E-learning in Community Care supported by Leonardo da Vinci. We used two different research methods, non-anonymous questionnaire and usage analysis that is in fact a relatively hidden form of gaining the data about the users in e-course. Our research question should be formed as follows: Is there a significant difference between the answers of questionnaire investigation and real behaviour of the students in the e-course? This question was stated after the first differences between how students responded in the questionnaire and what the analysis of log file proved emerged. Comparing the answers and outcomes of usage analysis we are able to find out the real process of the participants' study and eventually interpret the differences.
ER  - 

TY  - JOUR
JO  - IBM Journal of Research and Development
TI  - Analytics for resiliency in the mainframe
T2  - IBM Journal of Research and Development
IS  - 5
SN  - 0018-8646
VO  - 57
SP  - 8:1
EP  - 8:5
AU  - Ein-Dor, L.
AU  - Goldschmidt, Y.
AU  - Lavi, O.
AU  - Miller, G.E.
AU  - Ninio, M.
AU  - Dillenberger, D.
Y1  - Sept.-Oct. 2013
PY  - 2013
KW  - Algorithm design and analysis
KW  - Clustering algorithms
KW  - Data mining
KW  - Mainframes
KW  - Monitoring
KW  - Performance evaluation
VL  - 57
JA  - IBM Journal of Research and Development
DO  - 10.1147/JRD.2013.2263891
AB  - The IBM System z&#x00AE; mainframe computer is a direct descendant of the IBM System/360 family of computing systems initially made available in 1965. With each release of an IBM mainframe, its developers include new reliability, availability, and serviceability (RAS) functions. This paper describes recent technology added to the System z and z/OS&#x00AE; operating system to enhance the ability to provide high levels of availability. Specifically, we discuss the use of machine-learning algorithms to analyze log messages. We also describe combining temporal and textual information for system log-file analysis. Finally, we discuss the results of this analysis and the assistance it provides to mainframe users.
ER  - 

TY  - CONF
JO  - Advanced Language Processing and Web Information Technology, 2007. ALPIT 2007. Sixth International Conference on
TI  - Design of a Web System Using Context-Awareness
T2  - Advanced Language Processing and Web Information Technology, 2007. ALPIT 2007. Sixth International Conference on
IS  - 
SN  - 
VO  - 
SP  - 520
EP  - 525
AU  - Yun, Jong-Chan
AU  - Youn, Sung-Dae
Y1  - 22-24 Aug. 2007
PY  - 2007
KW  - Association rules
KW  - Computer science
KW  - Electronic commerce
KW  - Information analysis
KW  - Information retrieval
KW  - Information technology
KW  - Internet
KW  - Merchandise
KW  - Pattern analysis
KW  - Web pages
KW  - Association RuleData MiningContext
VL  - 
JA  - Advanced Language Processing and Web Information Technology, 2007. ALPIT 2007. Sixth International Conference on
DO  - 10.1109/ALPIT.2007.74
AB  - Currently, user requirements of the World Wide Web are diverse, growing more so with each passing year. Furthermore, each web user wants to retrieve data and/or merchandise more conveniently and quickly. As a result of different search criteria and dispositions of web users, data retrieval leads to unnecessarily repetitious operations in the use of the environment implemented by web designers. In this paper, we will suggest a system that analyzes user patterns on the Web using the technique of log file analysis and then transfers the web site information of users effectively. We analyzed the log files for customer data using EC-Miner, one of the tools used in data mining, and offered an appropriate layout, corresponding with personalization by giving a weight to each transport path.
ER  - 

TY  - CONF
JO  - System Sciences, 2006. HICSS '06. Proceedings of the 39th Annual Hawaii International Conference on
TI  - Barriers to Information Access across Languages on the Internet: Network and Language Effects
T2  - System Sciences, 2006. HICSS '06. Proceedings of the 39th Annual Hawaii International Conference on
IS  - 
SN  - 1530-1605
VO  - 3
SP  - 54b
EP  - 54b
AU  - Kralisch, A.
AU  - Mandl, T.
Y1  - 04-07 Jan. 2006
PY  - 2006
KW  - Costs
KW  - Crawlers
KW  - Data analysis
KW  - IP networks
KW  - Information analysis
KW  - Information retrieval
KW  - Internet
KW  - Investments
KW  - LAN interconnection
KW  - Natural languages
VL  - 3
JA  - System Sciences, 2006. HICSS '06. Proceedings of the 39th Annual Hawaii International Conference on
DO  - 10.1109/HICSS.2006.71
AB  - This paper investigates the role of language in accessing information on the Internet. We combined data about website visitors through log-file analysis with data about web-hosts and links obtained from a crawler. Results suggest that language may represent a double barrier: first, the number of native speakers determines the number of web-hosts, and hence the amount of information and the interconnectedness of information sources. Second, to access information on a particular website the languages offered are an even more important factor than network effects: non-native speakers and links from websites in other languages are always underrepresented. Our results are in line with the Information Foraging Theory, the Revised Hierarchy Model, network and market theories, and emphasize the role of language on the Internet. Insight into these processes is helpful when website translation represents important investment decisions, or when aiming to diminish the digital divide.
ER  - 

TY  - CONF
JO  - Professional Communication Conference, 2002. IPCC 2002. Proceedings. IEEE International
TI  - Aligning inner and outer visions of technical communication: reflections beyond traditional technical writing
T2  - Professional Communication Conference, 2002. IPCC 2002. Proceedings. IEEE International
IS  - 
SN  - 
VO  - 
SP  - 145
EP  - 156
AU  - Haselkorn, M.P.
AU  - Sauer, G.
AU  - Turns, J.
Y1  - 2002
PY  - 2002
KW  - technical presentation
KW  - academic assumptions
KW  - case studies
KW  - computer mediated communication
KW  - content analysis
KW  - content management
KW  - contextual inquiry
KW  - corporate domain
KW  - editing
KW  - file analysis
KW  - medicine
KW  - production
KW  - public service
KW  - research activities
KW  - strategic management
KW  - technical communication
KW  - user driven content
KW  - writing
KW  - Arthritis
KW  - Content management
KW  - Information management
KW  - Production
KW  - Professional communication
KW  - Project management
KW  - Reflection
KW  - Research and development
KW  - Software libraries
KW  - Writing
VL  - 
JA  - Professional Communication Conference, 2002. IPCC 2002. Proceedings. IEEE International
DO  - 10.1109/IPCC.2002.1049099
AB  - Technical communication is often misunderstood by those outside the profession or the academic field. These outside perceptions of our work, generally based on extremely limited and narrow notions of the field, can influence the opportunities available to technical communicators. In this paper, three faculty members from the University of Washington's Department of Technical Communication describe their academic assumptions and research activities that range far beyond traditional areas from technical writing such as writing, editing and production. They describe projects that represent the expanding boundaries of the field of technical communication, spanning domains (including medicine, corporate, and public service), methods (including contextual inquiry, content analysis, case studies, and log file analysis), and solution types (including content management, user driven content, computer mediated communication, and strategic management of systems). What these projects share is abroad vision of the field of technical communication and a broad vision of the contributions that technical communication professionals have to offer.
ER  - 

TY  - CONF
JO  - Computer and Information Sciences, 2009. ISCIS 2009. 24th International Symposium on
TI  - Comparison of balancing techniques for multimedia IR over imbalanced datasets
T2  - Computer and Information Sciences, 2009. ISCIS 2009. 24th International Symposium on
IS  - 
SN  - 
VO  - 
SP  - 674
EP  - 679
AU  - Bermejo, P.
AU  - Hopfgartner, F.
AU  - Gamez, J.A.
AU  - Callejon, J.M.P.
AU  - Jose, J.M.
Y1  - 14-16 Sept. 2009
PY  - 2009
KW  - information retrieval
KW  - multimedia systems
KW  - pattern classification
KW  - balancing techniques
KW  - data classification
KW  - inference relevance
KW  - multimedia information retrieval
KW  - supervised classification problem
KW  - text classification
KW  - Algorithm design and analysis
KW  - Bridges
KW  - Classification algorithms
KW  - Content based retrieval
KW  - Feedback
KW  - Image retrieval
KW  - Information retrieval
KW  - Large-scale systems
KW  - Music information retrieval
KW  - Video sharing
VL  - 
JA  - Computer and Information Sciences, 2009. ISCIS 2009. 24th International Symposium on
DO  - 10.1109/ISCIS.2009.5291904
AB  - A promising method to improve the performance of information retrieval systems is to approach retrieval tasks as a supervised classification problem. Previous user interactions, e.g. gathered from a thorough log file analysis, can be used to train classifiers which aim to inference relevance of retrieved documents based on user interactions. A problem in this approach is, however, the large imbalance ratio between relevant and non-relevant documents in the collection. In standard test collection as used in academic evaluation frameworks such as TREC, non-relevant documents outnumber relevant documents by far. In this work, we address this imbalance problem in the multimedia domain. We focus on the logs of two multimedia user studies which are highly imbalanced. We compare a naiinodotve solution of randomly deleting documents belonging to the majority class with various balancing algorithms coming from different fields: data classification and text classification. Our experiments indicate that all algorithms improve the classification performance of just deleting at random from the dominant class.
ER  - 


