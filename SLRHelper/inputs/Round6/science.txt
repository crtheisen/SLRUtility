TY  - JOUR
T1  - Towards an Analysis of Data Accountability and Auditing for Secure Cloud Data Storage
JO  - Procedia Computer Science
VL  - 50
IS  - 
SP  - 543
EP  - 550
PY  - 2015///
T2  - Big Data, Cloud and Computing Challenges
AU  - Prassanna, J.
AU  - Punitha, K.
AU  - Neelanarayanan, V.
SN  - 1877-0509
DO  - http://dx.doi.org/10.1016/j.procs.2015.04.101
UR  - http://www.sciencedirect.com/science/article/pii/S187705091500602X
AB  - Abstract
Cloud technology, runs off-premise, provides an extensive information technology support and service that can offer as an on-premise version if on demanded as a virtualized solution for customers’ technological requirements. Cloud computing put together the comprehensive solutions by integrating the different technology that provision Software, Platform and Infrastructure as services. Cloud, an integrated technology that delivers a complete, open and flexible solution, leads to a critical concern over its trust. The extensive use of virtualization in cloud especially in data storage capability put forth many security concerns to the customer. It is the time to prove the customer about the strength of the cloud security through enabling their data accountability and auditing mechanism. In this work we going to provide a systematic analysis of the data accountability mechanism and auditing approaches existing now in cloud. To strengthen the customers trust over the cloud technology and service, we provided a clear enhanced view on the cloud data security and also analyze the secured way of utilizing the advantages of cloud computing.
ER  - 

TY  - JOUR
T1  - A multi-resolution accountable logging and its applications
JO  - Computer Networks
VL  - 
IS  - 
SP  - 
EP  - 
PY  - 
T2  - 
AU  - Fu, Bo
AU  - Xiao, Yang
SN  - 1389-1286
DO  - http://dx.doi.org/10.1016/j.comnet.2015.06.011
UR  - http://www.sciencedirect.com/science/article/pii/S1389128615002042
KW  - Flow-net
KW  - Accountability
KW  - Multi-resolution
KW  - Computer networks
AB  - Abstract
Today's computer and network systems were not originally designed for accountability which plays a crucial role in information assurance systems. To assure accountability, each entity in the system should be held responsible for its own behaviors so that the entity is part of larger chains of the system's accountability. To achieve accountable logging, a flow-net methodology was proposed in our previous work. Moreover, the multi-layer feature of computer and network systems brings us the chance to achieve multiple degrees of accountability, which means that we are able to acknowledge the system's behaviors at different levels of accountability. To achieve multiple degrees of accountability, we propose designs of a multi-resolution flow-net scheme in this paper and find out the optimal design under different assumptions. Furthermore, we apply the multi-resolution flow-net on TCP/IP networks that are designed and organized with multiple layers. Finally, evaluation results are presented to verify the better performance provided by multi-resolution flow-net than other schemes.
ER  - 

TY  - JOUR
T1  - Modular design, application architecture, and usage of a self-service model for enterprise data delivery: The Duke Enterprise Data Unified Content Explorer (DEDUCE)
JO  - Journal of Biomedical Informatics
VL  - 52
IS  - 
SP  - 231
EP  - 242
PY  - 2014/12//
T2  - Special Section: Methods in Clinical Research Informatics
AU  - Horvath, Monica M.
AU  - Rusincovitch, Shelley A.
AU  - Brinson, Stephanie
AU  - Shang, Howard C.
AU  - Evans, Steve
AU  - Ferranti, Jeffrey M.
SN  - 1532-0464
DO  - http://dx.doi.org/10.1016/j.jbi.2014.07.006
UR  - http://www.sciencedirect.com/science/article/pii/S1532046414001531
KW  - Cohort definition
KW  - Research query tool
KW  - Medical informatics applications
KW  - Information systems
KW  - Application development
KW  - System design and architecture
AB  - AbstractPurpose
Data generated in the care of patients are widely used to support clinical research and quality improvement, which has hastened the development of self-service query tools. User interface design for such tools, execution of query activity, and underlying application architecture have not been widely reported, and existing tools reflect a wide heterogeneity of methods and technical frameworks. We describe the design, application architecture, and use of a self-service model for enterprise data delivery within Duke Medicine.
Methods
Our query platform, the Duke Enterprise Data Unified Content Explorer (DEDUCE), supports enhanced data exploration, cohort identification, and data extraction from our enterprise data warehouse (EDW) using a series of modular environments that interact with a central keystone module, Cohort Manager (CM). A data-driven application architecture is implemented through three components: an application data dictionary, the concept of “smart dimensions”, and dynamically-generated user interfaces.
Results
DEDUCE CM allows flexible hierarchies of EDW queries within a grid-like workspace. A cohort “join” functionality allows switching between filters based on criteria occurring within or across patient encounters. To date, 674 users have been trained and activated in DEDUCE, and logon activity shows a steady increase, with variability between months. A comparison of filter conditions and export criteria shows that these activities have different patterns of usage across subject areas.
Conclusions
Organizations with sophisticated EDWs may find that users benefit from development of advanced query functionality, complimentary to the user interfaces and infrastructure used in other well-published models. Driven by its EDW context, the DEDUCE application architecture was also designed to be responsive to source data and to allow modification through alterations in metadata rather than programming, allowing an agile response to source system changes.
ER  - 

TY  - JOUR
T1  - Audit-Based Access Control for Electronic Health Records
JO  - Electronic Notes in Theoretical Computer Science
VL  - 168
IS  - 
SP  - 221
EP  - 236
PY  - 2007/2/8/
T2  - Proceedings of the Second International Workshop on Views on Designing Complex Architectures (VODCA 2006)
AU  - Dekker, M.A.C.
AU  - Etalle, S.
SN  - 1571-0661
DO  - http://dx.doi.org/10.1016/j.entcs.2006.08.028
UR  - http://www.sciencedirect.com/science/article/pii/S1571066107000382
KW  - distributed access control
KW  - audit
KW  - accountability
KW  - Electronic Health Record (EHR) systems
AB  - Traditional access control mechanisms aim to prevent illegal actions a-priori occurrence, i.e. before granting a request for a document. There are scenarios however where the security decision can not be made on the fly. For these settings we developed a language and a framework for a-posteriori access control. I this paper we show how the framework can be used in a practical scenario. In particular, we work out the example of an Electronic Health Record (EHR) system, we outline the full architecture needed for audit-based access control and we discuss the requirements and limitations of this approach concerning the underlying infrastructure and its users.
ER  - 

TY  - JOUR
T1  - Medical image security in a HIPAA mandated PACS environment
JO  - Computerized Medical Imaging and Graphics
VL  - 27
IS  - 2–3
SP  - 185
EP  - 196
PY  - 2003/3//
Y2  - 2003/6//
T2  - Picture Archiving and Communication Systems 20 Years Later
AU  - Cao, F
AU  - Huang, H.K
AU  - Zhou, X.Q
SN  - 0895-6111
DO  - http://dx.doi.org/10.1016/S0895-6111(02)00073-3
UR  - http://www.sciencedirect.com/science/article/pii/S0895611102000733
KW  - Data encryption
KW  - Picture archiving and communication system security
KW  - Image integrity
KW  - Digital imaging and communication in medicine compliance
KW  - Health insurance portability and accountability act
AB  - Medical image security is an important issue when digital images and their pertinent patient information are transmitted across public networks. Mandates for ensuring health data security have been issued by the federal government such as Health Insurance Portability and Accountability Act (HIPAA), where healthcare institutions are obliged to take appropriate measures to ensure that patient information is only provided to people who have a professional need. Guidelines, such as digital imaging and communication in medicine (DICOM) standards that deal with security issues, continue to be published by organizing bodies in healthcare. However, there are many differences in implementation especially for an integrated system like picture archiving and communication system (PACS), and the infrastructure to deploy these security standards is often lacking. Over the past 6 years, members in the Image Processing and Informatics Laboratory, Childrens Hospital, Los Angeles/University of Southern California, have actively researched image security issues related to PACS and teleradiology. The paper summarizes our previous work and presents an approach to further research on the digital envelope (DE) concept that provides image integrity and security assurance in addition to conventional network security protection. The DE, including the digital signature (DS) of the image as well as encrypted patient information from the DICOM image header, can be embedded in the background area of the image as an invisible permanent watermark. The paper outlines the systematic development, evaluation and deployment of the DE method in a PACS environment. We have also proposed a dedicated PACS security server that will act as an image authority to check and certify the image origin and integrity upon request by a user, and meanwhile act also as a secure DICOM gateway to the outside connections and a PACS operation monitor for HIPAA supporting information.
ER  - 

TY  - JOUR
T1  - Enhancing Security in Cloud Using Trusted Monitoring Framework
JO  - Procedia Computer Science
VL  - 48
IS  - 
SP  - 198
EP  - 203
PY  - 2015///
T2  - International Conference on Computer, Communication and Convergence (ICCC 2015)
AU  - Fera, M. Arun
AU  - manikandaprabhu, C.
AU  - Natarajan, Ilakiya
AU  - Brinda, K.
AU  - Darathiprincy, R.
SN  - 1877-0509
DO  - http://dx.doi.org/10.1016/j.procs.2015.04.170
UR  - http://www.sciencedirect.com/science/article/pii/S1877050915006791
AB  - Abstract
Cloud computing is a technology that provides network based services on demand. Cloud computing technology provides advantages to end users and business organizations. Few notable advantages are cost efficiency, increased storage capacity, backup and recovery, continuous resource availability and location independence. Data owners host their private data in the cloud and worry about unauthorized access of their data. They feel uncomfortable about any user misusing their private data. This insecure feeling of data owners holds them back from using cloud services. Any unauthorized users accessing the owner's private data leads to accountability issues. We design a trusted monitoring framework, which provides a chain of trust that excludes the untrusted privileged domain, as well as utilizing the trusted computing technology to ensure the integrity of the monitoring environment. To solve the accountability issue, a mechanism to monitor the actual data usage is proposed. This approach grants access rights to users based on their role and also monitors every access to the owner's data, verifying that the service level agreements have been violated or not.
ER  - 

TY  - JOUR
T1  - End-to-end policy based encryption techniques for multi-party data management
JO  - Computer Standards & Interfaces
VL  - 36
IS  - 4
SP  - 689
EP  - 703
PY  - 2014/6//
T2  - Security in Information Systems: Advances and new Challenges.
AU  - Beiter, Michael
AU  - Casassa Mont, Marco
AU  - Chen, Liqun
AU  - Pearson, Siani
SN  - 0920-5489
DO  - http://dx.doi.org/10.1016/j.csi.2013.12.004
UR  - http://www.sciencedirect.com/science/article/pii/S0920548913001785
KW  - Cloud
KW  - Sticky policy
KW  - Policy enforcement
KW  - Privacy
KW  - Secret sharing
AB  - Abstract
We describe a data management solution and associated key management approaches to provide accountability within service provision networks, in particular addressing privacy issues in cloud computing applications. Our solution involves machine readable policies that stick to data to define allowed usage and obligations as data travels across multiple parties. Service providers have fine-grained access to specific data based on agreed policies, enforced by interactions with independent third parties that check for policy compliance before releasing decryption keys required for data access. We describe alternative solutions based upon Public Key Infrastructure (PKI), Identity Based Encryption (IBE) and advanced secret sharing schemes.
ER  - 

TY  - JOUR
T1  - Securing medical networks
JO  - Network Security
VL  - 2007
IS  - 6
SP  - 13
EP  - 16
PY  - 2007/6//
T2  - 
AU  - Dantu, Ram
AU  - Oosterwijk, Herman
AU  - Kolan, Prakash
AU  - Husna, Husain
SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/S1353-4858(07)70055-7
UR  - http://www.sciencedirect.com/science/article/pii/S1353485807700557
AB  - The Health Information Portability and Accountability Act of 1996 (HIPAA) imposes strict regulations on healthcare institutions and commercial vendors to indemnify clinical data against unscrupulous users. Security vulnerabilities concerning hospital information systems not only negatively impact patient healthcare, but may also represent a potential federal violation. For a comprehensive understanding of the security of a radiology communication network, a detailed survey of the Picture Archiving and Communication Systems (PACS) was compiled. In this paper, we present survey results and a set of recommendations for implementing PACS security.
ER  - 

TY  - JOUR
T1  - HIPAA compliant auditing system for medical images
JO  - Computerized Medical Imaging and Graphics
VL  - 29
IS  - 2–3
SP  - 235
EP  - 241
PY  - 2005/3//
Y2  - 2005/4//
T2  - Imaging Informatics
AU  - Zhou, Zheng
AU  - Liu, Brent J.
SN  - 0895-6111
DO  - http://dx.doi.org/10.1016/j.compmedimag.2004.09.009
UR  - http://www.sciencedirect.com/science/article/pii/S0895611104001223
KW  - HIPAA
KW  - Security
KW  - HIPAA compliant auditing system
KW  - Auditing
KW  - monitoring
AB  - As an official regulation for healthcare privacy and security, Health Insurance Portability and Accountability Act (HIPAA) mandates health institutions to protect health information against unauthorized use or disclosure. One such method proposed by HIPAA Security Standards is audit trail, which records and examines health information access activities. HIPAA mandates healthcare providers to have the ability to generate audit trails on data access activities for any specific patient. Although current medical imaging systems generate activity logs, there is a lack of formal methodology to interpret these large volumes of log data and generate HIPAA compliant auditing trails.

This paper outlines the design of a HIPAA compliant auditing system (HCAS) for medical images in imaging systems such as PACS and discusses the development of a security monitoring (SM) toolkit based on some of the partial components in HCAS.
ER  - 

TY  - JOUR
T1  - Security and privacy in electronic health records: A systematic literature review
JO  - Journal of Biomedical Informatics
VL  - 46
IS  - 3
SP  - 541
EP  - 562
PY  - 2013/6//
T2  - 
AU  - Fernández-Alemán, José Luis
AU  - Señor, Inmaculada Carrión
AU  - Lozoya, Pedro Ángel Oliver
AU  - Toval, Ambrosio
SN  - 1532-0464
DO  - http://dx.doi.org/10.1016/j.jbi.2012.12.003
UR  - http://www.sciencedirect.com/science/article/pii/S1532046412001864
KW  - Electronic health records
KW  - Systematic review
KW  - Privacy
KW  - Confidentiality
KW  - Security
KW  - Standards
AB  - Objective
To report the results of a systematic literature review concerning the security and privacy of electronic health record (EHR) systems.
Data sources
Original articles written in English found in MEDLINE, ACM Digital Library, Wiley InterScience, IEEE Digital Library, Science@Direct, MetaPress, ERIC, CINAHL and Trip Database.
Study selection
Only those articles dealing with the security and privacy of EHR systems.
Data extraction
The extraction of 775 articles using a predefined search string, the outcome of which was reviewed by three authors and checked by a fourth.
Results
A total of 49 articles were selected, of which 26 used standards or regulations related to the privacy and security of EHR data. The most widely used regulations are the Health Insurance Portability and Accountability Act (HIPAA) and the European Data Protection Directive 95/46/EC. We found 23 articles that used symmetric key and/or asymmetric key schemes and 13 articles that employed the pseudo anonymity technique in EHR systems. A total of 11 articles propose the use of a digital signature scheme based on PKI (Public Key Infrastructure) and 13 articles propose a login/password (seven of them combined with a digital certificate or PIN) for authentication. The preferred access control model appears to be Role-Based Access Control (RBAC), since it is used in 27 studies. Ten of these studies discuss who should define the EHR systems’ roles. Eleven studies discuss who should provide access to EHR data: patients or health entities. Sixteen of the articles reviewed indicate that it is necessary to override defined access policies in the case of an emergency. In 25 articles an audit-log of the system is produced. Only four studies mention that system users and/or health staff should be trained in security and privacy.
Conclusions
Recent years have witnessed the design of standards and the promulgation of directives concerning security and privacy in EHR systems. However, more work should be done to adopt these regulations and to deploy secure EHR systems.
ER  - 

TY  - JOUR
T1  - Secure Audit Log Management
JO  - Procedia Computer Science
VL  - 22
IS  - 
SP  - 1249
EP  - 1258
PY  - 2013///
T2  - 17th International Conference in Knowledge Based and Intelligent Information and Engineering Systems - KES2013
AU  - Söderström, Olof
AU  - Moradian, Esmiralda
SN  - 1877-0509
DO  - http://dx.doi.org/10.1016/j.procs.2013.09.212
UR  - http://www.sciencedirect.com/science/article/pii/S1877050913010053
KW  - Secure Log Management
KW  - Log Analysis
KW  - Log Server
KW  - Audit Log Event
AB  - Abstract
Log management and analysis is a vital part of organization's network management and system administration. Logs indicate current status of the system and contain information that refers to different security events, which occur within the system. Logs are used for different purposes, such as recording user activities, track authentication attempts, and other security events. Due to increasing number of threats against networks and systems, the number of security logs increases. However, many organizations that work in a distributed environment face following problems: log generation and storage, log protection, and log analysis. Moreover, ensuring that security, system and network administrators analyze log data in an effective way is another issue. In this research, we propose an approach for receiving, storing and administrating audit log events. Furthermore, we present a solution design that in a secure way allows organizations in distributed environments to send audit log transactions from different local networks to one centralized server.
ER  - 

TY  - JOUR
T1  - Logs may be found boring, but they are good: NIST
JO  - Computer Fraud & Security
VL  - 2006
IS  - 5
SP  - 2
EP  - 3
PY  - 2006/5//
T2  - 

SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(06)70351-7
UR  - http://www.sciencedirect.com/science/article/pii/S1361372306703517
AB  - A US standards body has said that the benefits of logs are being thrown away because system administrators find it “boring.” It said that the analysis of logs is often “treated as a low-priority task” by administrators because more urgent tasks like fixing vulnerabilities come first.
ER  - 

TY  - JOUR
T1  - German police issue new badges
JO  - Card Technology Today
VL  - 17
IS  - 10
SP  - 16
EP  - 
PY  - 2005/10//
T2  - 

SN  - 0965-2590
DO  - http://dx.doi.org/10.1016/S0965-2590(05)70391-1
UR  - http://www.sciencedirect.com/science/article/pii/S0965259005703911
AB  - Berlin Police Department has taken delivery of an advanced badge solution from French smart card manufacturer Axalto to secure physical and logical access to its computers and facilities. Delivered in collaboration with local provider PPC Card Systems, the smart ID cards will replace the conventional printed, plastic, photo ID pages currently used by the department.
ER  - 

TY  - JOUR
T1  - Privacy Threat Modeling for Emerging BiobankClouds
JO  - Procedia Computer Science
VL  - 37
IS  - 
SP  - 489
EP  - 496
PY  - 2014///
T2  - The 5th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN-2014)/ The 4th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH 2014)/ Affiliated Workshops
AU  - Gholami, Ali
AU  - Lind, Anna-Sara
AU  - Reichel, Jane
AU  - Litton, Jan-Eric
AU  - Edlund, Ake
AU  - Laure, Erwin
SN  - 1877-0509
DO  - http://dx.doi.org/10.1016/j.procs.2014.08.073
UR  - http://www.sciencedirect.com/science/article/pii/S1877050914010382
KW  - privacy-preservation
KW  - data security
KW  - cloud computing
KW  - threat modeling
KW  - requirement analysis
AB  - Abstract
There is an increased amount of data produced by next generation sequencing (NGS) machines which demand scalable storage and analysis of genomic data. In order to cope with this huge amount of information, many biobanks are interested in cloud computing capabilities such as on-demand elasticity of computing power and storage capacity. There are several security and privacy requirements mandated by personal data protection legislation which hinder biobanks from migrating big data generated by the NGS machines. This paper describes the privacy requirements of platform-as-service BiobankClouds according to the European Data Protection Directive (DPD). It identifies several key privacy threats which leave BiobankClouds vulnerable to an attack. This study benefits health-care application designers in the requirement elicitation cycle when building privacy-preserving BiobankCloud platforms.
ER  - 

TY  - JOUR
T1  - Making sense of log management for security purposes – an approach to best practice log collection, analysis and management
JO  - Computer Fraud & Security
VL  - 2007
IS  - 5
SP  - 5
EP  - 10
PY  - 2007/5//
T2  - 
AU  - Gorge, Mathieu
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(07)70047-7
UR  - http://www.sciencedirect.com/science/article/pii/S1361372307700477
AB  - Every computer action registers a log somewhere – giving a rich source of data that can help businesses identify any trace of corruption within their networks. Log collection is also a strong component of keeping in line with legislation such as Sarbanes-Oxley, HIPAA, GLBA in the US, and the European Data Protection Directive in the EU. Mathieu Gorge looks at what logs organizations need to keep and what standards require their storage.

He recommends proactive monitoring of firewalls, anti-virus, VPNs and IDS logs among other security systems. The main goal is to link a transaction back to an individual user in order to perform a forensic investigation. But it is important to be wary, as some countries do not allow companies to monitor staff usage of IT systems. See page 3 on how the European court ruled that a British college's monitoring of one employee was a breach of human rights. Therefore, linking a log with a person's actions may not stand up in court.

Gorge says logs can give as good an insight into external attacks as well as internally driven ones.

Logs should be analyzed for the following:
				•
User account activity: creation, elevation of privilege, changes, inactivity.
•
Client requests and server response.
•
Operational status: shutdown (planned or unplanned), system failure and automatic restart.
•
Usage information and trends – basic user behaviour analysis.


It is best practice to collect, store and analyze logs with a view to being able to get complete, accurate and verifiable information. This will improve the organization's ability to comply with key standards and legislation as regards e-evidence. It could save an organization from potential liability and repair costs and will give visibility over mission critical and security systems, performance and usage. The main advice is to remain proactive so as to be able to respond to a security incident and comply with legal requests should anything happen.

Mathieu Gorge looks at what logs can do for your business and how governance demands them.
ER  - 

TY  - JOUR
T1  - Insider Threat Detection Using Log Analysis and Event Correlation
JO  - Procedia Computer Science
VL  - 45
IS  - 
SP  - 436
EP  - 445
PY  - 2015///
T2  - International Conference on Advanced Computing Technologies and Applications (ICACTA)
AU  - Ambre, Amruta
AU  - Shekokar, Narendra
SN  - 1877-0509
DO  - http://dx.doi.org/10.1016/j.procs.2015.03.175
UR  - http://www.sciencedirect.com/science/article/pii/S1877050915004184
KW  - Security
KW  - log analysis
KW  - event correlation
KW  - Bayesian detection rate
KW  - false alarm rate
AB  - Abstract
Insider threat is one of the most dangerous security threat, and a much more complex issue. These insiders can be a former or a disgruntled employee or any business associate that has or had an authorised access to information for any particular organization. They have control and security measures. Hence continuous monitoring is essential to track each and every activity within the network. Log management is a strong technique which includes both Log analysis with event correlation which provides the root cause of any attack and network can be protected from security violations. Though intrusion detection is complex process, while checking the ability to detect intrusive behaviour within the internal environment, it has to take care of suppressing the false alarm rate. Some strong approach is required on the basis of which decisions can be taken fast. This paper proposes a probabilistic approach which illustrates the frequency of occurrence of event in percentage while still considering the false alarm rate at an acceptable level.
ER  - 

TY  - JOUR
T1  - Secured Temporal Log Management Techniques for Cloud
JO  - Procedia Computer Science
VL  - 46
IS  - 
SP  - 589
EP  - 595
PY  - 2015///
T2  - Proceedings of the International Conference on Information and Communication Technologies, ICICT 2014, 3-5 December 2014 at Bolgatty Palace &amp; Island Resort, Kochi, India
AU  - Muthurajkumar, S.
AU  - Ganapathy, S.
AU  - Vijayalakshmi, M.
AU  - Kannan, A.
SN  - 1877-0509
DO  - http://dx.doi.org/10.1016/j.procs.2015.02.098
UR  - http://www.sciencedirect.com/science/article/pii/S1877050915001623
KW  - Cloud computing
KW  - Log management
KW  - Cloud Storage
KW  - Cloud Security;
AB  - Abstract
Log Management has been an important service in Cloud Computing. In any business, maintaining the log records securely over a particular period of time is absolutely necessary for various reasons such as auditing, forensic analysis, evidence etc. In this work, Integrity and confidentiality of the log records are maintained at every stage of Log Management namely the Log Generation phase, Transmission phase and Storage phase. In addition to this, Log records may often contain sensitive information about the organization which should not be leaked to the outside world. In this paper, Temporal Secured Cloud Log Management Algorithm techniques are implemented to provide security to maintain transaction history in cloud within time period. In this work, security to temporal log management is provided by encrypting the log data before they are stored in the cloud storage. They are also stored in batches for easy retrieval. This work was implemented in Java programming language in the Google drive environment.
ER  - 

TY  - JOUR
T1  - Secure log management for privacy assurance in electronic communications
JO  - Computers & Security
VL  - 27
IS  - 7–8
SP  - 298
EP  - 308
PY  - 2008/12//
T2  - 
AU  - Stathopoulos, Vassilios
AU  - Kotzanikolaou, Panayiotis
AU  - Magkos, Emmanouil
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/j.cose.2008.07.010
UR  - http://www.sciencedirect.com/science/article/pii/S0167404808000400
KW  - System logging
KW  - Network providers
KW  - Internal attacks
KW  - Integrity
KW  - Digital signatures
AB  - In this paper we examine logging security in the environment of electronic communication providers. We review existing security threat models for system logging and we extend these to a new security model especially suited for communication network providers, which also considers internal modification attacks. We also propose a framework for secure log management in public communication networks as well as an implementation design, in order to provide traceability under the extended security model. A key role to the proposed framework is given to an independent Regulatory Authority, which is responsible to maintain log integrity proofs in a remote environment and verify the integrity of the provider's log files during security audits.
ER  - 

TY  - JOUR
T1  - Who's in control: a six-step strategy for secure IT
JO  - Network Security
VL  - 2011
IS  - 11
SP  - 18
EP  - 20
PY  - 2011/11//
T2  - 
AU  - Facey, Stuart
SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/S1353-4858(11)70121-0
UR  - http://www.sciencedirect.com/science/article/pii/S1353485811701210
AB  - As more and more organisations encourage flexible and remote working policies that allow employees to work outside the office, the complexity surrounding remote access and support mechanisms for the IT helpdesk has also increased. There is a growing and unregulated market for solutions that can ‘fix’ IT issues quickly and efficiently no matter where workers are located: however, as with many solutions, remote support and access products have their own inherent security risks that should not be underestimated.
ER  - 

TY  - JOUR
T1  - A survey on Advanced Metering Infrastructure
JO  - International Journal of Electrical Power & Energy Systems
VL  - 63
IS  - 
SP  - 473
EP  - 484
PY  - 2014/12//
T2  - 
AU  - Rashed Mohassel, Ramyar
AU  - Fung, Alan
AU  - Mohammadi, Farah
AU  - Raahemifar, Kaamran
SN  - 0142-0615
DO  - http://dx.doi.org/10.1016/j.ijepes.2014.06.025
UR  - http://www.sciencedirect.com/science/article/pii/S0142061514003743
KW  - Advanced Metering Infrastructure
KW  - Smart metering
KW  - Smart Grid
AB  - Abstract
This survey paper is an excerpt of a more comprehensive study on Smart Grid (SG) and the role of Advanced Metering Infrastructure (AMI) in SG. The survey was carried out as part of a feasibility study for creation of a Net-Zero community in a city in Ontario, Canada. SG is not a single technology; rather it is a combination of different areas of engineering, communication and management. This paper introduces AMI technology and its current status, as the foundation of SG, which is responsible for collecting all the data and information from loads and consumers. AMI is also responsible for implementing control signals and commands to perform necessary control actions as well as Demand Side Management (DSM). In this paper we introduce SG and its features, establish the relation between SG and AMI, explain the three main subsystems of AMI and discuss related security issues.
ER  - 

TY  - JOUR
T1  - Making the internet safe for e-commerce
JO  - Computers & Security
VL  - 15
IS  - 7
SP  - 587
EP  - 
PY  - 1996///
T2  - 
AU  - Meyer, Helen
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/S0167-4048(97)88121-7
UR  - http://www.sciencedirect.com/science/article/pii/S0167404897881217
ER  - 

TY  - JOUR
T1  - Making internet access safe
JO  - Computers & Security
VL  - 15
IS  - 7
SP  - 587
EP  - 
PY  - 1996///
T2  - 
AU  - Meyer, Helen
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/S0167-4048(97)88120-5
UR  - http://www.sciencedirect.com/science/article/pii/S0167404897881205
ER  - 

TY  - JOUR
T1  - From expert-derived user needs to user-perceived ease of use and usefulness: A two-phase mixed-methods evaluation framework
JO  - Journal of Biomedical Informatics
VL  - 52
IS  - 
SP  - 141
EP  - 150
PY  - 2014/12//
T2  - Special Section: Methods in Clinical Research Informatics
AU  - Boland, Mary Regina
AU  - Rusanov, Alexander
AU  - So, Yat
AU  - Lopez-Jimenez, Carlos
AU  - Busacca, Linda
AU  - Steinman, Richard C.
AU  - Bakken, Suzanne
AU  - Bigger, J. Thomas
AU  - Weng, Chunhua
SN  - 1532-0464
DO  - http://dx.doi.org/10.1016/j.jbi.2013.12.004
UR  - http://www.sciencedirect.com/science/article/pii/S1532046413001950
KW  - Evaluation studies
KW  - Clinical trials
KW  - Workflow
KW  - Needs assessment
KW  - Medical informatics
AB  - Abstract
Underspecified user needs and frequent lack of a gold standard reference are typical barriers to technology evaluation. To address this problem, this paper presents a two-phase evaluation framework involving usability experts (phase 1) and end-users (phase 2). In phase 1, a cross-system functionality alignment between expert-derived user needs and system functions was performed to inform the choice of “the best available” comparison system to enable a cognitive walkthrough in phase 1 and a comparative effectiveness evaluation in phase 2. During phase 2, five quantitative and qualitative evaluation methods are mixed to assess usability: time-motion analysis, software log, questionnaires – System Usability Scale and the Unified Theory of Acceptance of Use of Technology, think-aloud protocols, and unstructured interviews. Each method contributes data for a unique measure (e.g., time motion analysis contributes task-completion-time; software log contributes action transition frequency). The measures are triangulated to yield complementary insights regarding user-perceived ease-of-use, functionality integration, anxiety during use, and workflow impact. To illustrate its use, we applied this framework in a formative evaluation of a software called Integrated Model for Patient Care and Clinical Trials (IMPACT). We conclude that this mixed-methods evaluation framework enables an integrated assessment of user needs satisfaction and user-perceived usefulness and usability of a novel design. This evaluation framework effectively bridges the gap between co-evolving user needs and technology designs during iterative prototyping and is particularly useful when it is difficult for users to articulate their needs for technology support due to the lack of a baseline.
ER  - 

TY  - JOUR
T1  - Learning to love SIEM
JO  - Network Security
VL  - 2011
IS  - 4
SP  - 18
EP  - 19
PY  - 2011/4//
T2  - 
AU  - Jenkins, Steve
SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/S1353-4858(11)70041-1
UR  - http://www.sciencedirect.com/science/article/pii/S1353485811700411
AB  - In the 1964 motion picture, Dr Strangelove or: How I Learned to Stop Worrying and Love the Bomb, a paranoid general played by Sterling Hayden is able to hack into a system and initiate a nuclear attack on the Soviet Union without the knowledge of his superiors.
ER  - 

TY  - JOUR
T1  - Avoiding the five pitfalls of privileged accounts
JO  - Network Security
VL  - 2013
IS  - 5
SP  - 12
EP  - 14
PY  - 2013/5//
T2  - 
AU  - Grafton, Jane
SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/S1353-4858(13)70060-6
UR  - http://www.sciencedirect.com/science/article/pii/S1353485813700606
AB  - It is a rather human truth that when we hand out privileges they often get abused. Whether you are operating in the high reaches of government or the most basic market the sad fact is – where we find privilege we also find the abuse of privilege. In the world of IT, privileged accounts are identities that have elevated permission to access potentially sensitive data, run programs or change configuration settings. To put it simply, privileged accounts are the keys to the kingdom of IT.

When we hand out privileges they often get abused. In the world of IT, privileged accounts – identities that have elevated permission to access sensitive data, run programs or change settings – are found on every server, workstation and appliance.

In recent years we have witnessed more and more organisations fail to adequately secure such accounts, with catastrophic results. There are common practices that have lead to a number of security breaches and failed IT compliance audits, some of them very high-profile. Jane Grafton of Lieberman Software looks at the five most common errors with privileged accounts – and how to avoid them.
ER  - 

TY  - JOUR
T1  - Towards a forensic-aware database solution: Using a secured database replication protocol and transaction management for digital investigations
JO  - Digital Investigation
VL  - 11
IS  - 4
SP  - 336
EP  - 348
PY  - 2014/12//
T2  - 
AU  - Frühwirt, Peter
AU  - Kieseberg, Peter
AU  - Krombholz, Katharina
AU  - Weippl, Edgar
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2014.09.003
UR  - http://www.sciencedirect.com/science/article/pii/S1742287614001078
KW  - MySQL
KW  - InnoDB
KW  - Digital forensics
KW  - Databases
KW  - Data tempering
KW  - Replication
KW  - Transaction management
AB  - Abstract
Databases contain an enormous amount of structured data. While the use of forensic analysis on the file system level for creating (partial) timelines, recovering deleted data and revealing concealed activities is very popular and multiple forensic toolsets exist, the systematic analysis of database management systems has only recently begun. Databases contain a large amount of temporary data files and metadata which are used by internal mechanisms. These data structures are maintained in order to ensure transaction authenticity, to perform rollbacks, or to set back the database to a predefined earlier state in case of e.g. an inconsistent state or a hardware failure. However, these data structures are intended to be used by the internal system methods only and are in general not human-readable.

In this work we present a novel approach for a forensic-aware database management system using transaction- and replication sources. We use these internal data structures as a vital baseline to reconstruct evidence during a forensic investigation. The overall benefit of our method is that no additional logs (such as administrator logs) are needed. Furthermore, our approach is invariant to retroactive malicious modifications by an attacker. This assures the authenticity of the evidence and strengthens the chain of custody. To evaluate our approach, we present a formal description, a prototype implementation in MySQL alongside and a comprehensive security evaluation with respect to the most relevant attack scenarios.
ER  - 

TY  - JOUR
T1  - Security Log Management
JO  - Network Security
VL  - 2003
IS  - 11
SP  - 6
EP  - 9
PY  - 2003/11//
T2  - 
AU  - Lobo, Colin
SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/S1353-4858(03)01106-1
UR  - http://www.sciencedirect.com/science/article/pii/S1353485803011061
AB  - Have your systems been configured to write events to the system logs? Most people would answer “Yes”. Are they using the standard default settings? The answer to this will vary. How often are the logs examined? Most people would answer only “when there is a problem”.
ER  - 

TY  - JOUR
T1  - A brief overview and an experimental evaluation of data confidentiality measures on the cloud
JO  - Journal of Innovation in Digital Ecosystems
VL  - 1
IS  - 1–2
SP  - 1
EP  - 11
PY  - 2014/12//
T2  - 
AU  - Aljafer, Hussain
AU  - Malik, Zaki
AU  - Alodib, Mohammed
AU  - Rezgui, Abdelmounaam
SN  - 2352-6645
DO  - http://dx.doi.org/10.1016/j.jides.2015.02.001
UR  - http://www.sciencedirect.com/science/article/pii/S2352664515000024
KW  - Security and protection
KW  - Cloud
KW  - Encryption
KW  - Measurement
AB  - Abstract
Due to the many advantages offered by the cloud computing paradigm, it is fast becoming an enabling technology for many organizations, and even individual users. Flexibility and availability are two of the most important features that promote the wide spread adoption of this technology. In cloud-based data storage scenarios, where the data is controlled by a third party (i.e. the cloud service provider), the data owner usually does not have full control of its data at all stages. Consequently, this poses a prime security threat, and a major challenge is the development of a secure protocol for data storage, sharing, and retrieval. In recent years, a number of research works have targeted this problem. In this paper, we discuss some of the major approaches for secure data sharing in the cloud computing environment. The goal is to provide a concise survey of existing solutions, discuss their benefits, and point out any shortcomings for future research. Specifically, we focus on the use of encryption schemes, and provide a comparative study of the major schemes, through implementation of some representative frameworks.
ER  - 

TY  - JOUR
T1  - Forensic discovery auditing of digital evidence containers
JO  - Digital Investigation
VL  - 4
IS  - 2
SP  - 88
EP  - 97
PY  - 2007/6//
T2  - 
AU  - Richard III, Golden G.
AU  - Roussev, Vassil
AU  - Marziale, Lodovico
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2007.04.002
UR  - http://www.sciencedirect.com/science/article/pii/S1742287607000291
KW  - Digital forensics
KW  - Operating systems internals
KW  - Filesystems
KW  - Digital evidence containers Auditing
AB  - Current digital forensics methods capture, preserve, and analyze digital evidence in general-purpose electronic containers (typically, plain files) with no dedicated support to help establish that the evidence has been properly handled. Auditing of a digital investigation, from identification and seizure of evidence through duplication and investigation is, essentially, ad hoc, recorded in separate log files or in an investigator's case notebook. Auditing performed in this fashion is bound to be incomplete, because different tools provide widely disparate amounts of auditing information – including none at all – and there is ample room for human error. The latter is a particularly pressing concern given the fast growth of the size of forensic targets.

Recently, there has been a serious community effort to develop an open standard for specialized digital evidence containers (DECs). A DEC differs from a general purpose container in that, in addition to the actual evidence, it bundles arbitrary metadata associated with it, such as logs and notes, and provides the basic means to detect evidence-tampering through digital signatures. Current approaches consist of defining a container format and providing a specialized library that can be used to manipulate it. While a big step in the right direction, this approach has some non-trivial shortcomings – it requires the retooling of existing forensic software and, thereby, limits the number of tools available to the investigator. More importantly, however, it does not provide a complete solution since it only records snapshots of the state of the DEC without being able to provide a trusted log of all data operations actually performed on the evidence. Without a trusted log the question of whether a tool worked exactly as advertised cannot be answered with certainty, which opens the door to challenges (both legitimate and frivolous) of the results.

In this paper, we propose a complementary mechanism, called the Forensic Discovery Auditing Module (FDAM), aimed at closing this loophole in the discovery process. FDAM can be thought of as a ‘clean-room’ environment for the manipulation of digital evidence, where evidence from containers is placed for controlled manipulation. It functions as an operating system component, which monitors and logs all access to the evidence and enforces policy restrictions. This allows the immediate, safe, and verifiable use of any tool deemed necessary by the examiner. In addition, the module can provide transparent support for multiple DEC formats, thereby greatly simplifying the adoption of open standards.
ER  - 

TY  - JOUR
T1  - Crossing Borders: The Right Side of Wrong?
JO  - Infosecurity
VL  - 8
IS  - 5
SP  - 22
EP  - 25
PY  - 2011/9//
Y2  - 2011/10//
T2  - 
AU  - Grossman, Wendy M.
SN  - 1754-4548
DO  - http://dx.doi.org/10.1016/S1754-4548(11)70065-1
UR  - http://www.sciencedirect.com/science/article/pii/S1754454811700651
AB  - Most nations consider travel data to be crucial to protecting national security. How that data is collected, stored, and secured however seems to be a closely guarded secret. Wendy M. Grossman investigates
ER  - 

TY  - JOUR
T1  - Turning log files into a security asset
JO  - Network Security
VL  - 2008
IS  - 2
SP  - 4
EP  - 7
PY  - 2008/2//
T2  - 
AU  - Casey, Donal
SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/S1353-4858(08)70016-3
UR  - http://www.sciencedirect.com/science/article/pii/S1353485808700163
AB  - Despite most businesses having a range of devices which create log files of user and system activity, few actually analyse the logs for information.

As the internet becomes an integral part of the way we live and work, the number of logs generated and their importance has continued to increase. More homes now have broadband connections. Most businesses at least have email and many have an interactive web presence, or at the very least a static web ‘brochure’. All of the internet traffic these generate has the capacity to create useful information for marketers, and more importantly for security engineers, in the form of log entries.
ER  - 

TY  - JOUR
T1  - Security issues for third party games: Technical, business and legal perspectives
JO  - Computer Law & Security Review
VL  - 24
IS  - 2
SP  - 163
EP  - 168
PY  - 2008///
T2  - 
AU  - Davis, Steven B.
AU  - Price, W. Joseph
SN  - 0267-3649
DO  - http://dx.doi.org/10.1016/j.clsr.2008.01.004
UR  - http://www.sciencedirect.com/science/article/pii/S026736490800006X
AB  - There has been a substantial rise in security incidents for the multi-billion dollar, worldwide computer gaming industry. As the industry has grown, games are not just developed and operated by one company, but include multiple support, development, publishing, and other partners. This article will discuss how to include technical, business, and legal security issues in the game development process to manage security risks. This article also will discuss potential “long tail” opportunities, which game publishers and developers have not fully exploited or considered in their games, such as secondary commercial games licensing and other developmental and licensing strategies that would benefit from managing security issues early in the developmental lifecycle.
ER  - 

TY  - JOUR
T1  - Security in cloud computing: Opportunities and challenges
JO  - Information Sciences
VL  - 305
IS  - 
SP  - 357
EP  - 383
PY  - 2015/6/1/
T2  - 
AU  - Ali, Mazhar
AU  - Khan, Samee U.
AU  - Vasilakos, Athanasios V.
SN  - 0020-0255
DO  - http://dx.doi.org/10.1016/j.ins.2015.01.025
UR  - http://www.sciencedirect.com/science/article/pii/S0020025515000638
KW  - Cloud computing
KW  - Multi-tenancy
KW  - Security
KW  - Virtualization
KW  - Web services
AB  - Abstract
The cloud computing exhibits, remarkable potential to provide cost effective, easy to manage, elastic, and powerful resources on the fly, over the Internet. The cloud computing, upsurges the capabilities of the hardware resources by optimal and shared utilization. The above mentioned features encourage the organizations and individual users to shift their applications and services to the cloud. Even the critical infrastructure, for example, power generation and distribution plants are being migrated to the cloud computing paradigm. However, the services provided by third-party cloud service providers entail additional security threats. The migration of user’s assets (data, applications, etc.) outside the administrative control in a shared environment where numerous users are collocated escalates the security concerns. This survey details the security issues that arise due to the very nature of cloud computing. Moreover, the survey presents the recent solutions presented in the literature to counter the security issues. Furthermore, a brief view of security vulnerabilities in the mobile cloud computing are also highlighted. In the end, the discussion on the open issues and future research directions is also presented.
ER  - 

TY  - JOUR
T1  - Cheyenne, McAfee cure software viruses
JO  - Computers & Security
VL  - 15
IS  - 7
SP  - 587
EP  - 588
PY  - 1996///
T2  - 
AU  - Meyer, Helen
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/S0167-4048(97)88122-9
UR  - http://www.sciencedirect.com/science/article/pii/S0167404897881229
ER  - 

TY  - JOUR
T1  - How to protect your data
JO  - Computers & Security
VL  - 15
IS  - 7
SP  - 586
EP  - 587
PY  - 1996///
T2  - 
AU  - Meyer, Helen
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/S0167-4048(97)88119-9
UR  - http://www.sciencedirect.com/science/article/pii/S0167404897881199
ER  - 

TY  - JOUR
T1  - On Incident Handling and Response: A state-of-the-art approach
JO  - Computers & Security
VL  - 25
IS  - 5
SP  - 351
EP  - 370
PY  - 2006/7//
T2  - 
AU  - Mitropoulos, Sarandis
AU  - Patsos, Dimitrios
AU  - Douligeris, Christos
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/j.cose.2005.09.006
UR  - http://www.sciencedirect.com/science/article/pii/S0167404805001574
KW  - Incident Handling
KW  - Incident Response
KW  - Computer forensics
KW  - Internet forensics
KW  - Software forensics
KW  - Trace-back mechanisms
AB  - Incident Response has always been an important aspect of Information Security but it is often overlooked by security administrators. Responding to an incident is not solely a technical issue but has many management, legal, technical and social aspects that are presented in this paper. We propose a detailed management framework along with a complete structured methodology that contains best practices and recommendations for appropriately handling a security incident. We also present the state-of-the art technology in computer, network and software forensics as well as automated trace-back artifacts, schemas and protocols. Finally, we propose a generic Incident Response process within a corporate environment.
ER  - 

TY  - JOUR
T1  - Delegation and digital mandates: Legal requirements and security objectives
JO  - Computer Law & Security Review
VL  - 25
IS  - 5
SP  - 415
EP  - 431
PY  - 2009/9//
T2  - 
AU  - Van Alsenoy, Brendan
AU  - De Cock, Danny
AU  - Simoens, Koen
AU  - Dumortier, Jos
AU  - Preneel, Bart
SN  - 0267-3649
DO  - http://dx.doi.org/10.1016/j.clsr.2009.07.007
UR  - http://www.sciencedirect.com/science/article/pii/S0267364909001265
KW  - Delegation
KW  - Agency
KW  - Mandate
KW  - Identity management
KW  - Delegated user and access management
AB  - Now that more and more legal transactions are being performed online, it is increasingly necessary to enable integration of legal mandates within identity and information management systems. The purpose of this article is to outline the legal framework surrounding delegation and to identify basic requirements for any technical application which seeks to provide recognition to legal mandates and delegation processes. Special consideration is also given to the legal implications in situations where a (presumed) mandate holder acts without or outside his authority. Based on these considerations, this article attempts to outline an approach which can significantly reduce the potential risks for both mandate issuers and relying service providers.
ER  - 

TY  - JOUR
T1  - On a taxonomy of delegation
JO  - Computers & Security
VL  - 29
IS  - 5
SP  - 565
EP  - 579
PY  - 2010/7//
T2  - Challenges for Security, Privacy and Trust
AU  - Pham, Quan
AU  - Reid, Jason
AU  - McCullagh, Adrian
AU  - Dawson, Edward
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/j.cose.2009.12.009
UR  - http://www.sciencedirect.com/science/article/pii/S0167404809001473
KW  - Delegation
KW  - Authorisation
KW  - Taxonomy
KW  - Classification
KW  - Access control
AB  - Delegation, from a technical point of view, is widely considered as a potential approach in addressing the problem of providing dynamic access control decisions in activities with a high level of collaboration, either within a single security domain or across multiple security domains. Although delegation continues to attract significant attention from the research community, presently, there is no published work that presents a taxonomy of delegation concepts and models. This article intends to address this gap by presenting a set of taxonomic criteria relevant to the concept of delegation. This article also applies the taxonomy to a selection of significant delegation models published in the literature.
ER  - 

TY  - JOUR
T1  - Cloud forensics: Technical challenges, solutions and comparative analysis
JO  - Digital Investigation
VL  - 13
IS  - 
SP  - 38
EP  - 57
PY  - 2015/6//
T2  - 
AU  - Pichan, Ameer
AU  - Lazarescu, Mihai
AU  - Soh, Sie Teng
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2015.03.002
UR  - http://www.sciencedirect.com/science/article/pii/S1742287615000407
KW  - Cloud computing
KW  - Cloud forensics
KW  - Cloud service provider
KW  - Cloud customer
KW  - Digital forensics
KW  - Digital evidence
KW  - Service level agreement
KW  - Amazon EC2
AB  - Abstract
Cloud computing is arguably one of the most significant advances in information technology (IT) services today. Several cloud service providers (CSPs) have offered services that have produced various transformative changes in computing activities and presented numerous promising technological and economic opportunities. However, many cloud customers remain reluctant to move their IT needs to the cloud, mainly due to their concerns on cloud security and the threat of the unknown. The CSPs indirectly escalate their concerns by not letting customers see what is behind virtual wall of their clouds that, among others, hinders digital investigations. In addition, jurisdiction, data duplication and multi-tenancy in cloud platform add to the challenge of locating, identifying and separating the suspected or compromised targets for digital forensics. Unfortunately, the existing approaches to evidence collection and recovery in a non-cloud (traditional) system are not practical as they rely on unrestricted access to the relevant system and user data; something that is not available in the cloud due its decentralized data processing. In this paper we systematically survey the forensic challenges in cloud computing and analyze their most recent solutions and developments. In particular, unlike the existing surveys on the topic, we describe the issues in cloud computing using the phases of traditional digital forensics as the base. For each phase of the digital forensic process, we have included a list of challenges and analysis of their possible solutions. Our description helps identifying the differences between the problems and solutions for non-cloud and cloud digital forensics. Further, the presentation is expected to help the investigators better understand the problems in cloud environment. More importantly, the paper also includes most recent development in cloud forensics produced by researchers, National Institute of Standards and Technology and Amazon.
ER  - 

TY  - JOUR
T1  - Dealing with self-report bias in mobile Internet acceptance and usage studies
JO  - Information & Management
VL  - 52
IS  - 3
SP  - 287
EP  - 294
PY  - 2015/4//
T2  - 
AU  - de Reuver, Mark
AU  - Bouwman, Harry
SN  - 0378-7206
DO  - http://dx.doi.org/10.1016/j.im.2014.12.002
UR  - http://www.sciencedirect.com/science/article/pii/S0378720614001475
KW  - Self-report bias
KW  - Method bias
KW  - Mobile applications
KW  - Acceptance
KW  - Usage
AB  - Abstract
Self-report bias is a known validity threat to IS usage studies. Respondents may find it even more difficult to assess their actual usage of mobile services as these can be used in highly variable contexts. This paper examines the extent of self-report bias in mobile acceptance and usage studies and suggests counter-measures. We demonstrate that several Type-1 and Type-2 errors are made when relying on self-reports rather than log data. Weighing can partly mitigate self-report bias as older people are more accurate than younger people. Log data replace self-reports in order to reduce self-report bias in mobile acceptance studies.
ER  - 

TY  - JOUR
T1  - In brief
JO  - Network Security
VL  - 2010
IS  - 6
SP  - 3
EP  - 
PY  - 2010/6//
T2  - 

SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/S1353-4858(10)70079-9
UR  - http://www.sciencedirect.com/science/article/pii/S1353485810700799
ER  - 

TY  - JOUR
T1  - Security for eBusiness
JO  - Information Security Technical Report
VL  - 6
IS  - 2
SP  - 80
EP  - 94
PY  - 2001/6/1/
T2  - 
AU  - Davidson, Mary Ann
SN  - 1363-4127
DO  - http://dx.doi.org/10.1016/S1363-4127(01)00209-6
UR  - http://www.sciencedirect.com/science/article/pii/S1363412701002096
ER  - 

TY  - JOUR
T1  - Computer-aided design/drafting management within the united states air force logistics command civil engineering organization
JO  - Computers & Structures
VL  - 41
IS  - 6
SP  - 1169
EP  - 1173
PY  - 1991///
T2  - Special Issue: CIVIL-COMP 89
AU  - Alley, W.D.
SN  - 0045-7949
DO  - http://dx.doi.org/10.1016/0045-7949(91)90254-J
UR  - http://www.sciencedirect.com/science/article/pii/004579499190254J
AB  - The United States Air Force civil engineering organization is responsible for the design, construction, and maintenance of over $115 billion dollars worth of facilities at bases around the world. Many civil engineering functions and offices have procured or are in the process of procuring computer-aided design and drafting (CADD) technology to assist in accomplishing their assigned tasks. Specific areas where automation is anticipated as being most helpful include maintaining facility information, design and engineering, operations and maintenance, project management, master planning, and space planning and management. The actual achievement of these capabilities is less dependent on hardware or software and relies more on a comprehensive and achievable implementation and system management plan. The Air Force civil engineering implementation and management plan consists of four separate but related sections: the benefits/needs analysis; the application/action plan; the CADD standards manual; and the CADD user's guide.
ER  - 

TY  - JOUR
T1  - An integrated conceptual digital forensic framework for cloud computing
JO  - Digital Investigation
VL  - 9
IS  - 2
SP  - 71
EP  - 80
PY  - 2012/11//
T2  - 
AU  - Martini, Ben
AU  - Choo, Kim-Kwang Raymond
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2012.07.001
UR  - http://www.sciencedirect.com/science/article/pii/S174228761200059X
KW  - Cloud computing
KW  - Cloud forensics
KW  - Digital forensics
KW  - Forensic computing
KW  - Digital evidence
KW  - Computer forensics
AB  - Increasing interest in and use of cloud computing services presents both opportunities for criminal exploitation and challenges for law enforcement agencies (LEAs). For example, it is becoming easier for criminals to store incriminating files in the cloud computing environment but it may be extremely difficult for LEAs to seize these files as the latter could potentially be stored overseas. Two of the most widely used and accepted forensic frameworks – McKemmish (1999) and NIST (Kent et al., 2006) – are then reviewed to identify the required changes to current forensic practices needed to successfully conduct cloud computing investigations. We propose an integrated (iterative) conceptual digital forensic framework (based on McKemmish and NIST), which emphasises the differences in the preservation of forensic data and the collection of cloud computing data for forensic purposes. Cloud computing digital forensic issues are discussed within the context of this framework. Finally suggestions for future research are made to further examine this field and provide a library of digital forensic methodologies for the various cloud platforms and deployment models.
ER  - 

TY  - JOUR
T1  - Auditing and security
JO  - Computer Audit Update
VL  - 1989
IS  - 5
SP  - 2
EP  - 7
PY  - 1989/3//
Y2  - 1989/4//
T2  - 
AU  - Blatchford, Clive
SN  - 0960-2593
DO  - http://dx.doi.org/10.1016/S0960-2593(89)80027-8
UR  - http://www.sciencedirect.com/science/article/pii/S0960259389800278
AB  - Summary
The purpose of this short paper is to create a generic administrative framework in which the operational management of information systems may be supported by effective auditing process.

Emphasis is placed on agreeing a set of overall control principles which may be mapped from the human world to that of the technologically based information systems. Five intial principles have been recognised. This list may be extended as the practical problems of managing and auditing security information systems, especially in the non-Defence sector, are explored.

The objectives of functionally rich, IT ‘open systems’ solutions are used to illustrate the requirements. Where relevant, specific ‘open’ standards activities (especially ISO, CCITT, ect) are referenced as the basis of future development.
ER  - 

TY  - JOUR
T1  - Subject index to volume 2
JO  - Computers & Security
VL  - 2
IS  - 3
SP  - 305
EP  - 308
PY  - 1983/11//
T2  - 

SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/0167-4048(83)90023-8
UR  - http://www.sciencedirect.com/science/article/pii/0167404883900238
ER  - 

TY  - JOUR
T1  - A quantitative study of Public Key Infrastructures
JO  - Computers & Security
VL  - 22
IS  - 1
SP  - 56
EP  - 67
PY  - 2003/1//
T2  - 
AU  - Bruschi, D
AU  - Curti, A
AU  - Rosti, E
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/S0167-4048(03)00113-5
UR  - http://www.sciencedirect.com/science/article/pii/S0167404803001135
AB  - Public Key Infrastructures have not reached the widespread diffusion expected of them, although they are well understood from a security point of view, because, like many say, the killer application has not been found yet. The lack of a clear understanding of the performance of these systems also contributes significantly to their limited diffusion. Studies have appeared of specific aspects of the operations of PKIs, but no complete studies of the overall system are known.

In this paper we present an evaluation study of X.509-compliant Public Key Infrastructures using queuing network models. We focus our analysis on the performance of the subsystem in charge of generating and managing digital certificates, under a variety of load conditions, both in terms of the type of requests and their number. We also investigate the impact on the performance of the system of some implementation choices such as revocation mechanisms and auditing activities. The main result of our analysis is that the system we consider, given the current state of technology, can guarantee acceptable response time in steady state even in the presence of PKI with a consistent number of users. However, in order to guarantee such a performance level, throughput must not exceed 3.5 requests per second, where a request can be a certificate generation or revocation request. Such a limitation hinders the deployment of PKIs with large numbers of users, since recovering after a system compromise may require an unacceptable amount of time.
ER  - 

TY  - JOUR
T1  - Preventing software piracy
JO  - Network Security
VL  - 1994
IS  - 9
SP  - 17
EP  - 19
PY  - 1994/9//
T2  - 
AU  - Schifreen, Robert
SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/1353-4858(94)90179-1
UR  - http://www.sciencedirect.com/science/article/pii/1353485894901791
AB  - It's up to you, as the person in charge of network security, to ensure that users are not storing pirated software on the server or workstations. Regular audits can help you achieve this.
ER  - 

TY  - JOUR
T1  - Implementing enterprise security: a case study3
JO  - Computers & Security
VL  - 22
IS  - 2
SP  - 99
EP  - 114
PY  - 2003/2//
T2  - 
AU  - Doughty, Ken
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/S0167-4048(03)00205-0
UR  - http://www.sciencedirect.com/science/article/pii/S0167404803002050
AB  - Introduction

Information is an essential asset for organizations, because it supports the day-today operations, and facilitates decision-making by the organization’s key stakeholders. The challenge facing organizations is how to provide access to this asset without compromising its integrity. This asset is received and distributed by the organization through various distribution channels, which is connected together by the telecommunications network. These channels include:

•
Email
•
Internet
•
Applications (e.g. Financial, Logistics, Retail, Property and Construction, Energy etc.)
•
DBMS (MS SQL Server, Oracle, DB2, Sybase, etc.)
•
Operating systems (e.g. Unix, NT/Windows 2000, etc.)
ER  - 

TY  - JOUR
T1  - Gearing up for grid computing
JO  - Infosecurity Today
VL  - 2
IS  - 5
SP  - 22
EP  - 25
PY  - 2005/9//
Y2  - 2005/10//
T2  - 
AU  - Myerson, Judith M.
SN  - 1742-6847
DO  - http://dx.doi.org/10.1016/S1742-6847(05)70321-9
UR  - http://www.sciencedirect.com/science/article/pii/S1742684705703219
AB  - As an enterprise's infrastructure breaches the borders of both nations and its own direct control, application security becomes a hot issue, particularly in the new Europe.
ER  - 

TY  - JOUR
T1  - Random bits &amp; bytes
JO  - Computers & Security
VL  - 15
IS  - 1
SP  - 4
EP  - 11
PY  - 1996///
T2  - 
AU  - Highland, Harold Joseph
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/S0167-4048(96)90057-7
UR  - http://www.sciencedirect.com/science/article/pii/S0167404896900577
ER  - 

TY  - JOUR
T1  - ASE: A comprehensive pattern-driven security methodology for distributed systems
JO  - Computer Standards & Interfaces
VL  - 41
IS  - 
SP  - 112
EP  - 137
PY  - 2015/9//
T2  - 
AU  - Uzunov, Anton V.
AU  - Fernandez, Eduardo B.
AU  - Falkner, Katrina
SN  - 0920-5489
DO  - http://dx.doi.org/10.1016/j.csi.2015.02.011
UR  - http://www.sciencedirect.com/science/article/pii/S0920548915000276
KW  - Secure software engineering
KW  - Security methodologies
KW  - Distributed systems security
KW  - Security patterns
KW  - Security solution frames
AB  - Abstract
Incorporating security features is one of the most important and challenging tasks in designing distributed systems. Over the last decade, researchers and practitioners have come to recognize that the incorporation of security features should proceed by means of a structured, systematic approach, combining principles from both software and security engineering. Such systematic approaches, particularly those implying some sort of process aligned with the development life-cycle, are termed security methodologies. There are a number of security methodologies in the literature, of which the most flexible and, according to a recent survey, most satisfactory from an industry-adoption viewpoint are methodologies that encapsulate their security solutions in some fashion, especially via the use of security patterns. While the literature does present several mature pattern-driven security methodologies with either a general or a highly specific system applicability, there are currently no (pattern-driven) security methodologies specifically designed for general distributed systems. Going further, there are also currently no methodologies with mixed specific applicability, e.g. for both general and peer-to-peer distributed systems. In this paper we aim to fill these gaps by presenting a comprehensive pattern-driven security methodology – arrived at by applying a previously devised approach to engineering security methodologies – specifically designed for general distributed systems, which is also capable of taking into account the specifics of peer-to-peer systems as needed. Our methodology takes the principle of encapsulation several steps further, by employing patterns not only for the incorporation of security features (via security solution frames), but also for the modeling of threats, and even as part of its process. We illustrate and evaluate the presented methodology in detail via a realistic example – the development of a distributed system for file sharing and collaborative editing. In both the presentation of the methodology and example our focus is on the early life-cycle phases (analysis and design).
ER  - 

TY  - JOUR
T1  - Distributed component architectures security issues
JO  - Computer Standards & Interfaces
VL  - 27
IS  - 3
SP  - 269
EP  - 284
PY  - 2005/3//
T2  - 
AU  - Gousios, Giorgos
AU  - Aivaloglou, Efthimia
AU  - Gritzalis, Stefanos
SN  - 0920-5489
DO  - http://dx.doi.org/10.1016/j.csi.2004.08.003
UR  - http://www.sciencedirect.com/science/article/pii/S0920548904000984
KW  - Components
KW  - Component architectures security
KW  - CORBA
KW  - J2EE
KW  - .NET
AB  - Enterprise information systems and e-commerce applications are tightly integrated in today's modern enterprises. Component architectures are the base for building such multitier distributed applications. This paper examines the security threats those systems must confront and the solutions proposed by major existing component architectures. A comparative evaluation of both security features and implementation issues is carried out to determine each architecture's strong points and drawbacks.
ER  - 

TY  - JOUR
T1  - Design and implementation of FROST: Digital forensic tools for the OpenStack cloud computing platform
JO  - Digital Investigation
VL  - 10, Supplement
IS  - 
SP  - S87
EP  - S95
PY  - 2013/8//
T2  - The Proceedings of the Thirteenth Annual DFRWS Conference13th Annual Digital Forensics Research Conference
AU  - Dykstra, Josiah
AU  - Sherman, Alan T.
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2013.06.010
UR  - http://www.sciencedirect.com/science/article/pii/S174228761300056X
KW  - OpenStack
KW  - Cloud computing
KW  - Digital forensics
KW  - Cloud forensics
KW  - FROST
AB  - Abstract
We describe the design, implementation, and evaluation of FROST—three new forensic tools for the OpenStack cloud platform. Our implementation for the OpenStack cloud platform supports an Infrastructure-as-a-Service (IaaS) cloud and provides trustworthy forensic acquisition of virtual disks, API logs, and guest firewall logs. Unlike traditional acquisition tools, FROST works at the cloud management plane rather than interacting with the operating system inside the guest virtual machines, thereby requiring no trust in the guest machine. We assume trust in the cloud provider, but FROST overcomes non-trivial challenges of remote evidence integrity by storing log data in hash trees and returning evidence with cryptographic hashes. Our tools are user-driven, allowing customers, forensic examiners, and law enforcement to conduct investigations without necessitating interaction with the cloud provider. We demonstrate how FROST's new features enable forensic investigators to obtain forensically-sound data from OpenStack clouds independent of provider interaction. Our preliminary evaluation indicates the ability of our approach to scale in a dynamic cloud environment. The design supports an extensible set of forensic objectives, including the future addition of other data preservation, discovery, real-time monitoring, metrics, auditing, and acquisition capabilities.
ER  - 

TY  - JOUR
T1  - Information Security – The Fourth Wave
JO  - Computers & Security
VL  - 25
IS  - 3
SP  - 165
EP  - 168
PY  - 2006/5//
T2  - 
AU  - von Solms, Basie
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/j.cose.2006.03.004
UR  - http://www.sciencedirect.com/science/article/pii/S016740480600054X
KW  - Corporate Governance
KW  - Information Security
KW  - Information Security Management
KW  - Information Security Governance
KW  - Risk management
KW  - Sarbanes–Oxley
KW  - Social engineering
AB  - In a previous article [von Solms, 2000], the development of Information Security up to the year 2000 was characterized as consisting of three waves:•
the technical wave,
•
the management wave, and
•
the institutional wave.


This paper continues this development of Information Security by characterizing the Fourth Wave – that of Information Security Governance.
ER  - 

TY  - JOUR
T1  - A survey of security in multi-agent systems
JO  - Expert Systems with Applications
VL  - 39
IS  - 5
SP  - 4835
EP  - 4846
PY  - 2012/4//
T2  - 
AU  - Cavalcante, Rodolfo Carneiro
AU  - Bittencourt, Ig Ibert
AU  - da Silva, Alan Pedro
AU  - Silva, Marlos
AU  - Costa, Evandro
AU  - Santos, Robério
SN  - 0957-4174
DO  - http://dx.doi.org/10.1016/j.eswa.2011.09.130
UR  - http://www.sciencedirect.com/science/article/pii/S0957417411014539
KW  - Agents
KW  - Multi-agent systems
KW  - Security
KW  - Security in MAS
KW  - Multi-agents
AB  - Multi-agent systems (MAS) are a relatively new software paradigm that is being widely accepted in several application domains to address large and complex tasks. However, with the use of MAS in open, distributed and heterogeneous applications, the security issues may endanger the success of the application. The goal of this research is to identify the security issues faced by MAS and to survey the current state of the art of this field of knowledge. In order to do it, this paper examines the basic concepts of security in computing, and some characteristics of agents and multi-agent systems that introduce new threats and ways to attack. After this, some models and architectures proposed in the literature are presented and analyzed.
ER  - 

TY  - JOUR
T1  - Framework of a methodology for the life cycle of computer security in an organization
JO  - Computers & Security
VL  - 8
IS  - 5
SP  - 433
EP  - 442
PY  - 1989/8//
T2  - 
AU  - Badenhorst, K.P.
AU  - Eloff, Jan H.P.
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/0167-4048(89)90025-4
UR  - http://www.sciencedirect.com/science/article/pii/0167404889900254
KW  - Technological security
KW  - Applications security
KW  - Methodology
KW  - Life cycle
KW  - Management
AB  - The life cycle of computer security is a paradigm to the software development life cycle, a tool that provides structure and foundation for the planning, development and implementation of application software. Current “off-the-shelf” methodologies are mostly for conventional software system development. The objective of this paper is to design a methodology for the introduction, development and maintenance of computer security within major organizations. Both the following issues will be addressed: technological computer security such as physical and logical aspects, and applications computer security referring to the development of software.
ER  - 

TY  - JOUR
T1  - Inter-organizational future proof EHR systems: A review of the security and privacy related issues
JO  - International Journal of Medical Informatics
VL  - 78
IS  - 3
SP  - 141
EP  - 160
PY  - 2009/3//
T2  - 
AU  - van der Linden, Helma
AU  - Kalra, Dipak
AU  - Hasman, Arie
AU  - Talmon, Jan
SN  - 1386-5056
DO  - http://dx.doi.org/10.1016/j.ijmedinf.2008.06.013
UR  - http://www.sciencedirect.com/science/article/pii/S1386505608001081
KW  - Computerized Medical Records Systems
KW  - Data security
KW  - Access policy
KW  - Standards
KW  - Networked care
AB  - Objectives
Identification and analysis of privacy and security related issues that occur when health information is exchanged between health care organizations.
Methods
Based on a generic scenario questions were formulated to reveal the occurring issues. Possible answers were verified in literature.
Results
Ensuring secure health information exchange across organizations requires a standardization of security measures that goes beyond organizational boundaries, such as global definitions of professional roles, global standards for patient consent and semantic interoperable audit logs.
Conclusion
As to be able to fully address the privacy and security issues in interoperable EHRs and the long-life virtual EHR it is necessary to realize a paradigm shift from storing all incoming information in a local system to retrieving information from external systems whenever that information is deemed necessary for the care of the patient.
ER  - 

TY  - JOUR
T1  - Privacy and consent in pervasive networks
JO  - Information Security Technical Report
VL  - 14
IS  - 3
SP  - 138
EP  - 142
PY  - 2009/8//
T2  - The Changing Shape of Privacy and Consent
AU  - Malik, Nazir A.
AU  - Tomlinson, Allan
SN  - 1363-4127
DO  - http://dx.doi.org/10.1016/j.istr.2009.10.002
UR  - http://www.sciencedirect.com/science/article/pii/S1363412709000375
AB  - Pervasive networks and location based systems have the potential to provide many new services. However the user of these services often has to provide personal information to allow the service to operate effectively. This article considers the problem of protecting personal information in this environment, and reports on the legislative and technical efforts being made to protect user privacy.
ER  - 

TY  - JOUR
T1  - Network forensic frameworks: Survey and research challenges
JO  - Digital Investigation
VL  - 7
IS  - 1–2
SP  - 14
EP  - 27
PY  - 2010/10//
T2  - 
AU  - Pilli, Emmanuel S.
AU  - Joshi, R.C.
AU  - Niyogi, Rajdeep
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2010.02.003
UR  - http://www.sciencedirect.com/science/article/pii/S1742287610000113
KW  - Network forensics
KW  - NFATs
KW  - Distributed systems
KW  - Soft computing
KW  - Honeypots
KW  - Data fusion
KW  - Attribution
KW  - Traceback
KW  - Incident response
AB  - Network forensics is the science that deals with capture, recording, and analysis of network traffic for detecting intrusions and investigating them. This paper makes an exhaustive survey of various network forensic frameworks proposed till date. A generic process model for network forensics is proposed which is built on various existing models of digital forensics. Definition, categorization and motivation for network forensics are clearly stated. The functionality of various Network Forensic Analysis Tools (NFATs) and network security monitoring tools, available for forensics examiners is discussed. The specific research gaps existing in implementation frameworks, process models and analysis tools are identified and major challenges are highlighted. The significance of this work is that it presents an overview on network forensics covering tools, process models and framework implementations, which will be very much useful for security practitioners and researchers in exploring this upcoming and young discipline.
ER  - 

TY  - JOUR
T1  - Security in grid computing: A review and synthesis
JO  - Decision Support Systems
VL  - 44
IS  - 4
SP  - 749
EP  - 764
PY  - 2008/3//
T2  - 
AU  - Cody, Erin
AU  - Sharman, Raj
AU  - Rao, Raghav H.
AU  - Upadhyaya, Shambhu
SN  - 0167-9236
DO  - http://dx.doi.org/10.1016/j.dss.2007.09.007
UR  - http://www.sciencedirect.com/science/article/pii/S0167923607001728
KW  - Grid computing
KW  - Information assurance
KW  - Survey and synthesis
KW  - Security
AB  - This paper provides an extensive survey of the different methods of addressing security issues in the grid computing environment, and specifically contributes to the research environment by developing a comprehensive framework for classification of these research endeavors. The framework presented classifies security literature into System Solutions, Behavioral Solutions, Hybrid Solutions and Related Technologies. Each one of these categories is explained in detail in the paper to provide insight as to their unique methods of accomplishing grid security, the types of grid and security situations they apply best to, and the pros and cons for each type of solution. Further, several areas of research were identified in the course of the literature survey where more study is warranted. These avenues for future research are also discussed in this paper. Several types of grid systems exist currently, and the security needs and solutions to address those needs for each type vary as much as the types of systems themselves. This research framework will aid in future research efforts to define, analyze, and address grid security problems for the many varied types of grid setups, as well as the many security situations that each grid may face.
ER  - 

TY  - JOUR
T1  - Cyber Wars and other threats
JO  - Computers & Security
VL  - 17
IS  - 2
SP  - 115
EP  - 118
PY  - 1998///
T2  - 
AU  - Hinde, Stephen
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/S0167-4048(97)81979-7
UR  - http://www.sciencedirect.com/science/article/pii/S0167404897819797
ER  - 

TY  - JOUR
T1  - Analysis of recommended cloud security controls to validate OpenPMF “policy as a service”
JO  - Information Security Technical Report
VL  - 16
IS  - 3–4
SP  - 131
EP  - 141
PY  - 2011/8//
Y2  - 2011/11//
T2  - Cloud Security
AU  - Lang, Ulrich
AU  - Schreiner, Rudolf
SN  - 1363-4127
DO  - http://dx.doi.org/10.1016/j.istr.2011.08.001
UR  - http://www.sciencedirect.com/science/article/pii/S136341271100046X
KW  - Cloud
KW  - Security
KW  - Policy
KW  - Authorization management
KW  - Access policy
KW  - Compliance
KW  - Model-driven security
KW  - Accreditation
KW  - Audit policy
KW  - Application security
KW  - XACML
KW  - OpenPMF
KW  - NIST 800-53
KW  - NIST 800-147
KW  - NIST IR 7628
KW  - PCI-DSS
KW  - HIPAA
AB  - This paper describes some of the findings of a cloud research project the authors carried out in Q2/2011. As part of the project, the authors first identified security concerns related to cloud computing, and gaps in cloud-related standards/regulations. The authors then identified several hard-to-implement, but highly cloud-relevant, security requirements in numerous cloud (and non-cloud) regulations and guidance documents, especially related to “least privilege”, “information flow control”, and “incident monitoring/auditing/analysis”. Further study revealed that there are significant cloud technology gaps in cloud (and non-cloud) platforms, which make it difficult to effectively implement those security policy requirements. The project concluded that model-driven security policy automation offered as a cloud service and tied into the protected cloud platform is ideally suited to achieve correct, consistent, low-effort/cost policy implementation for cloud applications.
ER  - 

TY  - JOUR
T1  - Random bits &amp; bytes
JO  - Computers & Security
VL  - 13
IS  - 8
SP  - 622
EP  - 627
PY  - 1994///
T2  - 
AU  - Highland, HaroldJoseph
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/0167-4048(94)90041-8
UR  - http://www.sciencedirect.com/science/article/pii/0167404894900418
ER  - 

TY  - JOUR
T1  - Forensic collection of cloud storage data: Does the act of collection result in changes to the data or its metadata?
JO  - Digital Investigation
VL  - 10
IS  - 3
SP  - 266
EP  - 277
PY  - 2013/10//
T2  - 
AU  - Quick, Darren
AU  - Choo, Kim-Kwang Raymond
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2013.07.001
UR  - http://www.sciencedirect.com/science/article/pii/S1742287613000741
KW  - Cloud storage
KW  - Data collection
KW  - Cloud forensics
KW  - Preservation
KW  - Dropbox
KW  - Google Drive
KW  - Microsoft SkyDrive
KW  - Computer forensics
KW  - Digital forensics
KW  - Digital evidence
AB  - Abstract
The timely acquisition and preservation of data from cloud storage can be an issue for law enforcement agencies and other digital forensic practitioners. In a jurisdiction which has legal provisions to collect data available to a computer or device, the process may involve accessing an account to collect the data. Using three popular public cloud storage providers (Dropbox, Google Drive, and Microsoft SkyDrive) as case studies, this research explores the process of collecting data from a cloud storage account using a browser and also downloading files using client software. We then compare these with the original files and undertake analysis of the resulting data. We determined that there were no changes to the contents of files during the process of upload, storage, and download to the three cloud storage services. The timestamps of the files were also examined in relation to the files downloaded via a browser and via client software. It was observed that some of the timestamp information remained the same throughout the process of uploading, storing and downloading files. Timestamp information may be a crucial aspect of an investigation, prosecution, or civil action, and therefore it is important to record the information available, and to understand the circumstances relating to a timestamp on a file.
ER  - 

TY  - JOUR
T1  - Legal admissibility of evidence held in digital form
JO  - Computer Law & Security Review
VL  - 15
IS  - 3
SP  - 185
EP  - 187
PY  - 1999/5//
Y2  - 1999/6//
T2  - 
AU  - Kearsley, Amanda J.
SN  - 0267-3649
DO  - http://dx.doi.org/10.1016/S0267-3649(99)80037-5
UR  - http://www.sciencedirect.com/science/article/pii/S0267364999800375
AB  - Organizations faced with voluminous paper documents are increasingly turning to technology, scanning the paper documents and then destroying the originals, placing increased reliance on electronic document management systems for subsequent retrieval. One key difficulty these organizations face is the uncertain legal status of the electronic record, and whether, if necessary, the digital copy of the original paper document can be used in evidence in legal proceedings. This first part of a two part article considers the effects of the Civil Evidence Act 1995 and also examines current and recommended practices for ensuring the admissibility of digital evidence.
ER  - 

TY  - JOUR
T1  - Vulnerabilities and mitigation techniques toning in the cloud: A cost and vulnerabilities coverage optimization approach using Cuckoo search algorithm with Lévy flights
JO  - Computers & Security
VL  - 48
IS  - 
SP  - 1
EP  - 18
PY  - 2015/2//
T2  - 
AU  - Zineddine, Mhamed
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/j.cose.2014.09.002
UR  - http://www.sciencedirect.com/science/article/pii/S0167404814001333
KW  - Cloud computing
KW  - ICT security
KW  - Vulnerabilities mapping
KW  - Cuckoo search algorithm
KW  - Lévy flights algorithm
KW  - Optimization
AB  - Abstract
Information and Communication Technology (ICT) security issues have been a major concern for decades. Today's ICT infrastructure faces sophisticated attacks using combinations of multiple vulnerabilities to penetrate networks with devastating impact. With the recent rise of cloud computing as a new utility computing paradigm, organizations have been considering it as a viable option to outsource major IT services in order to cut costs. Some organizations have opted for a private or hybrid cloud to take advantage of the emerging technologies and services. However, ICT security issues have to be appropriately mitigated. This research proposes a cloud security framework and an approach for vulnerabilities coverage and cost optimization using Cuckoo search algorithm with Lévy flights as random walks. The objective is to mitigate an identified set of vulnerabilities using a selected set of techniques when minimizing cost and maximizing coverage. The results show that Cloud Computing providers and organizations implementing cloud technology within their premises can effectively balance IT security coverage and cost using the proposed approach.
ER  - 

TY  - JOUR
T1  - Data protection: why are organisations still missing the point?
JO  - Computer Fraud & Security
VL  - 2008
IS  - 6
SP  - 5
EP  - 8
PY  - 2008/6//
T2  - 
AU  - Gorge, Mathieu
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(08)70095-2
UR  - http://www.sciencedirect.com/science/article/pii/S1361372308700952
AB  - Mathieu Gorge argues that data protection should be second nature for companies.
ER  - 

TY  - JOUR
T1  - Information security in workstation environments
JO  - Computers & Security
VL  - 12
IS  - 2
SP  - 117
EP  - 122
PY  - 1993/3//
T2  - 
AU  - Stahl, Stanley H.
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/0167-4048(93)90090-R
UR  - http://www.sciencedirect.com/science/article/pii/016740489390090R
AB  - This article explores information security issues in a context directed to meeting the needs of distributed workstation environments. It is not intended to be self-contained. Many topics, particularly those that are generic to securing computer and communication systems, are hardly touched on. The intent has been to (i) provide a context for exploring information security issues associated with distributed workstation environments and (ii) explore a few of the most important technology-based counter-measures available in workstation security.
ER  - 

TY  - JOUR
T1  - Bootstrapping a de-identification system for narrative patient records: Cost-performance tradeoffs
JO  - International Journal of Medical Informatics
VL  - 82
IS  - 9
SP  - 821
EP  - 831
PY  - 2013/9//
T2  - 
AU  - Hanauer, David
AU  - Aberdeen, John
AU  - Bayer, Samuel
AU  - Wellner, Benjamin
AU  - Clark, Cheryl
AU  - Zheng, Kai
AU  - Hirschman, Lynette
SN  - 1386-5056
DO  - http://dx.doi.org/10.1016/j.ijmedinf.2013.03.005
UR  - http://www.sciencedirect.com/science/article/pii/S1386505613000634
KW  - Privacy [I01.880.604.473.352.500]
KW  - Natural language processing [L01.224.065.580]
KW  - NLP
KW  - Electronic health records [E05.318.308.940.968.625.500]
KW  - Medical record systems
KW  - Computerized [E05.318.308.940.968.625]
KW  - Medical informatics [L01.313.500]
AB  - AbstractPurpose
We describe an experiment to build a de-identification system for clinical records using the open source MITRE Identification Scrubber Toolkit (MIST). We quantify the human annotation effort needed to produce a system that de-identifies at high accuracy.
Methods
Using two types of clinical records (history and physical notes, and social work notes), we iteratively built statistical de-identification models by annotating 10 notes, training a model, applying the model to another 10 notes, correcting the model's output, and training from the resulting larger set of annotated notes. This was repeated for 20 rounds of 10 notes each, and then an additional 6 rounds of 20 notes each, and a final round of 40 notes. At each stage, we measured precision, recall, and F-score, and compared these to the amount of annotation time needed to complete the round.
Results
After the initial 10-note round (33 min of annotation time) we achieved an F-score of 0.89. After just over 8 h of annotation time (round 21) we achieved an F-score of 0.95. Number of annotation actions needed, as well as time needed, decreased in later rounds as model performance improved. Accuracy on history and physical notes exceeded that of social work notes, suggesting that the wider variety and contexts for protected health information (PHI) in social work notes is more difficult to model.
Conclusions
It is possible, with modest effort, to build a functioning de-identification system de novo using the MIST framework. The resulting system achieved performance comparable to other high-performing de-identification systems.
ER  - 

TY  - JOUR
T1  - A configurable cryptography subsystem in a middleware framework for embedded systems
JO  - Computer Networks
VL  - 46
IS  - 6
SP  - 771
EP  - 795
PY  - 2004/12/20/
T2  - 
AU  - McKinnon, A. David
AU  - Bakken, David E.
AU  - Shovic, John C.
SN  - 1389-1286
DO  - http://dx.doi.org/10.1016/j.comnet.2004.06.020
UR  - http://www.sciencedirect.com/science/article/pii/S1389128604001732
KW  - Middleware
KW  - Security
KW  - CORBA
KW  - Embedded systems
AB  - Computer and network security is becoming increasingly important as both large systems and, increasingly small, embedded systems are networked. Middleware frameworks aid the system developer who must interconnect individual systems into larger interconnected, distributed systems. However, there exist very few middleware frameworks that have been designed for use with embedded systems, which constitute the vast majority of CPUs produced each year, and none offer the range of security mechanisms required by the wide range of embedded system applications. This paper describes MicroQoSCORBA, a highly configurable middleware framework for embedded systems, and its security subsystem. It first presents an analysis of security requirements for embedded applications and what can and should be done in middleware. It then presents the design of MicroQoSCORBA’s security subsystem and the wide range of mechanisms it supports. Experimental results for these mechanisms are presented for two different embedded systems and one desktop computer that collectively represent a wide range of computational capabilities.
ER  - 

TY  - JOUR
T1  - It security testing, a practical guide — part 5: Security stress/loading testing
JO  - Computer Audit Update
VL  - 1993
IS  - 3
SP  - 7
EP  - 10
PY  - 1993/3//
T2  - 
AU  - Robertson, Bernard
AU  - Pullen, David
SN  - 0960-2593
DO  - http://dx.doi.org/10.1016/0960-2593(93)90041-X
UR  - http://www.sciencedirect.com/science/article/pii/096025939390041X
ER  - 

TY  - JOUR
T1  - Compliance by design – Bridging the chasm between auditors and IT architects
JO  - Computers & Security
VL  - 30
IS  - 6–7
SP  - 410
EP  - 426
PY  - 2011/9//
Y2  - 2011/10//
T2  - 
AU  - Julisch, Klaus
AU  - Suter, Christophe
AU  - Woitalla, Thomas
AU  - Zimmermann, Olaf
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/j.cose.2011.03.005
UR  - http://www.sciencedirect.com/science/article/pii/S0167404811000514
KW  - Information systems audit
KW  - CAVR
KW  - Compliance
KW  - Security architecture
KW  - Patterns
KW  - Service-oriented architecture
KW  - Business processes
KW  - Enterprise applications
AB  - System and process auditors assure – from an information processing perspective – the correctness and integrity of the data that is aggregated in a company’s financial statements. To do so, they assess whether a company’s business processes and information systems process financial data correctly. The audit process is a complex endeavor that in practice has to rely on simplifying assumptions. These simplifying assumptions mainly result from the need to restrict the audit scope and to focus it on the major risks. This article describes a generalized audit process. According to our experience with this process, there is a risk that material deficiencies remain undiscovered when said simplifying assumptions are not satisfied. To address this risk of deficiencies, the article compiles thirteen control patterns, which – according to our experience – are particularly suited to help information systems satisfy the simplifying assumptions. As such, use of these proven control patterns makes information systems easier to audit and IT architects can use them to build systems that meet audit requirements by design. Additionally, the practices and advice offered in this interdisciplinary article help bridge the gap between the architects and auditors of information systems and show either role how to benefit from an understanding of the other role’s terminology, techniques, and general work approach.
ER  - 

TY  - JOUR
T1  - Access control for smarter healthcare using policy spaces
JO  - Computers & Security
VL  - 29
IS  - 8
SP  - 848
EP  - 858
PY  - 2010/11//
T2  - 
AU  - Ardagna, Claudio A.
AU  - De Capitani di Vimercati, Sabrina
AU  - Foresti, Sara
AU  - Grandison, Tyrone W.
AU  - Jajodia, Sushil
AU  - Samarati, Pierangela
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/j.cose.2010.07.001
UR  - http://www.sciencedirect.com/science/article/pii/S0167404810000623
KW  - Access control
KW  - Break the glass
KW  - Policy spaces
KW  - Exceptions
KW  - Healthcare systems
AB  - A fundamental requirement for the healthcare industry is that the delivery of care comes first and nothing should interfere with it. As a consequence, the access control mechanisms used in healthcare to regulate and restrict the disclosure of data are often bypassed in case of emergencies. This phenomenon, called “break the glass”, is a common pattern in healthcare organizations and, though quite useful and mandatory in emergency situations, from a security perspective, it represents a serious system weakness. Malicious users, in fact, can abuse the system by exploiting the break the glass principle to gain unauthorized privileges and accesses.

In this paper, we propose an access control solution aimed at better regulating break the glass exceptions that occur in healthcare systems. Our solution is based on the definition of different policy spaces, a language, and a composition algebra to regulate access to patient data and to balance the rigorous nature of traditional access control systems with the “delivery of care comes first” principle.
ER  - 

TY  - JOUR
T1  - Analyzing multiple logs for forensic evidence
JO  - Digital Investigation
VL  - 4, Supplement
IS  - 
SP  - 82
EP  - 91
PY  - 2007/9//
T2  - 
AU  - Arasteh, Ali Reza
AU  - Debbabi, Mourad
AU  - Sakha, Assaad
AU  - Saleh, Mohamed
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2007.06.013
UR  - http://www.sciencedirect.com/science/article/pii/S1742287607000448
KW  - Forensic analysis
KW  - Log analysis
KW  - Formal methods
KW  - Model checking
KW  - Logging systems
KW  - Log correlation
AB  - Information stored in logs of a computer system is of crucial importance to gather forensic evidence of investigated actions or attacks. Analysis of this information should be rigorous and credible, hence it lends itself to formal methods. We propose a model checking approach to the formalization of the forensic analysis of logs. A set of logs is modeled as a tree whose labels are events extracted from the logs. In order to provide a structure to these events, we express each event as a term of algebra. The signature of the algebra is carefully chosen to include all relevant information necessary to conduct the analysis. Properties of the model, attack scenarios, and event sequences are expressed as formulas of a logic having dynamic, linear, temporal, and modal characteristics. Moreover, we provide a tableau-based proof system for this logic upon which a model checking algorithm can be developed. We use our model in a case study to demonstrate how events leading to an SYN attack can be reconstructed from a number of system logs.
ER  - 

TY  - JOUR
T1  - Sarbanes-Oxley: maybe a blessing, maybe a curse
JO  - Computer Fraud & Security
VL  - 2005
IS  - 9
SP  - 4
EP  - 7
PY  - 2005/9//
T2  - 
AU  - Power, Richard
AU  - Forte, Dario
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(05)70250-5
UR  - http://www.sciencedirect.com/science/article/pii/S1361372305702505
AB  - Sarbanes-Oxley can bring benefits and heartache to IT security managers. This article demonstrates the advantages and the headaches that the legislation can cause.
ER  - 

TY  - JOUR
T1  - A survey of information security incident handling in the cloud
JO  - Computers & Security
VL  - 49
IS  - 
SP  - 45
EP  - 69
PY  - 2015/3//
T2  - 
AU  - Ab Rahman, Nurul Hidayah
AU  - Choo, Kim-Kwang Raymond
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/j.cose.2014.11.006
UR  - http://www.sciencedirect.com/science/article/pii/S0167404814001680
KW  - Capability Maturity Model For Services (CMMI-SVC)
KW  - Cloud computing
KW  - Cloud response
KW  - Incident handling
KW  - Incident management
KW  - Incident response
AB  - Abstract
Incident handling strategy is one key strategy to mitigate risks to the confidentiality, integrity and availability (CIA) of organisation assets, as well as minimising loss (e.g. financial, reputational and legal) particularly as organisations move to the cloud. In this paper, we surveyed existing incident handling and digital forensic literature with the aims of contributing to the knowledge gap(s) in handling incidents in the cloud environment. 139 English language publications between January 2009 and May 2014 were located by searching various sources including the websites of standard bodies (e.g. National Institute of Standards and Technology) and academic databases (e.g. Google Scholar, IEEEXplore, ACM Digital Library, Springer and ScienceDirect). We then propose a conceptual cloud incident handling model that brings together incident handling, digital forensic and the Capability Maturity Model for Services to more effectively handle incidents for organisations using the cloud. A discussion of open research issues concludes this survey.
ER  - 

TY  - JOUR
T1  - Design and implementation of a mediation system enabling secure communication among Critical Infrastructures
JO  - International Journal of Critical Infrastructure Protection
VL  - 5
IS  - 2
SP  - 86
EP  - 97
PY  - 2012/7//
T2  - 
AU  - Castrucci, Marco
AU  - Neri, Alessandro
AU  - Caldeira, Filipe
AU  - Aubert, Jocelyn
AU  - Khadraoui, Djamel
AU  - Aubigny, Matthieu
AU  - Harpes, Carlo
AU  - Simões, Paulo
AU  - Suraci, Vincenzo
AU  - Capodieci, Paolo
SN  - 1874-5482
DO  - http://dx.doi.org/10.1016/j.ijcip.2012.04.001
UR  - http://www.sciencedirect.com/science/article/pii/S1874548212000194
KW  - Critical Infrastructure
KW  - Information sharing
KW  - Web services
KW  - MICIE
KW  - Secure mediation gateway
AB  - Nowadays, the increase of interdependencies among different Critical Infrastructures (CI) makes it more and more difficult to protect without using a systemic approach that considers a single infrastructure as part of a complex system of infrastructures. A strong collaboration among CI owners is required to avoid, or at least to limit the propagation of failures from one infrastructure to another and to put CI in safety mode. The key element enabling this required cooperation is the possibility for them to exchange relevant information related to the status of their infrastructures and to the services provided. In this paper, we present a middleware solution that allows CIs sharing real-time information, enabling the design and implementation of fault mitigation strategies and mechanisms to prevent the cascading phenomena generated by the failure propagation from one infrastructure to another.
ER  - 

TY  - JOUR
T1  - Flexible composition and execution of large scale applications on distributed e-infrastructures
JO  - Journal of Computational Science
VL  - 5
IS  - 1
SP  - 51
EP  - 62
PY  - 2014/1//
T2  - 
AU  - Zasada, Stefan J.
AU  - Chang, David C.W.
AU  - Haidar, Ali N.
AU  - Coveney, Peter V.
SN  - 1877-7503
DO  - http://dx.doi.org/10.1016/j.jocs.2013.10.009
UR  - http://www.sciencedirect.com/science/article/pii/S1877750313001269
KW  - E-infrastructure
KW  - High performance computing
KW  - Application virtualization
KW  - Usability
AB  - Abstract
Computer simulation is finding a role in an increasing number of scientific disciplines, concomitant with the rise in available computing power. Marshalling this power facilitates new, more effective and different research than has been hitherto possible. Realizing this inevitably requires access to computational power beyond the desktop, making use of clusters, supercomputers, data repositories, networks and distributed aggregations of these resources. The use of diverse e-infrastructure brings with it the ability to perform distributed multiscale simulations. Accessing one such resource entails a number of usability and security problems; when multiple geographically distributed resources are involved, the difficulty is compounded. In this paper we present a solution, the Application Hosting Environment,33
AHE is available to download under the LGPL license from: https://sourceforge.net/projects/ahe3/.
 which provides a Software as a Service layer on top of distributed e-infrastructure resources. We describe the performance and usability enhancements present in AHE version 3, and show how these have led to a high performance, easy to use gateway for computational scientists working in diverse application domains, from computational physics and chemistry, materials science to biology and biomedicine.
ER  - 

TY  - JOUR
T1  - Beyond RACF: Extending user authentication controls
JO  - Computers & Security
VL  - 10
IS  - 8
SP  - 711
EP  - 722
PY  - 1991/12//
T2  - 
AU  - Lynch, Paul
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/0167-4048(91)90090-Z
UR  - http://www.sciencedirect.com/science/article/pii/016740489190090Z
AB  - A discussion on passwords and user i.d.s as the central point for security control is presented, with recommendations for stricter control, and identifying weaknesses in this approach. Use of “tokens” for authentication is discussed, concentrating on currently available devices, including IBM's TSS. The impact of authentication on the US NCSC standards (the “Orange Book”) and the proposed ITSEC standards are considered.
ER  - 

TY  - JOUR
T1  - Security issues in system development
JO  - Computer Audit Update
VL  - 1992
IS  - 9
SP  - 4
EP  - 8
PY  - 1992/9//
T2  - 
AU  - Price, G.R.
SN  - 0960-2593
DO  - http://dx.doi.org/10.1016/0960-2593(92)90010-K
UR  - http://www.sciencedirect.com/science/article/pii/096025939290010K
ER  - 

TY  - JOUR
T1  - Client server architectures and security
JO  - Computer Audit Update
VL  - 1992
IS  - 9
SP  - 8
EP  - 12
PY  - 1992/9//
T2  - 
AU  - Pullen, David
AU  - Robertson, Bernard
SN  - 0960-2593
DO  - http://dx.doi.org/10.1016/0960-2593(92)90011-B
UR  - http://www.sciencedirect.com/science/article/pii/096025939290011B
ER  - 

TY  - JOUR
T1  - Towards a unified taxonomy and architecture of cloud frameworks
JO  - Future Generation Computer Systems
VL  - 29
IS  - 5
SP  - 1196
EP  - 1210
PY  - 2013/7//
T2  - Special section: Hybrid Cloud Computing
AU  - Dukaric, Robert
AU  - Juric, Matjaz B.
SN  - 0167-739X
DO  - http://dx.doi.org/10.1016/j.future.2012.09.006
UR  - http://www.sciencedirect.com/science/article/pii/S0167739X12001793
KW  - Cloud Computing
KW  - Infrastructure as a service
KW  - Taxonomy
KW  - Architectural framework
AB  - Infrastructure as a Service (IaaS) is one of the most important layers of Cloud Computing. However, there is an evident deficiency of mechanisms for analysis, comparison and evaluation of IaaS cloud implementations, since no unified taxonomy or reference architecture is available. In this article, we propose a unified taxonomy and an IaaS architectural framework. The taxonomy is structured around seven layers: core service layer, support layer, value-added services, control layer, management layer, security layer and resource abstraction. We survey various IaaS systems and map them onto our taxonomy to evaluate the classification. We then introduce an IaaS architectural framework that relies on the unified taxonomy. We provide a detailed description of each layer and define dependencies between the layers and components. Finally, we evaluate the proposed IaaS architectural framework on several real-world projects, while performing a comprehensive analysis of the most important commercial and open-source IaaS products. The evaluation results show notable distinction of feature support and capabilities between commercial and open-source IaaS platforms, significant deficiency of important architectural components in terms of fulfilling true promise of infrastructure clouds, and real-world usability of the proposed taxonomy and architectural framework.
ER  - 

TY  - JOUR
T1  - Security implications of implementing active network infrastructures using agent technology
JO  - Computer Networks
VL  - 36
IS  - 1
SP  - 87
EP  - 100
PY  - 2001/6//
T2  - Active Networks and Services
AU  - Karnouskos, Stamatis
SN  - 1389-1286
DO  - http://dx.doi.org/10.1016/S1389-1286(01)00155-4
UR  - http://www.sciencedirect.com/science/article/pii/S1389128601001554
KW  - Active networks
KW  - Security
KW  - Active code
KW  - Agent technology
AB  - Active networks (AN) are a rapid evolving area of research and in parallel an area of great industry interest. However, for this technology to make the step out of the labs and penetrate the market, the security problems have to be tackled effectively. This paper demonstrates why and how agent technology research, can and should be applied to active networks, in order to fulfill the new security challenges this infrastructure poses. First, we identify the key elements of AN, analyze the nature of active code, specify the role of agents in active networks and present a multi-execution environment active network architecture. Then, we target the security threats for active code and execution environment, and state the basic as well as the extended security requirements. Subsequently, we try to see how we can apply the security solutions and research done for agents to the context of active networks in order to satisfy their requirements.
ER  - 

TY  - JOUR
T1  - An empirical study of automatic event reconstruction systems
JO  - Digital Investigation
VL  - 3, Supplement
IS  - 
SP  - 108
EP  - 115
PY  - 2006/9//
T2  - The Proceedings of the 6th Annual Digital Forensic Research Workshop (DFRWS '06)
AU  - Jeyaraman, Sundararaman
AU  - Atallah, Mikhail J.
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2006.06.013
UR  - http://www.sciencedirect.com/science/article/pii/S1742287606000752
KW  - Intrusion analysis
KW  - Digital forensics
KW  - Event reconstruction
KW  - Incident response
AB  - Reconstructing the sequence of computer events that led to a particular event is an essential part of the digital investigation process. The ability to quantify the accuracy of automatic event reconstruction systems is an essential step in standardizing the digital investigation process thereby making it resilient to tactics such as the Trojan horse defense. In this paper, we present findings from an empirical study to measure and compare the accuracy and effectiveness of a suite of such event reconstruction techniques. We quantify (as applicable) the rates of false positives and false negatives, and scalability in terms of both computational burden and memory-usage. Some of our findings are quite surprising in the sense of not matching a priori expectations, and whereas other findings qualitatively match the a priori expectations they were never before quantitatively put to the test to determine the boundaries of their applicability. For example, our results show that automatic event reconstruction systems proposed in literature have very high false-positive rates (up to 96%).
ER  - 

TY  - JOUR
T1  - PalmCIS: A wireless handheld application for satisfying clinician information needs
JO  - Journal of the American Medical Informatics Association
VL  - 11
IS  - 1
SP  - 19
EP  - 28
PY  - 2004/1//
Y2  - 2004/2//
T2  - 
AU  - Chen, Elizabeth S
AU  - Mendonça, Eneida A
AU  - McKnight, Lawrence K
AU  - Stetson, Peter D
AU  - Lei, Jianbo
AU  - Cimino, James J
SN  - 1067-5027
DO  - http://dx.doi.org/10.1197/jamia.M1387
UR  - http://www.sciencedirect.com/science/article/pii/S1067502703002020
AB  - Wireless handheld technology provides new ways to deliver and present information. As with any technology, its unique features must be taken into consideration and its applications designed accordingly. In the clinical setting, availability of needed information can be crucial during the decision-making process. Preliminary studies performed at New York Presbyterian Hospital (NYPH) determined that there are inadequate access to information and ineffective communication among clinicians (potential proximal causes of medical errors). In response to these findings, the authors have been developing extensions to their Web-based clinical information system including PalmCIS, an application that provides access to needed patient information via a wireless personal digital assistant (PDA). The focus was on achieving end-to-end security and developing a highly usable system. This report discusses the motivation behind PalmCIS, design and development of the system, and future directions.
ER  - 

TY  - JOUR
T1  - Log management for effective incident response
JO  - Network Security
VL  - 2005
IS  - 9
SP  - 4
EP  - 7
PY  - 2005/9//
T2  - 
AU  - Forte, Dario
SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/S1353-4858(05)70279-8
UR  - http://www.sciencedirect.com/science/article/pii/S1353485805702798
AB  - Log file correlation is related to two distinct activities: Intrusion Detection and Network Forensics. It is more important than ever that these two disciplines work together, and in cooperation, to avoid points of failure. This article presents an overview of log analysis and correlation, with special emphasis on the tools and techniques for managing them within a network forensics context.
ER  - 

TY  - JOUR
T1  - DSS for computer security incident response applying CBR and collaborative response
JO  - Expert Systems with Applications
VL  - 37
IS  - 1
SP  - 852
EP  - 870
PY  - 2010/1//
T2  - 
AU  - Kim, Huy Kang
AU  - Im, Kwang Hyuk
AU  - Park, Sang Chan
SN  - 0957-4174
DO  - http://dx.doi.org/10.1016/j.eswa.2009.05.100
UR  - http://www.sciencedirect.com/science/article/pii/S0957417409005223
KW  - Log analysis
KW  - System security
KW  - Misuse detection
KW  - Anomaly detection
KW  - Decision support system
KW  - Expert system
KW  - RFM analysis methodology
KW  - CBR (case based reasoning)
AB  - Recently, as hacking attempts increase dramatically; most enterprises are forced to employ some safeguards for hacking proof. For example, firewall or IPS (Intrusion Prevention System) selectively accepts the incoming packets, and IDS (Intrusion Detection System) detects the attack attempts from network. The latest version of firewall works in cooperation with IDS to immediately response to hacking attempts. However, it may make false alarms that misjudge normal traffic as hacking traffic and cause network problems to block the normal IP address by false alarms. By these false alarms made by IDS, system administrators or CSOs make wrong decisions and important data may be exposed or the availability of network or server system may be exhausted. Therefore, it is important to minimize the false alarms.

As a way of minimizing false alarms and supporting adequate decisions, we suggest the RFM (Recency, Frequency, Monetary) analysis methodology, which analyzes log files with incorporating three criteria of recency, frequency and monetary with statistical process control chart, and thus leads to an intuitive detection of anomaly and misuse events. Moreover, to cope with hacking attempts proactively, we apply CBR (case based reasoning) to find out similarities between already known hacking patterns and new hacking patterns. With the RFM analysis methodology and CBR, we develop DSS which can minimize false alarms and decrease the time to respond to hacking events. In case that RFM analysis module finds out unknown viruses or worms occurred, this CBR system matches the most similar incident case from case-based database. System administrators can easily get information about how to fix and how we fixed in similar cases. And CSOs can build a blacklist of frequently detected IP addresses and users. This blacklist can be used for incident handling.

Finally, we propose collaborative incident response system with DSS, this distributed agent systems interactively exchange the suspicious users and source IP addresses data and decide who is true-anomalous users and which IP addresses is the most riskiest and then deny all connections from that users and IP addresses automatically with less false-positives.
ER  - 

TY  - JOUR
T1  - Trust in digital records: An increasingly cloudy legal area
JO  - Computer Law & Security Review
VL  - 28
IS  - 5
SP  - 522
EP  - 531
PY  - 2012/10//
T2  - 
AU  - Duranti, Luciana
AU  - Rogers, Corinne
SN  - 0267-3649
DO  - http://dx.doi.org/10.1016/j.clsr.2012.07.009
UR  - http://www.sciencedirect.com/science/article/pii/S0267364912001458
KW  - Digital records
KW  - Digital forensics
KW  - Cloud computing
KW  - Law of evidence
KW  - Digital documentary evidence
AB  - Trust has been defined in many ways, but at its core it involves acting without the knowledge needed to act. Trust in records depends on four types of knowledge about the creator or custodian of the records: reputation, past performance, competence, and the assurance of confidence in future performance. For over half a century society has been developing and adopting new computer technologies for business and communications in both the public and private realm. Frameworks for establishing trust have developed as technology has progressed. Today, individuals and organizations are increasingly saving and accessing records in cloud computing infrastructures, where we cannot assess our trust in records solely on the four types of knowledge used in the past. Drawing on research conducted at the University of British Columbia into the nature of digital records and their trustworthiness, this article presents the conceptual archival and digital forensic frameworks of trust in records and data, and explores the common law legal framework within which questions of trust in documentary evidence are being tested. Issues and challenges specific to cloud computing are introduced.
ER  - 

TY  - JOUR
T1  - Expert Provisioner: a range management aid
JO  - Knowledge-Based Systems
VL  - 11
IS  - 5–6
SP  - 339
EP  - 344
PY  - 1998/11/23/
T2  - 
AU  - Power, Rhys
AU  - Reynolds, Steve
AU  - Kingston, John
AU  - Harrison, Ian
AU  - Ann Macintosh
AU  - Tonberg, Jon
SN  - 0950-7051
DO  - http://dx.doi.org/10.1016/S0950-7051(98)00062-8
UR  - http://www.sciencedirect.com/science/article/pii/S0950705198000628
KW  - Knowledge engineering
KW  - Knowledge-based systems
KW  - Logistics
AB  - Expert Provisioner is a knowledge-based provisioning system developed by Royal Air Force (RAF) logistics research and AIAI at the University of Edinburgh for use by the RAF logistics command to support the procurement of consumable parts. The starting point for Expert Provisioner is an electronic purchase order form and its end point is a recommendation of whether to buy the item or not, its cost and due delivery date. Purchase recommendations are based on many factors including forecast demand, unit costs, shelf life and existing stock levels. Identified benefits of the system include improved speed and accuracy of the data checking and order quantity calculation processes; automatic recording of provisioning history data for use in financial management/analysis; and finally, the ability to allow trainees to work on real life problems and compare their results with the experts.
ER  - 

TY  - JOUR
T1  - Information security incident management: Current practice as reported in the literature
JO  - Computers & Security
VL  - 45
IS  - 
SP  - 42
EP  - 57
PY  - 2014/9//
T2  - 
AU  - Tøndel, Inger Anne
AU  - Line, Maria B.
AU  - Jaatun, Martin Gilje
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/j.cose.2014.05.003
UR  - http://www.sciencedirect.com/science/article/pii/S0167404814000819
KW  - Information security
KW  - Incident management
KW  - Incident response
KW  - ISO/IEC 27035
KW  - Systematic review
AB  - Abstract
This paper reports results of a systematic literature review on current practice and experiences with incident management, covering a wide variety of organisations. Identified practices are summarised according to the incident management phases of ISO/IEC 27035. The study shows that current practice and experience seem to be in line with the standard. We identify some inspirational examples that will be useful for organisations looking to improve their practices, and highlight which recommended practices generally are challenging to follow. We provide suggestions for addressing the challenges, and present identified research needs within information security incident management.
ER  - 

TY  - JOUR
T1  - Assessing insider threats to information security using technical, behavioural and organisational measures
JO  - Information Security Technical Report
VL  - 15
IS  - 3
SP  - 112
EP  - 133
PY  - 2010/8//
T2  - Computer Crime - A 2011 Update
AU  - Roy Sarkar, Kuheli
SN  - 1363-4127
DO  - http://dx.doi.org/10.1016/j.istr.2010.11.002
UR  - http://www.sciencedirect.com/science/article/pii/S1363412710000488
AB  - The UK government took a bruising in the headlines (Sep 2008) after a Home Office contractor lost a USB stick containing unencrypted data on all 84,000 prisoners in England and Wales. As a result, the Home Office terminated the £1.5 million contract with the management consultancy firm.

The world woke up to the largest attempted bank fraud ever when the UK’s National Hi-Tech Crime Unit foiled the world’s largest potential bank robbery in March 2005. With the help of the security supervisor, thieves masquerading as cleaning staff installed hardware keystroke loggers on computers within the London branch of a Japanese bank, to steal £220m.

It is indeed sobering to imagine that any organisation could fall victim to such events and the damage an insider can do. The consulting firm lost the contract worth £1.5 million due to a small mistake by an employee. The London branch of the Japanese Bank would have lost £220 million had not the crime been foiled.

Insider threat is a reality. Insiders commit fraud or steal sensitive information when motivated by money or revenge. Well-meaning employees can compromise the security of an organisation with their overzealousness in getting their job done. Every organisation has a varied mix of employees, consultants, management, partners and complex infrastructure and that makes handling insider threats a daunting challenge. With insider attacks, organisations face potential damage through loss of revenue, loss of reputation, loss of intellectual property or even loss of human life.

The insider threat problem is more elusive and perplexing than any other threat. Assessing the insider threat is the first step to determine the likelihood of any insider attack. Technical solutions do not suffice since insider threats are fundamentally a people issue. Therefore, a three-pronged approach - technological, behavioural and organisational assessment is essential in facilitating the prediction of insider threats and pre-empt any insider attack thus improving the organization’s security, survivability, and resiliency in light of insider threats.
ER  - 

TY  - JOUR
T1  - Security requirements for e-government services: a methodological approach for developing a common PKI-based security policy
JO  - Computer Communications
VL  - 26
IS  - 16
SP  - 1873
EP  - 1883
PY  - 2003/10/15/
T2  - Securing Computer Communications with Public Key Infrastructure.
AU  - Lambrinoudakis, Costas
AU  - Gritzalis, Stefanos
AU  - Dridi, Fredj
AU  - Pernul, Günther
SN  - 0140-3664
DO  - http://dx.doi.org/10.1016/S0140-3664(03)00082-3
UR  - http://www.sciencedirect.com/science/article/pii/S0140366403000823
KW  - e-Government
KW  - Security requirements
KW  - Public Key Infrastructure
AB  - The concept of one-stop on-line government is not science fiction any more. On the contrary, the high reliability and performance of communication links, combined with architectural models that facilitate transparent access to distributed computational and storage resources, propel the development of integrated e-government platforms that support increased citizen mobility. The price we have to pay is the complexity introduced in the design of the security mechanisms required for protecting several heterogeneous information systems—each one supporting some of the services offered through the e-government integrated environment—and ensuring user privacy.

This paper demonstrates that the security services offered by Public Key Infrastructure (PKI) can be employed for fulfilling most of the identified security requirements for an integrated e-government platform. The list of security requirements has been compiled by adopting an organisational framework that facilitates the classification of e-government services according to the security requirements they exhibit.

The proposed approach has been applied, as a case study, to the e-government system ‘Webocrat’, identifying its security requirements and then designing a PKI-based security architecture for fulfilling them.
ER  - 

TY  - JOUR
T1  - Administrative controls for password-based computer access control systems
JO  - Computer Fraud & Security Bulletin
VL  - 8
IS  - 3
SP  - 5
EP  - 13
PY  - 1986/1//
T2  - 
AU  - Wood, CharlesCresson
SN  - 0142-0496
DO  - http://dx.doi.org/10.1016/0142-0496(86)90043-3
UR  - http://www.sciencedirect.com/science/article/pii/0142049686900433
ER  - 

TY  - JOUR
T1  - Model-driven business process security requirement specification
JO  - Journal of Systems Architecture
VL  - 55
IS  - 4
SP  - 211
EP  - 223
PY  - 2009/4//
T2  - Secure Service-Oriented Architectures (Special Issue on Secure SOA)
AU  - Wolter, Christian
AU  - Menzel, Michael
AU  - Schaad, Andreas
AU  - Miseldine, Philip
AU  - Meinel, Christoph
SN  - 1383-7621
DO  - http://dx.doi.org/10.1016/j.sysarc.2008.10.002
UR  - http://www.sciencedirect.com/science/article/pii/S1383762108001471
KW  - Web service security
KW  - Business process
KW  - Model transformation
KW  - Security annotations
KW  - Access control
AB  - Various types of security goals, such as authentication or confidentiality, can be defined as policies for service-oriented architectures, typically in a manual fashion. Therefore, we foster a model-driven transformation approach from modelled security goals in the context of process models to concrete security implementations. We argue that specific types of security goals may be expressed in a graphical fashion at the business process modelling level which in turn can be transformed into corresponding access control and security policies. In this paper we present security policy and policy constraint models. We further discuss a translation of security annotated business processes into platform specific target languages, such as XACML or AXIS2 security configurations. To demonstrate the suitability of this approach an example transformation is presented based on an annotated process.
ER  - 

TY  - JOUR
T1  - User provisioning with SPML
JO  - Information Security Technical Report
VL  - 9
IS  - 1
SP  - 86
EP  - 96
PY  - 2004/1//
Y2  - 2004/3//
T2  - 
AU  - Sodhi, Gavenraj
SN  - 1363-4127
DO  - http://dx.doi.org/10.1016/S1363-4127(04)00018-4
UR  - http://www.sciencedirect.com/science/article/pii/S1363412704000184
ER  - 

TY  - JOUR
T1  - Automated consent through privacy agents: Legal requirements and technical architecture
JO  - Computer Law & Security Review
VL  - 25
IS  - 2
SP  - 136
EP  - 144
PY  - 2009///
T2  - 
AU  - Le Métayer, Daniel
AU  - Monteleone, Shara
SN  - 0267-3649
DO  - http://dx.doi.org/10.1016/j.clsr.2009.02.010
UR  - http://www.sciencedirect.com/science/article/pii/S0267364909000387
KW  - Privacy agents
KW  - Legal analysis of consent
KW  - Technical architecture
KW  - RFID tags
KW  - Pervasive computing
KW  - Ambient intelligence
KW  - Liability
KW  - PET (Privacy Enhancing Technologies)
AB  - The changes imposed by new information technologies, especially pervasive computing and the Internet, require a deep reflection on the fundamental values underlying privacy and the best way to achieve their protection. The explicit consent of the data subject, which is a cornerstone of most data protection regulations, is a typical example of requirement which is very difficult to put into practice in the new world of “pervasive computing” where many data communications necessarily occur without the users' notice. In this paper, we argue that an architecture based on “Privacy Agents” can make privacy rights protection more effective, provided however that this architecture meets a number of legal requirements to ensure the validity of consent delivered through such Privacy Agents. We first present a legal analysis of consent considering successively (1) its nature; (2) its essential features (qualities and defects) and (3) its formal requirements. Then we draw the lessons of this legal analysis for the design of a valid architecture based on Privacy Agents. To conclude, we suggest an implementation of this architecture proposed in a multidisciplinary project involving lawyers and computer scientists.
ER  - 

TY  - JOUR
T1  - Profiling software applications for forensic analysis
JO  - Computer Fraud & Security
VL  - 2015
IS  - 6
SP  - 13
EP  - 18
PY  - 2015/6//
T2  - 
AU  - Rafique, Mamoona
AU  - Khan, Muhammad Naeem Ahmed
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(15)30058-0
UR  - http://www.sciencedirect.com/science/article/pii/S1361372315300580
AB  - Computers are now a fundamental part of our professional lives. Although advanced technologies are being used to contain digital crimes, alongside these are other technologies that have expanded a criminal community that is constantly searching for new means to commit crimes in more sophisticated ways. Due to the availability of corporate data on the web, coupled with the open access nature of the web, digital miscreants can commit cybercrimes either as legitimate or illegitimate users.

Traditional digital forensics involves static analysis of the data available on permanent storage media, while live analysis allows running systems to be examined to analyse volatile data.

However, live analysis is not without its challenges, not least because each application has different effects on the system. Mamoona Rafique and Muhammad Naeem Ahmed Khan present a model for profiling the behaviour of application programs. This allows investigators to build a behavioural profile of each application in order to understand its effects on the system.
ER  - 

TY  - JOUR
T1  - Get ready for PCI DSS 3.0 with real-time monitoring
JO  - Computer Fraud & Security
VL  - 2015
IS  - 2
SP  - 17
EP  - 18
PY  - 2015/2//
T2  - 
AU  - Fernandes, Joel John
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(15)30009-9
UR  - http://www.sciencedirect.com/science/article/pii/S1361372315300099
AB  - PCI DSS 3.0 compliance has gained worldwide acceptance by card service providers – card issuers, banks, and merchants – that plan to protect their customers' cardholder data from being misused. PCI DSS 3.0 has 12 security requirements concerning the protection of cardholder data. All businesses that accept, store, process or transmit customers card data either online or offline have to adhere to those requirements.
ER  - 

TY  - JOUR
T1  - An ‘intelligent’ approach to audit trail analysis
JO  - Computer Audit Update
VL  - 1991
IS  - 11
SP  - 13
EP  - 17
PY  - 1991/11//
T2  - 
AU  - Hickman, Frank
SN  - 0960-2593
DO  - http://dx.doi.org/10.1016/0960-2593(91)90050-J
UR  - http://www.sciencedirect.com/science/article/pii/096025939190050J
ER  - 

TY  - JOUR
T1  - Economics and the cyber challenge
JO  - Information Security Technical Report
VL  - 17
IS  - 1–2
SP  - 9
EP  - 18
PY  - 2012/2//
T2  - Human Factors and Bio-metrics
AU  - Walker, Simon
SN  - 1363-4127
DO  - http://dx.doi.org/10.1016/j.istr.2011.12.003
UR  - http://www.sciencedirect.com/science/article/pii/S1363412711000860
AB  - Economics can be used as a tool to explain, describe, and to a certain extent predict many forms of human behaviour. However, there is only a limited body of work on its application to information security, much of which is acknowledged as partial or incomplete. As a consequence, there is a paucity of robust explanatory or predictive models that are tuned for the peculiarities of the “cyber” challenge, either to organisations, or, at a higher level, the nation state.

The effect of this is that the base arguments for information security business cases are often weak or flawed; as a result, there is an argument that both organisations and nation states will therefore tend to underinvest in information security. To improve this position, there would be benefits for information security, as a profession adopting economic models used in other areas of endeavour that historically have suffered similar problems. One potential model is full-cost accounting.

However, there are a number of further implications. These include an underlining of the importance of information security professional “speaking business language”. Also highlighted is the potential value of building a common knowledge base of the true cost of security failures, akin to the actuarial bodies of knowledge used in the insurance industry, rather than the partial and imperfect measures in use today.
ER  - 

TY  - JOUR
T1  - Learning relational policies from electronic health record access logs
JO  - Journal of Biomedical Informatics
VL  - 44
IS  - 2
SP  - 333
EP  - 342
PY  - 2011/4//
T2  - 
AU  - Malin, Bradley
AU  - Nyemba, Steve
AU  - Paulett, John
SN  - 1532-0464
DO  - http://dx.doi.org/10.1016/j.jbi.2011.01.007
UR  - http://www.sciencedirect.com/science/article/pii/S1532046411000098
KW  - Electronic health records
KW  - Organizational behavior
KW  - Knowledge discovery
KW  - Access logs
KW  - Auditing
AB  - Modern healthcare organizations (HCOs) are composed of complex dynamic teams to ensure clinical operations are executed in a quick and competent manner. At the same time, the fluid nature of such environments hinders administrators’ efforts to define access control policies that appropriately balance patient privacy and healthcare functions. Manual efforts to define these policies are labor-intensive and error-prone, often resulting in systems that endow certain care providers with overly broad access to patients’ medical records while restricting other providers from legitimate and timely use. In this work, we propose an alternative method to generate these policies by automatically mining usage patterns from electronic health record (EHR) systems. EHR systems are increasingly being integrated into clinical environments and our approach is designed to be generalizable across HCOs, thus assisting in the design and evaluation of local access control policies. Our technique, which is grounded in data mining and social network analysis theory, extracts a statistical model of the organization from the access logs of its EHRs. In doing so, our approach enables the review of predefined policies, as well as the discovery of unknown behaviors. We evaluate our approach with 5 months of access logs from the Vanderbilt University Medical Center and confirm the existence of stable social structures and intuitive business operations. Additionally, we demonstrate that there is significant turnover in the interactions between users in the HCO and that policies learned at the department-level afford greater stability over time.
ER  - 

TY  - JOUR
T1  - Information Systems Audit Trails in Legal Proceedings as Evidence
JO  - Computers & Security
VL  - 20
IS  - 5
SP  - 409
EP  - 421
PY  - 2001/7/1/
T2  - 
AU  - Allinson, Caroline
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/S0167-4048(01)00513-2
UR  - http://www.sciencedirect.com/science/article/pii/S0167404801005132
KW  - law enforcement
KW  - audit trails
KW  - evidence
KW  - investigation, expert witness
KW  - information security
KW  - policy
KW  - court
KW  - survey
KW  - computer
AB  - Australian State and Commonwealth Governments are interested in the collection, storage and presentation of audit trail information, particularly within a legal framework. Law enforcement agencies have a legal obligation to keep audit records of all activity on information systems used within their operations. Little to no research has been identified in relation to the use of internal audit systems for evidentiary purpose.

A brief history of audit trails is given with requirements for such audit trails beyond the year 2000.

The Queensland Police Service (QPS), Australia, is used as a major case study . Information on principles, techniques and processes used, and the reason for the recording, storing and releasing of audit information for evidentiary purposes have been studied.

To assist in determining current practice in the Australian Commonwealth and State Governments the results of an Australia wide survey of all government departments are given and contrasted to the major study for QPS.

Reference is also made to the legal obligations for authorization of audit analysis, expert witnessing and legal precedence in relation to court acceptance or rejection of audit information used in evidence.

It is shown that most organizations studied generate and retain audit trails but the approach is not consistent nor is it comprehensive. It is suggested that these materials would not withstand a serious legal challenge.
ER  - 

TY  - JOUR
T1  - A model-integrated authoring environment for privacy policies
JO  - Science of Computer Programming
VL  - 89, Part B
IS  - 
SP  - 105
EP  - 125
PY  - 2014/9/1/
T2  - Special issue on Success Stories in Model Driven Engineering
AU  - Nadas, Andras
AU  - Levendovszky, Tihamer
AU  - Jackson, Ethan K.
AU  - Madari, Istvan
AU  - Sztipanovits, Janos
SN  - 0167-6423
DO  - http://dx.doi.org/10.1016/j.scico.2013.05.004
UR  - http://www.sciencedirect.com/science/article/pii/S016764231300124X
KW  - Privacy policies
KW  - Model-integrated computing
KW  - Constraint logic programming
AB  - Abstract
Privacy policies are rules designed to ensure that individuals’ health data are properly protected. Health Information Systems (HIS) are legally required to adhere to these policies. Since privacy policies are imposed on complex software systems, it is extremely hard to reason about their conformance and consistency. In order to address this problem, we have created a model-driven authoring environment to formally specify privacy policies originally defined in legal terms. In our observation, appropriate formalization of our policy language enabled formal analysis of its policies; these features were key to a successful model-driven engineering process. In this paper we present our modeling language and show its semantic anchoring to analyzable logic programs. We report on several projects where our approach is being applied and validated.
ER  - 

TY  - JOUR
T1  - Intrusion detection aware component-based systems: A specification-based framework
JO  - Journal of Systems and Software
VL  - 80
IS  - 5
SP  - 700
EP  - 710
PY  - 2007/5//
T2  - Component-Based Software Engineering of Trustworthy Embedded Systems
AU  - Hussein, Mohammed
AU  - Zulkernine, Mohammad
SN  - 0164-1212
DO  - http://dx.doi.org/10.1016/j.jss.2006.08.017
UR  - http://www.sciencedirect.com/science/article/pii/S0164121206002263
KW  - Component-based software engineering
KW  - Component security
KW  - UML profile
KW  - Intrusion detection
AB  - Component-Based Software Engineering (CBSE) increases the reusability of software and hence decreases software development time and cost. Unfortunately, developing components for maximum reusability and acquiring third party components invite many security related concerns. The security related issues are more crucial for embedded and real-time systems. Currently, many approaches are proposed to aid the development and evaluation of secure components. However, it is well known among practitioners that, like any other software entities, components cannot be completely secure. This fact leads us to incorporate intrusion detection facilities to equip components with mechanisms to discover intrusions against components. In this paper, we present a framework for developing components with intrusion detection capabilities. This framework uses UMLintr, a UML profile for intrusion specifications. The profile allows developers to specify intrusion scenarios using UML diagrams. Specifying intrusion scenarios using the same language that is used for specifying software behavior eliminates the need for separate languages for describing intrusions. Other software specification languages can be easily adopted into this framework. The outcome of this framework are components equipped with intrusion detectors. Based on UMLintr, a prototype is built and used to generate signatures for some intrusions included in the benchmark DARPA attack datasets. Furthermore, we describe an Intrusion Detection System (IDS) which uses these signatures to detect component intrusions.
ER  - 

TY  - JOUR
T1  - Improving the scaffolds of a mobile-assisted Chinese character forming game via a design-based research cycle
JO  - Computers in Human Behavior
VL  - 27
IS  - 5
SP  - 1783
EP  - 1793
PY  - 2011/9//
T2  - 2009 Fifth International Conference on Intelligent ComputingICIC 20092009 Fifth International Conference on Intelligent Computing
AU  - Wong, Lung-Hsiang
AU  - Boticki, Ivica
AU  - Sun, Jizhen
AU  - Looi, Chee-Kit
SN  - 0747-5632
DO  - http://dx.doi.org/10.1016/j.chb.2011.03.005
UR  - http://www.sciencedirect.com/science/article/pii/S074756321100063X
KW  - Mobile Computer-supported Collaborative Learning (mCSCL)
KW  - Chinese language learning
KW  - Design-based research (DBR)
KW  - User interface design
AB  - This paper reports on one cycle of a design-based research (DBR) study in which mCSCL was explored through an iterative process of (re)designing and testing the collaboration and learning approach with students. A unique characteristic of our mCSCL approach is the student-led emergent formation of groups. The mCSCL application assigns each student a component of a Chinese character and requires them to form groups that can assemble a Chinese character using the components held by the group members. The enactment of the learning design in two modes (with and without the digital technology) was observed, and the actual process of students being scaffolded technologically or socially to accomplish their task was analyzed. Students were found to favor the card mode over the phone mode due to the emergent game strategy (social scaffold) of “trial and error” that they found it comfortable in applying. That triggered us to examine the scaffolding strategies by conducting another round of literature review. We explored domain-oriented theories (i.e. in Chinese character learning) to inform and guide them in deciding how they should further accommodate or rectify the students’ use of the strategy. This cycle of DBR in Chinese-PP project has effectively reshaped the overall learning model design. This paper brings to the fore the value of the interplay and iterations of theories, implementations and reflections, in no fixed order, as advocated by DBR.
ER  - 

TY  - JOUR
T1  - An Informatics Blueprint for Healthcare Quality Information Systems
JO  - Journal of the American Medical Informatics Association
VL  - 13
IS  - 4
SP  - 402
EP  - 417
PY  - 2006/7//
Y2  - 2006/8//
T2  - 
AU  - Niland, Joyce C.
AU  - Rouse, Layla
AU  - Stahl, Douglas C.
SN  - 1067-5027
DO  - http://dx.doi.org/10.1197/jamia.M2050
UR  - http://www.sciencedirect.com/science/article/pii/S1067502706000624
AB  - There is a critical gap in our nation’s ability to accurately measure and manage the quality of medical care. A robust healthcare quality information system (HQIS) has the potential to address this deficiency through the capture, codification, and analysis of information about patient treatments and related outcomes. Because non-technical issues often present the greatest challenges, this paper provides an overview of these socio-technical issues in building a successful HQIS, including the human, organizational, and knowledge management (KM) perspectives. Through an extensive literature review and direct experience in building a practical HQIS (the National Comprehensive Cancer Network Outcomes Research Database system), we have formulated an “informatics blueprint” to guide the development of such systems. While the blueprint was developed to facilitate healthcare quality information collection, management, analysis, and reporting, the concepts and advice provided may be extensible to the development of other types of clinical research information systems.
ER  - 

TY  - JOUR
T1  - Securing digital signatures for non-repudiation
JO  - Computer Communications
VL  - 22
IS  - 8
SP  - 710
EP  - 716
PY  - 1999/5/25/
T2  - 
AU  - Zhou, J.
AU  - Lam, K.Y.
SN  - 0140-3664
DO  - http://dx.doi.org/10.1016/S0140-3664(99)00031-6
UR  - http://www.sciencedirect.com/science/article/pii/S0140366499000316
KW  - Digital signature
KW  - Non-repudiation
KW  - Secure electronic commerce
AB  - Dispute of transactions is a common problem that could jeopardise business. Hence non-repudiation services are essential in business transactions which provide evidence to enable dispute resolution. To be eligible as non-repudiation evidence, the digital signature on an electronic document should remain valid until its expiry date which is specified by some non-repudiation policy. The conventional approaches are either inefficient or insecure to achieve non-repudiation in electronic commerce. This article presents a practical scheme to secure digital signatures as non-repudiation evidence with an adjustable degree of risk.
ER  - 

TY  - JOUR
T1  - A transaction flow approach to software security certification for document handling systems
JO  - Computers & Security
VL  - 7
IS  - 5
SP  - 495
EP  - 502
PY  - 1988/10//
T2  - 
AU  - Pfleeger, Charles P.
AU  - Pfleeger, Shari Lawrence
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/0167-4048(88)90203-9
UR  - http://www.sciencedirect.com/science/article/pii/0167404888902039
KW  - Security
KW  - Certification
KW  - Transaction
KW  - Data flow
KW  - Verification
KW  - Validation
KW  - Exposure
KW  - Control
AB  - A security certification method is described for a document handling system for a major government organization. The security evaluation process includes identification of the exposures of the system, determination of the controls that cover those exposures, and evaluation of the appropriateness and effectiveness of the controls. Included are the details of the analysis performed and the types of results expected in that analysis, both of which constitute the basic evaluation of the document handling system. The certification analysis approach can be extended naturally to other types of computing systems.
ER  - 

TY  - JOUR
T1  - Security and performance in service-oriented applications: Trading off competing objectives
JO  - Decision Support Systems
VL  - 50
IS  - 1
SP  - 336
EP  - 346
PY  - 2010/12//
T2  - 
AU  - Zo, Hangjung
AU  - Nazareth, Derek L.
AU  - Jain, Hemant K.
SN  - 0167-9236
DO  - http://dx.doi.org/10.1016/j.dss.2010.09.002
UR  - http://www.sciencedirect.com/science/article/pii/S0167923610001715
KW  - Service-oriented computing
KW  - Application composition
KW  - Performance
KW  - Security
KW  - Multiple criteria decision making
AB  - As service-oriented computing becomes more prevalent, an increasing number of applications will be developed using existing software components with standard interfaces. These components may be developed in-house, may represent purchased software, or may involve vendor located leased services. The use of multiple services, possibly utilizing different technologies and different sources, has significant implications for the performance and security of these applications to support a business process effectively. Estimating performance and security in this distributed environment is a hard problem. This paper examines how performance and security measures can be developed for service-based applications. Business processes are broken down into constituent tasks and a formal mechanism is developed for deriving performance and security measures for the application. Given the competing nature of these two objectives, a tradeoff strategy is utilized wherein managers can trade improved performance for reduced security or vice versa. As the number of alternative services for each task increases, the composition problem becomes combinatorially explosive. A genetic algorithm approach is adopted to find the Pareto optimal set of services that can be assembled to support the business process. An application to a real-world business process illustrates its effectiveness.
ER  - 

TY  - JOUR
T1  - Performance analysis of Bayesian networks and neural networks in classification of file system activities
JO  - Computers & Security
VL  - 31
IS  - 4
SP  - 391
EP  - 401
PY  - 2012/6//
T2  - 
AU  - Khan, Muhammad Naeem Ahmed
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/j.cose.2012.03.003
UR  - http://www.sciencedirect.com/science/article/pii/S0167404812000533
KW  - Digital forensics
KW  - Computer forensic analysis
KW  - Digital evidence
KW  - Neural networks
KW  - Bayesian learning
KW  - Bayesian decision theory
AB  - Precise comprehension of a file system state at any given time is vital for performing digital forensic analyses. To uncover evidence of the digital crime, the logical representation of file system activities helps reconstruct post-event timeline of the unauthorized or malicious accesses made on a system. This paper describes a comparative performance analysis of the Bayesian networks and neural networks techniques to classify the state of file system activities in terms of execution of applications based on the pattern of manipulation of specific files during certain period of time. In particular, this paper discusses the construction of a Bayesian networks and neural networks from the predetermined knowledge of the manipulation of file system artifacts and their corresponding metadata information by a set of software applications. The variability amongst the execution patterns of various applications indicate that the Bayesian network-based model is a more appropriate tool as compared to neural networks because of its ability to learn and detect patterns even from an incomplete dataset. The focus of this paper is to highlight intrinsic significance of the learning approach of Bayesian network methodology in comparison to the techniques used for supervised learning in ordinary neural networks. The paper also highlights the efficacy of Bayesian network technique to proficiently handle large volumes of datasets.
ER  - 

TY  - JOUR
T1  - General controls in computer systems
JO  - Computers & Security
VL  - 4
IS  - 1
SP  - 33
EP  - 45
PY  - 1985/3//
T2  - 
AU  - Cerullo, Michael J.
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/0167-4048(85)90007-0
UR  - http://www.sciencedirect.com/science/article/pii/0167404885900070
KW  - Internal control
KW  - plan of organization
KW  - operations controls
KW  - system development and planning controls
KW  - access controls
KW  - documentation
KW  - personnel controls
KW  - operating system controls
KW  - master planning
KW  - contingency planning
AB  - Because almost all business organizations own or will shortly own computer systems, auditors can no longer treat the computer as a “black box” to be ignored when attesting to the fairness with which financial statements present financial position and results of operations. This paper is primarily written for auditors who are becoming involved in auditing computerized accounting systems. It covers in detail general controls in computer systems, the first and most important category of controls evaluated by an auditor. General controls relate to all EDP activities; they span all jobs processed on the computer system. When they are weak or non-existing, the auditor must expand his testing of the entire computer system, often at considerable additional cost to the client.
ER  - 

TY  - JOUR
T1  - Leveraging CybOX™ to standardize representation and exchange of digital forensic information
JO  - Digital Investigation
VL  - 12, Supplement 1
IS  - 
SP  - S102
EP  - S110
PY  - 2015/3//
T2  - DFRWS 2015 EuropeProceedings of the Second Annual DFRWS Europe
AU  - Casey, Eoghan
AU  - Back, Greg
AU  - Barnum, Sean
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2015.01.014
UR  - http://www.sciencedirect.com/science/article/pii/S1742287615000158
KW  - Digital forensics
KW  - Standard representation
KW  - Digital forensic ontology
KW  - Digital forensic XML
KW  - CybOX
KW  - DFXML
KW  - DFAX
AB  - Abstract
With the growing number of digital forensic tools and the increasing use of digital forensics in various contexts, including incident response and cyber threat intelligence, there is a pressing need for a widely accepted standard for representing and exchanging digital forensic information. Such a standard representation can support correlation between different data sources, enabling more effective and efficient querying and analysis of digital evidence. This work summarizes the strengths and weaknesses of existing schemas, and proposes the open-source CybOX schema as a foundation for storing and sharing digital forensic information. The suitability of CybOX for representing objects and relationships that are common in forensic investigations is demonstrated with examples involving digital evidence. The capability to represent provenance by leveraging CybOX is also demonstrated, including specifics of the tool used to process digital evidence and the resulting output. An example is provided of an ongoing project that uses CybOX to record the state of a system before and after an event in order to capture cause and effect information that can be useful for digital forensics. An additional open-source schema and associated ontology called Digital Forensic Analysis eXpression (DFAX) is proposed that provides a layer of domain specific information overlaid on CybOX. DFAX extends the capability of CybOX to represent more abstract forensic-relevant actions, including actions performed by subjects and by forensic examiners, which can be useful for sharing knowledge and supporting more advanced forensic analysis. DFAX can be used in combination with other existing schemas for representing identity information (CIQ), and location information (KML). This work also introduces and leverages initial steps of a Unified Cyber Ontology (UCO) effort to abstract and express concepts/constructs that are common across the cyber domain.
ER  - 

TY  - JOUR
T1  - A remote interactive non-repudiation multimedia-based m-learning system
JO  - Telematics and Informatics
VL  - 27
IS  - 4
SP  - 377
EP  - 393
PY  - 2010/11//
T2  - 
AU  - Adibi, Sasan
SN  - 0736-5853
DO  - http://dx.doi.org/10.1016/j.tele.2010.01.001
UR  - http://www.sciencedirect.com/science/article/pii/S073658531000002X
KW  - m-Leaning
KW  - e-Learning
KW  - Non-repudiation
KW  - Identity management
KW  - Multimedia interactive communication
AB  - One of the current challenges regarding distance learning systems, from a performance point of view, is the efficient and timely delivery of multimedia-enriched learning materials. Providing guaranteed Class of Service (CoS) and Quality of Service (QoS) are also challenging especially for remote sites and rural areas where Internet coverage tends to be limited. On a different note, another challenge is to track the audience accessing the learning materials and more importantly to monitor the true identity of the examination attendees. This paper aims to investigate both of these issues simultaneously, with an introduction of a non-repudiation system that provides a security mechanism, as well as maintaining certain QoS measures. This system not only authenticates the intended party, but also integrates a digital signature scheme accompanied with the transmitted multimedia-based information. The included digital signature prevents a later dispute from the involved parties that the communication ever took place or they ever took part in the communication.

Therefore this paper introduces and discusses a multimedia-enriched interactive non-repudiation system involved in a mobile-based learning (m-learning) environment. The performance of this system is considered and discussed in terms of network-centric parameters, including end-to-end delays, overhead, and bandwidth, using Labview 8.5 mobile-transmitter and mobile-receiver testbeds.
ER  - 

TY  - JOUR
T1  - The future information system : The quality and security debate — Part 2
JO  - Computer Fraud & Security Bulletin
VL  - 1994
IS  - 9
SP  - 7
EP  - 14
PY  - 1994/9//
T2  - 
AU  - Blatchford, CliveW
SN  - 0142-0496
DO  - http://dx.doi.org/10.1016/0142-0496(94)90184-8
UR  - http://www.sciencedirect.com/science/article/pii/0142049694901848
AB  - This is the concluding part of the article in which Clive Blatchford outlines some of the current considerations in the Open Systems Information Security Integration.
ER  - 

TY  - JOUR
T1  - How secure is the next generation of IP-based emergency services architecture?
JO  - International Journal of Critical Infrastructure Protection
VL  - 3
IS  - 1
SP  - 41
EP  - 50
PY  - 2010/5//
T2  - 
AU  - Tschofenig, Hannes
AU  - Arumaithurai, Mayutan
AU  - Schulzrinne, Henning
AU  - Aboba, Bernard
SN  - 1874-5482
DO  - http://dx.doi.org/10.1016/j.ijcip.2010.02.001
UR  - http://www.sciencedirect.com/science/article/pii/S1874548210000028
KW  - Emergency services architecture
KW  - Ecrit
AB  - For some location-based applications, such as emergency calling or roadside assistance, it appears that the identity of the requester is less important than accurate and trustworthy location information for accomplishing the main function. Accurate and genuine location is important for these applications to avoid misuse.

In this paper we point to some ongoing efforts regarding transition emergency service architectures that could introduce security vulnerabilities unless countermeasures are developed. Furthermore, we summarize the ongoing work in providing cryptographic assertions for location.

We argue that many of the currently proposed ideas are difficult to deploy and to operate. Additionally, when used without ensuring that the underlying assumptions are met these mechanisms do not provide any additional benefit, but costs.

We conclude this article with a suggestion on what the research community and industry should be investigating to avoid potential problems with IP-based emergency services.
ER  - 

TY  - JOUR
T1  - How to supervise and control I/S Security Officers and Auditors
JO  - Computers & Security
VL  - 6
IS  - 1
SP  - 17
EP  - 21
PY  - 1987/2//
T2  - 
AU  - Moulton, Rolf T.
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/0167-4048(87)90121-0
UR  - http://www.sciencedirect.com/science/article/pii/0167404887901210
KW  - IS security
KW  - IS audit
KW  - Computer audit
KW  - Access control
KW  - IS supervision
KW  - IS management
KW  - IS responsibilities
AB  - Information system Security Officers (ISSO) and Information System Auditors (ISA) can have similar capabilities with respect to asset access. Their supervision and control requires special consideration by management because they may literally have the keys to all of the organization's information and physical assets. The managers of each of those employees must be directly accountable for the individual's supervision. The same suggestions which are described for ISSO supervision apply to ISA supervision. Further, each is in a unique position to audit the activities of the other. Recommendations are provided to accomplish this “audit of the auditors.”
ER  - 

TY  - JOUR
T1  - Security views
JO  - Computers & Security
VL  - 22
IS  - 8
SP  - 654
EP  - 663
PY  - 2003/12//
T2  - 
AU  - Schultz, Eugene
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/S0167-4048(03)00002-6
UR  - http://www.sciencedirect.com/science/article/pii/S0167404803000026
ER  - 

TY  - JOUR
T1  - Teleservice requirements for management
JO  - Computer Networks and ISDN Systems
VL  - 26, Supplement 4
IS  - 
SP  - S163
EP  - S178
PY  - 1995///
T2  - 
AU  - da Cruz, Alina
AU  - Lewis, Dave
AU  - Crowcroft, Jon
SN  - 0169-7552
DO  - http://dx.doi.org/10.1016/0169-7552(95)90004-7
UR  - http://www.sciencedirect.com/science/article/pii/0169755295900047
KW  - Teleservice
KW  - OSI
KW  - QoS
AB  - Future advances in conferencing will make it feasible to build servers which collaborate to provide services similar to those of an airline booking system over the networks. Such servers should be able to support real-time queries and modifications to the information stored. We present the management requirements for the services that are providd by these servers.
ER  - 

TY  - JOUR
T1  - IMENSE: An e-infrastructure environment for patient specific multiscale data integration, modelling and clinical treatment
JO  - Journal of Computational Science
VL  - 3
IS  - 5
SP  - 314
EP  - 327
PY  - 2012/9//
T2  - Advanced Computing Solutions for Health Care and Medicine
AU  - Zasada, Stefan J.
AU  - Wang, Tao
AU  - Haidar, Ali
AU  - Liu, Enjie
AU  - Graf, Norbert
AU  - Clapworthy, Gordon
AU  - Manos, Steven
AU  - Coveney, Peter V.
SN  - 1877-7503
DO  - http://dx.doi.org/10.1016/j.jocs.2011.07.001
UR  - http://www.sciencedirect.com/science/article/pii/S1877750311000639
KW  - Clinical decision support
KW  - Electronic health records
KW  - Virtual Physiological Human
KW  - Medical data management
AB  - Secure access to patient data and analysis tools to run on that data will revolutionize the treatment of a wide range of diseases, by using advanced simulation techniques to underpin the clinical decision making process. To achieve these goals, suitable e-Science infrastructures are required to allow clinicians and researchers to trivially access data and launch simulations. In this paper we describe the open source Individualized MEdiciNe Simulation Environment (IMENSE), which provides a platform to securely manage clinical data, and to perform wide ranging analysis on that data, ultimately with the intention of enhancing clinical decision making with direct impact on patient health care. We motivate the design decisions taken in the development of the IMENSE system by considering the needs of researchers in the ContraCancrum project, which provides a paradigmatic case in which clinicians and researchers require coordinated access to data and simulation tools. We show how the modular nature of the IMENSE system makes it applicable to a wide range of biomedical computing scenarios, from within a single hospital to major international research projects.
ER  - 

TY  - JOUR
T1  - Tree-formed verification data for trusted platforms
JO  - Computers & Security
VL  - 32
IS  - 
SP  - 19
EP  - 35
PY  - 2013/2//
T2  - 
AU  - Schmidt, Andreas U.
AU  - Leicher, Andreas
AU  - Brett, Andreas
AU  - Shah, Yogendra
AU  - Cha, Inhyok
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/j.cose.2012.09.004
UR  - http://www.sciencedirect.com/science/article/pii/S016740481200140X
KW  - Trusted platform
KW  - Remote attestation
KW  - Hash tree
KW  - Measurement log
KW  - Verification data
KW  - Validation
AB  - The establishment of trust relationships to a computing platform relies on validation processes. Validation allows an external entity to build trust in the expected behaviour of the platform based on provided evidence of the platform's configuration. In a process like remote attestation, the ‘trusted’ platform submits verification data created during a start up process. These data consist of hardware-protected values of platform configuration registers, containing nested measurement values, e.g., hash values, of loaded or started components. Commonly, the register values are created in linear order by a hardware-secured operation. Fine-grained diagnosis of components, based on the linear order of verification data and associated measurement logs, is not optimal. We propose a method to use tree-formed verification data to validate a platform. Component measurement values represent leaves, and protected registers represent roots of a hash tree. We describe the basic mechanism of validating a platform using tree-formed measurement logs and root registers and show a logarithmic speed-up for the search of faults. Secure creation of a tree is possible using a limited number of hardware-protected registers and a single protected operation. In this way, the security of tree-formed verification data is maintained.
ER  - 

TY  - JOUR
T1  - A triage framework for digital forensics
JO  - Computer Fraud & Security
VL  - 2015
IS  - 3
SP  - 8
EP  - 18
PY  - 2015/3//
T2  - 
AU  - Bashir, Muhammad Shamraiz
AU  - Khan, Muhammad Naeem Ahmed
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(15)30018-X
UR  - http://www.sciencedirect.com/science/article/pii/S136137231530018X
AB  - A sharp increase in malware and cyber-attacks has been observed in recent years. Analysing cyber-attacks on the affected digital devices falls under the purview of digital forensics. The Internet is the main source of cyber and malware attacks, which sometimes result in serious damage to the digital assets. The motive behind digital crimes varies – such as online banking fraud, information stealing, denial of services, security breaches, deceptive output of running programs and data distortion.

Digital forensics analysts use a variety of tools for data acquisition, evidence analysis and presentation of malicious activities. This leads to device diversity posing serious challenges for investigators.

For this reason, some attack scenarios have to be examined repeatedly, which entails tremendous effort on the part of the examiners when analysing the evidence. To counter this problem, Muhammad Shamraiz Bashir and Muhammad Naeem Ahmed Khan at the Shaheed Zulfikar Ali Bhutto Institute of Science and Technology, Islamabad, Pakistan propose a novel triage framework for digital forensics.
ER  - 

TY  - JOUR
T1  - Traceability in Systems Engineering – Review of industrial practices, state-of-the-art technologies and new research solutions
JO  - Advanced Engineering Informatics
VL  - 26
IS  - 4
SP  - 924
EP  - 940
PY  - 2012/10//
T2  - EG-ICE 2011 + SI: Modern Concurrent Engineering
AU  - Königs, Simon Frederick
AU  - Beier, Grischa
AU  - Figge, Asmus
AU  - Stark, Rainer
SN  - 1474-0346
DO  - http://dx.doi.org/10.1016/j.aei.2012.08.002
UR  - http://www.sciencedirect.com/science/article/pii/S1474034612000766
KW  - Traceability
KW  - Systems Engineering
KW  - Morphological schema
KW  - Systems modelling
KW  - Nomenclature
KW  - Efficient trace link modelling
AB  - This article discusses issues and solutions regarding traceability for Systems Engineering projects. A review of industrial Systems Engineering practice is presented based on observations and studies that have been carried out at different original equipment manufacturers (OEMs). The studies reveal challenges in communication, data-transparency and data-consistency resulting among others from diverse and inhomogeneous toolsets. Existing traceability solutions, which are one possibility to address these challenges, struggle to achieve satisfactory cost/benefit ratios. Thus, further improvements of existing approaches with new concepts and ideas are required.

The article presents a morphological schema for traceability approaches. Its aim is to support attribute recombination to new research solutions and to overcome existing problems with naming conventions. The application of the schema is shown at the example of two novel approaches from Fraunhofer IPK and Daimler AG which are focusing on issues in trace link recording within Systems Engineering projects. The implementation, evaluation and functional comparison for both approaches are presented based on an exemplary automotive system. First results show a significant reduction of required trace recording efforts and good acceptance from engineers in the automotive industry. The developed schema and the two approaches contribute to the establishment of traceability for process improvement in future Systems Engineering projects.
ER  - 

TY  - JOUR
T1  - Development of Information Security Baselines for Healthcare Information Systems in New Zealand
JO  - Computers & Security
VL  - 21
IS  - 2
SP  - 172
EP  - 192
PY  - 2002/3/31/
T2  - 
AU  - Janczewski, Lech
AU  - Xinli Shi, Frank
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/S0167-4048(02)00212-2
UR  - http://www.sciencedirect.com/science/article/pii/S0167404802002122
KW  - healthcare information systems
KW  - electronic medical records
KW  - information privacy
KW  - information security baselines
KW  - security model
AB  - In 1996 New Zealand had introduced security standard AS/NZCS 4444 based on the British Standard BS 7799, which has recently been accepted as an international standard ISO 17799. This standard is very often referred to as the ‘baseline lane approach’ to the issue of managing information security. On the other hand the health information systems (HIS) are undergoing rapid development both in the number of installed systems as in the law and regulations governing HIS developments and deployment. The project was aimed at reviewing the AS/NZCS 4444 standard from the HIS requirements point of view. In this paper, we began with an overview of healthcare information systems (HIS) infrastructure in New Zealand and associated security issues around privacy and confidentiality, followed by a general review of the security baseline approach. We analyzed each clause of the AS/NZS 4444 with the information collected about technical and non-technical approaches to protecting HIS, consisting of a series of multi-case studies of healthcare organizations that collect, process, store and transmit electronic medical records. Finally, we proposed a new set of information security baselines based on the research to build an information security model for healthcare organizations.
ER  - 

TY  - JOUR
T1  - Security Views - Malware Update
JO  - Computers & Security
VL  - 25
IS  - 5
SP  - 317
EP  - 324
PY  - 2006/7//
T2  - 

SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/j.cose.2006.06.003
UR  - http://www.sciencedirect.com/science/article/pii/S0167404806000952
ER  - 

TY  - JOUR
T1  - A higher level of computer security through active policies
JO  - Computers & Security
VL  - 14
IS  - 2
SP  - 147
EP  - 157
PY  - 1995///
T2  - 
AU  - Abrams, Marshall D.
AU  - Moffett, Jonathan D.
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/0167-4048(95)97048-F
UR  - http://www.sciencedirect.com/science/article/pii/016740489597048F
KW  - Monitor
KW  - Active
KW  - Passive
KW  - Reference Monitor
KW  - Policies
KW  - Distributed systems
KW  - Assurance
AB  - This paper views the Reference Monitor in a new framework that makes it possible to generalize from passive to active monitors. It describes a major trend in the evolution of information systems security. The concepts are a practical reflection of real-world needs, expressed in a theoretical framework. The approach of employing active and passive policies provides a higher level of security than would otherwise be possible. The passive traditional Reference Monitor that interprets security policies and permits or prohibits access requests is supplemented by an active monitor to initiate behavior, such as taking positive actions to maintain integrity, taking recovery actions to restore situations after failures, and regularly monitoring the system. This extension to enforcement of various policies supports distributed systems architectures as the appropriate model for thinking about information technology (IT) security.
ER  - 

TY  - JOUR
T1  - Investigating around mainframes and other high-end systems: the revenge of big iron
JO  - Digital Investigation
VL  - 1
IS  - 2
SP  - 90
EP  - 93
PY  - 2004/6//
T2  - 
AU  - Pemble, Matthew
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2004.04.006
UR  - http://www.sciencedirect.com/science/article/pii/S1742287604000349
KW  - Mainframe
KW  - Audit
KW  - Investigation
KW  - Forensic
KW  - Log
AB  - Normal computer forensic tools and techniques are ineffective or inefficient when applied to mainframes or other very large systems. Incidents involving mainframes are likely to have a significant business impact, so preparation is essential, both of any specialist tools and of staff with mainframe experience. Proper design of system audit logs can also provide admissible material of evidential weight, without the requirement for forensic treatment.
ER  - 

TY  - JOUR
T1  - Privacy, trust and policy-making: Challenges and responses
JO  - Computer Law & Security Review
VL  - 25
IS  - 1
SP  - 69
EP  - 83
PY  - 2009///
T2  - 
AU  - Wright, David
AU  - Gutwirth, Serge
AU  - Friedewald, Michael
AU  - De Hert, Paul
AU  - Langheinrich, Marc
AU  - Moscibroda, Anna
SN  - 0267-3649
DO  - http://dx.doi.org/10.1016/j.clsr.2008.11.004
UR  - http://www.sciencedirect.com/science/article/pii/S0267364908001672
KW  - Ambient intelligence
KW  - Profiling
KW  - RFID
KW  - Data Protection
KW  - Privacy
KW  - Information Society Policies
AB  - The authors contend that the emerging ubiquitous Information Society (aka ambient intelligence, pervasive computing, ubiquitous networking and so on) will raise many privacy and trust issues that are context dependent. These issues will pose many challenges for policy-makers and stakeholders because people's notions of privacy and trust are different and shifting. People's attitudes towards privacy and protecting their personal data can vary significantly according to differing circumstances. In addition, notions of privacy and trust are changing over time. The authors provide numerous examples of the challenges facing policy-makers and identify some possible responses, but they see a need for improvements in the policy-making process in order to deal more effectively with varying contexts. They also identify some useful policy-making tools. They conclude that the broad brush policies of the past are not likely to be adequate to deal with the new challenges and that we are probably entering an era that will require development of “micro-policies”. While the new technologies will pose many challenges, perhaps the biggest challenge of all will be to ensure coherence of these micro-policies.
ER  - 

TY  - JOUR
T1  - NADIR: An automated system for detecting network intrusion and misuse
JO  - Computers & Security
VL  - 12
IS  - 3
SP  - 235
EP  - 248
PY  - 1993/5//
T2  - 
AU  - Hochberg, Judith
AU  - Jackson, Kathleen
AU  - Stallings, Cathy
AU  - McClary, J.F.
AU  - DuBois, David
AU  - Ford, Josephine
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/0167-4048(93)90110-Q
UR  - http://www.sciencedirect.com/science/article/pii/016740489390110Q
KW  - Computer security
KW  - intrusion detection
KW  - misuse detection
KW  - anomaly detection
KW  - expert system
AB  - This paper describes a misuse detection system for Los Alamos National Laboratory's Integrated Computing Network (ICN). This automated expert system, the Network Anomaly Detection and Intrusion Reporter (NADIR), streamlines and supplements the manual audit record review traditionally performed by security auditors. NADIR compares network activity, as summarized in weekly profiles of individual users and the ICN as a whole, against expert rules that define security policy and improper or suspicious behaviour. NADIR reports suspicious behaviour to security auditors and provides tools to aid in follow-up investigations. This paper describes analysis by NADIR of two types of ICN activity: user authentication and access control, and mass file storage. It highlights system design issues of data handling, exploiting existing auditing systems, and performing audit analysis at the network level.
ER  - 

TY  - JOUR
T1  - A security framework for a workflow-based grid development platform
JO  - Computer Standards & Interfaces
VL  - 32
IS  - 5–6
SP  - 230
EP  - 245
PY  - 2010/10//
T2  - Information and communications security, privacy and trust: Standards and Regulations
AU  - Vivas, José L.
AU  - Fernández-Gago, Carmen
AU  - Lopez, Javier
AU  - Benjumea, Andrés
SN  - 0920-5489
DO  - http://dx.doi.org/10.1016/j.csi.2009.04.001
UR  - http://www.sciencedirect.com/science/article/pii/S0920548909000270
KW  - Grid
KW  - Security framework
KW  - Security services
KW  - Virtual organizations
AB  - This paper describes the security framework that is to be developed for the generic grid platform created for the project GREDIA. This platform is composed of several components that need to be secured. The platform uses the OGSA standards, so that the security framework will follow GSI, the portion of Globus that implements security. Thus, we will show the security features that GSI already provides and we will outline which others need to be created or enhanced.
ER  - 

TY  - JOUR
T1  - A framework for post-event timeline reconstruction using neural networks
JO  - Digital Investigation
VL  - 4
IS  - 3–4
SP  - 146
EP  - 157
PY  - 2007/9//
Y2  - 2007/12//
T2  - 
AU  - Khan, M.N.A.
AU  - Chatwin, C.R.
AU  - Young, R.C.D.
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2007.11.001
UR  - http://www.sciencedirect.com/science/article/pii/S1742287607000837
KW  - Computer forensics
KW  - Digital investigation
KW  - Event reconstruction
KW  - Digital evidence
KW  - Digital forensic analysis
KW  - Neural networks
AB  - Post-event timeline reconstruction plays a critical role in forensic investigation and serves as a means of identifying evidence of the digital crime. We present an artificial neural networks based approach for post-event timeline reconstruction using the file system activities. A variety of digital forensic tools have been developed during the past two decades to assist computer forensic investigators undertaking digital timeline analysis, but most of the tools cannot handle large volumes of data efficiently. This paper looks at the effectiveness of employing neural network methodology for computer forensic analysis by preparing a timeline of relevant events occurring on a computing machine by tracing the previous file system activities. Our approach consists of monitoring the file system manipulations, capturing file system snapshots at discrete intervals of time to characterise the use of different software applications, and then using this captured data to train a neural network to recognise execution patterns of the application programs. The trained version of the network may then be used to generate a post-event timeline of a seized hard disk to verify the execution of different applications at different time intervals to assist in the identification of available evidence.
ER  - 

TY  - JOUR
T1  - Forensic analysis of logs: Modeling and verification
JO  - Knowledge-Based Systems
VL  - 20
IS  - 7
SP  - 671
EP  - 682
PY  - 2007/10//
T2  - Special Issue on Techniques to Produce Intelligent Secure Software
AU  - Saleh, Mohamed
AU  - Arasteh, Ali Reza
AU  - Sakha, Assaad
AU  - Debbabi, Mourad
SN  - 0950-7051
DO  - http://dx.doi.org/10.1016/j.knosys.2007.05.002
UR  - http://www.sciencedirect.com/science/article/pii/S0950705107000561
KW  - Forensic analysis
KW  - Log analysis
KW  - Formal methods
KW  - Model checking
KW  - Logging systems
AB  - Information stored in logs of a computer system is of crucial importance to gather forensic evidence of investigated actions or attacks against the system. Analysis of this information should be rigorous and credible, hence it lends itself to formal methods. We propose a model checking approach to the formalization of the forensic analysis of logs. The set of logs of a certain system is modeled as a tree whose labels are events extracted from the logs. In order to provide a structure to these events, we express each event as a term of a term algebra. The signature of the algebra is carefully chosen to include all relevant information necessary to conduct the analysis. Properties of the model are expressed as formulas of a logic having dynamic, linear, temporal, and modal characteristics. Moreover, we provide a tableau-based proof system for this logic upon which a model checking algorithm can be developed. In order to illustrate the proposed approach, the Windows auditing system is studied. The properties that we capture in our logic include invariant properties of a system, forensic hypotheses, and generic or specific attack signatures. Moreover, we discuss the admissibility of forensics hypotheses and the underlying verification issues.
ER  - 

TY  - JOUR
T1  - Developing a Windows NT security policy
JO  - Information Security Technical Report
VL  - 3
IS  - 3
SP  - 31
EP  - 43
PY  - 1998///
T2  - 
AU  - White, Ian
SN  - 1363-4127
DO  - http://dx.doi.org/10.1016/S1363-4127(98)80029-0
UR  - http://www.sciencedirect.com/science/article/pii/S1363412798800290
AB  - The choice of Windows NT as the strategic platform for the desktop and shared application server is becoming more widespread. A challenge for the security team is to provide guidelines on the minimum level of security controls that should be implemented on these systems. This article discusses some of the controls that might be expected to be included within such a Windows NT Security Policy (the policy).
ER  - 

TY  - JOUR
T1  - A survey of password mechanisms: Weaknesses and potential improvements. Part 1
JO  - Computers & Security
VL  - 8
IS  - 7
SP  - 587
EP  - 604
PY  - 1989/11//
T2  - 
AU  - Jobusch, David L.
AU  - Oldehoeft, Arthur E.
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/0167-4048(89)90051-5
UR  - http://www.sciencedirect.com/science/article/pii/0167404889900515
KW  - Authentication
KW  - Passwords
KW  - UNIX
KW  - Pass-phrases
AB  - While research continues on more sophisticated methods of authentication, password mechanisms remain the predominant method of identifying computer system users. In this paper, the goals of authentication are reviewed, and the strengths and vulnerabilities of password mechanisms are discussed. The 4.3 Berkeley Software Distribution (4.3BSD) version of UNIX is used as a case study throughout the paper. Several recommendations are presented for the improvement of password mechanisms. In particular, a simple extension of the UNIX password system is described that permits the use of pass-phrases.
ER  - 

TY  - JOUR
T1  - Guide for authors
JO  - Computers & Security
VL  - 12
IS  - 4
SP  - 419
EP  - 
PY  - 1993/6//
T2  - 

SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/0167-4048(93)90030-9
UR  - http://www.sciencedirect.com/science/article/pii/0167404893900309
ER  - 

TY  - JOUR
T1  - With MVS/ESA security labels towards B1
JO  - Computers & Security
VL  - 10
IS  - 4
SP  - 309
EP  - 324
PY  - 1991/6//
T2  - 
AU  - Paans, Ronald
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/0167-4048(91)90106-N
UR  - http://www.sciencedirect.com/science/article/pii/016740489190106N
AB  - IBM's MVS operating system at the level MVS/ESA 3.1.3 supplemented by RACF 1.9 has intentionally been designed to function as part of a trusted computer base with B1 security. Among the enhancements there is full support for security labels for both subjects and objects, allowing implementation of a mandatory access control policy and the use of MVS/ESA systems as multi-level security systems. The evolution of the MVS security mechanisms ultimately resulting in the introduction of security labels will be discussed from the view- point of the technical EDP-auditor.
ER  - 

TY  - JOUR
T1  - Network intrusion investigation – Preparation and challenges
JO  - Digital Investigation
VL  - 3
IS  - 3
SP  - 118
EP  - 126
PY  - 2006/9//
T2  - 
AU  - Johnston, Andy
AU  - Reust, Jessica
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2006.08.001
UR  - http://www.sciencedirect.com/science/article/pii/S1742287606000922
KW  - Intrusion investigation
KW  - Incident response
KW  - Network forensics
KW  - Digital forensic examination
KW  - Compromise of sensitive information
KW  - Forensic preparedness
AB  - As new legislation is written mandating notification of affected parties following the compromise of confidential data, reliable investigative procedures into unauthorized access of such data assume increasing importance. The increasing costs and penalties associated with exposure of sensitive data can be mitigated through forensic preparation and the ability to employ digital forensics. A case study of the compromise of several systems containing sensitive data is outlined, with particular attention given to the procedures followed during the initial response and their impact on the subsequent digital forensic examination. Practical problems and challenges that arise in intrusion investigations are discussed, along with solutions and methodologies to address these issues. This case study illustrates both the importance of evaluating the evidence analyzed and of corroborating findings and conclusions with multiple independent sources of evidence. An initial response that incorporates forensic procedures provides a solid foundation for a successful and thorough forensic examination.
ER  - 

TY  - JOUR
T1  - Ideal log setting for database forensics reconstruction
JO  - Digital Investigation
VL  - 12
IS  - 
SP  - 27
EP  - 40
PY  - 2015/3//
T2  - 
AU  - Adedayo, Oluwasola Mary
AU  - Olivier, Martin S.
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2014.12.002
UR  - http://www.sciencedirect.com/science/article/pii/S1742287614001200
KW  - Database management system
KW  - Database forensics
KW  - Digital forensics
KW  - Reconstruction
KW  - Ideal log setting
AB  - Abstract
The ability to reconstruct the data stored in a database at an earlier time is an important aspect of database forensics. Past research shows that the log file in a database can be useful for reconstruction. However, in many database systems there are various options that control which information is included in the logs. This paper introduces the notion of the ideal log setting necessary for an effective reconstruction process in database forensics. The paper provides a survey of the default logging preferences in some of the popular database management systems and identifies the information that a database log should contain in order to be useful for reconstruction. The challenges that may be encountered in storing the information as well as ways of overcoming the challenges are discussed. Possible logging preferences that may be considered as the ideal log setting for the popular database systems are also proposed. In addition, the paper relates the identified requirements to the three dimensions of reconstruction in database forensics and points out the additional requirements and/or techniques that may be required in the different dimensions.
ER  - 

TY  - JOUR
T1  - The establishment of a pilot telemedical information society
JO  - Future Generation Computer Systems
VL  - 15
IS  - 2
SP  - 133
EP  - 156
PY  - 1999/3/11/
T2  - 
AU  - Marsh, Andy
SN  - 0167-739X
DO  - http://dx.doi.org/10.1016/S0167-739X(98)00059-4
UR  - http://www.sciencedirect.com/science/article/pii/S0167739X98000594
KW  - Telemedicine
KW  - Information society
KW  - World Wide Web
AB  - National and international telecommunication infrastructures have been set up through Europe to facilitate the movement of information. One major benefactor of the improved communication infrastructures is the health care community. The accessibility and interoperability of medical information systems is one of the grand challenges for the 21st century. Within Europe current developments in the application of telemedicine are being defined in separate initiatives. There are a number of pilot actions concentrating on various aspects of telemedicine. These actions involving the introduction of new technology or working practices rarely fail for technology related problems. However, in order to fully assess the likely take-up of telemedical technologies it is vital that all the aspects including non-technical are also addressed.

This paper describes how a complete pilot telemedical information society will be set up which facilitates to support secure and standardised remote diagnosis, teleconsultations and advanced medical facilities in a number of sectors covering a crucial spectrum of those required to support a complete telemedical information society. This pilot testbed will then be assessed in the context of a European environment identifying a business plan for its extension to other member states therefore promoting a truly international telemedical information society for the 21st century.
ER  - 

TY  - JOUR
T1  - What does “forensically sound” really mean?
JO  - Digital Investigation
VL  - 4
IS  - 2
SP  - 49
EP  - 50
PY  - 2007/6//
T2  - 
AU  - Casey, Eoghan
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2007.05.001
UR  - http://www.sciencedirect.com/science/article/pii/S1742287607000333
ER  - 

TY  - JOUR
T1  - Leaving a trace
JO  - Infosecurity
VL  - 5
IS  - 6
SP  - 30
EP  - 35
PY  - 2008/9//
T2  - 
AU  - Gold, Steve
SN  - 1754-4548
DO  - http://dx.doi.org/10.1016/S1754-4548(08)70102-5
UR  - http://www.sciencedirect.com/science/article/pii/S1754454808701025
AB  - IT forensics is seen by many in the industry as something of a black art. But it's actually a highly professional discipline, with professional software to assist, Steve Gold discovers
ER  - 

TY  - JOUR
T1  - SoTE: Strategy of Triple-E on solving Trojan defense in Cyber-crime cases
JO  - Computer Law & Security Review
VL  - 26
IS  - 1
SP  - 52
EP  - 60
PY  - 2010/1//
T2  - 
AU  - Kao, Da-Yu
AU  - Wang, Shiuh-Jeng
AU  - Fu-Yuan Huang, Frank
SN  - 0267-3649
DO  - http://dx.doi.org/10.1016/j.clsr.2009.09.008
UR  - http://www.sciencedirect.com/science/article/pii/S0267364909001575
KW  - Cyber-crime
KW  - Cyber criminology
KW  - Digital evidence
KW  - Trojan defense
KW  - Triple-E strategy
AB  - Cyber activity has become an essential part of the general public's everyday life. The hacking threats of Cyber-crime are becoming more sophisticated as internet communication services are more popular. To further confirm the final finding of Cyber-crime, this study proposes three analytical tools to clarify the Cyber-crime issues by means of Ideal Log, M-N model and MDFA (Multi-faceted Digital Forensics Analysis) strategy, where Ideal Log is identified as a traceable element of digital evidence including four elements of IP Address, Timestamp, Digital Action, and Response Message. M-N model applies a formal method for collating and analyzing data sets of investigation-relevant logs in view of connected time with ISP logs. MDFA strategy attempts to outline the basic elements of Cyber-crime using new procedural investigative steps, and combining universal types of evidential information in terms of Evidence, Scene, Victim, and Suspect. After researchers figure out what has happened in Cyber-crime events, it will be easier to communicate with offenders, victims or related people. SoTE (Strategy of Triple-E) is discussed to observe Cyber-crime from the viewpoints of Education, Enforcement and Engineering. That approach is further analyzed from the fields of criminology, investigation and forensics. Each field has its different focus in dealing with diverse topics, such as: the policy of 6W1H (What, Which, When, Where, Who, Why, and How) questions, the procedure of MDFA strategy, the process of ideal Logs and M-N model. In addition, the case study and proposed suggestion of this paper are presented to counter Cyber-crime.
ER  - 

TY  - JOUR
T1  - Computer forensics challenges in responding to incidents in real-life settings
JO  - Computer Fraud & Security
VL  - 2007
IS  - 12
SP  - 12
EP  - 16
PY  - 2007/12//
T2  - 
AU  - Schultz, E. Eugene
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(07)70169-0
UR  - http://www.sciencedirect.com/science/article/pii/S1361372307701690
AB  - Dr Eugene Shultz looks at the practice of computer forensics during a real-life incident.
ER  - 

TY  - JOUR
T1  - An efficient technique for enhancing forensic capabilities of Ext2 file system
JO  - Digital Investigation
VL  - 4, Supplement
IS  - 
SP  - 55
EP  - 61
PY  - 2007/9//
T2  - 
AU  - Barik, Mridul Sankar
AU  - Gupta, Gaurav
AU  - Sinha, Shubhro
AU  - Mishra, Alok
AU  - Mazumdar, Chandan
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2007.06.007
UR  - http://www.sciencedirect.com/science/article/pii/S1742287607000515
KW  - Electronic documents
KW  - Modification access and creation
KW  - date and time stamps (MAC DTS)
KW  - Authentic date and time stamps (ADTS)
KW  - Computer Frauds and Cyber Crimes (CFCC)
KW  - Ext2 file system
KW  - Loadable Kernel Module (LKM)
AB  - As electronic documents become more important and valuable in the modern era, attempts are invariably made to take undue-advantage by tampering with them. Tampering with the modification, access and creation date and time stamps (MAC DTS) of digital documents pose a great threat and proves to be a major handicap in digital forensic investigation. Authentic date and time stamps (ADTS) can provide crucial evidence in linking crime to criminal in cases of Computer Fraud and Cyber Crimes (CFCC) through reliable time lining of digital evidence. But the ease with which the MAC DTS of stored digital documents can be changed raises some serious questions about the integrity and admissibility of digital evidence, potentially leading to rejection of acquired digital evidence in the court of Law. MAC DTS procedures of popular operating systems are inherently flawed and were created only for the sake of convenience and not necessarily keeping in mind the security and digital forensic aspects. This paper explores these issues in the context of the Ext2 file system and also proposes one solution to tackle such issues for the scenario where systems have preinstalled plug-ins in the form of Loadable Kernel Modules, which provide the capability to preserve ADTS.
ER  - 

TY  - JOUR
T1  - Lost Opportunities
JO  - Computer Fraud & Security
VL  - 2003
IS  - 1
SP  - 4
EP  - 6
PY  - 2003/1//
T2  - 
AU  - Wilding, Edward
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(03)01009-1
UR  - http://www.sciencedirect.com/science/article/pii/S1361372303010091
AB  - A few months ago, a solicitor approached me on behalf of his client. The client suspected that an employee had stolen high value proprietary software, source code, customer databases, methodologies, and marketing and business development plans. The employee had been suspended and placed on what is euphemistically referred to as ‘gardening leave’. He had been ‘gardening’ for six weeks. The solicitor requested advice on how an investigation should proceed in order to prove, or disprove, these suspicions. His client was facing lengthy legal proceedings on the grounds of unfair dismissal. Significantly, neither the solicitor nor his client had any evidence that any information had actually been stolen other than rumours, office gossip and other hearsay.
ER  - 

TY  - JOUR
T1  - BYOD security challenges: control and protect your most sensitive data
JO  - Network Security
VL  - 2012
IS  - 12
SP  - 5
EP  - 8
PY  - 2012/12//
T2  - 
AU  - Morrow, Bill
SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/S1353-4858(12)70111-3
UR  - http://www.sciencedirect.com/science/article/pii/S1353485812701113
AB  - Several new trends in information access are impacting organisations' ability to control and secure sensitive corporate data. The increase in web applications, cloud computing and Software as a Service (SaaS) offerings, and the Bring Your Own Device (BYOD) phenomenon, means that employees, business partners and customers are increasingly accessing information using a web browser on a device not owned or managed by the organisation.

The increase in web applications, cloud computing and Software as a Service (SaaS) offerings, and the Bring Your Own Device (BYOD) phenomenon are driving employees, business partners and customers to increasingly access information on devices are not managed by IT departments.

This has resulted in security implications for data leakage, data theft and regulatory compliance. To protect valuable information, organisations must stop making a distinction between devices in the corporate network and devices outside of it, argues Bill Morrow of Quarri Technologies.
ER  - 

TY  - JOUR
T1  - Case study: Network intrusion investigation – lessons in forensic preparation
JO  - Digital Investigation
VL  - 2
IS  - 4
SP  - 254
EP  - 260
PY  - 2005/12//
T2  - 
AU  - Casey, Eoghan
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2005.11.007
UR  - http://www.sciencedirect.com/science/article/pii/S1742287605000940
KW  - Forensic readiness
KW  - Forensic preparation
KW  - Incident response
KW  - Incident handling
KW  - Network forensics
KW  - Computer intrusion
KW  - Rootkit
KW  - Forensic computing
KW  - Tracking intruders
KW  - Attribution
AB  - Investigations of network security breaches are both complex and costly. Even a moderate amount of forensic preparation in an organization can mitigate the impact of a major incident and can enable the organization to obtain restitution. A case study of an intrusion is outlined in which the victim organization worked with law enforcement agencies to apprehend the perpetrator. This case study contains examples of challenges that can arise during this type of investigation, and discusses practical steps that an organization can take to prepare for a major incident. The overlapping roles of System Administrators, Incident Handlers, and Forensic Examiners in a network intrusion are explored, with an emphasis on the need for collaboration and proper evidence handling. This case study also shows how effective case management and methodical reconstruction of events can help create a more complete picture of the crime and help establish links between computer intruders and their illegal activities.
ER  - 

TY  - JOUR
T1  - The first 10 years of the Trojan Horse defence
JO  - Computer Fraud & Security
VL  - 2015
IS  - 1
SP  - 5
EP  - 13
PY  - 2015/1//
T2  - 
AU  - Bowles, Stephen
AU  - Hernandez-Castro, Julio
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(15)70005-9
UR  - http://www.sciencedirect.com/science/article/pii/S1361372315700059
AB  - Apprehended criminals throughout history have always attempted to put the blame on someone else, a strategy popularly known as a SODDI defence (Some Other Dude Did It). When this defence is used, the act of the crime (actus reus) and the guilty mind (mens rea) is blamed on another party. A Trojan Horse Defence (THD) is a type of modern SODDI defence, where the mens rea and actus reus are blamed on a piece of software, known as a trojan.1

It has now become common for people accused of some computer-related crime to claim that the responsibility lies with malware placed on their machine without their knowledge.

This so-called Trojan Horse Defence (THD) was first used a decade ago. In this article, Stephen Bowles and Julio Hernandez-Castro of the University of Kent undertake a timely retrospective with an in-depth and critical literature review plus a detailed look at the peculiarities of many court cases from around the world.
ER  - 

TY  - JOUR
T1  - File Marshal: Automatic extraction of peer-to-peer data
JO  - Digital Investigation
VL  - 4, Supplement
IS  - 
SP  - 43
EP  - 48
PY  - 2007/9//
T2  - 
AU  - Adelstein, Frank
AU  - Joyce, Robert A.
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2007.06.016
UR  - http://www.sciencedirect.com/science/article/pii/S1742287607000400
KW  - Peer-to-peer
KW  - P2P
KW  - Forensics
KW  - LimeWire
KW  - File sharing
AB  - Digital forensic investigators often find peer-to-peer, or file sharing, software present on the computers, or the images of the disks, that they examine. Investigators must first determine what P2P software is present and where the associated information is stored, retrieve the information from the appropriate directories, and then analyze the results. File Marshal is a tool that will automatically detect and analyze peer-to-peer client use on a disk. The tool automates what is currently a manual and labor intensive process. It will determine what clients currently are or have been installed on a machine, and then extracts per-user usage information, specifically a list of peer servers contacted, and files that were shared and downloaded. The tool was designed to perform its actions in a forensically sound way, including maintaining a detailed audit trail of all actions performed. File Marshal is extensible, using a configuration file to specify details about specific peer-to-peer clients (e.g., location of log files and registry keys indicating installation). This paper describes the general design and features of File Marshal, its current status, and the plans for continued development and release. When complete, File Marshal, a National Institute of Justice funded effort, will be disseminated to law enforcement at no cost.
ER  - 

TY  - JOUR
T1  - Protecting critical control systems
JO  - Network Security
VL  - 2012
IS  - 3
SP  - 7
EP  - 10
PY  - 2012/3//
T2  - 
AU  - Brewer, Ross
SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/S1353-4858(12)70044-2
UR  - http://www.sciencedirect.com/science/article/pii/S1353485812700442
AB  - With cyber-attacks continuing to grow in sophistication and frequency, both public and private organisations have been forced to change their outlook on cyber-security and re-examine their strategies when it comes to protecting their networks. System breaches are no longer considered unlikely, and the mindset has shifted to cyber-attacks being a matter of ‘when’ rather than ‘if’.

With cyber-attacks continuing to grow in sophistication and frequency, both public and private organisations have been forced to change their outlook on cyber-security and re-examine their strategies when it comes to protecting their networks. System breaches are no longer considered unlikely, and the mindset has shifted to cyber-attacks being a matter of ‘when’ rather than ‘if’.

Many critical infrastructure installations are controlled by Supervisory Control And Data Acquisition (SCADA) solutions that were never designed to be secure. However, with the correct strategic focus and resources applied, SCADA systems can be secured. Ross Brewer of LogRhythm suggests that a ‘protective monitoring’ approach can be tailored around networks that support high-value cyber-assets.
ER  - 

TY  - JOUR
T1  - The 1999 DARPA off-line intrusion detection evaluation
JO  - Computer Networks
VL  - 34
IS  - 4
SP  - 579
EP  - 595
PY  - 2000/10//
T2  - Recent Advances in Intrusion Detection Systems
AU  - Lippmann, Richard
AU  - Haines, Joshua W
AU  - Fried, David J
AU  - Korba, Jonathan
AU  - Das, Kumar
SN  - 1389-1286
DO  - http://dx.doi.org/10.1016/S1389-1286(00)00139-0
UR  - http://www.sciencedirect.com/science/article/pii/S1389128600001390
KW  - Intrusion detection
KW  - Evaluate
KW  - Attack
KW  - Audit
KW  - Test bed
AB  - Eight sites participated in the second Defense Advanced Research Projects Agency (DARPA) off-line intrusion detection evaluation in 1999. A test bed generated live background traffic similar to that on a government site containing hundreds of users on thousands of hosts. More than 200 instances of 58 attack types were launched against victim UNIX and Windows NT hosts in three weeks of training data and two weeks of test data. False-alarm rates were low (less than 10 per day). The best detection was provided by network-based systems for old probe and old denial-of-service (DoS) attacks and by host-based systems for Solaris user-to-root (U2R) attacks. The best overall performance would have been provided by a combined system that used both host- and network-based intrusion detection. Detection accuracy was poor for previously unseen, new, stealthy and Windows NT attacks. Ten of the 58 attack types were completely missed by all systems. Systems missed attacks because signatures for old attacks did not generalize to new attacks, auditing was not available on all hosts, and protocols and TCP services were not analyzed at all or to the depth required. Promising capabilities were demonstrated by host-based systems, anomaly detection systems and a system that performs forensic analysis on file system data.
ER  - 

TY  - JOUR
T1  - AlmaNebula: A Computer Forensics Framework for the Cloud
JO  - Procedia Computer Science
VL  - 19
IS  - 
SP  - 139
EP  - 146
PY  - 2013///
T2  - The 4th International Conference on Ambient Systems, Networks and Technologies (ANT 2013), the 3rd International Conference on Sustainable Energy Information Technology (SEIT-2013)
AU  - Federici, Corrado
SN  - 1877-0509
DO  - http://dx.doi.org/10.1016/j.procs.2013.06.023
UR  - http://www.sciencedirect.com/science/article/pii/S1877050913006315
KW  - Forensics as a service
KW  - Computer forensics framework
KW  - Commodity computing
KW  - Big data
KW  - Web scale
KW  - Distributed processing
AB  - Abstract
Scalability, fault tolerance and collaborative processing across possibly dispersed sites are key enablers of modern computer forensics applications, that must be able to elastically accommodate all kinds of digital investigations, without wasting resources or fail to deliver timely outcomes. Traditional tools running in a standalone or client- server setups may fall short when handling the multi terabyte scale of a case just above average or, conversely, lie mainly underutilized when dealing with few digital evidences. A new category of applications that leverage the opportunities offered by modern Cloud Computing (CC) platforms, where scalable computational power and storage capacity can be engaged and decommissioned on demand, allow one to conveniently master huge amounts of information that otherwise could be impossible to wield. This paper discusses the design goals, technical requirements and architecture of AlmaNebula, a conceptual framework for the analysis of digital evidences built on top of a Cloud infrastructure, which aims to embody the concept of “Forensics as a service”.
ER  - 

TY  - JOUR
T1  - Control systems/SCADA forensics, what's the difference?
JO  - Digital Investigation
VL  - 11
IS  - 3
SP  - 160
EP  - 174
PY  - 2014/9//
T2  - Special Issue: Embedded Forensics
AU  - van der Knijff, R.M.
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2014.06.007
UR  - http://www.sciencedirect.com/science/article/pii/S1742287614000814
KW  - Forensics
KW  - Control systems
KW  - SCADA
KW  - ICS
KW  - Cyber security
AB  - Abstract
Immature IT security, increasing network connectivity and unwavering media attention is causing an increase in the number of control system cyber security incidents. For forensic examinations in these environments, knowledge and skills are needed in the field of hardware, networks and data analysis. For forensic examiners, this paper is meant to be a crash course on control systems and their forensic opportunities, focussing on the differences compared to regular IT systems. Assistance from experienced field engineers during forensic acquisition of control systems seems inevitable in order to guarantee process safety, business continuity and examination efficiency. For people working in the control system community, this paper may be helpful to get an idea about specific forensic issues about which they would normally not bother, but may be crucial as soon as their systems are under attack or become part of a law enforcement investigation. For analysis of acquired data, existing tools for network security monitoring have useful functionality for forensic applications but are designed for real-time acquisition and often not directly usable for post-mortem analysis of acquired data in a forensically sound way. The constant and predictable way in which control systems normally behave makes forensic application of anomaly-based threat detection an interesting topic for further research.
ER  - 

TY  - JOUR
T1  - Network forensics based on fuzzy logic and expert system
JO  - Computer Communications
VL  - 32
IS  - 17
SP  - 1881
EP  - 1892
PY  - 2009/11/15/
T2  - 
AU  - Liao, Niandong
AU  - Tian, Shengfeng
AU  - Wang, Tinghua
SN  - 0140-3664
DO  - http://dx.doi.org/10.1016/j.comcom.2009.07.013
UR  - http://www.sciencedirect.com/science/article/pii/S0140366409002060
KW  - Network forensics
KW  - Expert system
KW  - Fuzzy logic
KW  - Intrusion detection system
KW  - Vulnerability scanning
AB  - Network forensics is a research area that finds the malicious users by collecting and analyzing the intrusion or infringement evidence of computer crimes such as hacking. In the past, network forensics was only used by means of investigation. However, nowadays, due to the sharp increase of network traffic, not all the information captured or recorded will be useful for analysis or evidence. The existing methods and tools for network forensics show only simple results. The administrators have difficulty in analyzing the state of the damaged system without expert knowledge. Therefore, we need an effective and automated analyzing system for network forensics. In this paper, we firstly guarantee the evidence reliability as far as possible by collecting different forensic information of detection sensors. Secondly, we propose an approach based on fuzzy logic and expert system for network forensics that can analyze computer crimes in network environment and make digital evidences automatically. At the end of the paper, the experimental comparison results between our proposed method and other popular methods are presented. Experimental results show that the system can classify most kinds of attack types (91.5% correct classification rate on average) and provide analyzable and comprehensible information for forensic experts.
ER  - 

TY  - JOUR
T1  - Hacking: Myth or menace, Part I
JO  - Computer Fraud & Security
VL  - 1998
IS  - 2
SP  - 16
EP  - 18
PY  - 1998/2//
T2  - 
AU  - Blatchford, Clive
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(00)87011-6
UR  - http://www.sciencedirect.com/science/article/pii/S1361372300870116
AB  - Is hacking about the disposition of the perpetrator or the situation of the insecure computer network. Improving the security of the perceived target may reduce the opportunity for the hacker, but can it ever eradicate the problem? Criminologists increasingly insist that the solution must lie in understanding the perspective of the hacker and address the problem at the political, social and cultural levels. The following two part study aims to offer insights about the actual threat before embarking on more legislation, policing or prohibitively expensive and probably, inadequate target hardening.
ER  - 

TY  - JOUR
T1  - An empirical study of industrial security-engineering practices
JO  - Journal of Systems and Software
VL  - 61
IS  - 3
SP  - 225
EP  - 232
PY  - 2002/4/1/
T2  - 
AU  - Vaughn Jr., Rayford B.
AU  - Henning, Ronda
AU  - Fox, Kevin
SN  - 0164-1212
DO  - http://dx.doi.org/10.1016/S0164-1212(01)00150-9
UR  - http://www.sciencedirect.com/science/article/pii/S0164121201001509
KW  - Computer security
KW  - Information assurance
KW  - Security-engineering
KW  - Risk assessment
AB  - This paper presents lessons learned and observations noted about the state of security-engineering practices by three information security practitioners with different perspectives – two in industry and one in academia. All authors have more than 20-years experience in this field and two were former members of the US National Computer Security Center during the early days of the Trusted Computer System Evaluation Criteria and the strong promotion of trusted operating systems that accompanied the release of that document. In the last 20 years, it has been argued that security-engineering practices have not kept pace with the escalating threats to information systems. Much has occurred since that time – new security paradigms, failure of evaluated products to emerge into common use, new systemic threats, and an increased awareness of the risk faced by information systems. This paper presents an empirical view of lessons learned in security-engineering, experiences in applying the trade, and observations made about the successes and failures of security practices and technology. This work was sponsored in part by NSF Grant.
ER  - 

TY  - JOUR
T1  - The growing need for on-scene triage of mobile devices
JO  - Digital Investigation
VL  - 6
IS  - 3–4
SP  - 112
EP  - 124
PY  - 2010/5//
T2  - Embedded Systems Forensics: Smart Phones, GPS Devices, and Gaming Consoles
AU  - Mislan, Richard P.
AU  - Casey, Eoghan
AU  - Kessler, Gary C.
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2010.03.001
UR  - http://www.sciencedirect.com/science/article/pii/S1742287610000149
KW  - Mobile device forensics
KW  - Cell phone forensics
KW  - On-scene triage inspection
KW  - Mobile device technician
AB  - The increasing number of mobile devices being submitted to Digital Forensic Laboratories (DFLs) is creating a backlog that can hinder investigations and negatively impact public safety and the criminal justice system. In a military context, delays in extracting intelligence from mobile devices can negatively impact troop and civilian safety as well as the overall mission. To address this problem, there is a need for more effective on-scene triage methods and tools to provide investigators with information in a timely manner, and to reduce the number of devices that are submitted to DFLs for analysis. Existing tools that are promoted for on-scene triage actually attempt to fulfill the needs of both on-scene triage and in-lab forensic examination in a single solution. On-scene triage has unique requirements because it is a precursor to and distinct from the forensic examination process, and may be performed by mobile device technicians rather than forensic analysts. This paper formalizes the on-scene triage process, placing it firmly in the overall forensic handling process and providing guidelines for standardization of on-scene triage. In addition, this paper outlines basic requirements for automated triage tools.
ER  - 

TY  - JOUR
T1  - FORZA – Digital forensics investigation framework that incorporate legal issues
JO  - Digital Investigation
VL  - 3, Supplement
IS  - 
SP  - 29
EP  - 36
PY  - 2006/9//
T2  - The Proceedings of the 6th Annual Digital Forensic Research Workshop (DFRWS '06)
AU  - Ieong, Ricci S.C.
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2006.06.004
UR  - http://www.sciencedirect.com/science/article/pii/S1742287606000661
KW  - Digital forensics investigation framework
KW  - Digital forensics
KW  - FORZA framework
KW  - Forensics principles
KW  - Zachman framework
KW  - Legal aspects
AB  - What is Digital Forensics? Mark Pollitt highlighted in DFRWS 2004 [Politt MM. Six blind men from Indostan. Digital forensics research workshop (DFRWS); 2004] that digital forensics is not an elephant, it is a process and not just one process, but a group of tasks and processes in investigation. In fact, many digital forensics investigation processes and tasks were defined on technical implementation details Investigation procedures developed by traditional forensics scientist focused on the procedures in handling the evidence, while those developed by the technologist focused on the technical details in capturing evidence. As a result, many digital forensics practitioners simply followed technical procedures and forget about the actual purpose and core concept of digital forensics investigation.

With all these technical details and complicated procedures, legal practitioners may have difficulties in applying or even understanding their processes and tasks in digital forensics investigations.

In order to break the technical barrier between information technologists, legal practitioners and investigators, and their corresponding tasks together, a technical-independent framework would be required.

In this paper, we first highlighted the fundamental principle of digital forensics investigations (Reconnaissance, Reliability and Relevancy). Based on this principle, we re-visit the investigation tasks and outlined eight different roles and their responsibilities in a digital forensics investigation.

For each role, we defined the sets of six key questions. They are the What (the data attributes), Why (the motivation), How (the procedures), Who (the people), Where (the location) and When (the time) questions. In fact, among all the investigation processes, there are six main questions that each practitioner would always ask.

By incorporating these sets of six questions into the Zachman's framework, a digital forensics investigation framework – FORZA is composed. We will further explain how this new framework can incorporate legal advisors and prosecutors into a bigger picture of digital forensics investigation framework.

Usability of this framework will be illustrated in a web hacking example.

Finally, the road map that interconnects the framework to automatically zero-knowledge data acquisition tools will be briefly described.
ER  - 

TY  - JOUR
T1  - MEGA: A tool for Mac OS X operating system and application forensics
JO  - Digital Investigation
VL  - 5, Supplement
IS  - 
SP  - S83
EP  - S90
PY  - 2008/9//
T2  - The Proceedings of the Eighth Annual DFRWS Conference
AU  - Joyce, Robert A.
AU  - Powers, Judson
AU  - Adelstein, Frank
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2008.05.011
UR  - http://www.sciencedirect.com/science/article/pii/S1742287608000376
KW  - Mac OS X
KW  - Computer forensics
KW  - Spotlight
KW  - Disk image analysis
KW  - Application analysis
AB  - Computer forensic tools for Apple Mac hardware have traditionally focused on low-level file system details. Mac OS X and common applications on the Mac platform provide an abundance of information about the user's activities in configuration files, caches, and logs. We are developing MEGA, an extensible tool suite for the analysis of files on Mac OS X disk images. MEGA provides simple access to Spotlight metadata maintained by the operating system, yielding efficient file content search and exposing metadata such as digital camera make and model. It can also help investigators to assess FileVault encrypted home directories. MEGA support tools are under development to interpret files written by common Mac OS applications such as Safari, Mail, and iTunes.
ER  - 

TY  - JOUR
T1  - Acquiring forensic evidence from infrastructure-as-a-service cloud computing: Exploring and evaluating tools, trust, and techniques
JO  - Digital Investigation
VL  - 9, Supplement
IS  - 
SP  - S90
EP  - S98
PY  - 2012/8//
T2  - The Proceedings of the Twelfth Annual DFRWS Conference12th Annual Digital Forensics Research Conference
AU  - Dykstra, Josiah
AU  - Sherman, Alan T.
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2012.05.001
UR  - http://www.sciencedirect.com/science/article/pii/S1742287612000266
KW  - Computer security
KW  - Cloud computing
KW  - Digital forensics
KW  - Cloud forensics
KW  - EnCase
KW  - FTK
KW  - Amazon EC2
AB  - We expose and explore technical and trust issues that arise in acquiring forensic evidence from infrastructure-as-a-service cloud computing and analyze some strategies for addressing these challenges. First, we create a model to show the layers of trust required in the cloud. Second, we present the overarching context for a cloud forensic exam and analyze choices available to an examiner. Third, we provide for the first time an evaluation of popular forensic acquisition tools including Guidance EnCase and AccesData Forensic Toolkit, and show that they can successfully return volatile and non-volatile data from the cloud. We explain, however, that with those techniques judge and jury must accept a great deal of trust in the authenticity and integrity of the data from many layers of the cloud model. In addition, we explore four other solutions for acquisition—Trusted Platform Modules, the management plane, forensics-as-a-service, and legal solutions, which assume less trust but require more cooperation from the cloud service provider. Our work lays a foundation for future development of new acquisition methods for the cloud that will be trustworthy and forensically sound. Our work also helps forensic examiners, law enforcement, and the court evaluate confidence in evidence from the cloud.
ER  - 

TY  - JOUR
T1  - Security and privacy issues in implantable medical devices: A comprehensive survey
JO  - Journal of Biomedical Informatics
VL  - 55
IS  - 
SP  - 272
EP  - 289
PY  - 2015/6//
T2  - 
AU  - Camara, Carmen
AU  - Peris-Lopez, Pedro
AU  - Tapiador, Juan E.
SN  - 1532-0464
DO  - http://dx.doi.org/10.1016/j.jbi.2015.04.007
UR  - http://www.sciencedirect.com/science/article/pii/S153204641500074X
KW  - Implantable medical devices
KW  - Security
KW  - Privacy
KW  - m-Health
KW  - Survey
AB  - Abstract
Bioengineering is a field in expansion. New technologies are appearing to provide a more efficient treatment of diseases or human deficiencies. Implantable Medical Devices (IMDs) constitute one example, these being devices with more computing, decision making and communication capabilities. Several research works in the computer security field have identified serious security and privacy risks in IMDs that could compromise the implant and even the health of the patient who carries it. This article surveys the main security goals for the next generation of IMDs and analyzes the most relevant protection mechanisms proposed so far. On the one hand, the security proposals must have into consideration the inherent constraints of these small and implanted devices: energy, storage and computing power. On the other hand, proposed solutions must achieve an adequate balance between the safety of the patient and the security level offered, with the battery lifetime being another critical parameter in the design phase.
ER  - 

TY  - JOUR
T1  - A study of dynamic meta-learning for failure prediction in large-scale systems
JO  - Journal of Parallel and Distributed Computing
VL  - 70
IS  - 6
SP  - 630
EP  - 643
PY  - 2010/6//
T2  - 
AU  - Lan, Zhiling
AU  - Gu, Jiexing
AU  - Zheng, Ziming
AU  - Thakur, Rajeev
AU  - Coghlan, Susan
SN  - 0743-7315
DO  - http://dx.doi.org/10.1016/j.jpdc.2010.03.003
UR  - http://www.sciencedirect.com/science/article/pii/S0743731510000377
KW  - Failure prediction
KW  - Meta-learning
KW  - Dynamic techniques
KW  - Large-scale systems
KW  - Blue Gene
AB  - Despite years of study on failure prediction, it remains an open problem, especially in large-scale systems composed of vast amount of components. In this paper, we present a dynamic meta-learning framework for failure prediction. It intends to not only provide reasonable prediction accuracy, but also be of practical use in realistic environments. Two key techniques are developed to address technical challenges of failure prediction. One is meta-learning to boost prediction accuracy by combining the benefits of multiple predictive techniques. The other is a dynamic approach to dynamically obtain failure patterns from a changing training set and to dynamically extract effective rules by actively monitoring prediction accuracy at runtime. We demonstrate the effectiveness and practical use of this framework by means of real system logs collected from the production Blue Gene/L systems at Argonne National Laboratory and San Diego Supercomputer Center. Our case studies indicate that the proposed mechanism can provide reasonable prediction accuracy by forecasting up to 82% of the failures, with a runtime overhead less than 1.0 min.
ER  - 

TY  - JOUR
T1  - Handling Distributed Denial-of-Service Attacks
JO  - Information Security Technical Report
VL  - 6
IS  - 3
SP  - 37
EP  - 44
PY  - 2001/9/1/
T2  - 
AU  - Janczewski, Lech J
SN  - 1363-4127
DO  - http://dx.doi.org/10.1016/S1363-4127(01)00306-5
UR  - http://www.sciencedirect.com/science/article/pii/S1363412701003065
ER  - 

TY  - JOUR
T1  - Security in network attached storage (NAS) for workgroups
JO  - Network Security
VL  - 2004
IS  - 4
SP  - 8
EP  - 12
PY  - 2004/4//
T2  - 
AU  - Edelson, Eve
SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/S1353-4858(04)00065-0
UR  - http://www.sciencedirect.com/science/article/pii/S1353485804000650
AB  - Network-attached storage (NAS) is a relatively simple and inexpensive way to serve files over a network in a cross-platform environment. NAS devices face the same security challenges as other network components. This article discusses how NAS fits into the world of IP storage, some security features present in (and missing from) NAS devices, and some security considerations in choosing a NAS.
ER  - 

TY  - JOUR
T1  - Continuous auditing technologies and models: A discussion
JO  - Computers & Security
VL  - 25
IS  - 5
SP  - 325
EP  - 331
PY  - 2006/7//
T2  - 
AU  - Flowerday, S.
AU  - Blundell, A.W.
AU  - Von Solms, R.
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/j.cose.2006.06.004
UR  - http://www.sciencedirect.com/science/article/pii/S0167404806000964
KW  - Continuous auditing
KW  - Real-time assurances
KW  - Information integrity
KW  - Internal controls
KW  - Technology-based prevention
AB  - In the age of real-time accounting and real-time communication current audit practices, while effective, often provide audit results long after fraud and/or errors have occurred. Real-time assurances can assist in preventing intentional or unintentional errors. This can best be achieved through continuous auditing which relies heavily on technology. These technologies are embedded within and are crucial to continuous auditing models.
ER  - 

TY  - JOUR
T1  - Algorithms for anomaly detection of traces in logs of process aware information systems
JO  - Information Systems
VL  - 38
IS  - 1
SP  - 33
EP  - 44
PY  - 2013/3//
T2  - 
AU  - Bezerra, Fábio
AU  - Wainer, Jacques
SN  - 0306-4379
DO  - http://dx.doi.org/10.1016/j.is.2012.04.004
UR  - http://www.sciencedirect.com/science/article/pii/S0306437912000567
KW  - Anomaly detection
KW  - Process mining
KW  - Process-aware systems
AB  - This paper discusses four algorithms for detecting anomalies in logs of process aware systems. One of the algorithms only marks as potential anomalies traces that are infrequent in the log. The other three algorithms: threshold, iterative and sampling are based on mining a process model from the log, or a subset of it. The algorithms were evaluated on a set of 1500 artificial logs, with different profiles on the number of anomalous traces and the number of times each anomalous traces was present in the log. The sampling algorithm proved to be the most effective solution. We also applied the algorithm to a real log, and compared the resulting detected anomalous traces with the ones detected by a different procedure that relies on manual choices.
ER  - 

TY  - JOUR
T1  - A framework for incident response management in the petroleum industry
JO  - International Journal of Critical Infrastructure Protection
VL  - 2
IS  - 1–2
SP  - 26
EP  - 37
PY  - 2009/5//
T2  - 
AU  - Jaatun, Martin Gilje
AU  - Albrechtsen, Eirik
AU  - Line, Maria B.
AU  - Tøndel, Inger Anne
AU  - Longva, Odd Helge
SN  - 1874-5482
DO  - http://dx.doi.org/10.1016/j.ijcip.2009.02.004
UR  - http://www.sciencedirect.com/science/article/pii/S1874548209000043
KW  - Incident response
KW  - Process control systems
KW  - Learning
KW  - Security culture
AB  - Incident response is the process of responding to and handling security-related incidents involving information and communications technology (ICT) infrastructure and data. Incident response has traditionally been reactive in nature, focusing mainly on technical issues. This paper presents the Incident Response Management (IRMA) method, which combines traditional incident response with proactive learning and socio-technical perspectives. The IRMA method is targeted at integrated operations within the petroleum industry, but it is also applicable to other industries that rely on process control systems.
ER  - 

TY  - JOUR
T1  - An Integrated Model for Patient Care and Clinical Trials (IMPACT) to support clinical research visit scheduling workflow for future learning health systems
JO  - Journal of Biomedical Informatics
VL  - 46
IS  - 4
SP  - 642
EP  - 652
PY  - 2013/8//
T2  - 
AU  - Weng, Chunhua
AU  - Li, Yu
AU  - Berhe, Solomon
AU  - Boland, Mary Regina
AU  - Gao, Junfeng
AU  - Hruby, Gregory W.
AU  - Steinman, Richard C.
AU  - Lopez-Jimenez, Carlos
AU  - Busacca, Linda
AU  - Hripcsak, George
AU  - Bakken, Suzanne
AU  - Bigger, J. Thomas
SN  - 1532-0464
DO  - http://dx.doi.org/10.1016/j.jbi.2013.05.001
UR  - http://www.sciencedirect.com/science/article/pii/S1532046413000531
KW  - Workflow
KW  - Software
KW  - Personnel staffing and scheduling
KW  - Health resources
KW  - Clinical research informatics
KW  - Learning health systems
AB  - Abstract
We describe a clinical research visit scheduling system that can potentially coordinate clinical research visits with patient care visits and increase efficiency at clinical sites where clinical and research activities occur simultaneously. Participatory Design methods were applied to support requirements engineering and to create this software called Integrated Model for Patient Care and Clinical Trials (IMPACT). Using a multi-user constraint satisfaction and resource optimization algorithm, IMPACT automatically synthesizes temporal availability of various research resources and recommends the optimal dates and times for pending research visits. We conducted scenario-based evaluations with 10 clinical research coordinators (CRCs) from diverse clinical research settings to assess the usefulness, feasibility, and user acceptance of IMPACT. We obtained qualitative feedback using semi-structured interviews with the CRCs. Most CRCs acknowledged the usefulness of IMPACT features. Support for collaboration within research teams and interoperability with electronic health records and clinical trial management systems were highly requested features. Overall, IMPACT received satisfactory user acceptance and proves to be potentially useful for a variety of clinical research settings. Our future work includes comparing the effectiveness of IMPACT with that of existing scheduling solutions on the market and conducting field tests to formally assess user adoption.
ER  - 

TY  - JOUR
T1  - Wired and wireless intrusion detection system: Classifications, good characteristics and state-of-the-art
JO  - Computer Standards & Interfaces
VL  - 28
IS  - 6
SP  - 670
EP  - 694
PY  - 2006/9//
T2  - 
AU  - Sobh, Tarek S.
SN  - 0920-5489
DO  - http://dx.doi.org/10.1016/j.csi.2005.07.002
UR  - http://www.sciencedirect.com/science/article/pii/S092054890500098X
KW  - Wireless network
KW  - Network-based security
KW  - Host-based security
KW  - Intrusion detection system
KW  - Intrusion prevention systems
KW  - Sniffering
AB  - In computer and network security, standard approaches to intrusion detection and response attempt to detect and prevent individual attacks. Intrusion Detection System (IDS) and intrusion prevention systems (IPS) are real-time software for risk assessment by monitoring for suspicious activity at the network and system layer. Software scanner allows network administrator to audit the network for vulnerabilities and thus securing potential holes before attackers take advantage of them.

In this paper we try to define the intruder, types of intruders, detection behaviors, detection approaches and detection techniques. This paper presents a structural approach to the IDS by introducing a classification of IDS. It presents important features, advantages and disadvantages of each detection approach and the corresponding detection techniques. Furthermore, this paper introduces the wireless intrusion protection systems.

The goal of this paper is to place some characteristics of good IDS and examine the positioning of intrusion prevention as part of an overall layered security strategy and a review of evaluation criteria for identifying and selecting IDS and IPS. With this, we hope to introduce a good characteristic in order to improve the capabilities for early detection of distributed attacks in the preliminary phases against infrastructure and take a full spectrum of manual and automatic response actions against the source of attacks.
ER  - 

TY  - JOUR
T1  - Report highlights
JO  - Information Security Technical Report
VL  - 3
IS  - 4
SP  - 3
EP  - 14
PY  - 1998///
T2  - 

SN  - 1363-4127
DO  - http://dx.doi.org/10.1016/S1363-4127(98)80034-4
UR  - http://www.sciencedirect.com/science/article/pii/S1363412798800344
ER  - 

TY  - JOUR
T1  - CareWeb™, a web-based medical record for an integrated health care delivery system
JO  - International Journal of Medical Informatics
VL  - 54
IS  - 1
SP  - 1
EP  - 8
PY  - 1999/4//
T2  - 
AU  - Halamka, John D.
AU  - Osterland, Carsten
AU  - Safran, Charles
SN  - 1386-5056
DO  - http://dx.doi.org/10.1016/S1386-5056(98)00095-1
UR  - http://www.sciencedirect.com/science/article/pii/S1386505698000951
KW  - Patient records
KW  - Internet
KW  - Security
AB  - With the advent of Integrated Health care Delivery Systems, medical records are increasingly distributed across multiple institutions. Timely access to these medical records is a critical need for health care providers. The CareWeb™ project provides an architecture for World Wide Web-based retrieval of electronic medical records from heterogeneous data sources. Using Health Level 7 (HL7), web technologies and readily available software components, we consolidated the electronic records of Boston's Beth Israel and Deaconess Hospitals. We report on the creation of CareWeb™ (freya.bidmc.harvard.edu/careweb.htm) and propose it as a means to electronically link Integrated Health care Delivery Systems and geographically distant information resources.
ER  - 

TY  - JOUR
T1  - An extensible analysable system model
JO  - Information Security Technical Report
VL  - 13
IS  - 4
SP  - 235
EP  - 246
PY  - 2008/11//
T2  - 
AU  - Probst, Christian W.
AU  - Hansen, René Rydhof
SN  - 1363-4127
DO  - http://dx.doi.org/10.1016/j.istr.2008.10.012
UR  - http://www.sciencedirect.com/science/article/pii/S1363412708000502
AB  - Analysing real-world systems for vulnerabilities with respect to security and safety threats is a difficult undertaking, not least due to a lack of availability of formalisations for those systems. While both formalisations and analyses can be found for artificial systems such as software, this does not hold for real physical systems. Approaches such as threat modelling try to target the formalisation of the real-world domain, but still are far from the rigid techniques available in security research. Many currently available approaches to assurance of critical infrastructure security are based on (quite successful) ad-hoc techniques. We believe they can be significantly improved beyond the state-of-the-art by pairing them with static analyses techniques.

In this paper we present an approach to both formalising those real-world systems, as well as providing an underlying semantics, which allows for easy development of analyses for the abstracted systems. We briefly present one application of our approach, namely the analysis of systems for potential insider threats.
ER  - 

TY  - JOUR
T1  - NetHost-sensor: Monitoring a target host's application via system calls
JO  - Information Security Technical Report
VL  - 11
IS  - 4
SP  - 166
EP  - 175
PY  - 2006///
T2  - 
AU  - Abimbola, A.A.
AU  - Munoz, J.M.
AU  - Buchanan, W.J.
SN  - 1363-4127
DO  - http://dx.doi.org/10.1016/j.istr.2006.10.003
UR  - http://www.sciencedirect.com/science/article/pii/S1363412706000550
KW  - Intrusion detection
KW  - Network
KW  - Host
KW  - Application security
KW  - Dynamic link libraries
KW  - System calls
AB  - Intrusion detection has emerged as an important approach to network, host and application security. Network security includes analysing network packet payload and other inert network packet profiles for intrusive trends; whereas, host security may employ system logs for intrusion detection. In this paper, we contribute to the research community by tackling application security and attempt to detect intrusion via differentiating normal and abnormal application behaviour. A method for anomaly intrusion detection for applications is proposed based on deterministic system call traces derived from a monitored target application's dynamic link libraries (DLLs). We isolate associated DLLs of a monitored target application; log system call traces of the application in real time and use heuristic method to detect intrusion before the application is fully compromised. Our investigative research experiment methodology and set-up are reported, alongside our experimental procedure and results that show our research effort is effective and efficient, and can be used in practice to monitor a target application in real time.
ER  - 

TY  - JOUR
T1  - Dropbox analysis: Data remnants on user machines
JO  - Digital Investigation
VL  - 10
IS  - 1
SP  - 3
EP  - 18
PY  - 2013/6//
T2  - 
AU  - Quick, Darren
AU  - Choo, Kim-Kwang Raymond
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2013.02.003
UR  - http://www.sciencedirect.com/science/article/pii/S174228761300011X
KW  - Cloud storage
KW  - Cloud forensics
KW  - Dropbox analysis
KW  - Computer forensics
KW  - Digital forensics
KW  - Mobile forensics
AB  - Cloud storage has been identified as an emerging challenge to digital forensic researchers and practitioners in a range of literature. There are various types of cloud storage services with each type having a potentially different use in criminal activity. One area of difficulty is the identification, acquisition, and preservation of evidential data when disparate services can be utilised by criminals. Not knowing if a cloud service is being used, or which cloud service, can potentially impede an investigation. It would take additional time to contact all service providers to determine if data is being stored within their cloud service. Using Dropbox™ as a case study, research was undertaken to determine the data remnants on a Windows 7 computer and an Apple iPhone 3G when a user undertakes a variety of methods to store, upload, and access data in the cloud. By determining the data remnants on client devices, we contribute to a better understanding of the types of terrestrial artifacts that are likely to remain for digital forensics practitioners and examiners. Potential information sources identified during the research include client software files, prefetch files, link files, network traffic capture, and memory captures, with many data remnants available subsequent to the use of Dropbox by a user.
ER  - 

TY  - JOUR
T1  - Logging, auditing and filtering for Internet electronic commerce
JO  - Computer Fraud & Security
VL  - 1997
IS  - 8
SP  - 11
EP  - 16
PY  - 1997/8//
T2  - 
AU  - Cresson Wood, Charles
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(97)89846-6
UR  - http://www.sciencedirect.com/science/article/pii/S1361372397898466
AB  - This article provides a survey of the features and capabilities of commercial security products specifically designed for conducting business on the Internet. This article focuses on three not-so-glamorous but nonetheless essential, behind-the-scenes security activities. The author has no marketing, referral, or commission relationship with any of the vendors mentioned below.
ER  - 

TY  - JOUR
T1  - A latent class modeling approach to detect network intrusion
JO  - Computer Communications
VL  - 30
IS  - 1
SP  - 93
EP  - 100
PY  - 2006/12/15/
T2  - 
AU  - Wang, Yun
AU  - Kim, Inyoung
AU  - Mbateng, Gaston
AU  - Ho, Shih-Yieh
SN  - 0140-3664
DO  - http://dx.doi.org/10.1016/j.comcom.2006.07.018
UR  - http://www.sciencedirect.com/science/article/pii/S0140366406002891
KW  - Intrusion detection
KW  - Machine learning
KW  - Classification
KW  - Latent class model
KW  - Computer security
AB  - This study presents a latent class modeling approach to examine network traffic data when labeled abnormal events are absent in training data, or such events are insufficient to fit a conventional regression model. Using six anomaly-associated risk factors identified from previous studies, the latent class model based on an unlabeled sample yielded acceptable classification results compared with a logistic regression model based on a labeled sample (correctly classified: 0.95 vs. 0.98, sensitivity: 0.99 vs. 0.99, and specificity: 0.77 vs. 0.97). The study demonstrates a great potency for using the latent class modeling technique to analyze network traffic data.
ER  - 

TY  - JOUR
T1  - Design and implementation of a tool for the automatic construction of hypertexts for information retrieval
JO  - Information Processing & Management
VL  - 32
IS  - 4
SP  - 459
EP  - 476
PY  - 1996/7//
T2  - 
AU  - Agosti, Maristella
AU  - Crestani, Fabio
AU  - Melucci, Massimo
SN  - 0306-4573
DO  - http://dx.doi.org/10.1016/0306-4573(95)00075-5
UR  - http://www.sciencedirect.com/science/article/pii/0306457395000755
AB  - The paper describes the design and implementation of TACHIR, a tool for the automatic construction of hypertexts for Information Retrieval. Through the use of an authoring methodology employing a set of well known Information Retrieval techniques, TACHIR automatically builds up a hypertext from a document collection. The structure of the hypertext reflects a three level conceptual model that has proved to be quite effective for Information Retrieval. Using this model it is possible to navigate among documents, index terms, and concepts using automatically determined links. The hypertext is implemented using the HTML hypertext mark up language, the mark up language of the World Wide Web project. It can be distributed on different sites and different machines over the Internet, and it can be navigated using any of the interfaces developed in the framework World Wide Web project, for example NetScape.
ER  - 

TY  - JOUR
T1  - Assisting the design of a groupware system
JO  - The Journal of Logic and Algebraic Programming
VL  - 78
IS  - 4
SP  - 191
EP  - 232
PY  - 2009/4//
T2  - IFIP WG1.8 Workshop on Applying Concurrency Research in Industry
AU  - ter Beek, Maurice H.
AU  - Gnesi, Stefania
AU  - Latella, Diego
AU  - Massink, Mieke
AU  - Sebastianis, Maurizio
AU  - Trentanni, Gianluca
SN  - 1567-8326
DO  - http://dx.doi.org/10.1016/j.jlap.2008.11.004
UR  - http://www.sciencedirect.com/science/article/pii/S1567832608000945
KW  - Groupware
KW  - Concurrency
KW  - Formal methods
KW  - Verification
KW  - Model checking
AB  - Product Data Management (PDM) systems support the product/document management of design processes such as those typically used in the manufacturing industry. They allow enterprises to capture, organise, automate and share engineering information in an efficient way. The efficient handling of queries on product information and the uploading and downloading of families of related files for modification by designers are essential aspects of such systems. The efficiency of the system as perceived by its clients depends on its correct functioning, but also for a significant part on its performance aspects. In this article, we apply both qualitative and stochastic model-checking techniques to evaluate various usability and performance aspects of the thinkteam PDM system, and of several proposed extensions, thereby assisting the design phase of an industrial groupware system.
ER  - 


