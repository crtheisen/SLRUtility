TY  - JOUR
T1  - Towards a forensic-aware database solution: Using a secured database replication protocol and transaction management for digital investigations
JO  - Digital Investigation
VL  - 11
IS  - 4
SP  - 336
EP  - 348
PY  - 2014/12//
T2  - 
AU  - Frühwirt, Peter
AU  - Kieseberg, Peter
AU  - Krombholz, Katharina
AU  - Weippl, Edgar
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2014.09.003
UR  - http://www.sciencedirect.com/science/article/pii/S1742287614001078
KW  - MySQL
KW  - InnoDB
KW  - Digital forensics
KW  - Databases
KW  - Data tempering
KW  - Replication
KW  - Transaction management
AB  - Abstract
Databases contain an enormous amount of structured data. While the use of forensic analysis on the file system level for creating (partial) timelines, recovering deleted data and revealing concealed activities is very popular and multiple forensic toolsets exist, the systematic analysis of database management systems has only recently begun. Databases contain a large amount of temporary data files and metadata which are used by internal mechanisms. These data structures are maintained in order to ensure transaction authenticity, to perform rollbacks, or to set back the database to a predefined earlier state in case of e.g. an inconsistent state or a hardware failure. However, these data structures are intended to be used by the internal system methods only and are in general not human-readable.

In this work we present a novel approach for a forensic-aware database management system using transaction- and replication sources. We use these internal data structures as a vital baseline to reconstruct evidence during a forensic investigation. The overall benefit of our method is that no additional logs (such as administrator logs) are needed. Furthermore, our approach is invariant to retroactive malicious modifications by an attacker. This assures the authenticity of the evidence and strengthens the chain of custody. To evaluate our approach, we present a formal description, a prototype implementation in MySQL alongside and a comprehensive security evaluation with respect to the most relevant attack scenarios.
ER  - 

TY  - JOUR
T1  - Cloud forensics: Technical challenges, solutions and comparative analysis
JO  - Digital Investigation
VL  - 13
IS  - 0
SP  - 38
EP  - 57
PY  - 2015/6//
T2  - 
AU  - Pichan, Ameer
AU  - Lazarescu, Mihai
AU  - Soh, Sie Teng
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2015.03.002
UR  - http://www.sciencedirect.com/science/article/pii/S1742287615000407
KW  - Cloud computing
KW  - Cloud forensics
KW  - Cloud service provider
KW  - Cloud customer
KW  - Digital forensics
KW  - Digital evidence
KW  - Service level agreement
KW  - Amazon EC2
AB  - Abstract
Cloud computing is arguably one of the most significant advances in information technology (IT) services today. Several cloud service providers (CSPs) have offered services that have produced various transformative changes in computing activities and presented numerous promising technological and economic opportunities. However, many cloud customers remain reluctant to move their IT needs to the cloud, mainly due to their concerns on cloud security and the threat of the unknown. The CSPs indirectly escalate their concerns by not letting customers see what is behind virtual wall of their clouds that, among others, hinders digital investigations. In addition, jurisdiction, data duplication and multi-tenancy in cloud platform add to the challenge of locating, identifying and separating the suspected or compromised targets for digital forensics. Unfortunately, the existing approaches to evidence collection and recovery in a non-cloud (traditional) system are not practical as they rely on unrestricted access to the relevant system and user data; something that is not available in the cloud due its decentralized data processing. In this paper we systematically survey the forensic challenges in cloud computing and analyze their most recent solutions and developments. In particular, unlike the existing surveys on the topic, we describe the issues in cloud computing using the phases of traditional digital forensics as the base. For each phase of the digital forensic process, we have included a list of challenges and analysis of their possible solutions. Our description helps identifying the differences between the problems and solutions for non-cloud and cloud digital forensics. Further, the presentation is expected to help the investigators better understand the problems in cloud environment. More importantly, the paper also includes most recent development in cloud forensics produced by researchers, National Institute of Standards and Technology and Amazon.
ER  - 

TY  - JOUR
T1  - Secure Audit Log Management
JO  - Procedia Computer Science
VL  - 22
IS  - 0
SP  - 1249
EP  - 1258
PY  - 2013///
T2  - 17th International Conference in Knowledge Based and Intelligent Information and Engineering Systems - KES2013
AU  - Söderström, Olof
AU  - Moradian, Esmiralda
SN  - 1877-0509
DO  - http://dx.doi.org/10.1016/j.procs.2013.09.212
UR  - http://www.sciencedirect.com/science/article/pii/S1877050913010053
KW  - Secure Log Management
KW  - Log Analysis
KW  - Log Server
KW  - Audit Log Event
AB  - Abstract
Log management and analysis is a vital part of organization's network management and system administration. Logs indicate current status of the system and contain information that refers to different security events, which occur within the system. Logs are used for different purposes, such as recording user activities, track authentication attempts, and other security events. Due to increasing number of threats against networks and systems, the number of security logs increases. However, many organizations that work in a distributed environment face following problems: log generation and storage, log protection, and log analysis. Moreover, ensuring that security, system and network administrators analyze log data in an effective way is another issue. In this research, we propose an approach for receiving, storing and administrating audit log events. Furthermore, we present a solution design that in a secure way allows organizations in distributed environments to send audit log transactions from different local networks to one centralized server.
ER  - 

TY  - CHAP
AU  - Tan, Alan Y.S.
AU  - Ko, Ryan K.L.
AU  - Holmes, Geoff
AU  - Rogers, Bill
T1  - Chapter 8 - Provenance for cloud data accountability
A2  - Choo, Ryan KoKim-Kwang Raymond 
BT  - The Cloud Security Ecosystem
PB  - Syngress
CY  - Boston
PY  - 2015///
SP  - 171
EP  - 185
SN  - 978-0-12-801595-7
DO  - http://dx.doi.org/10.1016/B978-0-12-801595-7.00008-2
UR  - http://www.sciencedirect.com/science/article/pii/B9780128015957000082
KW  - Data Provenance
KW  - Provenance Model
KW  - Provenance Reconstruction
KW  - Cloud Computing
KW  - Cloud Data Accountability
AB  - Abstract
Although cloud adoption has increased in recent years, it is still hampered by the lack of means for data accountability. Data provenance, the information that describes the historical events surrounding the datum, can potentially address the data accountability issue in the cloud. While provenance research has produced tools that can actively collect data provenance in a cloud environment, these tools incur a substantial amount of overhead in terms of time and storage. This overhead, along with other disadvantages, render these tools untenable as a long-term solution.

In this chapter, we introduce the notion of provenance reconstruction as an alternative solution for active provenance collection. The idea is to mine pieces of information from readily available information sources in the relevant cloud environment and piece them together to form data provenance chains. However, to be able to piece the extracted information together, we need to know how each of these information pieces can be fitted together. We present a data provenance model that defines a list of provenance elements a data provenance for cloud data accountability should have, and a set of rules that defines the behavior of these elements. Through the data provenance model, we can then categorize the extracted information pieces into the different elements. Using the set of rules defined in the model, we have a rough idea of how these individual provenance elements can be fitted together to form provenance chains that describe the historical events related to data. We briefly discuss how the model can be used in our architecture and identify a list of challenges that needs to be addressed at the end.
ER  - 

TY  - JOUR
T1  - Performance analysis of Bayesian networks and neural networks in classification of file system activities
JO  - Computers & Security
VL  - 31
IS  - 4
SP  - 391
EP  - 401
PY  - 2012/6//
T2  - 
AU  - Khan, Muhammad Naeem Ahmed
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/j.cose.2012.03.003
UR  - http://www.sciencedirect.com/science/article/pii/S0167404812000533
KW  - Digital forensics
KW  - Computer forensic analysis
KW  - Digital evidence
KW  - Neural networks
KW  - Bayesian learning
KW  - Bayesian decision theory
AB  - Precise comprehension of a file system state at any given time is vital for performing digital forensic analyses. To uncover evidence of the digital crime, the logical representation of file system activities helps reconstruct post-event timeline of the unauthorized or malicious accesses made on a system. This paper describes a comparative performance analysis of the Bayesian networks and neural networks techniques to classify the state of file system activities in terms of execution of applications based on the pattern of manipulation of specific files during certain period of time. In particular, this paper discusses the construction of a Bayesian networks and neural networks from the predetermined knowledge of the manipulation of file system artifacts and their corresponding metadata information by a set of software applications. The variability amongst the execution patterns of various applications indicate that the Bayesian network-based model is a more appropriate tool as compared to neural networks because of its ability to learn and detect patterns even from an incomplete dataset. The focus of this paper is to highlight intrinsic significance of the learning approach of Bayesian network methodology in comparison to the techniques used for supervised learning in ordinary neural networks. The paper also highlights the efficacy of Bayesian network technique to proficiently handle large volumes of datasets.
ER  - 

TY  - JOUR
T1  - Forensic discovery auditing of digital evidence containers
JO  - Digital Investigation
VL  - 4
IS  - 2
SP  - 88
EP  - 97
PY  - 2007/6//
T2  - 
AU  - Richard III, Golden G.
AU  - Roussev, Vassil
AU  - Marziale, Lodovico
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2007.04.002
UR  - http://www.sciencedirect.com/science/article/pii/S1742287607000291
KW  - Digital forensics
KW  - Operating systems internals
KW  - Filesystems
KW  - Digital evidence containers Auditing
AB  - Current digital forensics methods capture, preserve, and analyze digital evidence in general-purpose electronic containers (typically, plain files) with no dedicated support to help establish that the evidence has been properly handled. Auditing of a digital investigation, from identification and seizure of evidence through duplication and investigation is, essentially, ad hoc, recorded in separate log files or in an investigator's case notebook. Auditing performed in this fashion is bound to be incomplete, because different tools provide widely disparate amounts of auditing information – including none at all – and there is ample room for human error. The latter is a particularly pressing concern given the fast growth of the size of forensic targets.

Recently, there has been a serious community effort to develop an open standard for specialized digital evidence containers (DECs). A DEC differs from a general purpose container in that, in addition to the actual evidence, it bundles arbitrary metadata associated with it, such as logs and notes, and provides the basic means to detect evidence-tampering through digital signatures. Current approaches consist of defining a container format and providing a specialized library that can be used to manipulate it. While a big step in the right direction, this approach has some non-trivial shortcomings – it requires the retooling of existing forensic software and, thereby, limits the number of tools available to the investigator. More importantly, however, it does not provide a complete solution since it only records snapshots of the state of the DEC without being able to provide a trusted log of all data operations actually performed on the evidence. Without a trusted log the question of whether a tool worked exactly as advertised cannot be answered with certainty, which opens the door to challenges (both legitimate and frivolous) of the results.

In this paper, we propose a complementary mechanism, called the Forensic Discovery Auditing Module (FDAM), aimed at closing this loophole in the discovery process. FDAM can be thought of as a ‘clean-room’ environment for the manipulation of digital evidence, where evidence from containers is placed for controlled manipulation. It functions as an operating system component, which monitors and logs all access to the evidence and enforces policy restrictions. This allows the immediate, safe, and verifiable use of any tool deemed necessary by the examiner. In addition, the module can provide transparent support for multiple DEC formats, thereby greatly simplifying the adoption of open standards.
ER  - 

TY  - JOUR
T1  - Secured Temporal Log Management Techniques for Cloud
JO  - Procedia Computer Science
VL  - 46
IS  - 0
SP  - 589
EP  - 595
PY  - 2015///
T2  - Proceedings of the International Conference on Information and Communication Technologies, ICICT 2014, 3-5 December 2014 at Bolgatty Palace &amp; Island Resort, Kochi, India
AU  - Muthurajkumar, S.
AU  - Ganapathy, S.
AU  - Vijayalakshmi, M.
AU  - Kannan, A.
SN  - 1877-0509
DO  - http://dx.doi.org/10.1016/j.procs.2015.02.098
UR  - http://www.sciencedirect.com/science/article/pii/S1877050915001623
KW  - Cloud computing
KW  - Log management
KW  - Cloud Storage
KW  - Cloud Security;
AB  - Abstract
Log Management has been an important service in Cloud Computing. In any business, maintaining the log records securely over a particular period of time is absolutely necessary for various reasons such as auditing, forensic analysis, evidence etc. In this work, Integrity and confidentiality of the log records are maintained at every stage of Log Management namely the Log Generation phase, Transmission phase and Storage phase. In addition to this, Log records may often contain sensitive information about the organization which should not be leaked to the outside world. In this paper, Temporal Secured Cloud Log Management Algorithm techniques are implemented to provide security to maintain transaction history in cloud within time period. In this work, security to temporal log management is provided by encrypting the log data before they are stored in the cloud storage. They are also stored in batches for easy retrieval. This work was implemented in Java programming language in the Google drive environment.
ER  - 

TY  - JOUR
T1  - A framework for post-event timeline reconstruction using neural networks
JO  - Digital Investigation
VL  - 4
IS  - 3–4
SP  - 146
EP  - 157
PY  - 2007/9//
Y2  - 2007/12//
T2  - 
AU  - Khan, M.N.A.
AU  - Chatwin, C.R.
AU  - Young, R.C.D.
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2007.11.001
UR  - http://www.sciencedirect.com/science/article/pii/S1742287607000837
KW  - Computer forensics
KW  - Digital investigation
KW  - Event reconstruction
KW  - Digital evidence
KW  - Digital forensic analysis
KW  - Neural networks
AB  - Post-event timeline reconstruction plays a critical role in forensic investigation and serves as a means of identifying evidence of the digital crime. We present an artificial neural networks based approach for post-event timeline reconstruction using the file system activities. A variety of digital forensic tools have been developed during the past two decades to assist computer forensic investigators undertaking digital timeline analysis, but most of the tools cannot handle large volumes of data efficiently. This paper looks at the effectiveness of employing neural network methodology for computer forensic analysis by preparing a timeline of relevant events occurring on a computing machine by tracing the previous file system activities. Our approach consists of monitoring the file system manipulations, capturing file system snapshots at discrete intervals of time to characterise the use of different software applications, and then using this captured data to train a neural network to recognise execution patterns of the application programs. The trained version of the network may then be used to generate a post-event timeline of a seized hard disk to verify the execution of different applications at different time intervals to assist in the identification of available evidence.
ER  - 

TY  - JOUR
T1  - A triage framework for digital forensics
JO  - Computer Fraud & Security
VL  - 2015
IS  - 3
SP  - 8
EP  - 18
PY  - 2015/3//
T2  - 
AU  - Bashir, Muhammad Shamraiz
AU  - Khan, Muhammad Naeem Ahmed
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(15)30018-X
UR  - http://www.sciencedirect.com/science/article/pii/S136137231530018X
AB  - A sharp increase in malware and cyber-attacks has been observed in recent years. Analysing cyber-attacks on the affected digital devices falls under the purview of digital forensics. The Internet is the main source of cyber and malware attacks, which sometimes result in serious damage to the digital assets. The motive behind digital crimes varies – such as online banking fraud, information stealing, denial of services, security breaches, deceptive output of running programs and data distortion.

Digital forensics analysts use a variety of tools for data acquisition, evidence analysis and presentation of malicious activities. This leads to device diversity posing serious challenges for investigators.

For this reason, some attack scenarios have to be examined repeatedly, which entails tremendous effort on the part of the examiners when analysing the evidence. To counter this problem, Muhammad Shamraiz Bashir and Muhammad Naeem Ahmed Khan at the Shaheed Zulfikar Ali Bhutto Institute of Science and Technology, Islamabad, Pakistan propose a novel triage framework for digital forensics.
ER  - 

TY  - JOUR
T1  - A system for the proactive, continuous, and efficient collection of digital forensic evidence
JO  - Digital Investigation
VL  - 8, Supplement
IS  - 0
SP  - S3
EP  - S13
PY  - 2011/8//
T2  - The Proceedings of the Eleventh Annual DFRWS Conference 11th Annual Digital Forensics Research Conference
AU  - Shields, Clay
AU  - Frieder, Ophir
AU  - Maloof, Mark
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2011.05.002
UR  - http://www.sciencedirect.com/science/article/pii/S1742287611000260
KW  - Proactive forensics
KW  - Evidence collection
KW  - Deleted file logging
KW  - Search
KW  - File similarity
AB  - The historical focus of forensics research and tools on digital systems that are seized from a suspect misses the fact that in centrally controlled networks it is possible to proactively and continuously collect evidence in advance of any known need. We present a proof-of-concept for PROOFS, the first proposed continuous forensic evidence collection system that applies information retrieval techniques to file system forensics. PROOFS creates and stores signatures for files that are deleted, edited, or copied within such a network. The heart of each signature is one or more fingerprints, generated based on statistical properties of file contents, maintaining semantics while requiring as little as 1.06% of the storage space of the original file. We focus on text documents and show that PROOFS has a high precision of 0.96 and recall of 0.85 with stored fingerprint sizes of less than 375 bytes. The two contributions of this work are that we show that common environments exist where proactive collection of forensic evidence is possible and that we demonstrate an efficient and accurate mechanism for collecting evidence in those environments.
ER  - 

TY  - CHAP
AU  - Liu, Dale
T1  - Chapter 1 - Digital Forensics and Analyzing Data
A2  - Liu, Dale 
BT  - Cisco Router and Switch Forensics
PB  - Syngress
CY  - Boston
PY  - 2009///
SP  - 15
EP  - 38
SN  - 978-1-59749-418-2
DO  - http://dx.doi.org/10.1016/B978-1-59749-418-2.00001-6
UR  - http://www.sciencedirect.com/science/article/pii/B9781597494182000016
AB  - Publisher Summary
This chapter focuses on digital forensics and the best practices that provide a foundation for digital forensic work. It also addresses some of the technical and procedural challenges a digital forensic examiner faces today. Digital forensics is probably the most intricate step of the cybercrime investigation process, and often yields the strongest evidence in terms of prosecutable cases. Digital forensics is the scientific acquisition, analysis, and preservation of data contained in electronic media whose information can be used as evidence in a court of law. The four main phases of the digital forensic process include: collection, examination, analysis, and reporting. It is a digital forensics best practice to make a full bitstream copy of the physical volume. This usually entails physically removing the hard drives from the suspect system and attaching the drives to another system for forensic duplication. The data must be unaltered and the chain of custody must be maintained. Documenting hardware configuration is a tedious but essential part of the forensic process. The magnitude of documentation is in direct correlation to the number and types of devices being acquired. Examination consists of the methodical sifting and combing of data. It may consist of examining dates, metadata, images, document content, or anything else. Every cybercrime incident will involve at least some analysis of data retrieved from systems, whether it's only a few small files from a system or two or terabytes from many machines. Some of the data analysis tools are GREP, PERL scripts, Spreadsheets, Structured Query Language (SQL), and commercial network forensic tools. The report is a compilation of all the documentation, evidence from examinations, and analysis obtained during a digital forensic investigation.
ER  - 

TY  - CHAP
AU  - Reyes, Anthony
AU  - O'Shea, Kevin
AU  - Steele, Jim
AU  - Hansen, Jon R.
AU  - Jean, Benjamin R.
AU  - Ralph, Thomas
T1  - Chapter 9 - Digital Forensics and Analyzing Data
A2  - O'Shea, Anthony ReyesKevin
A2  - Steele, Jim
A2  - Hansen, Jon R.
A2  - Jean, Benjamin R. 
A2  - Ralph, Thomas 
BT  - Cyber Crime Investigations
PB  - Syngress
CY  - Burlington
PY  - 2007///
SP  - 219
EP  - 259
SN  - 978-1-59749-133-4
DO  - http://dx.doi.org/10.1016/B978-159749133-4/50010-3
UR  - http://www.sciencedirect.com/science/article/pii/B9781597491334500103
AB  - Publisher Summary
Digital forensics is regarded as the most intricate part of the cyber crime investigation process. It is often where the strongest evidence can come from. It is the scientific acquisition, analysis, and preservation of data contained in electronic media whose information can be used as evidence in a court of law. Traditional digital forensics started with the seizure of a computer or some media. The drives and media were duplicated in a forensically sound manner bit by bit. The forensic practitioner essentially undeleted files, searched for temporary files, recovered e-mail, and performed other functions to try and find the evidence contained on the media. However, nowadays there are more user-friendly programs available that present data in a graphical user interface (GUI), and automate much of the extremely technical work that used to require in-depth knowledge and expertise with a hex editor.
ER  - 

TY  - CHAP
AU  - Conrad, Eric
AU  - Misenar, Seth
AU  - Feldman, Joshua
T1  - Chapter 2 - Domain 1: Access Control
A2  - Conrad, Eric
A2  - Misenar, Seth 
A2  - Feldman, Joshua 
BT  - CISSP Study Guide (Second Edition)
PB  - Syngress
CY  - Boston
PY  - 2012///
SP  - 9
EP  - 62
SN  - 978-1-59749-961-3
DO  - http://dx.doi.org/10.1016/B978-1-59749-961-3.00002-9
UR  - http://www.sciencedirect.com/science/article/pii/B9781597499613000029
AB  - Access control is the basis for all security disciplines, not just IT security. The purpose of access control is to allow authorized users access to appropriate data and deny access to unauthorized users. Seems simple, right? It would be easy to completely lock a system down to allow just predefined actions with no room for leeway. In fact, many organizations, including the U.S. military, are doing just that; restricting the access users have to systems to a very small functional capability.

Keywords: Subject, Object, Discretionary Access Control (DAC), Mandatory Access Control (MAC), Role-Based Access Control (RBAC)
ER  - 

TY  - JOUR
T1  - DEX: Digital evidence provenance supporting reproducibility and comparison
JO  - Digital Investigation
VL  - 6, Supplement
IS  - 0
SP  - S48
EP  - S56
PY  - 2009/9//
T2  - The Proceedings of the Ninth Annual DFRWS Conference
AU  - Levine, Brian Neil
AU  - Liberatore, Marc
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2009.06.011
UR  - http://www.sciencedirect.com/science/article/pii/S1742287609000395
KW  - Digital forensics
KW  - Digital evidence
KW  - Provenance
KW  - Forensic tools
AB  - The current standard and open formats for forensic data describe whole disk and memory image properties, but do not describe the products of detailed investigations. We propose a simple canonical description of digital evidence provenance that explicitly states the set of tools and transformations that led from acquired raw data to the resulting product. Our format, called Digital Evidence Exchange (DEX) is independent of the forensic tool that discovered the evidence, which has a number of advantages. Using a DEX description and the raw image file, evidence can be reproduced by other tools with the same functionality. Additionally, DEX descriptions can identify differences between two separate investigations of the same raw evidence. Finally, as a standard product of tools, DEX can allow quick fabrication of tool chains either as best-of-breed amalgams or for tool testing. We have implemented DEX as an open-source library.
ER  - 

TY  - JOUR
T1  - Secure log management for privacy assurance in electronic communications
JO  - Computers & Security
VL  - 27
IS  - 7–8
SP  - 298
EP  - 308
PY  - 2008/12//
T2  - 
AU  - Stathopoulos, Vassilios
AU  - Kotzanikolaou, Panayiotis
AU  - Magkos, Emmanouil
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/j.cose.2008.07.010
UR  - http://www.sciencedirect.com/science/article/pii/S0167404808000400
KW  - System logging
KW  - Network providers
KW  - Internal attacks
KW  - Integrity
KW  - Digital signatures
AB  - In this paper we examine logging security in the environment of electronic communication providers. We review existing security threat models for system logging and we extend these to a new security model especially suited for communication network providers, which also considers internal modification attacks. We also propose a framework for secure log management in public communication networks as well as an implementation design, in order to provide traceability under the extended security model. A key role to the proposed framework is given to an independent Regulatory Authority, which is responsible to maintain log integrity proofs in a remote environment and verify the integrity of the provider's log files during security audits.
ER  - 

TY  - JOUR
T1  - Ideal log setting for database forensics reconstruction
JO  - Digital Investigation
VL  - 12
IS  - 0
SP  - 27
EP  - 40
PY  - 2015/3//
T2  - 
AU  - Adedayo, Oluwasola Mary
AU  - Olivier, Martin S.
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2014.12.002
UR  - http://www.sciencedirect.com/science/article/pii/S1742287614001200
KW  - Database management system
KW  - Database forensics
KW  - Digital forensics
KW  - Reconstruction
KW  - Ideal log setting
AB  - Abstract
The ability to reconstruct the data stored in a database at an earlier time is an important aspect of database forensics. Past research shows that the log file in a database can be useful for reconstruction. However, in many database systems there are various options that control which information is included in the logs. This paper introduces the notion of the ideal log setting necessary for an effective reconstruction process in database forensics. The paper provides a survey of the default logging preferences in some of the popular database management systems and identifies the information that a database log should contain in order to be useful for reconstruction. The challenges that may be encountered in storing the information as well as ways of overcoming the challenges are discussed. Possible logging preferences that may be considered as the ideal log setting for the popular database systems are also proposed. In addition, the paper relates the identified requirements to the three dimensions of reconstruction in database forensics and points out the additional requirements and/or techniques that may be required in the different dimensions.
ER  - 

TY  - JOUR
T1  - Provenance-based reproducibility in the Semantic Web
JO  - Web Semantics: Science, Services and Agents on the World Wide Web
VL  - 9
IS  - 2
SP  - 202
EP  - 221
PY  - 2011/7//
T2  - Provenance in the Semantic Web
AU  - Moreau, Luc
SN  - 1570-8268
DO  - http://dx.doi.org/10.1016/j.websem.2011.03.001
UR  - http://www.sciencedirect.com/science/article/pii/S1570826811000163
KW  - Provenance
KW  - Reproducibility
KW  - Denotational semantics
KW  - Primitive environment
AB  - Reproducibility is a crucial property of data since it allows users to understand and verify how data were derived, and therefore allows them to put their trust in such data. Reproducibility is essential for science, because the reproducibility of experimental results is a tenet of the scientific method, but reproducibility is also beneficial in many other fields, including automated decision making, visualization, and automated data feeds. To achieve the vision of reproducibility, the workflow-based community has strongly advocated the use of provenance as an underpinning mechanism for reproducibility, since a rich representation of provenance allows steps to be reproduced and all intermediary and final results checked and validated. Concurrently, multiple ontology-based representations of provenance have been devised, to be able to describe past computations, uniformly across a variety of technologies. However, such Semantic Web representations of provenance do not have any formal link with execution. Even assuming a faithful and non-malicious environment, how can we claim that an ontology-based representation of provenance enables reproducibility, since it has not been given any execution semantics, and therefore has no formal way of expressing the reproduction of computations? This is the problem that this paper tackles by defining a denotational semantics for the Open Provenance Model, which is referred to as the reproducibility semantics. This semantics is used to implement a reproducibility service, leveraging multiple Semantic Web technologies, and offering a variety of reproducibility approaches, found in the literature. A series of empirical experiments were designed to exhibit the range of reproducibility capabilities of our approach; in particular, we demonstrate the ability to reproduce computations involving multiple technologies, as is commonly found on the Web.
ER  - 

TY  - JOUR
T1  - A survey of information security incident handling in the cloud
JO  - Computers & Security
VL  - 49
IS  - 0
SP  - 45
EP  - 69
PY  - 2015/3//
T2  - 
AU  - Ab Rahman, Nurul Hidayah
AU  - Choo, Kim-Kwang Raymond
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/j.cose.2014.11.006
UR  - http://www.sciencedirect.com/science/article/pii/S0167404814001680
KW  - Capability Maturity Model For Services (CMMI-SVC)
KW  - Cloud computing
KW  - Cloud response
KW  - Incident handling
KW  - Incident management
KW  - Incident response
AB  - Abstract
Incident handling strategy is one key strategy to mitigate risks to the confidentiality, integrity and availability (CIA) of organisation assets, as well as minimising loss (e.g. financial, reputational and legal) particularly as organisations move to the cloud. In this paper, we surveyed existing incident handling and digital forensic literature with the aims of contributing to the knowledge gap(s) in handling incidents in the cloud environment. 139 English language publications between January 2009 and May 2014 were located by searching various sources including the websites of standard bodies (e.g. National Institute of Standards and Technology) and academic databases (e.g. Google Scholar, IEEEXplore, ACM Digital Library, Springer and ScienceDirect). We then propose a conceptual cloud incident handling model that brings together incident handling, digital forensic and the Capability Maturity Model for Services to more effectively handle incidents for organisations using the cloud. A discussion of open research issues concludes this survey.
ER  - 

TY  - JOUR
T1  - Leveraging CybOX™ to standardize representation and exchange of digital forensic information
JO  - Digital Investigation
VL  - 12, Supplement 1
IS  - 0
SP  - S102
EP  - S110
PY  - 2015/3//
T2  - DFRWS 2015 Europe Proceedings of the Second Annual DFRWS Europe
AU  - Casey, Eoghan
AU  - Back, Greg
AU  - Barnum, Sean
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2015.01.014
UR  - http://www.sciencedirect.com/science/article/pii/S1742287615000158
KW  - Digital forensics
KW  - Standard representation
KW  - Digital forensic ontology
KW  - Digital forensic XML
KW  - CybOX
KW  - DFXML
KW  - DFAX
AB  - Abstract
With the growing number of digital forensic tools and the increasing use of digital forensics in various contexts, including incident response and cyber threat intelligence, there is a pressing need for a widely accepted standard for representing and exchanging digital forensic information. Such a standard representation can support correlation between different data sources, enabling more effective and efficient querying and analysis of digital evidence. This work summarizes the strengths and weaknesses of existing schemas, and proposes the open-source CybOX schema as a foundation for storing and sharing digital forensic information. The suitability of CybOX for representing objects and relationships that are common in forensic investigations is demonstrated with examples involving digital evidence. The capability to represent provenance by leveraging CybOX is also demonstrated, including specifics of the tool used to process digital evidence and the resulting output. An example is provided of an ongoing project that uses CybOX to record the state of a system before and after an event in order to capture cause and effect information that can be useful for digital forensics. An additional open-source schema and associated ontology called Digital Forensic Analysis eXpression (DFAX) is proposed that provides a layer of domain specific information overlaid on CybOX. DFAX extends the capability of CybOX to represent more abstract forensic-relevant actions, including actions performed by subjects and by forensic examiners, which can be useful for sharing knowledge and supporting more advanced forensic analysis. DFAX can be used in combination with other existing schemas for representing identity information (CIQ), and location information (KML). This work also introduces and leverages initial steps of a Unified Cyber Ontology (UCO) effort to abstract and express concepts/constructs that are common across the cyber domain.
ER  - 

TY  - CHAP
AU  - Snodgrass, Richard T.
AU  - Yao, Shilong Stanley
AU  - Collberg, Christian
T1  - Tamper Detection in Audit Logs
A2  - Nascimento, Mario A.
A2  - Özsu, M. Tamer
A2  - Kossmann, Donald
A2  - Miller, Renée J.
A2  - Blakeley, José A. 
A2  - Schiefer, Berni 
BT  - Proceedings 2004 VLDB Conference
PB  - Morgan Kaufmann
CY  - St Louis
PY  - 2004///
SP  - 504
EP  - 515
SN  - 978-0-12-088469-8
DO  - http://dx.doi.org/10.1016/B978-012088469-8.50046-2
UR  - http://www.sciencedirect.com/science/article/pii/B9780120884698500462
AB  - Publisher Summary
This chapter illustrates the tamper detection in audit logs. These logs are considered good practice for business systems, and are required by federal regulations for secure systems, drug approval data, medical information disclosure, financial records, and electronic voting. Given the central role of audit logs, it is critical that they are correct and inalterable. The chapter proposes mechanisms within a Database Management System (DBMS) based on cryptographically strong one-way hash functions that prevent an intruder, including an auditor or an employee or even an unknown bug within the DBMS itself, from silently corrupting the audit log. It also proposes that the DBMS store additional information in the database to enable a separate audit log validator to examine the database along with this extra information and state conclusively whether the audit log has been compromised.
ER  - 

TY  - JOUR
T1  - The increasing need for automation and validation in digital forensics
JO  - Digital Investigation
VL  - 7
IS  - 3–4
SP  - 103
EP  - 104
PY  - 2011/4//
T2  - 
AU  - Casey, Eoghan
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2011.02.002
UR  - http://www.sciencedirect.com/science/article/pii/S1742287611000053
ER  - 

TY  - JOUR
T1  - Distributed filesystem forensics: XtreemFS as a case study
JO  - Digital Investigation
VL  - 11
IS  - 4
SP  - 295
EP  - 313
PY  - 2014/12//
T2  - 
AU  - Martini, Ben
AU  - Choo, Kim-Kwang Raymond
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2014.08.002
UR  - http://www.sciencedirect.com/science/article/pii/S1742287614000942
KW  - Big data
KW  - Digital forensics
KW  - Distributed filesystem
KW  - Infrastructure as a Service (IaaS)
KW  - Storage as a Service (StaaS)
KW  - Distributed filesystem forensics
KW  - Cloud storage forensics
AB  - Abstract
Distributed filesystems provide a cost-effective means of storing high-volume, velocity and variety information in cloud computing, big data and other contemporary systems. These technologies have the potential to be exploited for illegal purposes, which highlights the need for digital forensic investigations. However, there have been few papers published in the area of distributed filesystem forensics. In this paper, we aim to address this gap in knowledge. Using our previously published cloud forensic framework as the underlying basis, we conduct an in-depth forensic experiment on XtreemFS, a Contrail EU-funded project, as a case study for distributed filesystem forensics. We discuss the technical and process issues regarding collection of evidential data from distributed filesystems, particularly when used in cloud computing environments. A number of digital forensic artefacts are also discussed. We then propose a process for the collection of evidential data from distributed filesystems.
ER  - 

TY  - JOUR
T1  - Electronic discovery: digital forensics and beyond
JO  - Computer Fraud & Security
VL  - 2006
IS  - 4
SP  - 8
EP  - 10
PY  - 2006/4//
T2  - 
AU  - Forte, Dario
AU  - Power, Richard
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(06)70332-3
UR  - http://www.sciencedirect.com/science/article/pii/S1361372306703323
AB  - Companies need to be ready to find and produce electronic records at the drop of a hat in the face of a lawsuit. Three-quarters of modern day lawsuits entail e-Discovery. Organisations need to salvage data from every remote corner of their systems. Information from email, office documents, log files, transactions and scanned files must be on standby for extraction and scrutiny.

And it isn't cheap - the average cost of an e-Discovery project is several hundred thousand dollars.

The idea is to provide a trusted copy of original documents requested by lawyers or chosen for presentation by the company.

“Litigation Lifecycle Management” is a very common legal procedure in the United States. This article provides an introduction to the topic with special reference to its complexities.
ER  - 

TY  - CHAP
AU  - Wiles, Jack
AU  - Alexander, Tammy
AU  - Ashlock, Stevee
AU  - Ballou, Susan
AU  - Depew, Larry
AU  - Dominguez, Greg
AU  - Ehuan, Art
AU  - Green, Ron
AU  - Long, Johnny
AU  - Reis, Kevin
AU  - Schroader, Amber
AU  - Schuler, Karen
AU  - Thompson, Eric
T1  - Chapter 1 - Authentication: Are You Investigating the Right Person?
A2  - Wiles, Jack
A2  - Alexander, Tammy
A2  - Ashlock, Stevee
A2  - Ballou, Susan
A2  - Depew, Larry
A2  - Dominguez, Greg
A2  - Ehuan, Art
A2  - Green, Ron
A2  - Long, Johnny
A2  - Reis, Kevin
A2  - Schroader, Amber
A2  - Schuler, Karen 
A2  - Thompson, Eric 
BT  - Techno Security's Guide to E-Discovery and Digital Forensics
PB  - Syngress
CY  - Burlington
PY  - 2007///
SP  - 1
EP  - 31
SN  - 978-1-59749-223-2
DO  - http://dx.doi.org/10.1016/B978-159749223-2.50005-4
UR  - http://www.sciencedirect.com/science/article/pii/B9781597492232500054
AB  - As we started to put together the various pieces of this book, I was tempted to call this chapter “The Chapter That Wasn't”. The subject matter for this chapter has been of major concern for close to two decades. That doesn't sound like a long time, but it is in the world of computer security. This subtle subject is one we don't often hear mentioned, but it could easily bring organizations to their techno knees if they don't take it seriously.
ER  - 

TY  - JOUR
T1  - Forensic analysis of logs: Modeling and verification
JO  - Knowledge-Based Systems
VL  - 20
IS  - 7
SP  - 671
EP  - 682
PY  - 2007/10//
T2  - Special Issue on Techniques to Produce Intelligent Secure Software
AU  - Saleh, Mohamed
AU  - Arasteh, Ali Reza
AU  - Sakha, Assaad
AU  - Debbabi, Mourad
SN  - 0950-7051
DO  - http://dx.doi.org/10.1016/j.knosys.2007.05.002
UR  - http://www.sciencedirect.com/science/article/pii/S0950705107000561
KW  - Forensic analysis
KW  - Log analysis
KW  - Formal methods
KW  - Model checking
KW  - Logging systems
AB  - Information stored in logs of a computer system is of crucial importance to gather forensic evidence of investigated actions or attacks against the system. Analysis of this information should be rigorous and credible, hence it lends itself to formal methods. We propose a model checking approach to the formalization of the forensic analysis of logs. The set of logs of a certain system is modeled as a tree whose labels are events extracted from the logs. In order to provide a structure to these events, we express each event as a term of a term algebra. The signature of the algebra is carefully chosen to include all relevant information necessary to conduct the analysis. Properties of the model are expressed as formulas of a logic having dynamic, linear, temporal, and modal characteristics. Moreover, we provide a tableau-based proof system for this logic upon which a model checking algorithm can be developed. In order to illustrate the proposed approach, the Windows auditing system is studied. The properties that we capture in our logic include invariant properties of a system, forensic hypotheses, and generic or specific attack signatures. Moreover, we discuss the admissibility of forensics hypotheses and the underlying verification issues.
ER  - 

TY  - JOUR
T1  - Trust in digital records: An increasingly cloudy legal area
JO  - Computer Law & Security Review
VL  - 28
IS  - 5
SP  - 522
EP  - 531
PY  - 2012/10//
T2  - 
AU  - Duranti, Luciana
AU  - Rogers, Corinne
SN  - 0267-3649
DO  - http://dx.doi.org/10.1016/j.clsr.2012.07.009
UR  - http://www.sciencedirect.com/science/article/pii/S0267364912001458
KW  - Digital records
KW  - Digital forensics
KW  - Cloud computing
KW  - Law of evidence
KW  - Digital documentary evidence
AB  - Trust has been defined in many ways, but at its core it involves acting without the knowledge needed to act. Trust in records depends on four types of knowledge about the creator or custodian of the records: reputation, past performance, competence, and the assurance of confidence in future performance. For over half a century society has been developing and adopting new computer technologies for business and communications in both the public and private realm. Frameworks for establishing trust have developed as technology has progressed. Today, individuals and organizations are increasingly saving and accessing records in cloud computing infrastructures, where we cannot assess our trust in records solely on the four types of knowledge used in the past. Drawing on research conducted at the University of British Columbia into the nature of digital records and their trustworthiness, this article presents the conceptual archival and digital forensic frameworks of trust in records and data, and explores the common law legal framework within which questions of trust in documentary evidence are being tested. Issues and challenges specific to cloud computing are introduced.
ER  - 

TY  - JOUR
T1  - Profiling software applications for forensic analysis
JO  - Computer Fraud & Security
VL  - 2015
IS  - 6
SP  - 13
EP  - 18
PY  - 2015/6//
T2  - 
AU  - Rafique, Mamoona
AU  - Khan, Muhammad Naeem Ahmed
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(15)30058-0
UR  - http://www.sciencedirect.com/science/article/pii/S1361372315300580
AB  - Computers are now a fundamental part of our professional lives. Although advanced technologies are being used to contain digital crimes, alongside these are other technologies that have expanded a criminal community that is constantly searching for new means to commit crimes in more sophisticated ways. Due to the availability of corporate data on the web, coupled with the open access nature of the web, digital miscreants can commit cybercrimes either as legitimate or illegitimate users.

Traditional digital forensics involves static analysis of the data available on permanent storage media, while live analysis allows running systems to be examined to analyse volatile data.

However, live analysis is not without its challenges, not least because each application has different effects on the system. Mamoona Rafique and Muhammad Naeem Ahmed Khan present a model for profiling the behaviour of application programs. This allows investigators to build a behavioural profile of each application in order to understand its effects on the system.
ER  - 

TY  - CHAP
AU  - Daniel, Larry E.
AU  - Daniel, Lars E.
T1  - Chapter 40 - Accounting Systems and Financial Software
A2  - Daniel, Larry E.  
A2  - Daniel, Lars E. 
BT  - Digital Forensics for Legal Professionals
PB  - Syngress
CY  - Boston
PY  - 2012///
SP  - 295
EP  - 300
SN  - 978-1-59749-643-8
DO  - http://dx.doi.org/10.1016/B978-1-59749-643-8.00040-7
UR  - http://www.sciencedirect.com/science/article/pii/B9781597496438000407
KW  - Accounting
KW  - QuickBooks
KW  - Money Management Software
KW  - Peachtree
KW  - Online Accounting
KW  - Financial Records
KW  - Audit Logs
AB  - Publisher Summary
You have probably heard the phrase, “follow the money.” This is true in all kinds of cases, from domestic disputes to Ponzi schemes and murder cases. Since so many people and businesses manage their money electronically today, there is a wealth of potential electronic evidence available residing anywhere from the Internet cache on a computer, to an e-mail attachment, inside an accounting program database, and even on a person's phone. In this chapter, the author looks at some of the places and ways that financial information is stored, some of the analysis techniques, and some cases where financial information has played a factor. This chapter discusses accounting and financial software along with the various issues associated with them. It also looks upon how the software works from the standpoint of evidence creation that can be recovered and analyzed in cases. From single file-based systems to older vertical market applications using multiple file database storage, accounting and financial systems can be a challenge for an examiner. Several cases have been looked at where different kinds of evidence played a part in the case, both civil and criminal.
 
You have probably heard the term, “follow the money.” This is true in all kinds of cases, from domestic disputes to Ponzi schemes and murder cases. Since so many people and businesses manage their money electronically today, there is a wealth of potential electronic evidence available residing anywhere from the Internet cache on a computer, to an e-mail attachment, inside an accounting program database, and even on a person’s phone.

In this chapter we will look at some of the places and ways that financial information is stored, some of the analysis techniques and some cases where financial information has played a factor.
ER  - 

TY  - CHAP
AU  - Watson, David
AU  - Jones, Andrew
T1  - Chapter 7 - IT Infrastructure
A2  - Jones, David WatsonAndrew 
BT  - Digital Forensics Processing and Procedures
PB  - Syngress
CY  - Boston
PY  - 2013///
SP  - 233
EP  - 312
SN  - 978-1-59749-742-8
DO  - http://dx.doi.org/10.1016/B978-1-59749-742-8.00007-8
UR  - http://www.sciencedirect.com/science/article/pii/B9781597497428000078
KW  - process
KW  - incident
KW  - change
KW  - release
KW  - hardware
KW  - software
KW  - capacity
KW  - service
KW  - management
AB  - Abstract
This chapter looks at the policies and issues related to the IT infrastructure within the laboratory. It looks at the hardware, the software, and the infrastructure in some detail. It then looks at process management, addressing issues including incident and problem management; change control; Release Management; and configuration, capacity, and service management.
ER  - 

TY  - CHAP
AU  - Watson, David
AU  - Jones, Andrew
T1  - Chapter 1 - Introduction
A2  - Jones, David WatsonAndrew 
BT  - Digital Forensics Processing and Procedures
PB  - Syngress
CY  - Boston
PY  - 2013///
SP  - 1
EP  - 12
SN  - 978-1-59749-742-8
DO  - http://dx.doi.org/10.1016/B978-1-59749-742-8.00001-7
UR  - http://www.sciencedirect.com/science/article/pii/B9781597497428000017
KW  - Digital Forensics
KW  - procedures
KW  - electronic evidence
KW  - nomenclature
AB  - Abstract
This chapter explains the purpose of the book and describes the rationale for the structure of the book. It contains a description of what Digital Forensics are and goes on to explain why there is a need for them. It explains who the target audience for this book is and gives a description of the principles of electronic evidence and some of the problems that have been encountered with it. It then gives an explanation of why there is a need for procedures in Digital Forensics. The chapter finishes with an explanation of the nomenclature that is used throughout the book.
ER  - 

TY  - JOUR
T1  - A secure log architecture to support remote auditing
JO  - Mathematical and Computer Modelling
VL  - 57
IS  - 7–8
SP  - 1578
EP  - 1591
PY  - 2013/4//
T2  - Public Key Services and Infrastructures EUROPKI-2010-Mathematical Modelling in Engineering &amp; Human Behaviour 2011
AU  - Accorsi, Rafael
SN  - 0895-7177
DO  - http://dx.doi.org/10.1016/j.mcm.2012.06.035
UR  - http://www.sciencedirect.com/science/article/pii/S0895717712001744
KW  - Secure log architecture
KW  - Public key cryptography
KW  - Remote auditing
KW  - Secure digital archiving
AB  - This paper presents BBox , a digital black box to provide for authentic archiving (and, consequently, forensic evidence) for remote auditing in distributed systems. Based upon public key cryptography and trusted computing platforms, the BBox employs standard primitives to ensure the authenticity of records during the transmission from devices to the collector, as well as during their storage on the collector and keyword retrieval by authorized auditors.
ER  - 

TY  - JOUR
T1  - Design and implementation of FROST: Digital forensic tools for the OpenStack cloud computing platform
JO  - Digital Investigation
VL  - 10, Supplement
IS  - 0
SP  - S87
EP  - S95
PY  - 2013/8//
T2  - The Proceedings of the Thirteenth Annual DFRWS Conference 13th Annual Digital Forensics Research Conference
AU  - Dykstra, Josiah
AU  - Sherman, Alan T.
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2013.06.010
UR  - http://www.sciencedirect.com/science/article/pii/S174228761300056X
KW  - OpenStack
KW  - Cloud computing
KW  - Digital forensics
KW  - Cloud forensics
KW  - FROST
AB  - Abstract
We describe the design, implementation, and evaluation of FROST—three new forensic tools for the OpenStack cloud platform. Our implementation for the OpenStack cloud platform supports an Infrastructure-as-a-Service (IaaS) cloud and provides trustworthy forensic acquisition of virtual disks, API logs, and guest firewall logs. Unlike traditional acquisition tools, FROST works at the cloud management plane rather than interacting with the operating system inside the guest virtual machines, thereby requiring no trust in the guest machine. We assume trust in the cloud provider, but FROST overcomes non-trivial challenges of remote evidence integrity by storing log data in hash trees and returning evidence with cryptographic hashes. Our tools are user-driven, allowing customers, forensic examiners, and law enforcement to conduct investigations without necessitating interaction with the cloud provider. We demonstrate how FROST's new features enable forensic investigators to obtain forensically-sound data from OpenStack clouds independent of provider interaction. Our preliminary evaluation indicates the ability of our approach to scale in a dynamic cloud environment. The design supports an extensible set of forensic objectives, including the future addition of other data preservation, discovery, real-time monitoring, metrics, auditing, and acquisition capabilities.
ER  - 

TY  - CHAP
AU  - Vidal, Chaz
AU  - Choo, Kim-Kwang Raymond
T1  - Chapter 18 - Cloud security and forensic readiness: The current state of an IaaS provider
A2  - Choo, Ryan KoKim-Kwang Raymond 
BT  - The Cloud Security Ecosystem
PB  - Syngress
CY  - Boston
PY  - 2015///
SP  - 401
EP  - 428
SN  - 978-0-12-801595-7
DO  - http://dx.doi.org/10.1016/B978-0-12-801595-7.00018-5
UR  - http://www.sciencedirect.com/science/article/pii/B9780128015957000185
KW  - Cloud forensics
KW  - Cloud forensic readiness
KW  - Cloud security
KW  - Cloud vulnerabilities and threat assessment
KW  - Forensic readiness
KW  - Private Infrastructure as a Service (IaaS) service
KW  - Third-party cloud service delivery management
AB  - Abstract
Security is a key challenge in the deployment and acceptance of cloud computing services, and existing research efforts include evaluating the effectiveness of various security solutions such as security policy implementations and technological solutions. This chapter examines the security controls of an existing organization’s Infrastructure as a Service (IaaS) service using existing information and cloud security standards and forensic readiness best practices. Areas for improvements are also identified in this chapter.
ER  - 

TY  - JOUR
T1  - Network intrusion investigation – Preparation and challenges
JO  - Digital Investigation
VL  - 3
IS  - 3
SP  - 118
EP  - 126
PY  - 2006/9//
T2  - 
AU  - Johnston, Andy
AU  - Reust, Jessica
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2006.08.001
UR  - http://www.sciencedirect.com/science/article/pii/S1742287606000922
KW  - Intrusion investigation
KW  - Incident response
KW  - Network forensics
KW  - Digital forensic examination
KW  - Compromise of sensitive information
KW  - Forensic preparedness
AB  - As new legislation is written mandating notification of affected parties following the compromise of confidential data, reliable investigative procedures into unauthorized access of such data assume increasing importance. The increasing costs and penalties associated with exposure of sensitive data can be mitigated through forensic preparation and the ability to employ digital forensics. A case study of the compromise of several systems containing sensitive data is outlined, with particular attention given to the procedures followed during the initial response and their impact on the subsequent digital forensic examination. Practical problems and challenges that arise in intrusion investigations are discussed, along with solutions and methodologies to address these issues. This case study illustrates both the importance of evaluating the evidence analyzed and of corroborating findings and conclusions with multiple independent sources of evidence. An initial response that incorporates forensic procedures provides a solid foundation for a successful and thorough forensic examination.
ER  - 

TY  - JOUR
T1  - Tool review—WinHex
JO  - Digital Investigation
VL  - 1
IS  - 2
SP  - 114
EP  - 128
PY  - 2004/6//
T2  - 
AU  - Casey, Eoghan
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2004.04.001
UR  - http://www.sciencedirect.com/science/article/pii/S1742287604000295
KW  - Digital forensics tool testing
KW  - Digital evidence preservation
KW  - Forensic examination
KW  - File systems
KW  - Data recovery
AB  - This paper presents strengths and shortcomings of WinHex Specialist Edition (version 11.25 SR-7) in the context of the overall digital forensics process, focusing on its ability to preserve and examine data on storage media. No serious problems were found during non-exhaustive testing of the tool's ability to create a forensic image of a disk, and to verify the integrity of an image. Generally accepted data sets were used to test WinHex's ability to reliably and accurately interpret file date–time stamps, recover deleted files, and search for keywords. The results of these tests are summarized in this paper. Certain advanced examination capabilities were also evaluated, including the creation of custom templates to interpret EXT2/EXT3 file systems. Based on this review, several enhancements are proposed. In addition to these results, this paper demonstrates a systematic approach to evaluating similar forensic tools.
ER  - 

TY  - CHAP
AU  - Casey, Eoghan
AU  - Daywalt, Christopher
AU  - Johnston, Andy
T1  - Chapter 4 - Intrusion Investigation
A2  - from, Eoghan CaseyWith contributions
A2  - Altheide, Cory
A2  - Daywalt, Christopher
A2  - Donno, Andrea de
A2  - Forte, Dario
A2  - Holley, James O.
A2  - Johnston, Andy
A2  - Knijff, Ronald van der
A2  - Kokocinski, Anthony
A2  - Luehr, Paul H.
A2  - Maguire, Terrance
A2  - Pittman, Ryan D.
A2  - Rose, Curtis W.
A2  - Schwerha, Joseph J.
A2  - Shaver, Dave 
A2  - Smith, Jessica Reust 
BT  - Handbook of Digital Forensics and Investigation
PB  - Academic Press
CY  - San Diego
PY  - 2010///
SP  - 135
EP  - 206
SN  - 978-0-12-374267-4
DO  - http://dx.doi.org/10.1016/B978-0-12-374267-4.00004-5
UR  - http://www.sciencedirect.com/science/article/pii/B9780123742674000045
AB  - Publisher Summary
This chapter explores basic methodologies for conducting an intrusion investigation, and describes how an organization can better prepare to facilitate future investigations. This chapter also assumes that one is familiar with basic network intrusion techniques. A theoretical intrusion investigation scenario is used throughout this chapter to demonstrate key points. Intrusion investigation is a specialized subset of digital forensic investigation that is focused on determining the nature and full extent of unauthorized access and usage of one or more computer systems. Intrusion investigation is a dynamic process that requires strong technical skills and effective case management, often requiring a team of digital investigators and forensic examiners. In practice, it sometimes seems like controlled chaos, particularly when an intruder is still active on the victim systems. Digital investigators have a better chance of navigating the challenges and complexities of intrusion investigations if they follow the scientific method and scope assessment cycle. In addition, the success of this type of investigation depends heavily on having a mechanism to keep track of the investigative and forensic subtasks. Investigating a security breach may require a combination of file system forensics, collecting evidence from various network devices, scanning hosts on a network for signs of compromise, searching and correlating network logs, combing through packet captures, and analyzing malware.
ER  - 

TY  - CHAP
AU  - Watson, David
AU  - Jones, Andrew
T1  - Chapter 15 - Effective Records Management
A2  - Jones, David WatsonAndrew 
BT  - Digital Forensics Processing and Procedures
PB  - Syngress
CY  - Boston
PY  - 2013///
SP  - 665
EP  - 699
SN  - 978-1-59749-742-8
DO  - http://dx.doi.org/10.1016/B978-1-59749-742-8.00015-7
UR  - http://www.sciencedirect.com/science/article/pii/B9781597497428000157
ER  - 

TY  - JOUR
T1  - Security and privacy in electronic health records: A systematic literature review
JO  - Journal of Biomedical Informatics
VL  - 46
IS  - 3
SP  - 541
EP  - 562
PY  - 2013/6//
T2  - 
AU  - Fernández-Alemán, José Luis
AU  - Señor, Inmaculada Carrión
AU  - Lozoya, Pedro Ángel Oliver
AU  - Toval, Ambrosio
SN  - 1532-0464
DO  - http://dx.doi.org/10.1016/j.jbi.2012.12.003
UR  - http://www.sciencedirect.com/science/article/pii/S1532046412001864
KW  - Electronic health records
KW  - Systematic review
KW  - Privacy
KW  - Confidentiality
KW  - Security
KW  - Standards
AB  - Objective
To report the results of a systematic literature review concerning the security and privacy of electronic health record (EHR) systems.
Data sources
Original articles written in English found in MEDLINE, ACM Digital Library, Wiley InterScience, IEEE Digital Library, Science@Direct, MetaPress, ERIC, CINAHL and Trip Database.
Study selection
Only those articles dealing with the security and privacy of EHR systems.
Data extraction
The extraction of 775 articles using a predefined search string, the outcome of which was reviewed by three authors and checked by a fourth.
Results
A total of 49 articles were selected, of which 26 used standards or regulations related to the privacy and security of EHR data. The most widely used regulations are the Health Insurance Portability and Accountability Act (HIPAA) and the European Data Protection Directive 95/46/EC. We found 23 articles that used symmetric key and/or asymmetric key schemes and 13 articles that employed the pseudo anonymity technique in EHR systems. A total of 11 articles propose the use of a digital signature scheme based on PKI (Public Key Infrastructure) and 13 articles propose a login/password (seven of them combined with a digital certificate or PIN) for authentication. The preferred access control model appears to be Role-Based Access Control (RBAC), since it is used in 27 studies. Ten of these studies discuss who should define the EHR systems’ roles. Eleven studies discuss who should provide access to EHR data: patients or health entities. Sixteen of the articles reviewed indicate that it is necessary to override defined access policies in the case of an emergency. In 25 articles an audit-log of the system is produced. Only four studies mention that system users and/or health staff should be trained in security and privacy.
Conclusions
Recent years have witnessed the design of standards and the promulgation of directives concerning security and privacy in EHR systems. However, more work should be done to adopt these regulations and to deploy secure EHR systems.
ER  - 

TY  - JOUR
T1  - Acquiring forensic evidence from infrastructure-as-a-service cloud computing: Exploring and evaluating tools, trust, and techniques
JO  - Digital Investigation
VL  - 9, Supplement
IS  - 0
SP  - S90
EP  - S98
PY  - 2012/8//
T2  - The Proceedings of the Twelfth Annual DFRWS Conference 12th Annual Digital Forensics Research Conference
AU  - Dykstra, Josiah
AU  - Sherman, Alan T.
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2012.05.001
UR  - http://www.sciencedirect.com/science/article/pii/S1742287612000266
KW  - Computer security
KW  - Cloud computing
KW  - Digital forensics
KW  - Cloud forensics
KW  - EnCase
KW  - FTK
KW  - Amazon EC2
AB  - We expose and explore technical and trust issues that arise in acquiring forensic evidence from infrastructure-as-a-service cloud computing and analyze some strategies for addressing these challenges. First, we create a model to show the layers of trust required in the cloud. Second, we present the overarching context for a cloud forensic exam and analyze choices available to an examiner. Third, we provide for the first time an evaluation of popular forensic acquisition tools including Guidance EnCase and AccesData Forensic Toolkit, and show that they can successfully return volatile and non-volatile data from the cloud. We explain, however, that with those techniques judge and jury must accept a great deal of trust in the authenticity and integrity of the data from many layers of the cloud model. In addition, we explore four other solutions for acquisition—Trusted Platform Modules, the management plane, forensics-as-a-service, and legal solutions, which assume less trust but require more cooperation from the cloud service provider. Our work lays a foundation for future development of new acquisition methods for the cloud that will be trustworthy and forensically sound. Our work also helps forensic examiners, law enforcement, and the court evaluate confidence in evidence from the cloud.
ER  - 

TY  - JOUR
T1  - A DSL for modeling application-specific functionalities of business applications
JO  - Computer Languages, Systems & Structures
VL  - 
IS  - 0
SP  - 
EP  - 
PY  - 
T2  - 
AU  - Popovic, Aleksandar
AU  - Lukovic, Ivan
AU  - Dimitrieski, Vladimir
AU  - Djukic, Verislav
SN  - 1477-8424
DO  - http://dx.doi.org/10.1016/j.cl.2015.03.003
UR  - http://www.sciencedirect.com/science/article/pii/S1477842415000263
KW  - Domain-specific languages
KW  - IIS⁎CFuncLang
KW  - Application-specific functionalities
KW  - Model transformations
KW  - IIS⁎Case
AB  - Abstract
Models have been widely used in the information system development process. Models are not just means for system analysis and documentation. They may be also transformed into system implementation, primarily program code. Generated program code of screen forms and transaction programs mainly implements generic functionalities that can be expressed by simple retrieval, insertion, update, or deletion operations over database records. Besides the program code of generic functionalities, each application usually includes program code for specific business logic that represents application-specific functionalities, which may include complex calculations, as well as a series of database operations. There is a lack of domain-specific and tool-supported techniques for specification of such application-specific functionalities at the level of platform-independent models (PIMs). In this paper, we propose an approach and a domain-specific language (DSL), named IIS⁎CFuncLang, aimed at enabling a complete specification of application-specific functionalities at the PIM level. We have developed algorithms for transformation of IIS⁎CFuncLang specifications into executable program code, such as PL/SQL program code. In order to support specification of application-specific functionalities using IIS⁎CFuncLang, we have also developed appropriate tree-based and textual editors. The language, editors, and the transformations are embedded into a Model-Driven Software Development tool, named Integrated Information Systems CASE (IIS⁎Case). IIS⁎Case supports platform-independent design and automated prototyping of information systems, which allows us to verify and test our approach in practice.
ER  - 

TY  - CHAP
AU  - Marinescu, Dan C.
T1  - Chapter 9 - Cloud Security
A2  - Marinescu, Dan C. 
BT  - Cloud Computing
PB  - Morgan Kaufmann
CY  - Boston
PY  - 2013///
SP  - 273
EP  - 300
SN  - 978-0-12-404627-6
DO  - http://dx.doi.org/10.1016/B978-0-12-404627-6.00009-9
UR  - http://www.sciencedirect.com/science/article/pii/B9780124046276000099
KW  - Trust
KW  - Security
KW  - Trusted computing base (TCB)
KW  - Xoar
KW  - Operating system security
KW  - Virtual machine security
KW  - Security of virtualization
KW  - Security risks posed by shared images
AB  - Abstract
A computer cloud is a target-rich environment for malicious individuals and criminal organizations. A cloud faces several types of security threat: traditional ones, threats related to system availability, and threats related to third-party data control.

Virtually all surveys report that security is the top concern for cloud users, who are accustomed to having full control of all systems where sensitive information is stored and processed. Users typically operate inside a secure perimeter protected by a corporate firewall. In spite of the potential threats, users have to extend their trust to the cloud service provider if they want to benefit from the economic advantages of utility computing.

Chapter 9 starts with an in-depth discussion of security, privacy, and trust in the cloud computing context. Operating system security and virtual machine security are analyzed next. Security of virtualization, the security risks posed by shared images and by a management OS, are the topics of the next sections. An overview of Xoar, a system designed to break the monolithic structure of the Trusted Computing Base (TCB), is followed by a survey of a trusted virtual machine monitor.
ER  - 

TY  - CHAP
AU  - Samani, Raj
AU  - Honan, Brian
AU  - Reavis, Jim
T1  - Chapter 8 - Cloud Security Alliance Research
A2  - Reavis, Raj SamaniBrian HonanJim 
BT  - CSA Guide to Cloud Computing
PB  - Syngress
CY  - Boston
PY  - 2015///
SP  - 149
EP  - 169
SN  - 978-0-12-420125-5
DO  - http://dx.doi.org/10.1016/B978-0-12-420125-5.00008-X
UR  - http://www.sciencedirect.com/science/article/pii/B978012420125500008X
KW  - Big Data
KW  - CloudCERT
KW  - CloudTrust
KW  - Governance
KW  - Innovation initiative
AB  - Abstract
As mentioned earlier, our intention is to provide a single reference for all Cloud Security Alliance (CSA) research. This chapter will provide the readers with an overview of the various working groups within the CSA and details of their current findings.
ER  - 

TY  - CHAP
AU  - Watson, David
AU  - Jones, Andrew
T1  - Chapter 9 - Case Processing
A2  - Jones, David WatsonAndrew 
BT  - Digital Forensics Processing and Procedures
PB  - Syngress
CY  - Boston
PY  - 2013///
SP  - 367
EP  - 420
SN  - 978-1-59749-742-8
DO  - http://dx.doi.org/10.1016/B978-1-59749-742-8.00009-1
UR  - http://www.sciencedirect.com/science/article/pii/B9781597497428000091
KW  - case processing
KW  - tool verification
KW  - equipment maintenance
KW  - management processes
KW  - imaging
KW  - examination
KW  - report
KW  - backup
KW  - archive disclosure
KW  - disposal
AB  - Abstract
This chapter addresses the whole spectrum of the processing of a case. It looks at the different types of cases that may be encountered and then discusses the preprocessing steps that will have to be undertaken before a case can be processed. It covers the maintenance of equipment that is required and also the management processes that must be in place. It then moves through all of the stages of processing a case from booking the exhibits into the laboratory to the setting up of a new case, preparing the forensic workstation, imaging the media, examining it, and preparing a report. It also addresses tool verification, archiving and backup disclosure, and media disposal.
ER  - 

TY  - JOUR
T1  - Automated computer forensics training in a virtualized environment
JO  - Digital Investigation
VL  - 5, Supplement
IS  - 0
SP  - S105
EP  - S111
PY  - 2008/9//
T2  - The Proceedings of the Eighth Annual DFRWS Conference
AU  - Brueckner, Stephen
AU  - Guaspari, David
AU  - Adelstein, Frank
AU  - Weeks, Joseph
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2008.05.009
UR  - http://www.sciencedirect.com/science/article/pii/S1742287608000406
KW  - Digital forensic training
KW  - Computer training
KW  - Virtualized training
KW  - Automated assessment
KW  - Automated evaluation
AB  - The CYber DEfenSe Trainer (CYDEST) is a virtualized training platform for network defense and computer forensics. It uses virtual machines to provide tactical level exercises for personnel such as network administrators, first responders, and digital forensics investigators. CYDEST incorporates a number of features to reduce instructor workload and to improve training realism, including: (1) automated assessment of trainee performance, (2) automated attacks that respond dynamically to the student's actions, (3) a full fidelity training environment, (4) an unrestricted user interface incorporating real tools, and (5) continuous, remote accessibility via the Web.
ER  - 

TY  - JOUR
T1  - File Marshal: Automatic extraction of peer-to-peer data
JO  - Digital Investigation
VL  - 4, Supplement
IS  - 0
SP  - 43
EP  - 48
PY  - 2007/9//
T2  - 
AU  - Adelstein, Frank
AU  - Joyce, Robert A.
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2007.06.016
UR  - http://www.sciencedirect.com/science/article/pii/S1742287607000400
KW  - Peer-to-peer
KW  - P2P
KW  - Forensics
KW  - LimeWire
KW  - File sharing
AB  - Digital forensic investigators often find peer-to-peer, or file sharing, software present on the computers, or the images of the disks, that they examine. Investigators must first determine what P2P software is present and where the associated information is stored, retrieve the information from the appropriate directories, and then analyze the results. File Marshal is a tool that will automatically detect and analyze peer-to-peer client use on a disk. The tool automates what is currently a manual and labor intensive process. It will determine what clients currently are or have been installed on a machine, and then extracts per-user usage information, specifically a list of peer servers contacted, and files that were shared and downloaded. The tool was designed to perform its actions in a forensically sound way, including maintaining a detailed audit trail of all actions performed. File Marshal is extensible, using a configuration file to specify details about specific peer-to-peer clients (e.g., location of log files and registry keys indicating installation). This paper describes the general design and features of File Marshal, its current status, and the plans for continued development and release. When complete, File Marshal, a National Institute of Justice funded effort, will be disseminated to law enforcement at no cost.
ER  - 

TY  - CHAP
AU  - Liu, Dale
T1  - Chapter 9 - Collecting the Volatile Data from a Router
A2  - Liu, Dale 
BT  - Cisco Router and Switch Forensics
PB  - Syngress
CY  - Boston
PY  - 2009///
SP  - 305
EP  - 389
SN  - 978-1-59749-418-2
DO  - http://dx.doi.org/10.1016/B978-1-59749-418-2.00009-0
UR  - http://www.sciencedirect.com/science/article/pii/B9781597494182000090
AB  - Publisher Summary
This chapter discusses how volatile data from a Cisco router's RAM can be collected for digital forensic investigation. Prior to connecting to the router one should ensure that one has all of the required equipment at hand and with planning what needs to be collected. The chapter covered how one can connect to the router, what one needs to record, and the commands that are associated with this process. It is generally best to make a direct connection to the router via the console port rather than accessing it through a network connection. Where a direct connection to the console port is not possible, use of the SSH encrypted protocol to remotely access the router is preferred, if enabled. There are a number of key points to remember when collecting volatile evidence from a router or switch. One should record the entire console session, run show commands from a script, record volatile information, and record actual time. While collecting, one should not reboot the router, access the router through network, and run configuration commands. For documentation, a log of all commands that have been run should be maintained. The most effective way to capture and analyze a router involves the creation of a core dump. A core dump will contain the complete memory image of the router at the time it was created. After collection, the data should be analyzed to determine the cause of the intrusion. Tools such as Nipper, RAT, and CREED can point to problems with the security of the router.
ER  - 

TY  - JOUR
T1  - The first 10 years of the Trojan Horse defence
JO  - Computer Fraud & Security
VL  - 2015
IS  - 1
SP  - 5
EP  - 13
PY  - 2015/1//
T2  - 
AU  - Bowles, Stephen
AU  - Hernandez-Castro, Julio
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(15)70005-9
UR  - http://www.sciencedirect.com/science/article/pii/S1361372315700059
AB  - Apprehended criminals throughout history have always attempted to put the blame on someone else, a strategy popularly known as a SODDI defence (Some Other Dude Did It). When this defence is used, the act of the crime (actus reus) and the guilty mind (mens rea) is blamed on another party. A Trojan Horse Defence (THD) is a type of modern SODDI defence, where the mens rea and actus reus are blamed on a piece of software, known as a trojan.1

It has now become common for people accused of some computer-related crime to claim that the responsibility lies with malware placed on their machine without their knowledge.

This so-called Trojan Horse Defence (THD) was first used a decade ago. In this article, Stephen Bowles and Julio Hernandez-Castro of the University of Kent undertake a timely retrospective with an in-depth and critical literature review plus a detailed look at the peculiarities of many court cases from around the world.
ER  - 

TY  - CHAP
AU  - Malin, Cameron H.
AU  - Casey, Eoghan
AU  - Aquilina, James M.
T1  - Chapter 1 - Malware Incident Response: Volatile Data Collection and Examination on a Live Linux System
A2  - Malin, Cameron H.  
A2  - Aquilina, Eoghan CaseyJames M. 
BT  - Malware Forensics Field Guide for Linux Systems
PB  - Syngress
CY  - Boston
PY  - 2014///
SP  - 1
EP  - 106
SN  - 978-1-59749-470-0
DO  - http://dx.doi.org/10.1016/B978-1-59749-470-0.00001-2
UR  - http://www.sciencedirect.com/science/article/pii/B9781597494700000012
ER  - 

TY  - JOUR
T1  - The growing need for on-scene triage of mobile devices
JO  - Digital Investigation
VL  - 6
IS  - 3–4
SP  - 112
EP  - 124
PY  - 2010/5//
T2  - Embedded Systems Forensics: Smart Phones, GPS Devices, and Gaming Consoles
AU  - Mislan, Richard P.
AU  - Casey, Eoghan
AU  - Kessler, Gary C.
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2010.03.001
UR  - http://www.sciencedirect.com/science/article/pii/S1742287610000149
KW  - Mobile device forensics
KW  - Cell phone forensics
KW  - On-scene triage inspection
KW  - Mobile device technician
AB  - The increasing number of mobile devices being submitted to Digital Forensic Laboratories (DFLs) is creating a backlog that can hinder investigations and negatively impact public safety and the criminal justice system. In a military context, delays in extracting intelligence from mobile devices can negatively impact troop and civilian safety as well as the overall mission. To address this problem, there is a need for more effective on-scene triage methods and tools to provide investigators with information in a timely manner, and to reduce the number of devices that are submitted to DFLs for analysis. Existing tools that are promoted for on-scene triage actually attempt to fulfill the needs of both on-scene triage and in-lab forensic examination in a single solution. On-scene triage has unique requirements because it is a precursor to and distinct from the forensic examination process, and may be performed by mobile device technicians rather than forensic analysts. This paper formalizes the on-scene triage process, placing it firmly in the overall forensic handling process and providing guidelines for standardization of on-scene triage. In addition, this paper outlines basic requirements for automated triage tools.
ER  - 

TY  - CHAP
AU  - Malin, Cameron H.
AU  - Casey, Eoghan
AU  - Aquilina, James M.
T1  - Chapter 3 - Postmortem Forensics: Discovering and Extracting Malware and Associated Artifacts from Linux Systems
A2  - Malin, Cameron H.  
A2  - Aquilina, Eoghan CaseyJames M. 
BT  - Malware Forensics Field Guide for Linux Systems
PB  - Syngress
CY  - Boston
PY  - 2014///
SP  - 163
EP  - 211
SN  - 978-1-59749-470-0
DO  - http://dx.doi.org/10.1016/B978-1-59749-470-0.00003-6
UR  - http://www.sciencedirect.com/science/article/pii/B9781597494700000036
ER  - 

TY  - CHAP
AU  - Gurkok, Cem
T1  - Chapter 4 - Securing Cloud Computing Systems
A2  - Vacca, John R. 
BT  - Network and System Security (Second Edition)
PB  - Syngress
CY  - Boston
PY  - 2014///
SP  - 83
EP  - 126
SN  - 978-0-12-416689-9
DO  - http://dx.doi.org/10.1016/B978-0-12-416689-9.00004-6
UR  - http://www.sciencedirect.com/science/article/pii/B9780124166899000046
KW  - cloud computing
KW  - data storage
KW  - cloud layers
KW  - public
KW  - private
KW  - virtual private
KW  - hybrid
KW  - cloud security
KW  - legal risks
KW  - general risks
AB  - Cloud computing is a method of delivering computing resources. Cloud computing services ranging from data storage and processing to software, such as customer relationship management systems, are now available instantly and on demand. In times of financial and economic hardship, this new low cost of ownership model for computing has gotten lots of attention and is seeing increasing global investment. Generally speaking, cloud computing provides implementation agility, lower capital expenditure, location independence, resource pooling, broad network access, reliability, scalability, elasticity, and ease of maintenance. While in most cases cloud computing can improve security due to ease of management, the provider’s lack of knowledge and experience can jeopardize customer environments. This chapter aims to discuss various cloud computing environments and methods to make them more secure for hosting companies and their customers.
ER  - 

TY  - JOUR
T1  - AlmaNebula: A Computer Forensics Framework for the Cloud
JO  - Procedia Computer Science
VL  - 19
IS  - 0
SP  - 139
EP  - 146
PY  - 2013///
T2  - The 4th International Conference on Ambient Systems, Networks and Technologies (ANT 2013), the 3rd International Conference on Sustainable Energy Information Technology (SEIT-2013)
AU  - Federici, Corrado
SN  - 1877-0509
DO  - http://dx.doi.org/10.1016/j.procs.2013.06.023
UR  - http://www.sciencedirect.com/science/article/pii/S1877050913006315
KW  - Forensics as a service
KW  - Computer forensics framework
KW  - Commodity computing
KW  - Big data
KW  - Web scale
KW  - Distributed processing
AB  - Abstract
Scalability, fault tolerance and collaborative processing across possibly dispersed sites are key enablers of modern computer forensics applications, that must be able to elastically accommodate all kinds of digital investigations, without wasting resources or fail to deliver timely outcomes. Traditional tools running in a standalone or client- server setups may fall short when handling the multi terabyte scale of a case just above average or, conversely, lie mainly underutilized when dealing with few digital evidences. A new category of applications that leverage the opportunities offered by modern Cloud Computing (CC) platforms, where scalable computational power and storage capacity can be engaged and decommissioned on demand, allow one to conveniently master huge amounts of information that otherwise could be impossible to wield. This paper discusses the design goals, technical requirements and architecture of AlmaNebula, a conceptual framework for the analysis of digital evidences built on top of a Cloud infrastructure, which aims to embody the concept of “Forensics as a service”.
ER  - 

TY  - CHAP

T1  - Index
A2  - from, Eoghan CaseyWith contributions
A2  - Altheide, Cory
A2  - Daywalt, Christopher
A2  - Donno, Andrea de
A2  - Forte, Dario
A2  - Holley, James O.
A2  - Johnston, Andy
A2  - Knijff, Ronald van der
A2  - Kokocinski, Anthony
A2  - Luehr, Paul H.
A2  - Maguire, Terrance
A2  - Pittman, Ryan D.
A2  - Rose, Curtis W.
A2  - Schwerha, Joseph J.
A2  - Shaver, Dave 
A2  - Smith, Jessica Reust 
BT  - Handbook of Digital Forensics and Investigation
PB  - Academic Press
CY  - San Diego
PY  - 2010///
SP  - 559
EP  - 567
SN  - 978-0-12-374267-4
DO  - http://dx.doi.org/10.1016/B978-0-12-374267-4.00020-3
UR  - http://www.sciencedirect.com/science/article/pii/B9780123742674000203
ER  - 

TY  - CHAP
AU  - Forte, Dario
AU  - Donno, Andrea de
T1  - Chapter 10 - Mobile Network Investigations
A2  - from, Eoghan CaseyWith contributions
A2  - Altheide, Cory
A2  - Daywalt, Christopher
A2  - Donno, Andrea de
A2  - Forte, Dario
A2  - Holley, James O.
A2  - Johnston, Andy
A2  - Knijff, Ronald van der
A2  - Kokocinski, Anthony
A2  - Luehr, Paul H.
A2  - Maguire, Terrance
A2  - Pittman, Ryan D.
A2  - Rose, Curtis W.
A2  - Schwerha, Joseph J.
A2  - Shaver, Dave 
A2  - Smith, Jessica Reust 
BT  - Handbook of Digital Forensics and Investigation
PB  - Academic Press
CY  - San Diego
PY  - 2010///
SP  - 517
EP  - 557
SN  - 978-0-12-374267-4
DO  - http://dx.doi.org/10.1016/B978-0-12-374267-4.00010-0
UR  - http://www.sciencedirect.com/science/article/pii/B9780123742674000100
AB  - Publisher Summary
This chapter discusses mobile networks and how they can be useful in digital investigations. The chapter focuses on cellular and PCS technologies. Methods of obtaining location information from mobile networks are described. The content and analysis of usage logs and billing records are covered, and the usefulness of text and multimedia messaging are demonstrated. This chapter also focuses on techniques and tools for capturing and analyzing traffic on mobile networks, culminating with a discussion of the legal and operational implication of interception. Mobile network investigations are also commonly performed for “conventional crimes,” often focusing on location information, logs of telephone calls, printouts of SMS messages, and associated metadata. A mobile device is generally defined as any instrument that can connect to and operate on a mobile network, including cellular telephones, wireless modems, and pagers. Although a significant amount of useful digital evidence associated with mobile devices like cellular telephones is stored in embedded flash memory “Embedded System Analysis,” the associated network is also a rich source of evidence. It is useful for investigators to understand the underlying network technologies, the types of data than can exist on mobile networks, and approaches to collecting and analyzing these sources of digital evidence.
ER  - 

TY  - CHAP
AU  - Chuvakin, Anton
AU  - Schmidt, Kevin
AU  - Phillips, Chris
T1  - Chapter 16 - Log Management Procedures: Log Review, Response, and Escalation
A2  - Chuvakin, Anton
A2  - Schmidt, Kevin 
A2  - Phillips, Chris 
BT  - Logging and Log Management
PB  - Syngress
CY  - Boston
PY  - 2013///
SP  - 267
EP  - 304
SN  - 978-1-59749-635-3
DO  - http://dx.doi.org/10.1016/B978-1-59-749635-3.00016-6
UR  - http://www.sciencedirect.com/science/article/pii/B9781597496353000166
AB  - Abstract

This chapter provides an introduction to log review, response, and escalation for log management. Examples using Payment Card Industry (PCI) Data Security Standard (DSS) will be a running theme throughout this chapter. The idea is to illustrate how to apply the concepts in the real world. This means examples are geared toward PCI standards, but they can easily be adapted and extended to fit any environment. In essence, this chapter develops a set of steps and procedures which you can begin using today. An added side benefit of this chapter is insight into interpreting and applying standards to log management.

Keywords

Escalation, Procedures, PCI, DSS, QSA, Compliance, Log review, Operational procedures, Log file, Event, Event id, Log, Log record, Log entry, Log record type, Log entry type, Situational awareness
ER  - 

TY  - CHAP

T1  - Appendix A - Free, Non-open Tools of Note
A2  - Altheide, Cory  
A2  - Carvey, Harlan 
BT  - Digital Forensics with Open Source Tools
PB  - Syngress
CY  - Boston
PY  - 2011///
SP  - 241
EP  - 255
SN  - 978-1-59749-586-8
DO  - http://dx.doi.org/10.1016/B978-1-59749-586-8.00012-1
UR  - http://www.sciencedirect.com/science/article/pii/B9781597495868000121
ER  - 

TY  - CHAP
AU  - Gurkok, Cem
T1  - Chapter 6 - Securing Cloud Computing Systems
A2  - Vacca, John R. 
BT  - Computer and Information Security Handbook (Second Edition)
PB  - Morgan Kaufmann
CY  - Boston
PY  - 2013///
SP  - 97
EP  - 123
SN  - 978-0-12-394397-2
DO  - http://dx.doi.org/10.1016/B978-0-12-394397-2.00006-4
UR  - http://www.sciencedirect.com/science/article/pii/B9780123943972000064
KW  - cloud computing
KW  - data storage
KW  - cloud layers
KW  - public
KW  - private
KW  - virtual private
KW  - hybrid
KW  - cloud security
KW  - legal risks
KW  - general risks
AB  - Cloud computing is a method of delivering computing resources. Cloud computing services ranging from data storage and processing to software, such as customer relationship management systems, are now available instantly and on demand. In times of financial and economic hardship, this new low cost of ownership model for computing has gotten lots of attention and is seeing increasing global investment. Generally speaking, cloud computing provides implementation agility, lower capital expenditure, location independence, resource pooling, broad network access, reliability, scalability, elasticity, and ease of maintenance. While in most cases cloud computing can improve security due to ease of management, the provider’s lack of knowledge and experience can jeopardize customer environments. This chapter aims to discuss various cloud computing environments and methods to make them more secure for hosting companies and their customers.
ER  - 

TY  - CHAP
AU  - Malin, Cameron H.
AU  - Casey, Eoghan
AU  - Aquilina, James M.
T1  - Chapter 1 - Malware Incident Response: Volatile Data Collection and Examination on a Live Windows System
A2  - Malin, Cameron H.
A2  - Casey, Eoghan 
A2  - Aquilina, James M. 
BT  - Malware Forensic Field Guide for Windows Systems
PB  - Syngress
CY  - Boston
PY  - 2012///
SP  - 1
EP  - 91
SN  - 978-1-59749-472-4
DO  - http://dx.doi.org/10.1016/B978-1-59749-472-4.00001-9
UR  - http://www.sciencedirect.com/science/article/pii/B9781597494724000019
AB  - Process memory associated with malware, together with passwords, Internet Protocol addresses, Security Event Log entries, and other contextual details, can provide digital investigators with an understanding of malware and its use on a Windows system. Therefore, it is important to preserve, in a forensically sound manner, both volatile data and select nonvolatile data memory on a system that is suspected to be infected with malware. This chapter provides a methodology for preserving volatile data on a Windows system during a malware incident, with the assumption that readers possess a live response toolkit consisting of trusted tools, or a tool suite designed to automatically collect digital evidence from Windows systems during incident response. The “Tool Box” section of the chapter discusses many live response tool suites available today. Ultimately, the chapter aims to provide readers with a walkthrough of the live response process and the digital evidence that should be collected.

Keywords: Auto-starting location, F-Response, FastDump, Live response, Local collection, NetBIOS, Nigilant32, Prefetch file, Remote collection, Stateful information, Volatile data
ER  - 

TY  - CHAP
AU  - Conrad, Eric
AU  - Misenar, Seth
AU  - Feldman, Joshua
T1  - Chapter 3 - Domain 2: Access control
A2  - Conrad, Eric
A2  - Misenar, Seth 
A2  - Feldman, Joshua 
BT  - CISSP Study Guide
PB  - Syngress
CY  - Boston
PY  - 2010///
SP  - 37
EP  - 89
SN  - 978-1-59749-563-9
DO  - http://dx.doi.org/10.1016/B978-1-59749-563-9.00003-2
UR  - http://www.sciencedirect.com/science/article/pii/B9781597495639000032
AB  - Publisher Summary
This chapter deals with access control, which is the basis for all security disciplines, not just IT security. The purpose of access control is to allow authorized users access to appropriate data and deny access to unauthorized users. Access controls protect against threats such as unauthorized access, inappropriate modification of data, and loss of confidentiality. Access control is performed by implementing strong technical, physical, and administrative measures. This chapter also defines the CIA triads that are comprised of protecting confidentiality, integrity, and availability. All three pieces work together to provide assurance that data and systems remain secure. It should not assume that one part of the triad is more important than another. Every IT system will require a different prioritization of the three, depending on the data, user community, and timeliness required for accessing the data.
ER  - 

TY  - CHAP
AU  - Pittman, Ryan D.
AU  - Shaver, Dave
T1  - Chapter 5 - Windows Forensic Analysis
A2  - from, Eoghan CaseyWith contributions
A2  - Altheide, Cory
A2  - Daywalt, Christopher
A2  - Donno, Andrea de
A2  - Forte, Dario
A2  - Holley, James O.
A2  - Johnston, Andy
A2  - Knijff, Ronald van der
A2  - Kokocinski, Anthony
A2  - Luehr, Paul H.
A2  - Maguire, Terrance
A2  - Pittman, Ryan D.
A2  - Rose, Curtis W.
A2  - Schwerha, Joseph J.
A2  - Shaver, Dave 
A2  - Smith, Jessica Reust 
BT  - Handbook of Digital Forensics and Investigation
PB  - Academic Press
CY  - San Diego
PY  - 2010///
SP  - 209
EP  - 300
SN  - 978-0-12-374267-4
DO  - http://dx.doi.org/10.1016/B978-0-12-374267-4.00005-7
UR  - http://www.sciencedirect.com/science/article/pii/B9780123742674000057
AB  - Publisher Summary
This chapter provides technical methods and techniques to help practitioners extract and interpret data of investigative value from computers running Windows operating systems. An important aspect of conducting advanced forensic analysis is understanding the mechanisms underlying fundamental operations on Windows systems such as the boot process, file creation and deletion, and use of removable storage media. By understanding how to aggregate and correlate data on Windows systems, digital investigators are better able to get the “big picture” (such as an overall theory of user action and a timeline), as well as overcoming specific technical obstacles. It is not surprising that the majority of systems that digital investigators are called upon to examine run a Windows operating system. Whether investigating child pornography, intellectual property theft, or Internet Relay Chat (IRC) bot infection, it is a safe bet that knowledge of Windows operating systems, and its associated artifacts, will aid investigators in their task. It is important for forensic examiners to understand the Windows startup process for a number of reasons beyond simply interrupting the boot process to view and document the CMOS configuration. Ever since examiners figured out that there might be more to a file than meets the eye, they have been interested in Metadata, the information that describes or places data in context, without being part of the data that is the primary focus of the user. There are two types of metadata: file system metadata and application (or file) metadata.
ER  - 

TY  - CHAP
AU  - Ko, Ryan K.L.
AU  - Choo, Kim-Kwang Raymond
T1  - Chapter 1 - Cloud security ecosystem
A2  - Choo, Ryan KoKim-Kwang Raymond 
BT  - The Cloud Security Ecosystem
PB  - Syngress
CY  - Boston
PY  - 2015///
SP  - 1
EP  - 14
SN  - 978-0-12-801595-7
DO  - http://dx.doi.org/10.1016/B978-0-12-801595-7.00001-X
UR  - http://www.sciencedirect.com/science/article/pii/B978012801595700001X
KW  - Cloud security ecosystem
KW  - Cloud security
KW  - Cloud data privacy
KW  - Cloud computing
AB  - Abstract
This chapter introduces the reader to the initial developments of the cloud computing industry, consolidated cloud-related terminologies, and concepts, and explains the main reasons and causes of the cloud security and privacy concerns.
ER  - 

TY  - JOUR
T1  - Inter-organizational future proof EHR systems: A review of the security and privacy related issues
JO  - International Journal of Medical Informatics
VL  - 78
IS  - 3
SP  - 141
EP  - 160
PY  - 2009/3//
T2  - 
AU  - van der Linden, Helma
AU  - Kalra, Dipak
AU  - Hasman, Arie
AU  - Talmon, Jan
SN  - 1386-5056
DO  - http://dx.doi.org/10.1016/j.ijmedinf.2008.06.013
UR  - http://www.sciencedirect.com/science/article/pii/S1386505608001081
KW  - Computerized Medical Records Systems
KW  - Data security
KW  - Access policy
KW  - Standards
KW  - Networked care
AB  - Objectives
Identification and analysis of privacy and security related issues that occur when health information is exchanged between health care organizations.
Methods
Based on a generic scenario questions were formulated to reveal the occurring issues. Possible answers were verified in literature.
Results
Ensuring secure health information exchange across organizations requires a standardization of security measures that goes beyond organizational boundaries, such as global definitions of professional roles, global standards for patient consent and semantic interoperable audit logs.
Conclusion
As to be able to fully address the privacy and security issues in interoperable EHRs and the long-life virtual EHR it is necessary to realize a paradigm shift from storing all incoming information in a local system to retrieving information from external systems whenever that information is deemed necessary for the care of the patient.
ER  - 

TY  - JOUR
T1  - On Incident Handling and Response: A state-of-the-art approach
JO  - Computers & Security
VL  - 25
IS  - 5
SP  - 351
EP  - 370
PY  - 2006/7//
T2  - 
AU  - Mitropoulos, Sarandis
AU  - Patsos, Dimitrios
AU  - Douligeris, Christos
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/j.cose.2005.09.006
UR  - http://www.sciencedirect.com/science/article/pii/S0167404805001574
KW  - Incident Handling
KW  - Incident Response
KW  - Computer forensics
KW  - Internet forensics
KW  - Software forensics
KW  - Trace-back mechanisms
AB  - Incident Response has always been an important aspect of Information Security but it is often overlooked by security administrators. Responding to an incident is not solely a technical issue but has many management, legal, technical and social aspects that are presented in this paper. We propose a detailed management framework along with a complete structured methodology that contains best practices and recommendations for appropriately handling a security incident. We also present the state-of-the art technology in computer, network and software forensics as well as automated trace-back artifacts, schemas and protocols. Finally, we propose a generic Incident Response process within a corporate environment.
ER  - 

TY  - CHAP
AU  - Ridge, Enda
T1  - Chapter 13 - Testing Data
A2  - Ridge, Enda 
BT  - Guerrilla Analytics
PB  - Morgan Kaufmann
CY  - Boston
PY  - 2015///
SP  - 159
EP  - 175
SN  - 978-0-12-800218-6
DO  - http://dx.doi.org/10.1016/B978-0-12-800218-6.00013-8
UR  - http://www.sciencedirect.com/science/article/pii/B9780128002186000138
KW  - Testing
KW  - Coherence
KW  - Data
KW  - Completeness
KW  - Correctness
KW  - Accountability
AB  - Summary
This chapter focuses on testing of data. Testing of data involves checking data that the team receives to detect defects. These defects can arise at data extraction, data receipt, and at data load into the team’s Data Manipulation Environment (DME).

You will learn about the five ways in which data can be tested in Guerrilla Analytics. These are (1) completeness, (2) correctness, (3) consistency, (4) coherence, and (5) accountability. This chapter will also describe some tips for successful data testing including scopes of tests, storage of test results, common test routines, and automation of testing.
ER  - 

TY  - JOUR
T1  - SoTE: Strategy of Triple-E on solving Trojan defense in Cyber-crime cases
JO  - Computer Law & Security Review
VL  - 26
IS  - 1
SP  - 52
EP  - 60
PY  - 2010/1//
T2  - 
AU  - Kao, Da-Yu
AU  - Wang, Shiuh-Jeng
AU  - Fu-Yuan Huang, Frank
SN  - 0267-3649
DO  - http://dx.doi.org/10.1016/j.clsr.2009.09.008
UR  - http://www.sciencedirect.com/science/article/pii/S0267364909001575
KW  - Cyber-crime
KW  - Cyber criminology
KW  - Digital evidence
KW  - Trojan defense
KW  - Triple-E strategy
AB  - Cyber activity has become an essential part of the general public's everyday life. The hacking threats of Cyber-crime are becoming more sophisticated as internet communication services are more popular. To further confirm the final finding of Cyber-crime, this study proposes three analytical tools to clarify the Cyber-crime issues by means of Ideal Log, M-N model and MDFA (Multi-faceted Digital Forensics Analysis) strategy, where Ideal Log is identified as a traceable element of digital evidence including four elements of IP Address, Timestamp, Digital Action, and Response Message. M-N model applies a formal method for collating and analyzing data sets of investigation-relevant logs in view of connected time with ISP logs. MDFA strategy attempts to outline the basic elements of Cyber-crime using new procedural investigative steps, and combining universal types of evidential information in terms of Evidence, Scene, Victim, and Suspect. After researchers figure out what has happened in Cyber-crime events, it will be easier to communicate with offenders, victims or related people. SoTE (Strategy of Triple-E) is discussed to observe Cyber-crime from the viewpoints of Education, Enforcement and Engineering. That approach is further analyzed from the fields of criminology, investigation and forensics. Each field has its different focus in dealing with diverse topics, such as: the policy of 6W1H (What, Which, When, Where, Who, Why, and How) questions, the procedure of MDFA strategy, the process of ideal Logs and M-N model. In addition, the case study and proposed suggestion of this paper are presented to counter Cyber-crime.
ER  - 

TY  - JOUR
T1  - An empirical study of automatic event reconstruction systems
JO  - Digital Investigation
VL  - 3, Supplement
IS  - 0
SP  - 108
EP  - 115
PY  - 2006/9//
T2  - The Proceedings of the 6th Annual Digital Forensic Research Workshop (DFRWS '06)
AU  - Jeyaraman, Sundararaman
AU  - Atallah, Mikhail J.
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2006.06.013
UR  - http://www.sciencedirect.com/science/article/pii/S1742287606000752
KW  - Intrusion analysis
KW  - Digital forensics
KW  - Event reconstruction
KW  - Incident response
AB  - Reconstructing the sequence of computer events that led to a particular event is an essential part of the digital investigation process. The ability to quantify the accuracy of automatic event reconstruction systems is an essential step in standardizing the digital investigation process thereby making it resilient to tactics such as the Trojan horse defense. In this paper, we present findings from an empirical study to measure and compare the accuracy and effectiveness of a suite of such event reconstruction techniques. We quantify (as applicable) the rates of false positives and false negatives, and scalability in terms of both computational burden and memory-usage. Some of our findings are quite surprising in the sense of not matching a priori expectations, and whereas other findings qualitatively match the a priori expectations they were never before quantitatively put to the test to determine the boundaries of their applicability. For example, our results show that automatic event reconstruction systems proposed in literature have very high false-positive rates (up to 96%).
ER  - 

TY  - JOUR
T1  - Analyzing multiple logs for forensic evidence
JO  - Digital Investigation
VL  - 4, Supplement
IS  - 0
SP  - 82
EP  - 91
PY  - 2007/9//
T2  - 
AU  - Arasteh, Ali Reza
AU  - Debbabi, Mourad
AU  - Sakha, Assaad
AU  - Saleh, Mohamed
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2007.06.013
UR  - http://www.sciencedirect.com/science/article/pii/S1742287607000448
KW  - Forensic analysis
KW  - Log analysis
KW  - Formal methods
KW  - Model checking
KW  - Logging systems
KW  - Log correlation
AB  - Information stored in logs of a computer system is of crucial importance to gather forensic evidence of investigated actions or attacks. Analysis of this information should be rigorous and credible, hence it lends itself to formal methods. We propose a model checking approach to the formalization of the forensic analysis of logs. A set of logs is modeled as a tree whose labels are events extracted from the logs. In order to provide a structure to these events, we express each event as a term of algebra. The signature of the algebra is carefully chosen to include all relevant information necessary to conduct the analysis. Properties of the model, attack scenarios, and event sequences are expressed as formulas of a logic having dynamic, linear, temporal, and modal characteristics. Moreover, we provide a tableau-based proof system for this logic upon which a model checking algorithm can be developed. We use our model in a case study to demonstrate how events leading to an SYN attack can be reconstructed from a number of system logs.
ER  - 

TY  - JOUR
T1  - Self-sustaining, efficient and forward-secure cryptographic constructions for Unattended Wireless Sensor Networks
JO  - Ad Hoc Networks
VL  - 10
IS  - 7
SP  - 1204
EP  - 1220
PY  - 2012/9//
T2  - 
AU  - Yavuz, Attila Altay
AU  - Ning, Peng
SN  - 1570-8705
DO  - http://dx.doi.org/10.1016/j.adhoc.2012.03.006
UR  - http://www.sciencedirect.com/science/article/pii/S1570870512000479
KW  - Applied cryptography
KW  - Unattended Wireless Sensor Networks (UWSNs)
KW  - Digital signatures
KW  - Forward security
KW  - Aggregate signatures
AB  - Unattended Wireless Sensor Networks (UWSNs) operating in hostile environments face great security and performance challenges due to the lack of continuous real-time communication with the final data receivers (e.g., mobile data collectors). The lack of real-time communication forces sensors to accumulate sensed data possibly for long time periods, along with the corresponding authentication tags. It also makes UWSNs vulnerable to active adversaries, which can compromise sensors and manipulate the collected data. Hence, it is critical to have forward security property such that even if the adversary can compromise the current keying materials, she cannot forge authentication tags generated before the compromise. Forward secure and aggregate signature schemes are developed to address these issues. Unfortunately, existing schemes either impose substantial overhead, or do not allow public verifiability, thereby impractical for resource-constrained UWSNs.

In this paper, we propose a new class of cryptographic schemes, referred to as Hash-BasedSequentialAggregate andForwardSecureSignature (HaSAFSS), which allows a signer to sequentially generate a compact, fixed-size, and publicly verifiable signature efficiently. We develop three HaSAFSS schemes, Symmetric HaSAFSS (Sym-HaSAFSS), Elliptic Curve Cryptography (ECC) based HaSAFSS (ECC-HaSAFSS) and self-SUstaining HaSAFSS (SU-HaSAFSS). These schemes integrate the efficiency of MAC-based aggregate signatures and the public verifiability of Public Key Cryptography (PKC)-based signatures by preserving forward security via Timed-Release Encryption (TRE). We demonstrate that our schemes are secure and also significantly more efficient than previous approaches.
ER  - 

TY  - JOUR
T1  - Log management for effective incident response
JO  - Network Security
VL  - 2005
IS  - 9
SP  - 4
EP  - 7
PY  - 2005/9//
T2  - 
AU  - Forte, Dario
SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/S1353-4858(05)70279-8
UR  - http://www.sciencedirect.com/science/article/pii/S1353485805702798
AB  - Log file correlation is related to two distinct activities: Intrusion Detection and Network Forensics. It is more important than ever that these two disciplines work together, and in cooperation, to avoid points of failure. This article presents an overview of log analysis and correlation, with special emphasis on the tools and techniques for managing them within a network forensics context.
ER  - 

TY  - JOUR
T1  - Enhancing the core scientific metadata model to incorporate derived data
JO  - Future Generation Computer Systems
VL  - 29
IS  - 2
SP  - 612
EP  - 623
PY  - 2013/2//
T2  - Special section: Recent advances in e-Science
AU  - Yang, Erica
AU  - Matthews, Brian
AU  - Wilson, Michael
SN  - 0167-739X
DO  - http://dx.doi.org/10.1016/j.future.2011.08.003
UR  - http://www.sciencedirect.com/science/article/pii/S0167739X11001427
KW  - Data management
KW  - Information management
KW  - Derived data management
KW  - Data analysis
KW  - Data provenance
KW  - Large scale facilities
KW  - Neutron sources
KW  - Scientific process
AB  - Much of the value in scientific data is provided not only in the raw data but through the analysis of that data to derive published results. A study of the data analysis process for structural science has shown that various data sets derived from the raw data are of use to scientists and should be stored with the raw data. The Core Scientific MetaData model (CSMD) is used by a number of large scientific facilities to catalogue scientific data. The current version provides support to experimental scientists to access their raw data, facility managers for accounting for facility usage and other scientists who wish to re-use raw experimental data. In this paper, extensions to the CSMD are presented to describe the analysis process so that the provenance of the derived data can be captured. A pilot implementation incorporating derived data through this extended CSMD model has been trialled with experimental scientists. Remaining challenges to the adoption of CSMD and the tools it supports are considered.
ER  - 

TY  - CHAP
AU  - Grasdal, Martin
AU  - Hunter, Laura E.
AU  - Cross, Michael
AU  - Hunter, Laura
AU  - Shinder, Debra Littlejohn
AU  - Shinder, Thomas W.
T1  - Chapter 11 - MCSE 70-293: Planning, Implementing, and Maintaining a Security Framework
A2  - Hunter, Martin GrasdalLaura E.  
A2  - Shinder, Michael CrossLaura HunterDebra Littlejohn ShinderThomas W. 
BT  - MCSE (Exam 70-293) Study Guide
PB  - Syngress
CY  - Rockland
PY  - 2003///
SP  - 781
EP  - 859
SN  - 978-1-931836-93-7
DO  - http://dx.doi.org/10.1016/B978-193183693-7/50015-4
UR  - http://www.sciencedirect.com/science/article/pii/B9781931836937500154
ER  - 

TY  - JOUR
T1  - Incorporating collaboratory concepts into informatics in support of translational interdisciplinary biomedical research
JO  - International Journal of Medical Informatics
VL  - 78
IS  - 1
SP  - 10
EP  - 21
PY  - 2009/1//
T2  - 
AU  - Lee, E. Sally
AU  - McDonald, David W.
AU  - Anderson, Nicholas
AU  - Tarczy-Hornoch, Peter
SN  - 1386-5056
DO  - http://dx.doi.org/10.1016/j.ijmedinf.2008.06.011
UR  - http://www.sciencedirect.com/science/article/pii/S138650560800107X
KW  - Collaboration
KW  - Biomedical informatics
KW  - Computer supported collaborative work
KW  - Collaboratories
KW  - Social and technical issues
KW  - Bioinformatics
AB  - Due to its complex nature, modern biomedical research has become increasingly interdisciplinary and collaborative in nature. Although a necessity, interdisciplinary biomedical collaboration is difficult. There is, however, a growing body of literature on the study and fostering of collaboration in fields such as computer supported cooperative work (CSCW) and information science (IS). These studies of collaboration provide insight into how to potentially alleviate the difficulties of interdisciplinary collaborative research. We, therefore, undertook a cross cutting study of science and engineering collaboratories to identify emergent themes. We review many relevant collaboratory concepts: (a) general collaboratory concepts across many domains: communication, common workspace and coordination, and data sharing and management, (b) specific collaboratory concepts of particular biomedical relevance: data integration and analysis, security structure, metadata and data provenance, and interoperability and data standards, (c) environmental factors that support collaboratories: administrative and management structure, technical support, and available funding as critical environmental factors, and (d) future considerations for biomedical collaboration: appropriate training and long-term planning. In our opinion, the collaboratory concepts we discuss can guide planning and design of future collaborative infrastructure by biomedical informatics researchers to alleviate some of the difficulties of interdisciplinary biomedical collaboration.
ER  - 

TY  - CHAP
AU  - Malin, Cameron
AU  - Casey, Eoghan
AU  - Aquilina, James
T1  - Chapter 1 - Linux Malware Incident Response
A2  - Malin, Cameron
A2  - Casey, Eoghan 
A2  - Aquilina, James 
BT  - Linux Malware Incident Response
PB  - Syngress
CY  - Boston
PY  - 2013///
SP  - 1
EP  - 66
SN  - 978-0-12-409507-6
DO  - http://dx.doi.org/10.1016/B978-0-12-409507-6.00001-7
UR  - http://www.sciencedirect.com/science/article/pii/B9780124095076000017
ER  - 

TY  - JOUR
T1  - A practical off-line taint analysis framework and its application in reverse engineering of file format
JO  - Computers & Security
VL  - 51
IS  - 0
SP  - 1
EP  - 15
PY  - 2015/6//
T2  - 
AU  - Cui, Baojiang
AU  - Wang, Fuwei
AU  - Guo, Tao
AU  - Dong, Guowei
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/j.cose.2015.02.006
UR  - http://www.sciencedirect.com/science/article/pii/S0167404815000218
KW  - Taint analysis
KW  - Data flow tracking
KW  - Binary instrumentation
KW  - Format reverse engineering
KW  - Fuzzing test
KW  - Virtualized execution
KW  - Parallelization
AB  - Abstract
This paper presents FlowWalker, a novel dynamic taint analysis framework that aims to extract the complete taint data flow while eliminating the bottlenecks that occur in existing tools, with applications to file-format reverse engineering. The framework proposes a multi-taint-tag assembly-level taint propagation strategy. FlowWalker separates taint tracking operations from execution with an off-line structure, utilizes memory-mapped files to enhance I/O efficiency, processes taint paths during virtual execution playback, and uses parallelization and pipelining mechanisms to achieve speedup. Based on the semantic correlations implied by the taint path information, this paper presents an algorithm for extracting the structures of unknown file formats. According to test data, the overall program runtime ranges from 92.98% to 208.01% of the length of the underlying instrumentation alone, while the speed enhancement is 60% compared to another well-featured tool in Windows. Medium-complexity file formats are correctly partitioned, and the constant fields are extracted. Due to its efficiency and scalability, FlowWalker can address the needs of further security-related research.
ER  - 

TY  - CHAP

T1  - Index
A2  - Choo, Ryan KoKim-Kwang Raymond 
BT  - The Cloud Security Ecosystem
PB  - Syngress
CY  - Boston
PY  - 2015///
SP  - 519
EP  - 530
SN  - 978-0-12-801595-7
DO  - http://dx.doi.org/10.1016/B978-0-12-801595-7.09992-4
UR  - http://www.sciencedirect.com/science/article/pii/B9780128015957099924
ER  - 

TY  - JOUR
T1  - Privacy and security in teleradiology
JO  - European Journal of Radiology
VL  - 73
IS  - 1
SP  - 31
EP  - 35
PY  - 2010/1//
T2  - Teleradiology
AU  - Ruotsalainen, Pekka
SN  - 0720-048X
DO  - http://dx.doi.org/10.1016/j.ejrad.2009.10.018
UR  - http://www.sciencedirect.com/science/article/pii/S0720048X09005828
KW  - Security
KW  - Privacy protection
KW  - Security policy
KW  - Security enhancing technology
KW  - Risk analysis
KW  - Access control
KW  - Audit-logs
KW  - Certification
AB  - Teleradiology is probably the most successful eHealth service available today. Its business model is based on the remote transmission of radiological images (e.g. X-ray and CT-images) over electronic networks, and on the interpretation of the transmitted images for diagnostic purpose. Two basic service models are commonly used teleradiology today. The most common approach is based on the message paradigm (off-line model), but more developed teleradiology systems are based on the interactive use of PACS/RIS systems. Modern teleradiology is also more and more cross-organisational or even cross-border service between service providers having different jurisdictions and security policies. This paper defines the requirements needed to make different teleradiology models trusted. Those requirements include a common security policy that covers all partners and entities, common security and privacy protection principles and requirements, controlled contracts between partners, and the use of security controls and tools that supporting the common security policy. The security and privacy protection of any teleradiology system must be planned in advance, and the necessary security and privacy enhancing tools should be selected (e.g. strong authentication, data encryption, non-repudiation services and audit-logs) based on the risk analysis and requirements set by the legislation. In any case the teleradiology system should fulfil ethical and regulatory requirements. Certification of the whole teleradiology service system including security and privacy is also proposed. In the future, teleradiology services will be an integrated part of pervasive eHealth. Security requirements for this environment including dynamic and context aware security services are also discussed in this paper.
ER  - 

TY  - CHAP
AU  - Williams, Branden R.
AU  - Chuvakin, Anton A.
AU  - Milroy, Derek
T1  - Chapter 10 - Logging events and monitoring the cardholder data environment
A2  - Milroy, Branden R. WilliamsAnton A. ChuvakinDerek 
BT  - PCI Compliance (Fourth Edition)
PB  - Syngress
CY  - Boston
PY  - 2015///
SP  - 197
EP  - 234
SN  - 978-0-12-801579-7
DO  - http://dx.doi.org/10.1016/B978-0-12-801579-7.00010-8
UR  - http://www.sciencedirect.com/science/article/pii/B9780128015797000108
KW  - Logging
KW  - SIEM
KW  - Intelligence
KW  - Infrastructure Visibility
AB  - Abstract
This chapter discusses how to configure logging and event data to capture the information you need to be able to show and maintain PCI compliance, as well as how to perform other security monitoring tasks.
ER  - 

TY  - CHAP
AU  - Grimes, Stephen L.
T1  - 104 - Health Insurance Portability and Accountability Act (HIPAA) and Its Implications for Clinical Engineering
A2  - Dyro, Joseph F 
BT  - Clinical Engineering Handbook
PB  - Academic Press
CY  - Burlington
PY  - 2004///
SP  - 498
EP  - 506
T2  - Biomedical Engineering
SN  - 978-0-12-226570-9
DO  - http://dx.doi.org/10.1016/B978-012226570-9/50114-9
UR  - http://www.sciencedirect.com/science/article/pii/B9780122265709501149
ER  - 

TY  - JOUR
T1  - Forensic implications of System Resource Usage Monitor (SRUM) data in Windows 8
JO  - Digital Investigation
VL  - 12
IS  - 0
SP  - 53
EP  - 65
PY  - 2015/3//
T2  - 
AU  - Khatri, Yogesh
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2015.01.002
UR  - http://www.sciencedirect.com/science/article/pii/S1742287615000031
KW  - System Resource Usage Monitor
KW  - SRUM
KW  - Windows 8
KW  - Forensics
KW  - Process metrics
AB  - Abstract
The Microsoft Windows 8 operating system has a newly added feature to track system resource usage, specifically process and network metrics over time. Process related information such as process owner, CPU cycles used, data bytes read/written, and network data (sent/received) are continuously recorded by a mechanism called System Resource Usage Monitor (SRUM). This paper describes the SRUM mechanism, its databases, Windows registry entries, data logging, and potential uses in a forensic examination. Prior to this applied research, no tools were available to parse the SRUM data to a usable format. As part of this paper, two scripts have been developed to aid forensic examiners who would want to read, parse, and decode this information from a forensic disk image.
ER  - 

TY  - JOUR
T1  - Advances in Onion Routing: Description and backtracing/investigation problems
JO  - Digital Investigation
VL  - 3
IS  - 2
SP  - 85
EP  - 88
PY  - 2006/6//
T2  - 
AU  - Forte, Dario
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2006.04.007
UR  - http://www.sciencedirect.com/science/article/pii/S1742287606000272
KW  - Onion routing
KW  - Antiforensic
KW  - Anonymizers
KW  - Privacy
KW  - Security
KW  - Network investigation
KW  - Log files analysis
KW  - Traffic analysis
KW  - Network intelligence
KW  - Digital forensics
AB  - Onion routers were born about 10 years ago as a sort of blended military/research project. The main goal of it was the avoidance of traffic analysis (TA). TA is used in part to identify the remote IP addresses that a given host seeks to contact. This technique may have various purposes, from simple statistical analysis to illegal interception. In response, to protect personnel whose communications are being monitored by hostile forces, researchers from the US Naval Research Laboratory conceived a system, dubbed “Onion Routing”, that eludes the above two operations.

In this author's opinion, the original “owners” of the project lost control of it. Software is freely available that enables anyone to utilize a network of onion routers. The result is a strong evolution (in terms of “privacy”) that is very difficult to manage, especially when an international investigation must be performed.
ER  - 

TY  - JOUR
T1  - From the Semantic Web to social machines: A research challenge for AI on the World Wide Web
JO  - Artificial Intelligence
VL  - 174
IS  - 2
SP  - 156
EP  - 161
PY  - 2010/2//
T2  - Special Review Issue
AU  - Hendler, Jim
AU  - Berners-Lee, Tim
SN  - 0004-3702
DO  - http://dx.doi.org/10.1016/j.artint.2009.11.010
UR  - http://www.sciencedirect.com/science/article/pii/S0004370209001404
AB  - The advent of social computing on the Web has led to a new generation of Web applications that are powerful and world-changing. However, we argue that we are just at the beginning of this age of “social machines” and that their continued evolution and growth requires the cooperation of Web and AI researchers. In this paper, we show how the growing Semantic Web provides necessary support for these technologies, outline the challenges we see in bringing the technology to the next level, and propose some starting places for the research.
ER  - 

TY  - CHAP
AU  - Taylor, Laura P.
T1  - Chapter 20 - Independent Assessor Audit Guide
A2  - Taylor, Laura P. 
BT  - FISMA Compliance Handbook (Second Edition)
PB  - Syngress
CY  - Boston
PY  - 2013///
SP  - 239
EP  - 273
SN  - 978-0-12-405871-2
DO  - http://dx.doi.org/10.1016/B978-0-12-405871-2.00020-8
UR  - http://www.sciencedirect.com/science/article/pii/B9780124058712000208
KW  - Independent assessor
KW  - Integrity
KW  - Confidentiality
KW  - Availability
KW  - Audit
KW  - Scanners
KW  - Security testing tools
KW  - Tools
KW  - GAO
KW  - Government Accountability Office
AB  - Abstract
Independent assessors review the Security Package and test the security controls for compliance. Tests for confidentiality, integrity, and availability are performed. Tests can be manual or automated, or a combination of both. Independent assessors commonly use checklists for performing security reviews and audits. Audits are also performed by Inspector Generals and the Government Accountability Office.
ER  - 

TY  - CHAP
AU  - Metheny, Matthew
T1  - Chapter 9 - The FedRAMP Cloud Computing Security Requirements
A2  - Metheny, Matthew 
BT  - Federal Cloud Computing
PB  - Syngress
CY  - Boston
PY  - 2013///
SP  - 241
EP  - 327
SN  - 978-1-59749-737-4
DO  - http://dx.doi.org/10.1016/B978-1-59-749737-4.00009-5
UR  - http://www.sciencedirect.com/science/article/pii/B9781597497374000095
AB  - Abstract

This chapter gives an overview of the Federal Risk and Authorization Management Program (FedRAMP) security control selection process. The FedRAMP security control requirements are discussed in detail. Specific topics include role and responsibilities, policies and procedures, and the harmonization governance process for the continual review and updates. Finaly a detailed catalog of the FedRAMP security control baseline is provided as a reference.

Keywords

FedRAMP, Cloud computing, Security requirements, Control selection process, JAB, Security control baseline
ER  - 

TY  - CHAP
AU  - Hosmer, Chet
T1  - Chapter 5 - Forensic Evidence Extraction (JPEG and TIFF)
A2  - Hosmer, Chet 
BT  - Python Forensics
PB  - Syngress
CY  - Boston
PY  - 2014///
SP  - 125
EP  - 163
SN  - 978-0-12-418676-7
DO  - http://dx.doi.org/10.1016/B978-0-12-418676-7.00005-0
UR  - http://www.sciencedirect.com/science/article/pii/B9780124186767000050
KW  - Python Image Library
KW  - Dictionary
KW  - Exchangeable Image File Format
KW  - Global Positioning System
KW  - Longitude
KW  - Latitude
KW  - JPEG
KW  - TIFF
KW  - FCC
KW  - Key value pairs
KW  - csv
KW  - argparse
KW  - Logging
KW  - Class
AB  - Abstract
This chapter takes a deep dive into performing data extraction from digital images. I employ the Python Image Library to perform the extraction and rendering of the resulting data. I explore, demonstrate, and experiment with code that uses the Python Dictionary structure and integrate that knowledge into a new Python p-gpsExtraction application.
ER  - 

TY  - CHAP
AU  - Hosmer, Chet
T1  - Chapter 4 - Forensic Searching and Indexing Using Python
A2  - Hosmer, Chet 
BT  - Python Forensics
PB  - Syngress
CY  - Boston
PY  - 2014///
SP  - 91
EP  - 123
SN  - 978-0-12-418676-7
DO  - http://dx.doi.org/10.1016/B978-0-12-418676-7.00004-9
UR  - http://www.sciencedirect.com/science/article/pii/B9780124186767000049
KW  - Searching
KW  - Indexing
KW  - Bytearray
KW  - Set
KW  - List
KW  - Hex
KW  - Deductive reasoning
KW  - Inductive reasoning
KW  - Inculpatory
KW  - Exculpatory
KW  - Keywords
KW  - Buffer
KW  - Buffer printing
KW  - Hex/ASCII printing
KW  - p-Search
KW  - argparse
KW  - Class
KW  - Methods
AB  - Abstract
This chapter takes an in-depth look at text searching and indexing using Python. This chapter covers new language elements such as bytearrays and sets. The focus is on the development of a combined search and indexing program that leverages key Python capabilities to produce a fast, efficient, easy to use, and extensible application that executes unmodified on multiple platforms.
ER  - 

TY  - CHAP
AU  - Samani, Raj
AU  - Honan, Brian
AU  - Reavis, Jim
T1  - Chapter 9 - Dark Clouds, What to Do In The Event of a Security Incident
A2  - Reavis, Raj SamaniBrian HonanJim 
BT  - CSA Guide to Cloud Computing
PB  - Syngress
CY  - Boston
PY  - 2015///
SP  - 171
EP  - 190
SN  - 978-0-12-420125-5
DO  - http://dx.doi.org/10.1016/B978-0-12-420125-5.00009-1
UR  - http://www.sciencedirect.com/science/article/pii/B9780124201255000091
KW  - Human resources
KW  - Incident response
KW  - Network operations
KW  - Service provider
AB  - Abstract
With corporate resources now stored and managed (to some extent) by third parties, the need to have a strong security incident management policy is imperative. This chapter will recommend the steps required to address the fundamental question, what happens when something does go wrong?
ER  - 

TY  - CHAP
AU  - Rittinghouse, John W.
AU  - Ransome, James F.
T1  - 6 - IM Security Risk Management
A2  - Rittinghouse, John W.  
A2  - Ransome, James F. 
BT  - IM Instant Messaging Security
PB  - Digital Press
CY  - Burlington
PY  - 2005///
SP  - 165
EP  - 194
SN  - 978-1-55558-338-5
DO  - http://dx.doi.org/10.1016/B978-155558338-5/50008-4
UR  - http://www.sciencedirect.com/science/article/pii/B9781555583385500084
AB  - Publisher Summary
This chapter focuses on the general requirements for an instant messaging (IM) Risk Management program and describes the various regulatory requirements that drive the need for risk mitigation in an enterprise. Risk management challenges associated with the use of IM include revealing confidential information over an unsecured delivery channel, spreading viruses and worms, and exposing the network to backdoor Trojan horses. IM is also vulnerable to denial-of-service attacks, hijacking sessions, and legal liability resulting from downloading copyrighted files. The numerous vulnerabilities inherent in IM dictate that senior management perform a risk assessment on the business benefit of allowing the use of public IM on corporate networks. Corporations should establish a policy to restrict public IM usage and require employees to sign an acknowledgment of receipt of the policy. They should include the vulnerabilities of public IM in information security awareness training, ensure a strong virus protection program, ensure a strong patch (software update) management program, and create firewall rules to block IM delivery and file sharing. Technology vendors have released various enterprise IM products for corporate use that authenticate, encrypt, audit, log, and monitor IM communication, and provide an alternative to public IM solutions used in a corporate environment. Risk management considerations for IM include antivirus, privacy, antihijacking, firewall, intrusion detection, and other risk mitigation controls and practices. Regulations imposed by the Securities Exchange Commission (SEC), the Freedom of Information Act and the Sarbanes-Oxley Act require the financial institutions to meet security compliance mandates, failing to which can result in significant financial and legal liabilities. In July 2004, the Federal Deposit Insurance Corporation (FDIC) issued its 5,300 member banks and financial institutions a warning about unmanaged IM access.
ER  - 

TY  - JOUR
T1  - A model-integrated authoring environment for privacy policies
JO  - Science of Computer Programming
VL  - 89, Part B
IS  - 0
SP  - 105
EP  - 125
PY  - 2014/9/1/
T2  - Special issue on Success Stories in Model Driven Engineering
AU  - Nadas, Andras
AU  - Levendovszky, Tihamer
AU  - Jackson, Ethan K.
AU  - Madari, Istvan
AU  - Sztipanovits, Janos
SN  - 0167-6423
DO  - http://dx.doi.org/10.1016/j.scico.2013.05.004
UR  - http://www.sciencedirect.com/science/article/pii/S016764231300124X
KW  - Privacy policies
KW  - Model-integrated computing
KW  - Constraint logic programming
AB  - Abstract
Privacy policies are rules designed to ensure that individuals’ health data are properly protected. Health Information Systems (HIS) are legally required to adhere to these policies. Since privacy policies are imposed on complex software systems, it is extremely hard to reason about their conformance and consistency. In order to address this problem, we have created a model-driven authoring environment to formally specify privacy policies originally defined in legal terms. In our observation, appropriate formalization of our policy language enabled formal analysis of its policies; these features were key to a successful model-driven engineering process. In this paper we present our modeling language and show its semantic anchoring to analyzable logic programs. We report on several projects where our approach is being applied and validated.
ER  - 

TY  - CHAP
AU  - Bourne, Kelly C.
T1  - Chapter 15 - Security
A2  - Bourne, Kelly C. 
BT  - Application Administrators Handbook
PB  - Morgan Kaufmann
CY  - Boston
PY  - 2014///
SP  - 242
EP  - 267
SN  - 978-0-12-398545-3
DO  - http://dx.doi.org/10.1016/B978-0-12-398545-3.00015-7
UR  - http://www.sciencedirect.com/science/article/pii/B9780123985453000157
KW  - security
KW  - authentication
KW  - LDAP
KW  - SSO
KW  - terminated users
KW  - security groups
KW  - VPN
KW  - shared accounts
KW  - test accounts
KW  - passwords
KW  - National Vulnerability Database
KW  - ACL
KW  - log files
KW  - encryption
KW  - firewalls
KW  - ports
KW  - SSL
KW  - HTTPS
KW  - hacking
KW  - social engineering
KW  - penetration testing
AB  - Abstract
Applications and their data need to be secure. This chapter describes steps that can be taken to help ensure that an application and its data are as secure as possible. Some steps are putting users into appropriate security groups, removing users leave the organization, limiting access to the server, applying the concept of least privilege, not sharing accounts, requiring strong passwords, controlling the admin account, applying patches, turning off unused features, and closing unused ports.
ER  - 

TY  - JOUR
T1  - Aspects of privacy for electronic health records
JO  - International Journal of Medical Informatics
VL  - 80
IS  - 2
SP  - e26
EP  - e31
PY  - 2011/2//
T2  - Special Issue: Security in Health Information Systems
AU  - Haas, Sebastian
AU  - Wohlgemuth, Sven
AU  - Echizen, Isao
AU  - Sonehara, Noboru
AU  - Müller, Günter
SN  - 1386-5056
DO  - http://dx.doi.org/10.1016/j.ijmedinf.2010.10.001
UR  - http://www.sciencedirect.com/science/article/pii/S1386505610001723
KW  - Privacy
KW  - Electronic health records
KW  - Disclosure of personal data to third parties
KW  - Usage control
AB  - Patients’ medical data have been originally generated and maintained by health professionals in several independent electronic health records (EHRs). Centralized electronic health records accumulate medical data of patients to improve their availability and completeness; EHRs are not tied to a single medical institution anymore. Nowadays enterprises with the capacity and knowledge to maintain this kind of databases offer the services of maintaining EHRs and adding personal health data by the patients. These enterprises get access on the patients’ medical data and act as a main point for collecting and disclosing personal data to third parties, e.g. among others doctors, healthcare service providers and drug stores. Existing systems like Microsoft HealthVault and Google Health comply with data protection acts by letting the patients decide on the usage and disclosure of their data. But they fail in satisfying essential requirements to privacy. We propose a privacy-protecting information system for controlled disclosure of personal data to third parties. Firstly, patients should be able to express and enforce obligations regarding a disclosure of health data to third parties. Secondly, an organization providing EHRs should neither be able to gain access to these health data nor establish a profile about patients.
ER  - 

TY  - CHAP
AU  - Watson, David
AU  - Jones, Andrew
T1  - Chapter 10 - Case Management
A2  - Jones, David WatsonAndrew 
BT  - Digital Forensics Processing and Procedures
PB  - Syngress
CY  - Boston
PY  - 2013///
SP  - 421
EP  - 508
SN  - 978-1-59749-742-8
DO  - http://dx.doi.org/10.1016/B978-1-59749-742-8.00010-8
UR  - http://www.sciencedirect.com/science/article/pii/B9781597497428000108
KW  - rules of evidence
KW  - evidence preparation
KW  - testimony
KW  - expert witness
AB  - Abstract
This chapter looks at the presentation of evidence. It addresses the issues of the rules of evidence and the handling of the evidence throughout its lifecycle. It discusses the issues of report preparation and the level of detail that is normally required. It also looks at the types of report that may be required and addresses the issue of report and presentation quality. It also addresses testimony in court and expert witnesses and reviews some of the main reasons why cases in the courts fail.
ER  - 

TY  - CHAP
AU  - Liu, Dale
T1  - Chapter 7 - Understanding the Methods and Mindset of the Attacker
A2  - Liu, Dale 
BT  - Cisco Router and Switch Forensics
PB  - Syngress
CY  - Boston
PY  - 2009///
SP  - 207
EP  - 249
SN  - 978-1-59749-418-2
DO  - http://dx.doi.org/10.1016/B978-1-59749-418-2.00007-7
UR  - http://www.sciencedirect.com/science/article/pii/B9781597494182000077
AB  - Publisher Summary
Digital forensic investigators have to find out how the network was compromised. If an investigator understands a hacker's mindset, methodology and tools, he or she would be more aware of steps he or she can take to detect these types of activities, and would be better prepared to detect when a malicious activity has taken place. This chapter discusses the typical phases of attack that an attacker may consider in pursuit of a mark, including Google hacking, social engineering, and data mining. Network scanning is a way to determine the state of a system's network connection and listening services, whether they are open, closed, or filtered. Attackers may use a scanner to perform a denial of service (DoS) attack, but for the most part they will use it to determine the composition and state of the hosts on the network. Network scanning tools including Nmap, Netcat, Nessus, and Maltego are discussed. An attacker can exploit weaknesses of network through tools such as the Metasploit Framework. The chapter discusses the critical changes to the operating system known typically as rootkits as well as anti-discovery methods attackers that can use like tunneling and backdoors. Furthermore, the chapter looks at some efforts to protect data, as well as the tactics attackers may try on systems and how they can try to cover their tracks.
ER  - 

TY  - JOUR
T1  - The Clinical Value of Large Neuroimaging Data Sets in Alzheimer's Disease
JO  - Neuroimaging Clinics of North America
VL  - 22
IS  - 1
SP  - 107
EP  - 118
PY  - 2012/2//
T2  - Imaging in Alzheimer's Disease and Other Dementias
AU  - Toga, Arthur W.
SN  - 1052-5149
DO  - http://dx.doi.org/10.1016/j.nic.2011.11.008
UR  - http://www.sciencedirect.com/science/article/pii/S105251491100178X
KW  - Neuroimaging
KW  - Database
KW  - Alzheimer's disease
KW  - Informatics
ER  - 

TY  - JOUR
T1  - Enhancing workflow with a semantic description of scientific intent
JO  - Web Semantics: Science, Services and Agents on the World Wide Web
VL  - 9
IS  - 2
SP  - 222
EP  - 244
PY  - 2011/7//
T2  - Provenance in the Semantic Web
AU  - Pignotti, Edoardo
AU  - Edwards, Peter
AU  - Gotts, Nick
AU  - Polhill, Gary
SN  - 1570-8268
DO  - http://dx.doi.org/10.1016/j.websem.2011.05.001
UR  - http://www.sciencedirect.com/science/article/pii/S1570826811000229
KW  - Workflow
KW  - Semantic Grid
KW  - Provenance
KW  - Scientist’s intent
KW  - Rules
AB  - Scientists are becoming increasingly dependent upon resources available through the Internet including, for example, datasets and computational modelling services, which are changing the way they conduct their research activities. This paper investigates the use of workflow tools enhanced with semantics to facilitate the design, execution, analysis and interpretation of workflow experiments and exploratory studies. Current workflow technologies do not incorporate any representation of experimental constraints and goals, which we refer to in this paper as scientist’s intent. This paper proposes an abstract model of intent based on the Open Provenance Model (OPM) specification. To realise this model a framework based upon a number of Semantic Web technologies has been developed, including the OWL ontology language and the Semantic Web Rule Language (SWRL). Through the use of social simulation case studies the paper illustrates the benefits of using this framework in terms of workflow monitoring, workflow provenance and annotation of experimental results.
ER  - 

TY  - CHAP
AU  - Wheeler, Evan
T1  - Chapter 1 - The Security Evolution
A2  - Wheeler, Evan 
BT  - Security Risk Management
PB  - Syngress
CY  - Boston
PY  - 2011///
SP  - 3
EP  - 19
SN  - 978-1-59749-615-5
DO  - http://dx.doi.org/10.1016/B978-1-59749-615-5.00001-3
UR  - http://www.sciencedirect.com/science/article/pii/B9781597496155000013
KW  - confidentiality
KW  - integrity
KW  - availability
KW  - accountability
KW  - best practices
KW  - risk management
KW  - least privilege
KW  - defense in depth
KW  - separation of duties
KW  - threats
KW  - checklist
KW  - perimeter
AB  - Publisher Summary
Information Security (or Information Assurance) needs to be viewed through the lens of business context to see the added value of basing security program on a risk model. Risk management is by no means a ubiquitous foundation for information security programs, but many visionaries in the field recognize that the future of information security has to be focused on risk decisions if one has any hope of combating the ever-changing threat landscape and constantly increasing business demands. Traditional information security practices were primarily concerned with keeping the “bad guys” out. The assumption was that anything outside your network (or physical walls) was un-trusted and anything inside could be trusted. Although this perspective can be very comforting and simplifies your protection activities (in an “ignorance is bliss” kind of way), unfortunately, it is also greatly flawed. The goal of Information Security is to ensure the confidentiality, integrity, availability, and accountability of the resources.
 
Anyone who is new to the Information Security field may wonder why there is so much controversy about the value of a risk-focused program. To really appreciate both why these debates continue and how risk management is so fundamentally tied to the most core security principles, we need to start with a brief history about the evolution of information security as a discipline and review a few foundational concepts. This chapter summarizes the struggles of checklist-oriented practitioners trying to move security initiatives forward without the clear business focus and lays out a new vision for how risk management can better support the dynamic business environments. Once you understand some of the basic security principles, models, and concepts, it will help you to choose risk assessment activities that will most benefit your organization. This chapter also lays the general foundation to introduce the specific risk methodologies in Chapter 2.
ER  - 

TY  - CHAP

T1  - Index
A2  - Johnson, Leighton R. 
BT  - Computer Incident Response and Forensics Team Management
PB  - Syngress
CY  - Boston
PY  - 2014///
SP  - 329
EP  - 334
SN  - 978-1-59749-996-5
DO  - http://dx.doi.org/10.1016/B978-1-59749-996-5.00037-6
UR  - http://www.sciencedirect.com/science/article/pii/B9781597499965000376
ER  - 

TY  - JOUR
T1  - Vulnerabilities and mitigation techniques toning in the cloud: A cost and vulnerabilities coverage optimization approach using Cuckoo search algorithm with Lévy flights
JO  - Computers & Security
VL  - 48
IS  - 0
SP  - 1
EP  - 18
PY  - 2015/2//
T2  - 
AU  - Zineddine, Mhamed
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/j.cose.2014.09.002
UR  - http://www.sciencedirect.com/science/article/pii/S0167404814001333
KW  - Cloud computing
KW  - ICT security
KW  - Vulnerabilities mapping
KW  - Cuckoo search algorithm
KW  - Lévy flights algorithm
KW  - Optimization
AB  - Abstract
Information and Communication Technology (ICT) security issues have been a major concern for decades. Today's ICT infrastructure faces sophisticated attacks using combinations of multiple vulnerabilities to penetrate networks with devastating impact. With the recent rise of cloud computing as a new utility computing paradigm, organizations have been considering it as a viable option to outsource major IT services in order to cut costs. Some organizations have opted for a private or hybrid cloud to take advantage of the emerging technologies and services. However, ICT security issues have to be appropriately mitigated. This research proposes a cloud security framework and an approach for vulnerabilities coverage and cost optimization using Cuckoo search algorithm with Lévy flights as random walks. The objective is to mitigate an identified set of vulnerabilities using a selected set of techniques when minimizing cost and maximizing coverage. The results show that Cloud Computing providers and organizations implementing cloud technology within their premises can effectively balance IT security coverage and cost using the proposed approach.
ER  - 

TY  - CHAP

T1  - Glossary
A2  - Jones, David WatsonAndrew 
BT  - Digital Forensics Processing and Procedures
PB  - Syngress
CY  - Boston
PY  - 2013///
SP  - e1
EP  - e33
SN  - 978-1-59749-742-8
DO  - http://dx.doi.org/10.1016/B978-1-59749-742-8.09989-1
UR  - http://www.sciencedirect.com/science/article/pii/B9781597497428099891
ER  - 

TY  - CHAP
AU  - Daniel, Larry E.
AU  - Daniel, Lars E.
T1  - Chapter 39 - Databases
A2  - Daniel, Larry E.  
A2  - Daniel, Lars E. 
BT  - Digital Forensics for Legal Professionals
PB  - Syngress
CY  - Boston
PY  - 2012///
SP  - 287
EP  - 293
SN  - 978-1-59749-643-8
DO  - http://dx.doi.org/10.1016/B978-1-59749-643-8.00039-0
UR  - http://www.sciencedirect.com/science/article/pii/B9781597496438000390
KW  - Database Metadata
KW  - Relational Databases
KW  - Database Types
KW  - Database Recovery
AB  - Publisher Summary
Databases have been around as long as man has had a need to store and retrieve information. Beginning as simple lists of accounts and transactions to ledger books and finally to electronic form, databases are in many cases the number one data asset of a company or person. This chapter discusses what databases are, a little about how they work, and finally, how they can contain evidence that can be a factor in criminal and civil cases. It also talks about databases and database management systems and also examines a relational database and how the data are structured. Finally, the chapter looks at the methods and issues involved in getting evidence from databases and about the data themselves in the form of database metadata. The challenges presented by database applications and the need to use the same application the user has to get data out of a database system whenever possible are also discussed.
 
The sole purpose of database systems is to allow for fast and accurate storage and retrieval of records. Data is the lifeblood of businesses from the smallest home business to global mega-corporations. This chapter explores how data is stored, how it is retrieved, and how it can be a factor in electronic evidence. We will also look at the challenges involved in getting data as evidence and at metadata, or data about the data and its value as evidence.
ER  - 

TY  - CHAP
AU  - Wright, Craig
T1  - Chapter 22 - Operations Security
A2  - Wright, Craig 
BT  - The IT Regulatory and Standards Compliance Handbook
PB  - Syngress
CY  - Burlington
PY  - 2008///
SP  - 673
EP  - 693
SN  - 978-1-59749-266-9
DO  - http://dx.doi.org/10.1016/B978-1-59749-266-9.00022-9
UR  - http://www.sciencedirect.com/science/article/pii/B9781597492669000229
AB  - Publisher Summary
This chapter defines operations security. Organizational Operations Security is about maximizing the Confidentiality, Integrity and Availability of the systems used by the organization using a risk-based approach. To be effective, it is important that all standards, guidelines, and procedures are clearly defined in advance and are aligned to the organization's management practices. Operations Security consists of a series of controls that are designed to maintain the security of the organization's resources from design to deployment to disposal. This is achieved through minimizing the effects of the threats and extent of the vulnerabilities faced by the organization. This leads to reduced asset losses and thus lower risk. Operational controls are also described that are implemented to protect the day-to-day running of the organization. These involve everything from hardware controls (such as maintenance) through to controls designed to monitor privileged-entities. Operational controls include the monitoring and general review of systems. Media controls expand on the idea of controls that cover the handling of sensitive information.
ER  - 

TY  - CHAP
AU  - Van Horn, J.D.
T1  - Databases
A2  - Toga, Arthur W. 
BT  - Brain Mapping
PB  - Academic Press
CY  - Waltham
PY  - 2015///
SP  - 685
EP  - 695
SN  - 978-0-12-397316-0
DO  - http://dx.doi.org/10.1016/B978-0-12-397025-1.00351-1
UR  - http://www.sciencedirect.com/science/article/pii/B9780123970251003511
KW  - Data sharing
KW  - Databases
KW  - Informatics
KW  - Multisite trials
KW  - Neuroimaging
AB  - Abstract
The advent of in vivo neuroimaging has led to the gathering of incredible quantities of digital information about the form and function of the human brain. In many cases, multisite collaborative teams obtain data from unique subject samples with an eye toward sharing that data with others. A range of neuroimaging databasing approaches have streamlined the transmission, storage, and dissemination of data from such brain imaging studies. This article reviews the history of neuroimaging databases and illustrates a specific example of a fully curated archive used as a resource for active data sharing. Finally, specific use-case examples are provided for how data have been reused and repurposed toward obtaining new knowledge about the living brain in health and disease.
ER  - 

TY  - CHAP
AU  - Altheide, Cory
AU  - Carvey, Harlan
T1  - Chapter 5 - Linux Systems and Artifacts
A2  - Altheide, Cory  
A2  - Carvey, Harlan 
BT  - Digital Forensics with Open Source Tools
PB  - Syngress
CY  - Boston
PY  - 2011///
SP  - 95
EP  - 121
SN  - 978-1-59749-586-8
DO  - http://dx.doi.org/10.1016/B978-1-59749-586-8.00005-4
UR  - http://www.sciencedirect.com/science/article/pii/B9781597495868000054
KW  - linux forensics
KW  - ext2 forensics
KW  - ext3 forensics
KW  - gnome artifacts
AB  - Publisher Summary
Linux has found its way into everything—to children's toys and networking devices to the most powerful supercomputing clusters in the world. Most current Linux systems use the Ext3 file system. This chapter explores some of the Ext2 and 3 specific structures and forensically interesting information available, using the file system abstraction model. Ext file systems have two major components that make up their file system layer structures: the superblock and the group descriptor tables. Understanding the Linux boot process is important when performing an investigation of a Linux system. Knowledge of the files user during system startup can help the examiner determine which version of the operating system was running and when it was installed. This chapter also discusses how directories and files are organized in the file system, how users are managed, and the meaning of file metadata being examined. Because each Linux system can be quite different from any other Linux system, attempting to create an exhaustive list of possible user artifacts would be an exercise in futility. Linux is becoming more and more popular on embedded devices such as mobile phones and tablet computers. Examiners capable of exploiting these data sources for artifacts of interest will be in high demand in the years to come.
 
This chapter discusses the analysis of Linux systems using open source tools. This includes analysis of the Ext3 file system, as well as artifacts of system and user activity, including items generated by the GNOME desktop.
ER  - 

TY  - JOUR
T1  - Access control for smarter healthcare using policy spaces
JO  - Computers & Security
VL  - 29
IS  - 8
SP  - 848
EP  - 858
PY  - 2010/11//
T2  - 
AU  - Ardagna, Claudio A.
AU  - De Capitani di Vimercati, Sabrina
AU  - Foresti, Sara
AU  - Grandison, Tyrone W.
AU  - Jajodia, Sushil
AU  - Samarati, Pierangela
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/j.cose.2010.07.001
UR  - http://www.sciencedirect.com/science/article/pii/S0167404810000623
KW  - Access control
KW  - Break the glass
KW  - Policy spaces
KW  - Exceptions
KW  - Healthcare systems
AB  - A fundamental requirement for the healthcare industry is that the delivery of care comes first and nothing should interfere with it. As a consequence, the access control mechanisms used in healthcare to regulate and restrict the disclosure of data are often bypassed in case of emergencies. This phenomenon, called “break the glass”, is a common pattern in healthcare organizations and, though quite useful and mandatory in emergency situations, from a security perspective, it represents a serious system weakness. Malicious users, in fact, can abuse the system by exploiting the break the glass principle to gain unauthorized privileges and accesses.

In this paper, we propose an access control solution aimed at better regulating break the glass exceptions that occur in healthcare systems. Our solution is based on the definition of different policy spaces, a language, and a composition algebra to regulate access to patient data and to balance the rigorous nature of traditional access control systems with the “delivery of care comes first” principle.
ER  - 

TY  - CHAP
AU  - Carvey, Harlan
T1  - 2 - Tools
A2  - Carvey, Harlan 
BT  - Windows Registry Forensics
PB  - Syngress
CY  - Boston
PY  - 2011///
SP  - 35
EP  - 83
SN  - 978-1-59749-580-6
DO  - http://dx.doi.org/10.1016/B978-1-59749-580-6.00002-4
UR  - http://www.sciencedirect.com/science/article/pii/B9781597495806000024
KW  - Registry
KW  - analysis
KW  - RegRipper
KW  - tools
KW  - F-Response
KW  - autoruns
KW  - rip
KW  - ripxp
AB  - Publisher Summary
This chapter addresses some of the possible scenarios that an analyst may encounter and presents some tools that may be used in various situations focusing on the use of open source and freely available tools. There are a couple of reasons for this, one of them being that such tools are generally accessible to a much wider audience than commercial forensic analysis applications. It is important for analysts to understand the mechanics of what they are trying to achieve and what is going on "under the hood" before using the commercial forensic analysis applications. There are a number of tools available that provide functionality, either in and of themselves, or through process, that commercial forensic analysis applications do not provide. There are tools and mechanisms available, which allow administrators and responders alike to collect and correlate just about any Registry information they may need. An analyst wants to conduct Registry analysis by first monitoring the Registry while conducting some sort of action, such as launching an exploit against a live system, or when installing an application or launching Malware on a system. Monitoring tools are most often used in testing scenarios. RegRipper has proved to be extremely useful and flexible. It is an interface to an "engine" of sorts. RegRipper processes sets of instructions listed in plug-ins files and consists of a scanning engine and a series of plug-ins that contain instructions. The tools or techniques employed depend on how one engages and interacts with the Registry, as well as the goals of the interaction and analysis.
 
The purpose of this chapter is to present some of the methods and tools for interacting with the Windows Registry, from both a live and “postmortem” perspective.
ER  - 

TY  - JOUR
T1  - Tree-formed verification data for trusted platforms
JO  - Computers & Security
VL  - 32
IS  - 0
SP  - 19
EP  - 35
PY  - 2013/2//
T2  - 
AU  - Schmidt, Andreas U.
AU  - Leicher, Andreas
AU  - Brett, Andreas
AU  - Shah, Yogendra
AU  - Cha, Inhyok
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/j.cose.2012.09.004
UR  - http://www.sciencedirect.com/science/article/pii/S016740481200140X
KW  - Trusted platform
KW  - Remote attestation
KW  - Hash tree
KW  - Measurement log
KW  - Verification data
KW  - Validation
AB  - The establishment of trust relationships to a computing platform relies on validation processes. Validation allows an external entity to build trust in the expected behaviour of the platform based on provided evidence of the platform's configuration. In a process like remote attestation, the ‘trusted’ platform submits verification data created during a start up process. These data consist of hardware-protected values of platform configuration registers, containing nested measurement values, e.g., hash values, of loaded or started components. Commonly, the register values are created in linear order by a hardware-secured operation. Fine-grained diagnosis of components, based on the linear order of verification data and associated measurement logs, is not optimal. We propose a method to use tree-formed verification data to validate a platform. Component measurement values represent leaves, and protected registers represent roots of a hash tree. We describe the basic mechanism of validating a platform using tree-formed measurement logs and root registers and show a logarithmic speed-up for the search of faults. Secure creation of a tree is possible using a limited number of hardware-protected registers and a single protected operation. In this way, the security of tree-formed verification data is maintained.
ER  - 

TY  - CHAP
AU  - Fichera, Joe
AU  - Bolt, Steven
T1  - Chapter 6 - Host Analysis
A2  - Fichera, Joe  
A2  - Bolt, Steven 
BT  - Network Intrusion Analysis
PB  - Syngress
CY  - Boston
PY  - 2013///
SP  - 153
EP  - 167
SN  - 978-1-59749-962-0
DO  - http://dx.doi.org/10.1016/B978-1-59-749962-0.00006-7
UR  - http://www.sciencedirect.com/science/article/pii/B9781597499620000067
AB  - Abstract

Often referred to as “Deadbox” forensics, this part of the examination focuses on locating any artifacts, malware, registry keys and any other evidence that can be found on the host or “victim” machine. You may here the initial point of infection referred to as “ground zero.” In this chapter we will examine the more common locations for locating evidence.

Keywords

EnCase, FTK, X-Ways, F-Response, Registry, ADS, Timestomp, Malware, Imaging, Hash, Hash analysis, Signature analysis, Keyword searching, Autorun, Timeline, FireEye, Gold Build
ER  - 

TY  - JOUR
T1  - FriendlyRoboCopy: A GUI to RoboCopy for computer forensic investigators
JO  - Digital Investigation
VL  - 4
IS  - 1
SP  - 16
EP  - 23
PY  - 2007/3//
T2  - 
AU  - LaVelle, Claire
AU  - Konrad, Almudena
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2007.01.001
UR  - http://www.sciencedirect.com/science/article/pii/S1742287607000023
KW  - Digital forensics
KW  - Network forensics
KW  - Drive mapping
KW  - RoboCopy application
KW  - Microsoft OS forensics
KW  - Network system administration
KW  - NAS
KW  - Computer cluster
KW  - Graphical User Interface
KW  - Perl
KW  - Open Source application
AB  - One of the most pressing challenges in digital investigations today is the extraction and forensic preservation of a subset of data on computer clusters and other large storage systems. As the number and capacity of computer systems increases, it is no longer feasible to create forensic duplicates of every system in their entirety. Although forensic tools are being developed to cope with such situations, they do not support all file systems. Experienced digital investigators use tools such as RoboCopy to preserve a subset of data on target systems, and take steps to document their process and results. This paper explores the need for these tools in digital investigations, and demonstrates the strengths and weaknesses of using RoboCopy to acquire data on a network share. This paper then introduces FriendlyRoboCopy, which provides an effective, user-friendly interface to RoboCopy that addresses the requirements of forensic preservation.
ER  - 

TY  - JOUR
T1  - Integrating intrusion alert information to aid forensic explanation: An analytical intrusion detection framework for distributive IDS
JO  - Information Fusion
VL  - 10
IS  - 4
SP  - 325
EP  - 341
PY  - 2009/10//
T2  - Special Issue on Information Fusion in Computer Security
AU  - Sy, Bon K.
SN  - 1566-2535
DO  - http://dx.doi.org/10.1016/j.inffus.2009.01.001
UR  - http://www.sciencedirect.com/science/article/pii/S1566253509000153
KW  - Probabilistic inference
KW  - Model discovery
KW  - Intrusion detection
KW  - Forensic analysis
AB  - The objective of this research is to show an analytical intrusion detection framework (AIDF) comprised of (i) a probability model discovery approach, and (ii) a probabilistic inference mechanism for generating the most probable forensic explanation based on not only just the observed intrusion detection alerts, but also the unreported signature rules that are revealed in the probability model. The significance of the proposed probabilistic inference is its ability to integrate alert information available from IDS sensors distributed across subnets. We choose the open source Snort to illustrate its feasibility, and demonstrate the inference process applied to the intrusion detection alerts produced by Snort. Through a preliminary experimental study, we illustrate the applicability of AIDF for information integration and the realization of (i) a distributive IDS environment comprised of multiple sensors, and (ii) a mechanism for selecting and integrating the probabilistic inference results from multiple models for composing the most probable forensic explanation.
ER  - 

TY  - CHAP
AU  - Lillard, Terrence V.
AU  - Garrison, Clint P.
AU  - Schiller, Craig A.
AU  - Steele, James
T1  - Chapter 1 - What Is Network Forensics?
A2  - Lillard, Terrence V.
A2  - Garrison, Clint P.
A2  - Schiller, Craig A. 
A2  - Steele, James 
BT  - Digital Forensics for Network, Internet, and Cloud Computing
PB  - Syngress
CY  - Boston
PY  - 2010///
SP  - 3
EP  - 20
SN  - 978-1-59749-537-0
DO  - http://dx.doi.org/10.1016/B978-1-59749-537-0.00001-6
UR  - http://www.sciencedirect.com/science/article/pii/B9781597495370000016
AB  - Publisher Summary
Network forensics in the cloud-computing environment could be focused only on data that go to and from the systems that the company has access to, but that would miss other data. Network forensics needs to be part of and work with all the other components that comprise the entire system within the cloud. The network forensics investigator needs to understand that the cloud environment is the space that the company rents on another company's computer systems to perform the work. The cloud-computing model can also be very useful for forensics by allowing storage of very large log files on a storage instance or in a very large database for easy data retrieval and discovery. Network forensics can also have an influence on the outcome of an investigation into an event as long as data is collected at the box and at the entry and exit points of the company network. The use of in-built firewall logs, system logs, and other logs will generally point to an entry time, place, and IP address that can be used to help determine how the event is propagated through the network and what steps can be taken to help minimize any future event by providing solid data on the event. A large part of network forensics is being able to monitor the network traffic in order to isolate the number of servers that need to be taken down for the traditional forensics process.
ER  - 

TY  - CHAP
AU  - Chuvakin, Anton
AU  - Schmidt, Kevin
AU  - Phillips, Chris
T1  - Chapter 19 - Logs and Compliance
A2  - Chuvakin, Anton
A2  - Schmidt, Kevin 
A2  - Phillips, Chris 
BT  - Logging and Log Management
PB  - Syngress
CY  - Boston
PY  - 2013///
SP  - 343
EP  - 366
SN  - 978-1-59749-635-3
DO  - http://dx.doi.org/10.1016/B978-1-59-749635-3.00019-1
UR  - http://www.sciencedirect.com/science/article/pii/B9781597496353000191
AB  - Abstract

This chapter is about logging and compliance with regulations and policies.

Keywords

Logs, PCI DSS, Logging, Compliance, Security monitoring
ER  - 

TY  - CHAP
AU  - Johnson III, Leighton R.
T1  - Section 14 - Forensics Tools
A2  - Johnson, Leighton R. 
BT  - Computer Incident Response and Forensics Team Management
PB  - Syngress
CY  - Boston
PY  - 2014///
SP  - 145
EP  - 165
SN  - 978-1-59749-996-5
DO  - http://dx.doi.org/10.1016/B978-1-59749-996-5.00014-5
UR  - http://www.sciencedirect.com/science/article/pii/B9781597499965000145
KW  - Forensics tools
KW  - live capture
KW  - case management
AB  - Detailed discussion of the various types and kinds of Forensics tools available and what their uses are is found in this section.
ER  - 

TY  - JOUR
T1  - The informatics core of the Alzheimer's Disease Neuroimaging Initiative
JO  - Alzheimer's & Dementia
VL  - 6
IS  - 3
SP  - 247
EP  - 256
PY  - 2010/5//
T2  - 
AU  - Toga, Arthur W.
AU  - Crawford, Karen L.
SN  - 1552-5260
DO  - http://dx.doi.org/10.1016/j.jalz.2010.03.001
UR  - http://www.sciencedirect.com/science/article/pii/S1552526010000610
KW  - ADNI
KW  - Informatics
KW  - Alzheimer's disease
KW  - Data management
AB  - The Alzheimer's Diseases Neuroimaging Initiative project has brought together geographically distributed investigators, each collecting data on the progression of Alzheimer's disease. The quantity and diversity of the imaging, clinical, cognitive, biochemical, and genetic data acquired and generated throughout the study necessitated sophisticated informatics systems to organize, manage, and disseminate data and results. We describe, here, a successful and comprehensive system that provides powerful mechanisms for processing, integrating, and disseminating these data not only to support the research needs of the investigators who make up the Alzheimer's Diseases Neuroimaging Initiative cores, but also to provide widespread data access to the greater scientific community for the study of Alzheimer's Disease.
ER  - 

TY  - JOUR
T1  - Enabling long-term oceanographic research: Changing data practices, information management strategies and informatics
JO  - Deep Sea Research Part II: Topical Studies in Oceanography
VL  - 55
IS  - 18–19
SP  - 2132
EP  - 2142
PY  - 2008/9//
T2  - Palmer, Antarctica Long Term Ecological Research
AU  - Baker, Karen S.
AU  - Chandler, Cynthia L.
SN  - 0967-0645
DO  - http://dx.doi.org/10.1016/j.dsr2.2008.05.009
UR  - http://www.sciencedirect.com/science/article/pii/S0967064508001628
KW  - Data collections
KW  - Data management
KW  - Informatics
KW  - Information centers
KW  - Information systems
KW  - Oceanographic data
AB  - Interdisciplinary global ocean science requires new ways of thinking about data and data management. With new data policies and growing technological capabilities, datasets of increasing variety and complexity are being made available digitally and data management is coming to be recognized as an integral part of scientific research. To meet the changing expectations of scientists collecting data and of data reuse by others, collaborative strategies involving diverse teams of information professionals are developing. These changes are stimulating the growth of information infrastructures that support multi-scale sampling, data repositories, and data integration. Two examples of oceanographic projects incorporating data management in partnership with science programs are discussed: the Palmer Station Long-Term Ecological Research program (Palmer LTER) and the United States Joint Global Ocean Flux Study (US JGOFS). Lessons learned from a decade of data management within these communities provide an experience base from which to develop information management strategies—short-term and long-term. Ocean Informatics provides one example of a conceptual framework for managing the complexities inherent to sharing oceanographic data. Elements are introduced that address the economies-of-scale and the complexities-of-scale pertinent to a broader vision of information management and scientific research.
ER  - 

TY  - JOUR
T1  - Understanding documentation and reconstruction requirements for computer-assisted decision processes
JO  - Decision Support Systems
VL  - 50
IS  - 1
SP  - 316
EP  - 324
PY  - 2010/12//
T2  - 
AU  - Bajcsy, Peter
AU  - Kooper, Rob
AU  - Lee, Sang-Chul
SN  - 0167-9236
DO  - http://dx.doi.org/10.1016/j.dss.2010.08.033
UR  - http://www.sciencedirect.com/science/article/pii/S0167923610001570
KW  - Preservation decisions
KW  - Information granularity
KW  - Evaluations
AB  - Challenges related to documenting and reconstructing computer-assisted decision processes include the selection of information granularity, design of information gathering and reconstruction mechanisms, evaluation of reconstruction value, and storage and computational costs. This article surveys these challenges and explicates an approach to designing and prototyping an evaluation framework for decisions based on image inspection. The framework explored here allows users to analyze storage and computational costs of information gathering as a function of information granularity and then assesses the potential value of decision process reconstructions. We illustrate how evaluations of decision process reconstructions could potentially improve our understanding of future archival needs by simultaneously documenting, preserving and reconstructing computer-assisted decision processes, and evaluating and forecasting computational and storage requirements of the documentation and reconstruction processes over time.
ER  - 

TY  - JOUR
T1  - End-to-end policy based encryption techniques for multi-party data management
JO  - Computer Standards & Interfaces
VL  - 36
IS  - 4
SP  - 689
EP  - 703
PY  - 2014/6//
T2  - Security in Information Systems: Advances and new Challenges.
AU  - Beiter, Michael
AU  - Casassa Mont, Marco
AU  - Chen, Liqun
AU  - Pearson, Siani
SN  - 0920-5489
DO  - http://dx.doi.org/10.1016/j.csi.2013.12.004
UR  - http://www.sciencedirect.com/science/article/pii/S0920548913001785
KW  - Cloud
KW  - Sticky policy
KW  - Policy enforcement
KW  - Privacy
KW  - Secret sharing
AB  - Abstract
We describe a data management solution and associated key management approaches to provide accountability within service provision networks, in particular addressing privacy issues in cloud computing applications. Our solution involves machine readable policies that stick to data to define allowed usage and obligations as data travels across multiple parties. Service providers have fine-grained access to specific data based on agreed policies, enforced by interactions with independent third parties that check for policy compliance before releasing decryption keys required for data access. We describe alternative solutions based upon Public Key Infrastructure (PKI), Identity Based Encryption (IBE) and advanced secret sharing schemes.
ER  - 

TY  - CHAP

T1  - Appendix B - Relevant Incident Response and Forensics Publications from Governmental Agencies and Organizations
A2  - Johnson, Leighton R. 
BT  - Computer Incident Response and Forensics Team Management
PB  - Syngress
CY  - Boston
PY  - 2014///
SP  - 227
EP  - 243
SN  - 978-1-59749-996-5
DO  - http://dx.doi.org/10.1016/B978-1-59749-996-5.00033-9
UR  - http://www.sciencedirect.com/science/article/pii/B9781597499965000339
ER  - 

TY  - CHAP
AU  - Carvey, Harlan
T1  - Chapter 4 - File Analysis
A2  - Carvey, Harlan 
BT  - Windows Forensic Analysis Toolkit (Fourth Edition)
PB  - Syngress
CY  - Boston
PY  - 2014///
SP  - 75
EP  - 118
SN  - 978-0-12-417157-2
DO  - http://dx.doi.org/10.1016/B978-0-12-417157-2.00004-7
UR  - http://www.sciencedirect.com/science/article/pii/B9780124171572000047
KW  - File analysis
KW  - MFT
KW  - Jump List
KW  - DestList
KW  - Prefetch
KW  - Scheduled Tasks
KW  - Recycle Bin
KW  - hibernation
KW  - Apple
KW  - image
KW  - Skype
KW  - EXIF
KW  - metadata
AB  - Windows systems contain a number of files of various formats. These files can also contain data consisting of various structures, some of which are well documented, while others have been discovered through analysis.
ER  - 

TY  - CHAP
AU  - Andress, Jason
AU  - Winterfeld, Steve
T1  - Chapter 6 - Logical Weapons
A2  - Winterfeld, Jason AndressSteve 
BT  - Cyber Warfare (Second Edition)
PB  - Syngress
CY  - Boston
PY  - 2014///
SP  - 103
EP  - 136
SN  - 978-0-12-416672-1
DO  - http://dx.doi.org/10.1016/B978-0-12-416672-1.00006-4
UR  - http://www.sciencedirect.com/science/article/pii/B9780124166721000064
KW  - Access and privilege escalation tool
KW  - Assault tool
KW  - Backdoor
KW  - Computer Network Operations
KW  - Exfiltration tool
KW  - Google hacking
KW  - Logical tool
KW  - Obfuscation tool
KW  - Reconnaissance tool
KW  - Scanning tool
AB  - Abstract
A variety of tools are available for use in cyber warfare, penetration testing, and security in general. And most of them are free (or have free versions) and are available to the general public. For instance, reconnaissance tools can gather general information, search whois and DNS records, and search metadata from media and documents. Scanning tools, such as Nmap and Nessus, can find systems and detect areas where vulnerabilities might exist. Access and privilege escalation tools, such as Metasploit and CANVAS, can penetrate system accounts. Exfiltration tools can encrypt, hide, or smuggle data over common protocols to remove them from a compromised system. Backdoors can sustain a connection to a compromised system. Assault tools can damage or disrupt compromised systems. And obfuscation tools can hide one’s location, logically and physically, during an attack. This chapter discusses how to use these tools and how to defend against an attacker using them.
ER  - 

TY  - CHAP
AU  - Cope, Bill
AU  - Kalantzis, Mary
T1  - 2 - Changing knowledge ecologies and the transformation of the scholarly journal
A2  - Cope, Bill  
A2  - Phillips, Angus 
BT  - The Future of the Academic Journal (Second edition)
PB  - Chandos Publishing
CY  - 
PY  - 2014///
SP  - 9
EP  - 83
SN  - 978-1-84334-783-5
DO  - http://dx.doi.org/10.1533/9781780634647.9
UR  - http://www.sciencedirect.com/science/article/pii/B9781843347835500021
KW  - academic journals
KW  - knowledge ecologies
KW  - publishing technologies
KW  - journal publishing business models
KW  - open access publishing
KW  - peer review
KW  - knowledge evaluation
KW  - citation analyses
KW  - impact metrics
KW  - impact factor
AB  - Abstract:
This chapter is an overview of the current state of scholarly journals, not (just) as an activity to be described in terms of its changing business processes but more fundamentally as the pivot point in a broader knowledge system that is itself in a process of transformation. After locating journals in what we characterize as a process of knowledge design, the chapter goes on to discuss some of the deeply disruptive aspects of the contemporary moment. These not only portend potential transformations in the form of the journal, but possibly also in the knowledge systems that the journal in its heritage form has supported. These disruptive forces are represented by changing technological, economic, distributional, geographic, interdisciplinary and social relations to knowledge.

The chapter goes on to examine three specific breaking points. The first breaking point is in business models – the unsustainable costs and inefficiencies of traditional commercial publishing, the rise of open access and the challenge of developing sustainable publishing models. The second potential breaking point is the credibility of the peer-review system: its accountability, its textual practices, the validity of its measures and its exclusionary network effects. The third breaking point is post-publication evaluation, centred primarily on citation analysis as a proxy for impact. We argue that the prevailing system of impact analysis is deeply flawed. Its validity as a measure of knowledge is questionable, as is the reliability of the data used as evidence.

The chapter ends with suggestions intended to contribute to discussion about the transformation of the academic journal and the creation of new knowledge systems: sustainable publishing models, frameworks for guardianship of intellectual property, criterion-referenced peer review, greater reflexivity in the review process, incremental knowledge refinement, more widely distributed sites of knowledge production and inclusive knowledge cultures, new types of scholarly text, and more reliable impact metrics.
ER  - 

TY  - CHAP
AU  - Lillard, Terrence V.
AU  - Garrison, Clint P.
AU  - Schiller, Craig A.
AU  - Steele, James
T1  - Chapter 3 - Other Network Evidence
A2  - Lillard, Terrence V.
A2  - Garrison, Clint P.
A2  - Schiller, Craig A. 
A2  - Steele, James 
BT  - Digital Forensics for Network, Internet, and Cloud Computing
PB  - Syngress
CY  - Boston
PY  - 2010///
SP  - 59
EP  - 92
SN  - 978-1-59749-537-0
DO  - http://dx.doi.org/10.1016/B978-1-59749-537-0.00003-X
UR  - http://www.sciencedirect.com/science/article/pii/B978159749537000003X
AB  - Publisher Summary
This chapter presents the workflow of a typical virus infection or botnet security incident describing the various repositories of potential evidence, and the tools used to find and extract them. Many virus infections or bot client cases start as potential computer crime cases until one could determine the potential damage or intent. Most organizations have started using first responder tools. The first responder tool, Rapid Assessment and Potential Incident Examination Report (RAPIER), is adapted from a tool that is used by Intel to collect a consistent set of data from a machine involved in an incident, no matter where in the world the incident had occurred and regardless of the skill of the first responder. RAPIER enables investigators to have the desktop support techs gather the information as part of normal response to suspected bot clients or virus-infected systems. Investigators are also able to determine the identity of other infected machines by examining security event and firewall logs. RAPIER has the ability to identify a forensics server. Each client can be configured to send its result to the forensics server across the network. Analysis of these logs may indicate a need for deeper forensics. RAPIER requires the presence of .NET 2.0, so it is well suited to a corporate environment in which .NET 2.0 is deployed by policy and less suited for a less-regulated environment.
ER  - 

TY  - JOUR
T1  - Compliance by design – Bridging the chasm between auditors and IT architects
JO  - Computers & Security
VL  - 30
IS  - 6–7
SP  - 410
EP  - 426
PY  - 2011/9//
Y2  - 2011/10//
T2  - 
AU  - Julisch, Klaus
AU  - Suter, Christophe
AU  - Woitalla, Thomas
AU  - Zimmermann, Olaf
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/j.cose.2011.03.005
UR  - http://www.sciencedirect.com/science/article/pii/S0167404811000514
KW  - Information systems audit
KW  - CAVR
KW  - Compliance
KW  - Security architecture
KW  - Patterns
KW  - Service-oriented architecture
KW  - Business processes
KW  - Enterprise applications
AB  - System and process auditors assure – from an information processing perspective – the correctness and integrity of the data that is aggregated in a company’s financial statements. To do so, they assess whether a company’s business processes and information systems process financial data correctly. The audit process is a complex endeavor that in practice has to rely on simplifying assumptions. These simplifying assumptions mainly result from the need to restrict the audit scope and to focus it on the major risks. This article describes a generalized audit process. According to our experience with this process, there is a risk that material deficiencies remain undiscovered when said simplifying assumptions are not satisfied. To address this risk of deficiencies, the article compiles thirteen control patterns, which – according to our experience – are particularly suited to help information systems satisfy the simplifying assumptions. As such, use of these proven control patterns makes information systems easier to audit and IT architects can use them to build systems that meet audit requirements by design. Additionally, the practices and advice offered in this interdisciplinary article help bridge the gap between the architects and auditors of information systems and show either role how to benefit from an understanding of the other role’s terminology, techniques, and general work approach.
ER  - 

TY  - JOUR
T1  - The structural, connectomic and network covariance of the human brain
JO  - NeuroImage
VL  - 66
IS  - 0
SP  - 489
EP  - 499
PY  - 2013/2/1/
T2  - 
AU  - Irimia, Andrei
AU  - Van Horn, John D.
SN  - 1053-8119
DO  - http://dx.doi.org/10.1016/j.neuroimage.2012.10.066
UR  - http://www.sciencedirect.com/science/article/pii/S1053811912010695
KW  - Neuroimaging
KW  - MRI
KW  - DTI
KW  - Connectivity
KW  - Correlation
AB  - Though it is widely appreciated that complex structural, functional and morphological relationships exist between distinct areas of the human cerebral cortex, the extent to which such relationships coincide remains insufficiently appreciated. Here we determine the extent to which correlations between brain regions are modulated by either structural, connectomic or network-theoretic properties using a structural neuroimaging data set of magnetic resonance imaging (MRI) and diffusion tensor imaging (DTI) volumes acquired from N = 110 healthy human adults. To identify the linear relationships between all available pairs of regions, we use canonical correlation analysis to test whether a statistically significant correlation exists between each pair of cortical parcels as quantified via structural, connectomic or network-theoretic measures. In addition to this, we investigate (1) how each group of canonical variables (whether structural, connectomic or network-theoretic) contributes to the overall correlation and, additionally, (2) whether each individual variable makes a significant contribution to the test of the omnibus null hypothesis according to which no correlation between regions exists across subjects. We find that, although region-to-region correlations are extensively modulated by structural and connectomic measures, there are appreciable differences in how these two groups of measures drive inter-regional correlation patterns. Additionally, our results indicate that the network-theoretic properties of the cortex are strong modulators of region-to-region covariance. Our findings are useful for understanding the structural and connectomic relationship between various parts of the brain, and can inform theoretical and computational models of cortical information processing.
ER  - 

TY  - CHAP
AU  - Ghassemlouei, Alijohn
AU  - Bathurst, Robert
AU  - Rogers, Russ
T1  - Chapter 3 - The Filesystem
A2  - Ghassemlouei, Alijohn
A2  - Bathurst, Robert 
A2  - Rogers, Russ 
BT  - The Hacker's Guide to OS X
PB  - Syngress
CY  - 
PY  - 2013///
SP  - 17
EP  - 47
SN  - 978-1-59749-950-7
DO  - http://dx.doi.org/10.1016/B978-1-59-749950-7.00003-4
UR  - http://www.sciencedirect.com/science/article/pii/B9781597499507000034
AB  - Abstract

In this chapter we will cover HFS/HFS+ and how the filesystem provides an abstraction layer to the user. We will also cover the organization of the filesystem, including inodes and file caching.

Keywords

Filesystem, Inodes, OSX, File system, File caching, HFS, HFS+
ER  - 

TY  - JOUR
T1  - Modular design, application architecture, and usage of a self-service model for enterprise data delivery: The Duke Enterprise Data Unified Content Explorer (DEDUCE)
JO  - Journal of Biomedical Informatics
VL  - 52
IS  - 0
SP  - 231
EP  - 242
PY  - 2014/12//
T2  - Special Section: Methods in Clinical Research Informatics
AU  - Horvath, Monica M.
AU  - Rusincovitch, Shelley A.
AU  - Brinson, Stephanie
AU  - Shang, Howard C.
AU  - Evans, Steve
AU  - Ferranti, Jeffrey M.
SN  - 1532-0464
DO  - http://dx.doi.org/10.1016/j.jbi.2014.07.006
UR  - http://www.sciencedirect.com/science/article/pii/S1532046414001531
KW  - Cohort definition
KW  - Research query tool
KW  - Medical informatics applications
KW  - Information systems
KW  - Application development
KW  - System design and architecture
AB  - AbstractPurpose
Data generated in the care of patients are widely used to support clinical research and quality improvement, which has hastened the development of self-service query tools. User interface design for such tools, execution of query activity, and underlying application architecture have not been widely reported, and existing tools reflect a wide heterogeneity of methods and technical frameworks. We describe the design, application architecture, and use of a self-service model for enterprise data delivery within Duke Medicine.
Methods
Our query platform, the Duke Enterprise Data Unified Content Explorer (DEDUCE), supports enhanced data exploration, cohort identification, and data extraction from our enterprise data warehouse (EDW) using a series of modular environments that interact with a central keystone module, Cohort Manager (CM). A data-driven application architecture is implemented through three components: an application data dictionary, the concept of “smart dimensions”, and dynamically-generated user interfaces.
Results
DEDUCE CM allows flexible hierarchies of EDW queries within a grid-like workspace. A cohort “join” functionality allows switching between filters based on criteria occurring within or across patient encounters. To date, 674 users have been trained and activated in DEDUCE, and logon activity shows a steady increase, with variability between months. A comparison of filter conditions and export criteria shows that these activities have different patterns of usage across subject areas.
Conclusions
Organizations with sophisticated EDWs may find that users benefit from development of advanced query functionality, complimentary to the user interfaces and infrastructure used in other well-published models. Driven by its EDW context, the DEDUCE application architecture was also designed to be responsive to source data and to allow modification through alterations in metadata rather than programming, allowing an agile response to source system changes.
ER  - 

TY  - CHAP
AU  - Ridge, Enda
T1  - Chapter 15 - Testing Work Products
A2  - Ridge, Enda 
BT  - Guerrilla Analytics
PB  - Morgan Kaufmann
CY  - Boston
PY  - 2015///
SP  - 194
EP  - 201
SN  - 978-0-12-800218-6
DO  - http://dx.doi.org/10.1016/B978-0-12-800218-6.00015-1
UR  - http://www.sciencedirect.com/science/article/pii/B9780128002186000151
KW  - Testing
KW  - Work Products
KW  - Statistical Models
AB  - Summary
In this chapter, you will learn about testing work products. A work product is any output from the team’s analytics work. This may be a sample of some data, visualizations, a presentation, or a sophisticated predictive model. Like any of the team’s activities, work products should be tested for defects. This chapter will describe how to do that testing.
ER  - 

TY  - CHAP
AU  - Freund, Jack
AU  - Jones, Jack
T1  - Chapter 11 - Controls
A2  - Freund, Jack  
A2  - Jones, Jack 
BT  - Measuring and Managing Information Risk
PB  - Butterworth-Heinemann
CY  - Boston
PY  - 2015///
SP  - 241
EP  - 272
SN  - 978-0-12-420231-3
DO  - http://dx.doi.org/10.1016/B978-0-12-420231-3.00011-7
UR  - http://www.sciencedirect.com/science/article/pii/B9780124202313000117
KW  - Control
KW  - Decision-making
KW  - Variance
AB  - Abstract
As risk management professionals, we spend a lot of time and energy developing and evaluating controls. What we do not tend to do as much of is measuring their effectiveness. Part of this may be because of the inherent difficulty in establishing real-world testing methods. Another possible challenge is that, as a profession, it seems like we have not really taken the time to think deeply about the nature of controls, how they work (and do not work) both independently and in combination with other controls. Most of the time, controls are simply categorized in some high-level manner (e.g., internal controls versus technical controls, or “prevention” versus “compensating” controls).

In this chapter, we will introduce an ontology for controls that sets the stage for more effective measurement and as a way to help people understand controls at a deeper level. We will also describe how to think about controls within the context of performing Factor Analysis of Information Risk analyses.
ER  - 

TY  - CHAP
AU  - Knapp, Eric
T1  - Chapter 10 - Standards and Regulations
A2  - Knapp, Eric 
BT  - Industrial Network Security
PB  - Syngress
CY  - Boston
PY  - 2011///
SP  - 249
EP  - 302
SN  - 978-1-59749-645-2
DO  - http://dx.doi.org/10.1016/B978-1-59749-645-2.00010-0
UR  - http://www.sciencedirect.com/science/article/pii/B9781597496452000100
AB  - Publisher Summary
There are several cyber security standards and regulations imposed by governments and industry, which provide everything from “best practices” recommendations to hard requirements that are enforced through penalties and fines. Some common standards are the North American Electric Reliability Corporation's (NERC's) Critical Infrastructure Protection (CIP) Reliability Standards, the US Department of Homeland Security's Chemical Facility Anti-Terrorism Standards (CFATS), and the Regulated Security of Nuclear Facilities by the United States. The Risk-Based Performance Standards (RBPS) for the Chemical Facilities Anti-Terrorism Standards (CFATS) outlines various controls for securing the cyber systems of chemical facilities. Specifically, RBPS Metric 8 (“Cyber”) outlines controls for security policies, access control, personnel security, awareness and training, monitoring and incident response, disaster recovery and business continuity, system development and acquisition, configuration management, and audits. Nuclear Regulatory Commission (NRC) Regulation 5.71 (RG 5.71) provides security recommendations for complying with Title 10 of the Code of Federal Regulations (CFR) 73.54. It consists of an in-depth discussion of the general requirements of cyber security, to specific requirements of planning, establishing, and implementing a cyber security program.
ER  - 

TY  - JOUR
T1  - They look good but don't work: a case study of global performance indicators in crime prevention
JO  - Evaluation and Program Planning
VL  - 26
IS  - 3
SP  - 237
EP  - 248
PY  - 2003/8/1/
T2  - Longitudinal Evaluations of Substance Abuse Treatment
AU  - van den Eynde, Julie
AU  - Veno, Arthur
AU  - Hart, Alison
SN  - 0149-7189
DO  - http://dx.doi.org/10.1016/S0149-7189(03)00028-4
UR  - http://www.sciencedirect.com/science/article/pii/S0149718903000284
KW  - Partnerships with police
KW  - Community development
KW  - Performance indicators
KW  - Crime prevention
KW  - Community and police relations
KW  - Evaluating crime prevention
AB  - Around the western world global performance indicators are widely used by governments and bureaucrats to determine the effectiveness of community development programs. The worldwide shift to crime prevention models, relying upon the formation of community partnerships with police to effect safety in neighbourhoods, is no exception. Police and governments seem especially committed to using global performance indicators for measuring and assessing the effectiveness of these partnerships, probably due to the traditional measures of effectiveness of ‘crime fighting’ by police. The Waratah Crime Prevention Project was a pilot community based crime prevention project carried out in Victoria, Australia, which required the use of global performance indicators as a measure of effectiveness on the directive of government and police. This paper reports on the reasons for the total failure of these global performance indicators to measure the changes effected by the project in the three key performance areas of reducing violence in and around licensed premises; reducing violence in families; and reducing violence by young people. The paper discusses the problems with assigning only global measures to community crime prevention programs, and suggests recommendations for future projects.
ER  - 

TY  - JOUR
T1  - A quantitative study of Public Key Infrastructures
JO  - Computers & Security
VL  - 22
IS  - 1
SP  - 56
EP  - 67
PY  - 2003/1//
T2  - 
AU  - Bruschi, D
AU  - Curti, A
AU  - Rosti, E
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/S0167-4048(03)00113-5
UR  - http://www.sciencedirect.com/science/article/pii/S0167404803001135
AB  - Public Key Infrastructures have not reached the widespread diffusion expected of them, although they are well understood from a security point of view, because, like many say, the killer application has not been found yet. The lack of a clear understanding of the performance of these systems also contributes significantly to their limited diffusion. Studies have appeared of specific aspects of the operations of PKIs, but no complete studies of the overall system are known.

In this paper we present an evaluation study of X.509-compliant Public Key Infrastructures using queuing network models. We focus our analysis on the performance of the subsystem in charge of generating and managing digital certificates, under a variety of load conditions, both in terms of the type of requests and their number. We also investigate the impact on the performance of the system of some implementation choices such as revocation mechanisms and auditing activities. The main result of our analysis is that the system we consider, given the current state of technology, can guarantee acceptable response time in steady state even in the presence of PKI with a consistent number of users. However, in order to guarantee such a performance level, throughput must not exceed 3.5 requests per second, where a request can be a certificate generation or revocation request. Such a limitation hinders the deployment of PKIs with large numbers of users, since recovering after a system compromise may require an unacceptable amount of time.
ER  - 

TY  - JOUR
T1  - MEGA: A tool for Mac OS X operating system and application forensics
JO  - Digital Investigation
VL  - 5, Supplement
IS  - 0
SP  - S83
EP  - S90
PY  - 2008/9//
T2  - The Proceedings of the Eighth Annual DFRWS Conference
AU  - Joyce, Robert A.
AU  - Powers, Judson
AU  - Adelstein, Frank
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2008.05.011
UR  - http://www.sciencedirect.com/science/article/pii/S1742287608000376
KW  - Mac OS X
KW  - Computer forensics
KW  - Spotlight
KW  - Disk image analysis
KW  - Application analysis
AB  - Computer forensic tools for Apple Mac hardware have traditionally focused on low-level file system details. Mac OS X and common applications on the Mac platform provide an abundance of information about the user's activities in configuration files, caches, and logs. We are developing MEGA, an extensible tool suite for the analysis of files on Mac OS X disk images. MEGA provides simple access to Spotlight metadata maintained by the operating system, yielding efficient file content search and exposing metadata such as digital camera make and model. It can also help investigators to assess FileVault encrypted home directories. MEGA support tools are under development to interpret files written by common Mac OS applications such as Safari, Mail, and iTunes.
ER  - 

TY  - JOUR
T1  - Delegation and digital mandates: Legal requirements and security objectives
JO  - Computer Law & Security Review
VL  - 25
IS  - 5
SP  - 415
EP  - 431
PY  - 2009/9//
T2  - 
AU  - Van Alsenoy, Brendan
AU  - De Cock, Danny
AU  - Simoens, Koen
AU  - Dumortier, Jos
AU  - Preneel, Bart
SN  - 0267-3649
DO  - http://dx.doi.org/10.1016/j.clsr.2009.07.007
UR  - http://www.sciencedirect.com/science/article/pii/S0267364909001265
KW  - Delegation
KW  - Agency
KW  - Mandate
KW  - Identity management
KW  - Delegated user and access management
AB  - Now that more and more legal transactions are being performed online, it is increasingly necessary to enable integration of legal mandates within identity and information management systems. The purpose of this article is to outline the legal framework surrounding delegation and to identify basic requirements for any technical application which seeks to provide recognition to legal mandates and delegation processes. Special consideration is also given to the legal implications in situations where a (presumed) mandate holder acts without or outside his authority. Based on these considerations, this article attempts to outline an approach which can significantly reduce the potential risks for both mandate issuers and relying service providers.
ER  - 

TY  - CHAP

T1  - Index
A2  - Conrad, Eric
A2  - Misenar, Seth 
A2  - Feldman, Joshua 
BT  - CISSP Study Guide
PB  - Syngress
CY  - Boston
PY  - 2010///
SP  - 525
EP  - 567
SN  - 978-1-59749-563-9
DO  - http://dx.doi.org/10.1016/B978-1-59749-563-9.00022-6
UR  - http://www.sciencedirect.com/science/article/pii/B9781597495639000226
ER  - 

TY  - CHAP
AU  - Loukas, George
T1  - 4 - Cyber-Physical Attacks on Industrial Control Systems
A2  - Loukas, George 
BT  - Cyber-Physical Attacks
PB  - Butterworth-Heinemann
CY  - Boston
PY  - 2015///
SP  - 105
EP  - 144
SN  - 978-0-12-801290-1
DO  - http://dx.doi.org/10.1016/B978-0-12-801290-1.00004-7
UR  - http://www.sciencedirect.com/science/article/pii/B9780128012901000047
KW  - Cyber-physical attack
KW  - SCADA
KW  - RTU
KW  - PLC
KW  - DNP3
KW  - Modbus
KW  - Aurora vulnerability
KW  - Stuxnet
KW  - smart grid
KW  - smart meter
AB  - Chapter Summary
Being an area of engineering that the information security community had largely ignored in the past, industrial control systems traditionally have been built with an emphasis on efficiency and safety but not on security. In recent years, scientific experiments, such as the Aurora Generator Test, and high-profile real-world attacks, such as Stuxnet, have contributed both in raising awareness and, indirectly, in generating investment in cyber-physical security research and development. Nevertheless, SCADA and other industrial control systems continue to present a number of challenges that make them particularly difficult to protect against determined attackers. In this chapter, we describe some common security threats to SCADA systems, followed by an overview of Stuxnet and a discussion on its significance, before moving on to the smart grid and the associated security challenges.
ER  - 

TY  - CHAP
AU  - Gordon, Gary R.
AU  - McBride, R. Bruce
T1  - 12 - Using Information and Technology as Crime-Fighting Tools
A2  - McBride, Gary R. GordonR. Bruce 
BT  - Criminal Justice Internships (Seventh Edition)
PB  - Anderson Publishing, Ltd.
CY  - Boston
PY  - 2012///
SP  - 145
EP  - 154
SN  - 978-1-4377-3502-4
DO  - http://dx.doi.org/10.1016/B978-1-4377-3502-4.00012-8
UR  - http://www.sciencedirect.com/science/article/pii/B9781437735024000128
AB  - Technology plays a key role in the prevention, detection, investigation, and prosecution of crime. Employees who are well versed in technological advances will be highly valued by their organizations. Therefore, it is imperative that interns be exposed to as much of this technology as possible during their internship. Those interns who bring technical skills to the internship setting or who can develop these skills during their internship will be attractive candidates for openings in a wide variety of organizations. There are many new and exciting career areas in criminal justice and the private sector. Many organizations are providing extensive training to individuals who show an interest and capability in technological areas.
ER  - 

TY  - JOUR
T1  - Making sense of log management for security purposes – an approach to best practice log collection, analysis and management
JO  - Computer Fraud & Security
VL  - 2007
IS  - 5
SP  - 5
EP  - 10
PY  - 2007/5//
T2  - 
AU  - Gorge, Mathieu
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(07)70047-7
UR  - http://www.sciencedirect.com/science/article/pii/S1361372307700477
AB  - Every computer action registers a log somewhere – giving a rich source of data that can help businesses identify any trace of corruption within their networks. Log collection is also a strong component of keeping in line with legislation such as Sarbanes-Oxley, HIPAA, GLBA in the US, and the European Data Protection Directive in the EU. Mathieu Gorge looks at what logs organizations need to keep and what standards require their storage.

He recommends proactive monitoring of firewalls, anti-virus, VPNs and IDS logs among other security systems. The main goal is to link a transaction back to an individual user in order to perform a forensic investigation. But it is important to be wary, as some countries do not allow companies to monitor staff usage of IT systems. See page 3 on how the European court ruled that a British college's monitoring of one employee was a breach of human rights. Therefore, linking a log with a person's actions may not stand up in court.

Gorge says logs can give as good an insight into external attacks as well as internally driven ones.

Logs should be analyzed for the following: •
User account activity: creation, elevation of privilege, changes, inactivity.
•
Client requests and server response.
•
Operational status: shutdown (planned or unplanned), system failure and automatic restart.
•
Usage information and trends – basic user behaviour analysis.


It is best practice to collect, store and analyze logs with a view to being able to get complete, accurate and verifiable information. This will improve the organization's ability to comply with key standards and legislation as regards e-evidence. It could save an organization from potential liability and repair costs and will give visibility over mission critical and security systems, performance and usage. The main advice is to remain proactive so as to be able to respond to a security incident and comply with legal requests should anything happen.

Mathieu Gorge looks at what logs can do for your business and how governance demands them.
ER  - 

TY  - JOUR
T1  - Information Security – The Fourth Wave
JO  - Computers & Security
VL  - 25
IS  - 3
SP  - 165
EP  - 168
PY  - 2006/5//
T2  - 
AU  - von Solms, Basie
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/j.cose.2006.03.004
UR  - http://www.sciencedirect.com/science/article/pii/S016740480600054X
KW  - Corporate Governance
KW  - Information Security
KW  - Information Security Management
KW  - Information Security Governance
KW  - Risk management
KW  - Sarbanes–Oxley
KW  - Social engineering
AB  - In a previous article [von Solms, 2000], the development of Information Security up to the year 2000 was characterized as consisting of three waves:•
the technical wave,
•
the management wave, and
•
the institutional wave.


This paper continues this development of Information Security by characterizing the Fourth Wave – that of Information Security Governance.
ER  - 

TY  - JOUR
T1  - The “Art” of log correlation: Tools and Techniques for Correlating Events and Log Files
JO  - Computer Fraud & Security
VL  - 2004
IS  - 8
SP  - 15
EP  - 17
PY  - 2004/8//
T2  - 
AU  - Forte, Dario Valentino
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(04)00101-0
UR  - http://www.sciencedirect.com/science/article/pii/S1361372304001010
AB  - This is the last article in a three part series on log correlation. Log file correlation is related to two distinct activities: Intrusion Detection and Network Forensics. It is more important than ever that these two disciplines work together in a mutualistic relationship in order to avoid Points of Failure. This paper, intended as a tutorial for those dealing with such issues, presents an overview of log analysis and correlation, with special emphasis on the tools and techniques for managing them within a network forensics context.
ER  - 

TY  - CHAP
AU  - Casey, Eoghan
T1  - Chapter 17 - Reconstructing Digital Evidence
A2  - Turvey, W. Jerry ChisumBrent E. 
BT  - Crime Reconstruction (Second Edition)
PB  - Academic Press
CY  - San Diego
PY  - 2011///
SP  - 531
EP  - 548
SN  - 978-0-12-386460-4
DO  - http://dx.doi.org/10.1016/B978-0-12-386460-4.00017-5
UR  - http://www.sciencedirect.com/science/article/pii/B9780123864604000175
AB  - Publisher Summary
This chapter presents the use of digital evidence to reconstruct actions taken in furtherance of a crime, providing case examples to demonstrate key concepts. Digital evidence is defined as any data stored or transmitted using a computer that support or refute a theory of how an offense occurred or that addresses critical elements of the offense, such as intent or alibi. Homicide, sexual assault, and other violent crimes can involve digital evidence from a wide range of sources, including personal computers, handheld devices, servers, and the internet, helping investigators reconstruct events and gain insight into the state of mind of individuals. Computers and networks should be considered an extension of the crime scene, even when they are not involved directly in facilitating the crime. A single computer can contain e-mail communications between the victim and the offender, evidence of intent to commit a crime, incriminating digital photographs taken by the offender as trophies, and software applications used to conceal digital evidence. It is suggested that digital evidence that is handled and interpreted properly can be used to apprehend offenders, authenticate documents, assess alibis and statements, and determine intent.
 
Digital evidence is defined as any data stored or transmitted using a computer that support or refute a theory of how an offense occurred or that address critical elements of the offense, such as intent or alibi. Homicide, sexual assault, and other violent crimes can involve digital evidence from a wide range of sources, including personal computers, handheld devices, servers, and the Internet, helping investigators reconstruct events and gain insight into the state of mind of individuals. A basic knowledge of these, and how they operate, is required for a complete investigation and reconstruction. Computers and networks should be considered an extension of the crime scene, even when they are not involved directly in facilitating the crime. Information stored and created on computers can be used to answer fundamental questions relating to a crime, including what happened when (sequencing), who was responsible (attribution), and origination of a particular item (evaluation of source).
ER  - 

TY  - CHAP
AU  - Andress, Jason
AU  - Winterfeld, Steve
T1  - Chapter 5 - Logical Weapons
A2  - Andress, Jason  
A2  - Winterfeld, Steve 
BT  - Cyber Warfare
PB  - Syngress
CY  - Boston
PY  - 2011///
SP  - 83
EP  - 118
SN  - 978-1-59749-637-7
DO  - http://dx.doi.org/10.1016/B978-1-59749-637-7.00005-8
UR  - http://www.sciencedirect.com/science/article/pii/B9781597496377000058
KW  - Access and privilege escalation tool
KW  - Assault tool
KW  - Backdoor
KW  - Computer Network Operations
KW  - Exfiltration tool
KW  - Google hacking
KW  - Logical tool
KW  - Obfuscation tool
KW  - Reconnaissance tool
KW  - Scanning tool
AB  - Publisher Summary
Logical tools are the weapons that one likely envisions when discussing cyber warfare. These sets of tools are used to conduct reconnaissance, scout out the networks and systems of the opponents, and attack the various targets one might find. Reconnaissance tools are those tools that one uses to gather information, usually in a passive state, about the networks and systems that one might plan to take action against in a logical sense. Such efforts may include gathering information from public websites, looking up Domain Name Server records, collecting metadata from accessible documents, and retrieving very specific information through the use of search engine or any of a number of other similar activities. When looking for general information that can be used to provide intelligence on a target, there are a variety of sources that one can turn to. All manners of interesting information can be found on the websites of individuals and organizations. Some information such as corporate organizational information may be intentionally displayed, and some of it may be shared in an unintentional or unauthorized manner.
 
A variety of tools are available for use in cyber warfare, penetration testing, and security in general. And most of them are free (or have free versions) and are available to the general public. For instance, reconnaissance tools can gather general information, search whois and DNS records, and search metadata from media and documents. Scanning tools, such as Nmap and Nessus, can find systems and detect areas where vulnerabilities might exist. Access and privilege escalation tools, such as Metasploit and CANVAS, can penetrate system accounts. Exfiltration tools can encrypt, hide, or smuggle data over common protocols to remove it from a compromised system. Backdoors can sustain a connection to a compromised system. Assault tools can damage or disrupt compromised systems. And obfuscation tools can hide one's location, logically and physically, during an attack. This chapter discusses how to use these tools, and how to defend against an attacker using them.
ER  - 

TY  - JOUR
T1  - Improving evidence acquisition from live network sources
JO  - Digital Investigation
VL  - 3
IS  - 2
SP  - 89
EP  - 96
PY  - 2006/6//
T2  - 
AU  - Nikkel, Bruce J.
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2006.05.002
UR  - http://www.sciencedirect.com/science/article/pii/S1742287606000363
KW  - Network forensics
KW  - Live network evidence
KW  - Live network acquisition
KW  - Live network forensics
KW  - NFAT
AB  - The pervasiveness of network technology is causing a shift in the location of digital evidence. What was once largely found on individual disks tied to single individuals is now becoming distributed across remote networked machines, under the control of multiple organizations, and scattered over multiple jurisdictions. The network interactions between these machines are also becoming recognized as a source of network evidence. These live network sources of evidence bring additional challenges which need to be addressed. This paper discusses these issues and suggests some improvements in the methods used for the collection of evidence from live network sources.
ER  - 

TY  - CHAP

T1  - Index
A2  - Wiles, Jack  
A2  - Rogers, Russ 
BT  - Techno Security's Guide to Managing Risks for IT Managers, Auditors, and Investigators
PB  - Syngress
CY  - Rockland
PY  - 2007///
SP  - 383
EP  - 391
SN  - 978-1-59749-138-9
DO  - http://dx.doi.org/10.1016/B978-159749138-9/50018-1
UR  - http://www.sciencedirect.com/science/article/pii/B9781597491389500181
ER  - 

TY  - JOUR
T1  - Network traffic as a source of evidence: tool strengths, weaknesses, and future needs
JO  - Digital Investigation
VL  - 1
IS  - 1
SP  - 28
EP  - 43
PY  - 2004/2//
T2  - 
AU  - Casey, Eoghan
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2003.12.002
UR  - http://www.sciencedirect.com/science/article/pii/S1742287603000033
KW  - Network traffic
KW  - Network investigations
KW  - Digital evidence
KW  - Forensic examination
KW  - Computer crime
AB  - Digital investigators require specialized knowledge and tools to process network traffic as a source of evidence. Existing open source tools can be used for basic tasks in simple cases but lack the functionality of commercial tools that are specifically designed to process network traffic as evidence. These commercial tools reduce the amount of time and specialized technical knowledge required to examine large quantities of network traffic but even these tools are lacking from a forensic standpoint. This paper discusses the strengths and shortcomings of existing tools in the context of the overall digital investigation process—specifically the collection, documentation, preservation, examination and analysis stages. In addition to highlighting the capabilities of different tools, this paper familiarizes digital investigators with different aspects of network traffic as a source of evidence. Based on this discussion, a set of requirements is proposed for tools used to process network traffic as evidence in the hope that existing developers will enhance the capabilities of their tools to address the weaknesses.
ER  - 

TY  - CHAP
AU  - Shinder, Thomas W.
T1  - Chapter 1 - Network Security Basics
A2  - Shinder, Thomas W. 
BT  - Dr. Tom Shinder's ISA Server 2006 Migration Guide
PB  - Syngress
CY  - Burlington
PY  - 2007///
SP  - 1
EP  - 45
SN  - 978-1-59749-199-0
DO  - http://dx.doi.org/10.1016/B978-159749199-0/00001-0
UR  - http://www.sciencedirect.com/science/article/pii/B9781597491990000010
AB  - Publisher Summary
This chapter discusses the network security basics. A good network security system will help to remove the temptations easily, and will be as transparent to the users as possible. File servers on which sensitive data is stored and infrastructure servers that provide mission critical services such as logon authentication and access control should be placed in a highly secure location. At the minimum, servers should be in a locked room where only those who need to work directly with the servers have access. Denial of Service (DOS) attacks are one of the most popular choices of Internet hackers who want to disrupt a network's operations. The objective of the DOS attacker is to bring down the network, denying service to its legitimate users. DOS attacks are easy to initiate; software is readily available from hacker Web sites and warez newsgroups that will allow anyone to launch a DOS attack with little or no technical expertise. The Domain Name System (DNS) DOS attack exploits the difference in size between a DNS query and a DNS response, in which all of the network's bandwidth is tied up by bogus DNS queries. The security plan should also address the procedures for reporting security breaches, both internally, and if the police or other outside agencies are to be brought in. It is suggested that password change policies should prevent users from making only slight changes.
ER  - 

TY  - CHAP

T1  - Index
A2  - Aquilina, James M. 
BT  - Malware Forensics
PB  - Syngress
CY  - Burlington
PY  - 2008///
SP  - 661
EP  - 674
SN  - 978-1-59749-268-3
DO  - http://dx.doi.org/10.1016/B978-1-59749-268-3.00016-5
UR  - http://www.sciencedirect.com/science/article/pii/B9781597492683000165
ER  - 

TY  - JOUR
T1  - Geologic Sequestration Software Suite (GS3 ): A collaborative approach to the management of geological GHG storage projects
JO  - Energy Procedia
VL  - 4
IS  - 0
SP  - 3825
EP  - 3832
PY  - 2011///
T2  - 10th International Conference on Greenhouse Gas Control Technologies
AU  - Bonneville, Alain
AU  - Black, Gary D.
AU  - Gorton, Ian
AU  - Hui, Peter
AU  - Murphy, Ellyn M.
AU  - Murray, Chris J.
AU  - Rockhold, Mark L.
AU  - Schuchardt, Karen L.
AU  - Sivaramakrishnan, Chandrika
AU  - White, Mark D.
AU  - Williams, Mark D.
AU  - Wurstner, Signe K.
SN  - 1876-6102
DO  - http://dx.doi.org/10.1016/j.egypro.2011.02.318
UR  - http://www.sciencedirect.com/science/article/pii/S1876610211005972
KW  - Geological storage of CO2
KW  - CCS
KW  - Project management
KW  - CO2 injection
KW  - Reservoir modeling
AB  - Geologic storage projects associated with large anthropogenic sources of greenhouse gases (GHG) will have lifecycles that may easily span a century, involve several numerical simulation cycles, and have distinct modeling teams. The process used for numerical simulation of the fate of GHG in the subsurface follows a generally consistent sequence of steps that often are replicated by scientists and engineers around the world. Site data is gathered, assembled, interpreted, and assimilated into conceptualizations of a solid-earth model; assumptions are made about the processes to be modeled; a computational domain is specified and spatially discretized; driving forces and initial conditions are defined; the conceptual models, computational domain, and driving forces are translated into input files; simulations are executed; and results are analyzed. Then, during and after the GHG injection, a continuous monitoring of the reservoir is done and models are updated with the newly collected data. Typically the working files generated during all these steps are maintained on workstations with local backups and archived once the project has concluded along with any modeling notes and records. We are proposing a new concept for supporting the management of full-scale GHG storage projects where collaboration, flexibility, accountability and long-term access will be essential features: The Geologic Sequestration Software Suite, GS3.
ER  - 

TY  - JOUR
T1  - Assessing insider threats to information security using technical, behavioural and organisational measures
JO  - Information Security Technical Report
VL  - 15
IS  - 3
SP  - 112
EP  - 133
PY  - 2010/8//
T2  - Computer Crime - A 2011 Update
AU  - Roy Sarkar, Kuheli
SN  - 1363-4127
DO  - http://dx.doi.org/10.1016/j.istr.2010.11.002
UR  - http://www.sciencedirect.com/science/article/pii/S1363412710000488
AB  - The UK government took a bruising in the headlines (Sep 2008) after a Home Office contractor lost a USB stick containing unencrypted data on all 84,000 prisoners in England and Wales. As a result, the Home Office terminated the £1.5 million contract with the management consultancy firm.

The world woke up to the largest attempted bank fraud ever when the UK’s National Hi-Tech Crime Unit foiled the world’s largest potential bank robbery in March 2005. With the help of the security supervisor, thieves masquerading as cleaning staff installed hardware keystroke loggers on computers within the London branch of a Japanese bank, to steal £220m.

It is indeed sobering to imagine that any organisation could fall victim to such events and the damage an insider can do. The consulting firm lost the contract worth £1.5 million due to a small mistake by an employee. The London branch of the Japanese Bank would have lost £220 million had not the crime been foiled.

Insider threat is a reality. Insiders commit fraud or steal sensitive information when motivated by money or revenge. Well-meaning employees can compromise the security of an organisation with their overzealousness in getting their job done. Every organisation has a varied mix of employees, consultants, management, partners and complex infrastructure and that makes handling insider threats a daunting challenge. With insider attacks, organisations face potential damage through loss of revenue, loss of reputation, loss of intellectual property or even loss of human life.

The insider threat problem is more elusive and perplexing than any other threat. Assessing the insider threat is the first step to determine the likelihood of any insider attack. Technical solutions do not suffice since insider threats are fundamentally a people issue. Therefore, a three-pronged approach - technological, behavioural and organisational assessment is essential in facilitating the prediction of insider threats and pre-empt any insider attack thus improving the organization’s security, survivability, and resiliency in light of insider threats.
ER  - 

TY  - CHAP
AU  - Yee, George O.M.
AU  - Korba, Larry
T1  - Chapter 44 - Personal Privacy Policies1
A2  - Vacca, John R. 
BT  - Computer and Information Security Handbook (Second Edition)
PB  - Morgan Kaufmann
CY  - Boston
PY  - 2013///
SP  - 773
EP  - 792
SN  - 978-0-12-394397-2
DO  - http://dx.doi.org/10.1016/B978-0-12-394397-2.00044-1
UR  - http://www.sciencedirect.com/science/article/pii/B9780123943972000441
KW  - privacy
KW  - e-services
KW  - data privacy
KW  - directives
KW  - requirements
KW  - privacy principles
KW  - privacy policy specification
KW  - personal privacy policies
KW  - semiautomated derivation
KW  - retrieval
AB  - The rapid growth of the Internet has been accompanied by a similar growth in the availability of Internet e-services (such as online booksellers and stockbrokers). This proliferation of e-services has in turn fueled the need to protect the personal privacy of e-service users or consumers. This chapter proposes the use of personal privacy policies to protect privacy. It is evident that the content must match the user’s privacy preferences as well as privacy legislation. It is also evident that the construction of a personal privacy policy must be as easy as possible for the consumer. Further, the content and construction must not result in negative unexpected outcomes (an unexpected outcome that harms the user in some manner). The chapter begins with the derivation of policy content based on privacy legislation, followed by a description of how a personal privacy policy may be constructed semiautomatically. It then shows how to additionally specify policies so that negative unexpected outcomes can be avoided. Finally, it describes our Privacy Management Model that explains how to use personal privacy policies to protect privacy, including what is meant by a “match” of consumer and service provider policies and how nonmatches can be resolved through negotiation. difficulty. Hence, it has become hard for individuals to manage and control their personal spheres. Both legal and technical means are needed to protect privacy and to (re-)establish the individuals’ control. This chapter provides an overview to the area of Privacy-enhancing Technologies (PETs), which help to protect privacy by technically enforcing legal privacy principles. It will start with defining the legal foundations of PETs, and will present a classification of PETs as well as a definition of traditional privacy properties that PETs are addressing and metrics for measuring the level of privacy that PETs are providing. Then, a selection of the most relevant PETs is presented.
ER  - 

TY  - CHAP

T1  - Chapter 6 - Windows and Linux Forensics
A2  - Kleiman, Dave
A2  - Cardwell, Kevin
A2  - Clinton, Timothy
A2  - Cross, Michael
A2  - Gregg, Michael
A2  - Varsalone, Jesse 
A2  - Wright, Craig 
BT  - The Official CHFI Study Guide (Exam 312-49)
PB  - Syngress
CY  - Rockland
PY  - 2007///
SP  - 287
EP  - 349
SN  - 978-1-59749-197-6
DO  - http://dx.doi.org/10.1016/B978-159749197-6.50007-9
UR  - http://www.sciencedirect.com/science/article/pii/B9781597491976500079
AB  - In this chapter we introduce the reader to both Windows and Linux-based forensic tools. The chapter is split into two sections which reflect Module 10:Windows Forensics and Module 11: Linux Forensics respectively. The forensic analyst should be conversant with both Windows and Linux. In the course of a career you will be expected to work with both fairly regularly. Although it is likely that you will have personal preferences, a thorough understanding of the strengths and weaknesses of each type of system will aid you in understanding both of them and also provide you with a wealth of additional tools.
ER  - 

TY  - CHAP

T1  - Chapter 13 - Forensic Software and Hardware
A2  - Kleiman, Dave
A2  - Cardwell, Kevin
A2  - Clinton, Timothy
A2  - Cross, Michael
A2  - Gregg, Michael
A2  - Varsalone, Jesse 
A2  - Wright, Craig 
BT  - The Official CHFI Study Guide (Exam 312-49)
PB  - Syngress
CY  - Rockland
PY  - 2007///
SP  - 543
EP  - 616
SN  - 978-1-59749-197-6
DO  - http://dx.doi.org/10.1016/B978-159749197-6.50014-6
UR  - http://www.sciencedirect.com/science/article/pii/B9781597491976500146
AB  - In this chapter we introduce to the hardware and software-based forensic tools. Due to the large number of hardware and software tools, few can be covered in great detail. We shall list the software products you will need to be familiar with to pass the exam in Part One.
ER  - 

TY  - CHAP
AU  - Shinder, Littlejohn
AU  - Cross, Michael
T1  - Chapter 6 - Computer Forensic Software and Hardware
A2  - Shinder, Littlejohn  
A2  - Cross, Michael 
BT  - Scene of the Cybercrime (Second Edition)
PB  - Syngress
CY  - Burlington
PY  - 2008///
SP  - 243
EP  - 303
SN  - 978-1-59749-276-8
DO  - http://dx.doi.org/10.1016/B978-1-59749-276-8.00006-6
UR  - http://www.sciencedirect.com/science/article/pii/B9781597492768000066
AB  - Publisher Summary
This chapter discusses disk imaging and introduces hardware- and software-based forensic tools. Disk imaging refers to the process of making an exact copy of a disk. Imaging is sometimes also called disk cloning or ghosting, but the latter terms usually refer to images created for purposes other than evidence preservation. Disk imaging is accepted as standard practice in computer forensics to preserve the integrity of the original evidence. Disk imaging differs from creating a standard backup of a disk in that ambient data is not copied to a backup; only active files are copied. Because a backup created with popular backup programs such as the Windows built-in backup utility, Backup Exec, ARCserve, and the like is not an exact duplicate these programs should not be used for disk imaging. Programs such as Norton Ghost include switches that allow one to make a bitstream copy, but these programs were not originally designed for forensic use. In some cases, the tools are software-based, but one can also use hardware to acquire evidence from suspect machines. The hardware- and software-based solutions create bitstream images of the disk, and include any data that is visible to the file system, as well as hidden and deleted files and fragments of files, the MBR, the partition table, and any number of other items on a disk.
ER  - 

TY  - CHAP
AU  - Wilhelm, Thomas
T1  - CHAPTER 3 - Hacking as a Career
A2  - Wilhelm, Thomas 
BT  - Professional Penetration Testing
PB  - Syngress
CY  - Boston
PY  - 2010///
SP  - 43
EP  - 99
SN  - 978-1-59749-425-0
DO  - http://dx.doi.org/10.1016/B978-1-59749-425-0.00007-5
UR  - http://www.sciencedirect.com/science/article/pii/B9781597494250000075
AB  - Publisher Summary
This chapter focuses on hacking as a career and discusses how one can develop a career as a penetration tester. Penetration testing expertise can be narrowed down into three different fields: networks, systems, and applications. Penetration testers with a network architecture background can identify deficiencies in a large variety of network designs, penetration testers who specialize in system administration often expand the knowledge of an operating system by learning about things such as secure communication protocols, file sharing, directory services, system hardening, backup processes, and more. Penetration testers who specialize in application and databases typically understand what it takes to create applications and how they interact with databases. The chapter offers information on some of the certifications of information systems security (ISS), such as the (ISC)2, the (ISC)2 CBK, Systems Security Certified Practitioner (SSCP), Certification and Accreditation Professional (CAP), Certified Secure Software Lifecycle Professional (CSSLP), and CISSP–ISSEP. Certifications are helpful in employment and government related exams and contracts. There are a variety of information security organizations that disseminate news about the happenings within the industry, including local and national associations. Many security conferences provide training opportunities, while some mailing lists provide the latest in news and vulnerabilities related to information security.
ER  - 

TY  - CHAP
AU  - Aquilina, James M.
T1  - Chapter 2 - Malware Incident Response: Volatile Data Collection and Examination on a Live Linux System
A2  - Aquilina, James M. 
BT  - Malware Forensics
PB  - Syngress
CY  - Burlington
PY  - 2008///
SP  - 93
EP  - 120
SN  - 978-1-59749-268-3
DO  - http://dx.doi.org/10.1016/B978-1-59749-268-3.00002-5
UR  - http://www.sciencedirect.com/science/article/pii/B9781597492683000025
AB  - Publisher Summary
This chapter provides an overall methodology for preserving volatile data on a Linux machine in a forensically sound manner, and uses case examples to demonstrate the strengths and shortcomings of the information that is available through the operating system. There are various native Linux commands that are useful for collecting volatile data from a live computer. Since the commands on a compromised system can be undermined by Malware and cannot be trusted, it is necessary to use a toolkit of utilities for capturing volatile data that have minimal interaction with the subject operating system. Using trusted binaries is a critical part of any live examination, and can reveal information that is hidden by a rootkit. However, when a loadable kernel module (LKM) rootkit is involved, even statically compiled binaries that do not rely on components of the subject system are ineffective, making it necessary to explore creative countermeasures and rely on memory forensics and file system forensics.
ER  - 

TY  - JOUR
T1  - Network forensics based on fuzzy logic and expert system
JO  - Computer Communications
VL  - 32
IS  - 17
SP  - 1881
EP  - 1892
PY  - 2009/11/15/
T2  - 
AU  - Liao, Niandong
AU  - Tian, Shengfeng
AU  - Wang, Tinghua
SN  - 0140-3664
DO  - http://dx.doi.org/10.1016/j.comcom.2009.07.013
UR  - http://www.sciencedirect.com/science/article/pii/S0140366409002060
KW  - Network forensics
KW  - Expert system
KW  - Fuzzy logic
KW  - Intrusion detection system
KW  - Vulnerability scanning
AB  - Network forensics is a research area that finds the malicious users by collecting and analyzing the intrusion or infringement evidence of computer crimes such as hacking. In the past, network forensics was only used by means of investigation. However, nowadays, due to the sharp increase of network traffic, not all the information captured or recorded will be useful for analysis or evidence. The existing methods and tools for network forensics show only simple results. The administrators have difficulty in analyzing the state of the damaged system without expert knowledge. Therefore, we need an effective and automated analyzing system for network forensics. In this paper, we firstly guarantee the evidence reliability as far as possible by collecting different forensic information of detection sensors. Secondly, we propose an approach based on fuzzy logic and expert system for network forensics that can analyze computer crimes in network environment and make digital evidences automatically. At the end of the paper, the experimental comparison results between our proposed method and other popular methods are presented. Experimental results show that the system can classify most kinds of attack types (91.5% correct classification rate on average) and provide analyzable and comprehensible information for forensic experts.
ER  - 

TY  - CHAP
AU  - Carvey, Harlan
T1  - Chapter 3 - Windows Memory Analysis
A2  - Carvey, Harlan 
BT  - Windows Forensic Analysis
PB  - Syngress
CY  - Rockland
PY  - 2007///
SP  - 87
EP  - 124
SN  - 978-1-59749-156-3
DO  - http://dx.doi.org/10.1016/B978-159749156-3/50007-1
UR  - http://www.sciencedirect.com/science/article/pii/B9781597491563500071
AB  - Publisher Summary
There are several options for collecting physical or process memory from a system during incident response. The chapter examines a number of tools for collecting various portions of volatile memory during live response, keeping in mind that there's always the potential for the Windows application programming interfaces (APIs) being compromised by an attacker. This is true in any case where live response is being performed, and therefore, one might decide to use multiple disparate means of collecting volatile information. A root kit can hide the existence of a process from most tools that enumerate the list of active processes, but dumping the contents of random access memory (RAM) allows the investigator to list active and exited processes, as well as processes hidden using kernel-mode root kits.
ER  - 

TY  - CHAP
AU  - Aquilina, James M.
T1  - Chapter 1 - Malware Incident Response: Volatile Data Collection and Examination on a Live Windows System
A2  - Aquilina, James M. 
BT  - Malware Forensics
PB  - Syngress
CY  - Burlington
PY  - 2008///
SP  - 1
EP  - 91
SN  - 978-1-59749-268-3
DO  - http://dx.doi.org/10.1016/B978-1-59749-268-3.00001-3
UR  - http://www.sciencedirect.com/science/article/pii/B9781597492683000013
AB  - Publisher Summary
This chapter demonstrates the value of preserving volatile data, and provides practical guidance for preserving such data in a forensically sound manner. The value of volatile data is not limited to process memory associated with Malware, and includes passwords, Internet Protocol (IP) addresses, Security Event Log entries, and other contextual details that can provide a more complete understanding of the Malware and its use on a system. In a powered-up state, a subject system contains critical ephemeral information that reveals the state of the system. This volatile data is sometimes referred to as stateful information. Incident response forensics, or live response, is the process of acquiring the stateful information from the subject system while it remains powered on. The Order of Volatility is considered when collecting data from a live system to ensure that critical system data is acquired before it is lost or the system is powered down. The chapter provides methodology for preserving volatile data on a Windows system during a Malware incident, and uses case scenarios to demonstrate the collection process as well as the strengths and shortcoming of the data acquired in the process.
ER  - 

TY  - CHAP
AU  - Shinder, Littlejohn
AU  - Cross, Michael
T1  - Chapter 10 - Understanding Network Intrusions and Attacks
A2  - Shinder, Littlejohn  
A2  - Cross, Michael 
BT  - Scene of the Cybercrime (Second Edition)
PB  - Syngress
CY  - Burlington
PY  - 2008///
SP  - 419
EP  - 465
SN  - 978-1-59749-276-8
DO  - http://dx.doi.org/10.1016/B978-1-59749-276-8.00010-8
UR  - http://www.sciencedirect.com/science/article/pii/B9781597492768000108
AB  - Publisher Summary
This chapter provides overview of the technical aspects of various types of intrusions and attacks. The sheer number of ways that a hacker can intrude or attack a network can be overwhelming. As soon as one security “hole” is plugged, dozens more are discovered or created. Some of these methods are so subtle that no one might ever realize that the network's security has been compromised. Others are so blatant that everyone will know instantly. Attackers range from charmers with lots of “people skills” who can persuade legitimate users to provide the credentials they need to break into the system, to technical “whiz kids” who can exploit the characteristics of network protocols, applications, and operating systems, and technically unsophisticated hacker “wannabes” who use scripts, GUI tools, and Web sites created by others to carry out their attacks. The attacks themselves can range from denials of service that disrupt communications on the entire network, to “benign” viruses that do no more than pop up an annoying message window. In many cases, the goal of an attack is to plant a “back door” in the system that will allow the hacker to reenter later at will. The state of hacking has reached the point at which anyone and everyone who wants to launch an attack can do so, and the incidence of “drive-by hacking” has increased with the advent of easy-to-use hacking tools. To protect systems, network administrators and management should use the same methods used by hackers to identify weaknesses in their networks.
ER  - 

TY  - CHAP
AU  - Barnes, Christian
AU  - Bautts, Tony
AU  - Lloyd, Donald
AU  - Ouellet, Eric
AU  - Posluns, Jeffrey
AU  - Zendzian, David M.
AU  - O'Farrell, Neal
T1  - Chapter 2 - A Security Primer
A2  - Barnes, Christian
A2  - Bautts, Tony
A2  - Lloyd, Donald
A2  - Ouellet, Eric
A2  - Posluns, Jeffrey
A2  - Zendzian, David M. 
A2  - O'Farrell, Neal 
BT  - Hackproofing Your Wireless Network
PB  - Syngress
CY  - Rockland
PY  - 2002///
SP  - 75
EP  - 124
SN  - 978-1-928994-59-6
DO  - http://dx.doi.org/10.1016/B978-192899459-6/50022-8
UR  - http://www.sciencedirect.com/science/article/pii/B9781928994596500228
AB  - Publisher Summary
The rush to deploy the latest generation of technology today, along with inadequate security reviews, not only allows for the inclusion of security vulnerabilities in products, but also creates new and unknown challenges. Wireless networking is not exempt from this, many security flaws have been identified and new methods of exploiting these flaws are published regularly. Utilizing security fundamentals developed over the last few decades, it is possible to review and protect the wireless networks from known and unknown threats. This chapter recapitulates security fundamentals and principles that are the foundation of any good security strategy. Security protection starts with the preservation of the confidentiality, integrity, and availability (CIA) of data and computing resources. The chapter addresses a range of issues from authentication and authorization to control and audit. It also examines the common security standards alongside the emerging privacy standards and their implications for the wireless exchange of information. The existing and anticipated threats to wireless networks, and the principles of protection that are fundamental to a wireless security strategy, are also presented.
ER  - 

TY  - CHAP

T1  - Index
A2  - Kleiman, Dave
A2  - Cardwell, Kevin
A2  - Clinton, Timothy
A2  - Cross, Michael
A2  - Gregg, Michael
A2  - Varsalone, Jesse 
A2  - Wright, Craig 
BT  - The Official CHFI Study Guide (Exam 312-49)
PB  - Syngress
CY  - Rockland
PY  - 2007///
SP  - 921
EP  - 939
SN  - 978-1-59749-197-6
DO  - http://dx.doi.org/10.1016/B978-159749197-6.50020-1
UR  - http://www.sciencedirect.com/science/article/pii/B9781597491976500201
ER  - 

TY  - CHAP

T1  - Contributing Authors
A2  - Caceres, Dale LiuMax
A2  - Robichaux, Tim
A2  - Forte, Dario V.
A2  - Seagren, Eric S.
A2  - Ganger, Devin L.
A2  - Smith, Brad
A2  - Jayawickrama, Wipul
A2  - Stokes, Christopher 
A2  - Kanclirz, Jan 
BT  - Next Generation SSH2 Implementation
PB  - Syngress
CY  - Burlington
PY  - 2009///
SP  - vi
EP  - x
SN  - 978-1-59749-283-6
DO  - http://dx.doi.org/10.1016/B978-1-59749-283-6.00016-7
UR  - http://www.sciencedirect.com/science/article/pii/B9781597492836000167
ER  - 

TY  - CHAP
AU  - Casey, Eoghan
T1  - Chapter 11 - Reconstructing digital evidence
A2  - Chisum, W.Jerry  
A2  - Turvey, Brent E. 
BT  - Crime Reconstruction
PB  - Academic Press
CY  - Burlington
PY  - 2007///
SP  - 419
EP  - 439
SN  - 978-0-12-369375-4
DO  - http://dx.doi.org/10.1016/B978-012369375-4/50014-7
UR  - http://www.sciencedirect.com/science/article/pii/B9780123693754500147
AB  - Publisher Summary
This chapter presents the use of digital evidence to reconstruct actions taken in the furtherance of a crime. It focuses on how digital evidence can be useful in violent crime investigations. Specifically, how digital evidence that is properly handled and interpreted can be used to apprehend offenders, authenticate documents, assess alibis and statements, and determine intent is discussed. Digital evidence can help answer many questions in an investigation, ranging from the whereabouts of a victim at a given time to the state of mind of the offender. Therefore, evidence on computers and networks should be included whenever feasible in crime reconstructions. At the same time, care must be taken when interpreting abstracted behavioral evidence that is stored on computers. The risk of missing or misinterpreting important details highlights the importance of utilizing this scientific method to reach objective conclusions that are solidly based on evidence.
ER  - 

TY  - CHAP

T1  - Contributing Authors
A2  - Liu, Dale 
BT  - Cisco Router and Switch Forensics
PB  - Syngress
CY  - Boston
PY  - 2009///
SP  - iv
EP  - ix
SN  - 978-1-59749-418-2
DO  - http://dx.doi.org/10.1016/B978-1-59749-418-2.00017-X
UR  - http://www.sciencedirect.com/science/article/pii/B978159749418200017X
ER  - 

TY  - CHAP

T1  - Index
A2  - Miller, Drew 
BT  - Black Hat Physical Device Security
PB  - Syngress
CY  - Burlington
PY  - 2004///
SP  - 353
EP  - 363
SN  - 978-1-932266-81-8
DO  - http://dx.doi.org/10.1016/B978-193226681-8/50017-8
UR  - http://www.sciencedirect.com/science/article/pii/B9781932266818500178
ER  - 

TY  - CHAP
AU  - Virtue, Timothy
AU  - Rainey, Justin
T1  - Chapter 7 - Third-Party Risk Management
A2  - Virtue, Timothy  
A2  - Rainey, Justin 
BT  - HCISPP Study Guide
PB  - Syngress
CY  - Boston
PY  - 2015///
SP  - 167
EP  - 183
SN  - 978-0-12-802043-2
DO  - http://dx.doi.org/10.1016/B978-0-12-802043-2.00007-0
UR  - http://www.sciencedirect.com/science/article/pii/B9780128020432000070
KW  - Third-party risk management
KW  - Security
KW  - Privacy
KW  - Risk assessment
KW  - Incident notification
KW  - Incident response
KW  - Third-party connectivity
AB  - Abstract
This chapter discusses the importance and purpose of managing risk associated with third parties. This includes understanding the definition of third parties, risk assessment and management activities, and requirements for maintaining a third-party inventory, applying security standards and practices, determining assessment requirements, and addressing incident response and connectivity requirements.
ER  - 

TY  - CHAP

T1  - Index
A2  - Khnaser, Elias N.
A2  - Snedaker, Susan
A2  - Peiris, Chris
A2  - Amini, Rob 
A2  - Hunter, Laura E. 
BT  - MCSE (Exam 70-298) Study Guide
PB  - Syngress
CY  - Rockland
PY  - 2004///
SP  - 759
EP  - 774
SN  - 978-1-932266-55-9
DO  - http://dx.doi.org/10.1016/B978-193226655-9/50015-6
UR  - http://www.sciencedirect.com/science/article/pii/B9781932266559500156
ER  - 

TY  - CHAP

T1  - Chapter 3 - Security Concepts and Security Policies
A2  - Grasdal, Thomas W. ShinderDebra Littlejohn ShinderMartin 
BT  - Configuring ISA Server 2000
PB  - Syngress
CY  - Burlington
PY  - 2001///
SP  - 121
EP  - 200
SN  - 978-1-928994-29-9
DO  - http://dx.doi.org/10.1016/B978-192899429-9/50007-5
UR  - http://www.sciencedirect.com/science/article/pii/B9781928994299500075
AB  - Publisher Summary
Computer security involves many aspects of safekeeping, from protection of the physical equipment to protection of the electronic bits and bytes, which make up the information that resides on the network. Network security solutions can be loosely divided into three categories: hardware, software, and human. This chapter provides an overview of the basic security concepts and examines all four security objectives and the three categories of security solution. It discusses Internet security and acceleration's (ISA's) firewall functionality and how Microsoft has improved security in Windows 2000. To get the most out of ISA's features, one must be able to recognize the security threats to which network is subject and understand a little about the motivations of typical intruders. It is not necessary to be a hacker to prevent network from hacking attempts, but it benefits to know something about how unscrupulous hackers think and how they do their dirty work. A number of hardware-based security solutions and even more software-based firewalls are on the market. A good security plan is one that meets the needs of IT administration, company management, and network users. The best way to ensure that security plan meets these criteria is to involve people from all levels of the organization in the planning process.
ER  - 

TY  - CHAP
AU  - Mallery, John
T1  - Chapter 1 - Building a Secure Organization
A2  - Vacca, John R. 
BT  - Computer and Information Security Handbook
PB  - Morgan Kaufmann
CY  - Boston
PY  - 2009///
SP  - 1
EP  - 21
SN  - 978-0-12-374354-1
DO  - http://dx.doi.org/10.1016/B978-0-12-374354-1.00001-7
UR  - http://www.sciencedirect.com/science/article/pii/B9780123743541000017
AB  - Publisher Summary
Building a secure organization is important to long-term success. When a business implements and maintains a strong security posture, it can take advantage of numerous benefits. An organization that can demonstrate an infrastructure protected by robust security mechanisms can potentially see a reduction in insurance premiums being paid. A secure organization can use its security program as a marketing tool, demonstrating to clients that it values their business so much that it takes a very aggressive stance on protecting their information. Security breaches can cost an organization significantly through a tarnished reputation, lost business, and legal fees. Numerous regulations, such as the Health Insurance Portability and Accountability Act, the Gramm-Leach-Bliley Act, and the Sarbanes-Oxley Act, require businesses to maintain the security of information. Security, by its very nature, is inconvenient, and the more robust the security mechanisms, the more inconvenient the process becomes. Most security mechanisms, from passwords to multifactor authentication, are seen as roadblocks to productivity. Despite the benefits of maintaining a secure organization and the potentially devastating consequences of not doing so, many organizations have poor security mechanisms, implementations, policies, and culture.
ER  - 

TY  - JOUR
T1  - Is it time to re-prioritize neuroimaging databases and digital repositories?
JO  - NeuroImage
VL  - 47
IS  - 4
SP  - 1720
EP  - 1734
PY  - 2009/10/1/
T2  - 
AU  - Van Horn, John Darrell
AU  - Toga, Arthur W.
SN  - 1053-8119
DO  - http://dx.doi.org/10.1016/j.neuroimage.2009.03.086
UR  - http://www.sciencedirect.com/science/article/pii/S1053811909003632
AB  - The development of in vivo brain imaging has lead to the collection of large quantities of digital information. In any individual research article, several tens of gigabytes-worth of data may be represented—collected across normal and patient samples. With the ease of collecting such data, there is increased desire for brain imaging datasets to be openly shared through sophisticated databases. However, very often the raw and pre-processed versions of these data are not available to researchers outside of the team that collected them. A range of neuroimaging databasing approaches has streamlined the transmission, storage, and dissemination of data from such brain imaging studies. Though early sociological and technical concerns have been addressed, they have not been ameliorated altogether for many in the field. In this article, we review the progress made in neuroimaging databases, their role in data sharing, data management, potential for the construction of brain atlases, recording data provenance, and value for re-analysis, new publication, and training. We feature the LONI IDA as an example of an archive being used as a source for brain atlas workflow construction, list several instances of other successful uses of image databases, and comment on archive sustainability. Finally, we suggest that, given these developments, now is the time for the neuroimaging community to re-prioritize large-scale databases as a valuable component of brain imaging science.
ER  - 

TY  - JOUR
T1  - A survey of password mechanisms: Weaknesses and potential improvements. Part 1
JO  - Computers & Security
VL  - 8
IS  - 7
SP  - 587
EP  - 604
PY  - 1989/11//
T2  - 
AU  - Jobusch, David L.
AU  - Oldehoeft, Arthur E.
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/0167-4048(89)90051-5
UR  - http://www.sciencedirect.com/science/article/pii/0167404889900515
KW  - Authentication
KW  - Passwords
KW  - UNIX
KW  - Pass-phrases
AB  - While research continues on more sophisticated methods of authentication, password mechanisms remain the predominant method of identifying computer system users. In this paper, the goals of authentication are reviewed, and the strengths and vulnerabilities of password mechanisms are discussed. The 4.3 Berkeley Software Distribution (4.3BSD) version of UNIX is used as a case study throughout the paper. Several recommendations are presented for the improvement of password mechanisms. In particular, a simple extension of the UNIX password system is described that permits the use of pass-phrases.
ER  - 

TY  - JOUR
T1  - The enemy within: the inherent security risks of temporary staff
JO  - Computer Fraud & Security
VL  - 2014
IS  - 5
SP  - 5
EP  - 7
PY  - 2014/5//
T2  - 
AU  - Liu, Ching
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(14)70489-0
UR  - http://www.sciencedirect.com/science/article/pii/S1361372314704890
AB  - Since the credit crunch and the subsequent tentative recovery, there has been a boom in the use of temporary staff. According to the Chartered Institute of Personnel and Development's (CIPD) Labour Market Outlook Survey last year, employers reported that 29% of new recruits will be employed on this basis.1

There is a growing issue with fraudsters taking positions with companies as temporary staff with the sole intention of learning their systems and procedures so they can later commit fraud.

The risk of fraud has been exacerbated by the growth of ‘Bring Your Own Device’ (BYOD) on many company networks. IT departments need to effectively control BYOD users, especially temporary ones, and look at what privileges temporary staff are given, explains Ching Liu of Control Risks.
ER  - 

TY  - JOUR
T1  - Secloud: A cloud-based comprehensive and lightweight security solution for smartphones
JO  - Computers & Security
VL  - 37
IS  - 0
SP  - 215
EP  - 227
PY  - 2013/9//
T2  - 
AU  - Zonouz, Saman
AU  - Houmansadr, Amir
AU  - Berthier, Robin
AU  - Borisov, Nikita
AU  - Sanders, William
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/j.cose.2013.02.002
UR  - http://www.sciencedirect.com/science/article/pii/S016740481300031X
KW  - Smartphone security
KW  - Intrusion detection
KW  - Cloud computing
KW  - Energy-aware security
KW  - Real-time intrusion response
AB  - As smartphones are becoming more complex and powerful to provide better functionalities, concerns are increasing regarding security threats against their users. Since smartphones use a software architecture similar to PCs, they are vulnerable to the same classes of security risks. Unfortunately, smartphones are constrained by their limited resources that prevent the integration of advanced security monitoring solutions that work with traditional PCs. We propose Secloud, a cloud-based security solution for smartphone devices. Secloud emulates a registered smartphone device inside a designated cloud and keeps it synchronized by continuously passing the device inputs and network connections to the cloud. This allows Secloud to perform a resource-intensive security analysis on the emulated replica that would otherwise be infeasible to run on the device itself. We demonstrate the practical feasibility of Secloud through a prototype for Android devices and illustrate its resource effectiveness by comparing it with on-device solutions.
ER  - 

TY  - CHAP

T1  - Index
A2  - Ridge, Enda 
BT  - Guerrilla Analytics
PB  - Morgan Kaufmann
CY  - Boston
PY  - 2015///
SP  - 253
EP  - 261
SN  - 978-0-12-800218-6
DO  - http://dx.doi.org/10.1016/B978-0-12-800218-6.00027-8
UR  - http://www.sciencedirect.com/science/article/pii/B9780128002186000278
ER  - 

TY  - CHAP
AU  - Virtue, Timothy
AU  - Rainey, Justin
T1  - Chapter 6 - Information Risk Assessment
A2  - Virtue, Timothy  
A2  - Rainey, Justin 
BT  - HCISPP Study Guide
PB  - Syngress
CY  - Boston
PY  - 2015///
SP  - 131
EP  - 166
SN  - 978-0-12-802043-2
DO  - http://dx.doi.org/10.1016/B978-0-12-802043-2.00006-9
UR  - http://www.sciencedirect.com/science/article/pii/B9780128020432000069
KW  - Information governance
KW  - Risk management
KW  - Security
KW  - Privacy
KW  - Risk assessment
KW  - Life cycle
KW  - Risk management activities
AB  - Abstract
This chapter discusses the importance and purpose of conducting information risk assessments. This includes identifying assessment control procedures, the process for conducting a risk assessment, and the process to respond and remediate risk.
ER  - 

TY  - CHAP
AU  - Ridge, Enda
T1  - Chapter 19 - Closing Remarks
A2  - Ridge, Enda 
BT  - Guerrilla Analytics
PB  - Morgan Kaufmann
CY  - Boston
PY  - 2015///
SP  - 239
EP  - 242
SN  - 978-0-12-800218-6
DO  - http://dx.doi.org/10.1016/B978-0-12-800218-6.00019-9
UR  - http://www.sciencedirect.com/science/article/pii/B9780128002186000199
ER  - 

TY  - JOUR
T1  - Privacy Threat Modeling for Emerging BiobankClouds
JO  - Procedia Computer Science
VL  - 37
IS  - 0
SP  - 489
EP  - 496
PY  - 2014///
T2  - The 5th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN-2014)/ The 4th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH 2014)/ Affiliated Workshops
AU  - Gholami, Ali
AU  - Lind, Anna-Sara
AU  - Reichel, Jane
AU  - Litton, Jan-Eric
AU  - Edlund, Ake
AU  - Laure, Erwin
SN  - 1877-0509
DO  - http://dx.doi.org/10.1016/j.procs.2014.08.073
UR  - http://www.sciencedirect.com/science/article/pii/S1877050914010382
KW  - privacy-preservation
KW  - data security
KW  - cloud computing
KW  - threat modeling
KW  - requirement analysis
AB  - Abstract
There is an increased amount of data produced by next generation sequencing (NGS) machines which demand scalable storage and analysis of genomic data. In order to cope with this huge amount of information, many biobanks are interested in cloud computing capabilities such as on-demand elasticity of computing power and storage capacity. There are several security and privacy requirements mandated by personal data protection legislation which hinder biobanks from migrating big data generated by the NGS machines. This paper describes the privacy requirements of platform-as-service BiobankClouds according to the European Data Protection Directive (DPD). It identifies several key privacy threats which leave BiobankClouds vulnerable to an attack. This study benefits health-care application designers in the requirement elicitation cycle when building privacy-preserving BiobankCloud platforms.
ER  - 

TY  - CHAP
AU  - McCrie, Robert D.
T1  - 9 - Operating Personnel-Intensive Programs
A2  - McCrie, Robert D. 
BT  - Security Operations Management (Second Edition)
PB  - Butterworth-Heinemann
CY  - Burlington
PY  - 2007///
SP  - 249
EP  - 284
SN  - 978-0-7506-7882-7
DO  - http://dx.doi.org/10.1016/B978-075067882-7/50048-5
UR  - http://www.sciencedirect.com/science/article/pii/B9780750678827500485
AB  - Publisher Summary
This chapter discusses the macro level of managing people in functioning programs. It considers two significant categories of security employment—security officers and investigators—and the programs that make them productive and efficient. Consequently, the optimally performing manager will seek to assure that security functions are achieved with the minimum number of people required. Proprietary and contract security services both have advantages therefore, managers sometimes plan to employ both in large operations. Software programs specifically written for security applications have improved accountability and decreased costs since their introduction. A request for proposal (RFP) is a bureaucratic, costly, and burdensome means of selecting a security services vendor. However, the process sets out a fair basis for identifying the best security company for the client and also helps to determine the most favorable arrangements for the client. The alarm monitoring and installation business is an important part of the security industry. Some of the typical aspects to an alarm monitoring contract include: terms of agreement, notification for termination, and ownership of the security system.
ER  - 

TY  - CHAP
AU  - Rittinghouse, John W.
AU  - Hancock, William M.
T1  - 11 - Forensics, Investigation, and Response
A2  - Rittinghouse, John W.  
A2  - Hancock, William M. 
BT  - Cybersecurity Operations Handbook
PB  - Digital Press
CY  - Burlington
PY  - 2004///
SP  - 365
EP  - 425
SN  - 978-1-55558-306-4
DO  - http://dx.doi.org/10.1016/B978-155558306-4/50016-0
UR  - http://www.sciencedirect.com/science/article/pii/B9781555583064500160
AB  - Publisher Summary
The truth about all investigations is that if perpetrators do not want to get caught, they most likely will not. Computer forensic science addresses the specific and articulated needs of law enforcement to make the most of this new form of electronic evidence. Forensics depends on the skills of the investigator, and it is quite common not to have enough evidence. Sometimes, however, by just plain luck, investigators may uncover some piece of evidence that allows them to build a case against the suspect. It is hard to explain this phenomenon, but even the best investigators are always learning about new methodologies criminals use to hide what they have done. Use of tools described in the chapter may cause the transition from “plain luck” in labs across the world. Better tools make for better investigations, and tools have come a long way in the past 15 years.
ER  - 

TY  - CHAP
AU  - Gantz, Stephen D.
T1  - Chapter 10 - Audit-Related Organizations, Standards, and Certifications
A2  - Gantz, Stephen D. 
BT  - The Basics of IT Audit
PB  - Syngress
CY  - Boston
PY  - 2014///
SP  - 197
EP  - 219
SN  - 978-0-12-417159-6
DO  - http://dx.doi.org/10.1016/B978-0-12-417159-6.00010-9
UR  - http://www.sciencedirect.com/science/article/pii/B9780124171596000109
KW  - Audit standards
KW  - certifications
KW  - audit organizations
KW  - audit guidance
AB  - This chapter provides summary information about major associations, agencies, and organizations engaged in the development and dissemination of standards, certifications, or procedures and guidelines related to IT auditing. The material in this chapter serves primarily as a point of reference to help readers identify organizations, audit standards, and auditor certifications relevant to their own IT audit needs. The organizations included here do not (in general) perform audits, but instead have supporting or oversight roles in the practice of IT auditing at national or international levels.
ER  - 

TY  - CHAP
AU  - Talburt, John R.
AU  - Zhou, Yinle
T1  - Chapter 1 - The Value Proposition for MDM and Big Data
A2  - Zhou, John R. TalburtYinle 
BT  - Entity Information Life Cycle for Big Data
PB  - Morgan Kaufmann
CY  - Boston
PY  - 2015///
SP  - 1
EP  - 16
SN  - 978-0-12-800537-8
DO  - http://dx.doi.org/10.1016/B978-0-12-800537-8.00001-6
UR  - http://www.sciencedirect.com/science/article/pii/B9780128005378000016
KW  - Master data
KW  - master data management
KW  - MDM
KW  - Big Data
KW  - reference data management
KW  - RDM
AB  - Abstract
This chapter gives a definition of master data management (MDM) and describes how it generates value for organizations. It also provides an overview of Big Data and the challenges it brings to MDM.
ER  - 

TY  - CHAP
AU  - Virtue, Timothy
AU  - Rainey, Justin
T1  - Chapter 4 - Privacy and Security in Healthcare
A2  - Virtue, Timothy  
A2  - Rainey, Justin 
BT  - HCISPP Study Guide
PB  - Syngress
CY  - Boston
PY  - 2015///
SP  - 61
EP  - 89
SN  - 978-0-12-802043-2
DO  - http://dx.doi.org/10.1016/B978-0-12-802043-2.00004-5
UR  - http://www.sciencedirect.com/science/article/pii/B9780128020432000045
KW  - Privacy
KW  - Security
KW  - Confidentiality
KW  - Integrity
KW  - Availability
KW  - Access control
KW  - Data encryption
KW  - Data classification
KW  - Logging and monitoring
KW  - Least privilege
KW  - OECD
KW  - GAPP
KW  - PIPEDA
KW  - UK Data Protection Act
AB  - Abstract
This chapter discusses the fundamental concepts of security and privacy and their impact on the healthcare industry. It will also review the relationship between privacy and security for healthcare organizations when managing and safeguarding healthcare information. General privacy and security principles in relation to specific healthcare industry standards will also be discussed.
ER  - 

TY  - CHAP

T1  - Index
A2  - Payne, Thomas H. 
BT  - Practical Guide to Clinical Computing Systems (Second Edition)
PB  - Academic Press
CY  - Oxford
PY  - 2015///
SP  - 221
EP  - 228
SN  - 978-0-12-420217-7
DO  - http://dx.doi.org/10.1016/B978-0-12-420217-7.09990-9
UR  - http://www.sciencedirect.com/science/article/pii/B9780124202177099909
ER  - 

TY  - CHAP
AU  - Williams, Branden R.
AU  - Chuvakin, Anton A.
AU  - Milroy, Derek
T1  - Chapter 19 - Myths and misconceptions of PCI DSS
A2  - Milroy, Branden R. WilliamsAnton A. ChuvakinDerek 
BT  - PCI Compliance (Fourth Edition)
PB  - Syngress
CY  - Boston
PY  - 2015///
SP  - 333
EP  - 356
SN  - 978-0-12-801579-7
DO  - http://dx.doi.org/10.1016/B978-0-12-801579-7.00024-8
UR  - http://www.sciencedirect.com/science/article/pii/B9780128015797000248
KW  - Myths
KW  - Misconceptions
KW  - False Information about PCI DSS
AB  - Abstract
This final chapter explains common but damaging PCI myths and misconceptions, as well as the reality behind them.
ER  - 

TY  - CHAP
AU  - Gonzalez, Deborah
T1  - Chapter 1 - Risk Management Digital Style
A2  - Gonzalez, Deborah 
BT  - Managing Online Risk
PB  - Butterworth-Heinemann
CY  - Boston
PY  - 2015///
SP  - 1
EP  - 24
SN  - 978-0-12-420055-5
DO  - http://dx.doi.org/10.1016/B978-0-12-420055-5.00001-3
UR  - http://www.sciencedirect.com/science/article/pii/B9780124200555000013
KW  - BlueWave computing
KW  - Critical security controls
KW  - Incident response
KW  - Models
KW  - Risk analysis
KW  - Risk assessment
KW  - Risk identification
KW  - Risk level
KW  - Risk management
KW  - Risk management apps
KW  - Risk mitigation
KW  - Risk remediation
KW  - Risk response
KW  - SANS
KW  - Security
KW  - Socially legal audit
KW  - Threat
AB  - Abstract
This introductory chapter lays out the context of the book by giving an overview of risk management concepts and how they apply in a digital environment. It goes over risk management models and the risk management process.
ER  - 

TY  - CHAP
AU  - Barnes, Christian
AU  - Bautts, Tony
AU  - Lloyd, Donald
AU  - Ouellet, Eric
AU  - Posluns, Jeffrey
AU  - Zendzian, David M.
AU  - O'Farrell, Neal
T1  - Chapter 8 - Auditing
A2  - Barnes, Christian
A2  - Bautts, Tony
A2  - Lloyd, Donald
A2  - Ouellet, Eric
A2  - Posluns, Jeffrey
A2  - Zendzian, David M. 
A2  - O'Farrell, Neal 
BT  - Hackproofing Your Wireless Network
PB  - Syngress
CY  - Rockland
PY  - 2002///
SP  - 363
EP  - 406
SN  - 978-1-928994-59-6
DO  - http://dx.doi.org/10.1016/B978-192899459-6/50028-9
UR  - http://www.sciencedirect.com/science/article/pii/B9781928994596500289
AB  - Publisher Summary
Audits are the most fundamental tools used for establishing a baseline and understanding how a system behaves after it has been installed. An audit is a methodology used to test systems or components against the predefined standards of operation or industry accepted best practices. Audits provide a means of assessing accountability and establishing metrics through performance measurements. Auditing is the most overlooked activity when deploying any technology system or application. It is an activity that should be performed continuously over the lifetime of a wireless network system. Doing an audit once does not guarantee a system performs as advertised in perpetuity. Systems are constantly being stretched and expanded to meet the ever-changing roles of an organization. Audits ensure that as new features and functionalities are added, they do not inversely affect the system. This chapter discusses the fundamental principles of security auditing. It considers industry “best practices” and commonly used standards employed in auditing wireless networks. The base methodology applied when auditing other systems is similar. The guidelines provided in this chapter are generally applicable to most wireless networks. One can choose to add or remove auditing components to fit one's own specific environment and systems.
ER  - 

TY  - CHAP
AU  - Gantz, Stephen D.
T1  - Chapter 1 - IT Audit Fundamentals
A2  - Gantz, Stephen D. 
BT  - The Basics of IT Audit
PB  - Syngress
CY  - Boston
PY  - 2014///
SP  - 1
EP  - 19
SN  - 978-0-12-417159-6
DO  - http://dx.doi.org/10.1016/B978-0-12-417159-6.00001-8
UR  - http://www.sciencedirect.com/science/article/pii/B9780124171596000018
KW  - IT audit
KW  - auditors
KW  - information assurance
KW  - governance
AB  - This chapter gives a broad overview of IT auditing, explaining what auditing is, why auditing is performed, the subjects of audits, and who conducts audits, and defining key terms and concepts referenced throughout the book. It seeks to answer the basic questions someone new to IT auditing would ask—the who, what, when, where, and why—and subsequently sets up more detailed chapters that go into more depth as to how auditing is done. This chapter distinguishes between internal and external auditing in terms of the purposes, rationale, and requirements for each and carries this distinction through to the types of organizations and auditors involved. It also describes the various career paths and professional development activities associated with developing IT auditors.
ER  - 

TY  - CHAP
AU  - Cole, Eric
AU  - Ring, Sandra
T1  - Chapter 6 - Banking and Financial Sector
A2  - Ring, Eric ColeSandra 
BT  - Insider Threat
PB  - Syngress
CY  - Burlington
PY  - 2006///
SP  - 241
EP  - 274
SN  - 978-1-59749-048-1
DO  - http://dx.doi.org/10.1016/B978-159749048-1/50008-X
UR  - http://www.sciencedirect.com/science/article/pii/B978159749048150008X
AB  - Publisher Summary
Activities of financial sector insiders include sabotage, stealing of proprietary information, embezzlement of money, and so on. Insiders cause damage by taking undue risks and covering them up. Repeatedly, the lessons learned call for auditing and outside accountability. This is true in all cases but is especially necessary for employees whose jobs require high levels of risk. The chapter also highlights the threat of identity theft and what institutions can do to help prevent insiders from participating in fraud rings. The chapter focuses on the benefit of legislation, such as the California Notice of Security Breach Law that protects citizens by forcing organizations that have incidents to expose them to the victims. Theft involving banking and finance is complex. Insiders can steal, embezzle, make poor decisions with money, and compromise intellectual property. There are several types of threats and one of them is financial threat. Financial theft is fairly straightforward. An insider who knows the system and knows how to exploit it is likely to do so. The insider either helps to arrange a robbery at a large institution or transfers money to secret accounts. Some cases are even more intriguing.
ER  - 

TY  - CHAP
AU  - XYPRO Technology
T1  - 1 - Compliance Concepts
A2  - Technology, XYPRO 
BT  - Securing HP NonStop Servers in an Open Systems World
PB  - Digital Press
CY  - Burlington
PY  - 2006///
SP  - 1
EP  - 19
SN  - 978-1-55558-344-6
DO  - http://dx.doi.org/10.1016/B978-155558344-6/50004-4
UR  - http://www.sciencedirect.com/science/article/pii/B9781555583446500044
AB  - Publisher Summary
This chapter explores the key concepts of commercial security compliance and provides a high-level framework to organize compliance requirements. This includes the detailed components fundamental to efficiently securing data and systems in real-world NonStop Server environments. The new standards and legislative regulations introduced recently in the IT departments aim to affix responsibility for protecting the privacy of customers' personal data firmly on those who direct and "police" the companies that hold such data. They also ensure that customers (both consumers and companies) are notified in the event their private data has been compromised. The chapter mainly focuses on six samples representing a variety of regulatory organizations; and studies various data security regulations and aspects like representative regulation, common criteria for information technology security evaluation, federal information processing standard 140-2, health insurance portability &amp; accountability act, and payment card industry (PCI) data security standard. All of these acts and regulation ensure that security activity is logged and any indication of imminent security violation is reported immediately to all who may be concerned, internally and externally, and is acted upon in a timely manner; and that violation and security activity is logged, reported, reviewed, and appropriately escalated on a regular basis to identify and resolve incidents involving unauthorized activity. The chapter also describes the various authentication, authorization, auditing, integrity and confidentiality requirements as per the Native HP NonStop Guardian security foundation.
ER  - 

TY  - CHAP
AU  - Rittinghouse, John W.
AU  - Ransome, James F.
T1  - 7 - Testing, Auditing, and Training
A2  - Rittinghouse, John W.  
A2  - Ransome, James F. 
BT  - Business Continuity and Disaster Recovery for InfoSec Managers
PB  - Digital Press
CY  - Burlington
PY  - 2006///
SP  - 193
EP  - 239
SN  - 978-1-55558-339-2
DO  - http://dx.doi.org/10.1016/B978-155558339-2/50009-7
UR  - http://www.sciencedirect.com/science/article/pii/B9781555583392500097
AB  - Publisher Summary
The chapter discusses the basic diligence efforts needed to keep a security program healthy. The purpose of a security audit is to assess the quantity of risk and the effectiveness of the organization's risk management processes as they relate to the security measures instituted to ensure the confidentiality, integrity, and availability of information and to instill accountability for the actions taken on the organization's systems. Information security is the process by which an organization protects and secures systems and media, facilities that process, and maintains information vital to its operations. The security of systems and information is essential for the privacy of organizational and corporate customer information. Security professionals must maintain effective security programs adequate for their organization's operational complexity. These security programs must have a strong board and senior-management-level support, integration of security responsibilities and controls throughout the organization's business processes, and clear accountability for carrying out security responsibilities. The chapter provides guidance to security professionals and organizations on determining the level of security risks to the organization and evaluating the adequacy of the organization's risk management.
ER  - 

TY  - CHAP
AU  - Savino, John O.
AU  - Turvey, Brent E.
T1  - Chapter 18 - Sex Crimes on Trial
A2  - Savino, John O.  
A2  - Turvey, Brent E. 
BT  - Rape Investigation Handbook (Second Edition)
PB  - Academic Press
CY  - San Diego
PY  - 2011///
SP  - 463
EP  - 488
SN  - 978-0-12-386029-3
DO  - http://dx.doi.org/10.1016/B978-0-12-386029-3.00018-8
UR  - http://www.sciencedirect.com/science/article/pii/B9780123860293000188
KW  - Controlled meet
KW  - Ethics
KW  - Investigative reports
KW  - Pretext call
KW  - Reasonable doubt
KW  - Wade hearing
AB  - Publisher Summary
This chapter discusses various ways to help investigators prepare their case for court and present it effectively at trial. Investigators are entirely responsible for assembling the initial case against an accused sexual assault suspect. This is best accomplished from the beginning, with a comprehensive investigation: gathering evidence, having evidence tested, conducting effective interviews, and investigating each and every investigative lead that is developed. A prosecutor is ultimately going to be responsible for deciding whether or not criminal charges will be filed against any suspects, and must be capable of presenting the case against the suspect in court. For prosecutors to succeed, they must convince the judge or jury with proof of a suspect's guilt that is beyond a reasonable doubt. Prosecutors are under no obligation to accept cases presented to them by an investigator. Criminal defendants are entitled to due process of law which means adequate access to the evidence, the ability to confront their accusers, and competent legal representation. It is suggested that the investigators should view the trial process, and the accused's defense, as a necessary challenge to their investigative stamina and competence.
 
Sex crimes investigators should be thinking about and preparing for trial from the moment that a complaint is received. Specifically, they should assume that their case will result in a trial and that supervisors, forensic experts, prosecutors, defense attorneys, judges, and juries will scrutinize every decision made. The purpose of this chapter is to help investigators prepare their case for court and present it effectively at trial. If sex crime investigators follow even some of the suggestions provided in this text, all of which are born out of years of trial and error by the authors and their colleagues, then they will have a better chance of increasing their clearance rates, of increasing their rate of prosecutable cases, and of protecting the communities that they serve.
ER  - 

TY  - CHAP
AU  - Gonzalez, Deborah
T1  - Chapter 2 - Internal and External Risks
A2  - Gonzalez, Deborah 
BT  - Managing Online Risk
PB  - Butterworth-Heinemann
CY  - Boston
PY  - 2015///
SP  - 25
EP  - 52
SN  - 978-0-12-420055-5
DO  - http://dx.doi.org/10.1016/B978-0-12-420055-5.00002-5
UR  - http://www.sciencedirect.com/science/article/pii/B9780124200555000025
KW  - App malware
KW  - BYOD
KW  - BYOD policy
KW  - External security risk
KW  - Hot spot
KW  - Internal security risk
KW  - Mobile
KW  - Mobile app management
KW  - Mobile data protection
KW  - Mobile device management
KW  - Password alternatives
KW  - Passwords
KW  - Priority
KW  - Security budget
KW  - Security perception
KW  - Shadow IT
AB  - Abstract
This chapter gives an overview of various internal and external risks associated with digital and online activity. The internal explores corporate security perception, priority and budget setting, traditional and shadow information technology, mobile and the Bring-Your-Own-Device trend, and people, including employees, vendors, and third parties that can lead to cyber risks such as computer security, computer viruses, computer fraud, etc. The external looks at issues with a lesser element of control for the corporation such as technology advances and new devices, cloud computing, hacking, regulation, and natural disasters and squirrels.
ER  - 

TY  - CHAP
AU  - Nathans, David
T1  - Chapter 6 - Daily operations
A2  - Nathans, David 
BT  - Designing and Building Security Operations Center
PB  - Syngress
CY  - 
PY  - 2015///
SP  - 125
EP  - 149
SN  - 978-0-12-800899-7
DO  - http://dx.doi.org/10.1016/B978-0-12-800899-7.00006-9
UR  - http://www.sciencedirect.com/science/article/pii/B9780128008997000069
KW  - shift schedule
KW  - workflow
KW  - checklists
KW  - daily operations
KW  - shift change
KW  - critical bridge
KW  - escalations
AB  - Abstract
What you do on a daily basis has direct impact to the security of your organization. From shift changes to critical notifications, the SOC has to have a precise recipe of action to ensure critical countermeasures are taken to protect the organization. Proper workflow, checklists, and schedules must be created to ensure that everyone executes properly
ER  - 

TY  - CHAP
AU  - Wu, Caesar
AU  - Buyya, Rajkumar
T1  - Chapter 10 - Fire Suppression and On-Site Security
A2  - Wu, Caesar  
A2  - Buyya, Rajkumar 
BT  - Cloud Data Centers and Cost Modeling
PB  - Morgan Kaufmann
CY  - 
PY  - 2015///
SP  - 341
EP  - 367
SN  - 978-0-12-801413-4
DO  - http://dx.doi.org/10.1016/B978-0-12-801413-4.00010-6
UR  - http://www.sciencedirect.com/science/article/pii/B9780128014134000106
KW  - Fire suppression
KW  - fire suppression solutions
KW  - fire classification
KW  - fire detection
KW  - alarm
KW  - sprinkler
KW  - extinguisher
KW  - flammable objects
KW  - smoke detection
KW  - physical security
KW  - SSAE
KW  - SOX
KW  - CCTV
KW  - security zone
AB  - This chapter consists of two main topics. One is the data center fire suppression system and the other is the on-site security system. Regarding the fire suppression issue, this chapter discusses how to select the best solution for a fire suppression system for a particular data center. One of the criteria for selection is cost considerations.

The second topic is on-site or physical security systems. Based on the TIA-942 standard, different tiers of data center should have different physical security systems. The physical security system will be organized with three layers, the physical layer, the organizational layer, and the infrastructure layer.
ER  - 

TY  - JOUR
T1  - HIPAA compliant auditing system for medical images
JO  - Computerized Medical Imaging and Graphics
VL  - 29
IS  - 2–3
SP  - 235
EP  - 241
PY  - 2005/3//
Y2  - 2005/4//
T2  - Imaging Informatics
AU  - Zhou, Zheng
AU  - Liu, Brent J.
SN  - 0895-6111
DO  - http://dx.doi.org/10.1016/j.compmedimag.2004.09.009
UR  - http://www.sciencedirect.com/science/article/pii/S0895611104001223
KW  - HIPAA
KW  - Security
KW  - HIPAA compliant auditing system
KW  - Auditing
KW  - monitoring
AB  - As an official regulation for healthcare privacy and security, Health Insurance Portability and Accountability Act (HIPAA) mandates health institutions to protect health information against unauthorized use or disclosure. One such method proposed by HIPAA Security Standards is audit trail, which records and examines health information access activities. HIPAA mandates healthcare providers to have the ability to generate audit trails on data access activities for any specific patient. Although current medical imaging systems generate activity logs, there is a lack of formal methodology to interpret these large volumes of log data and generate HIPAA compliant auditing trails.

This paper outlines the design of a HIPAA compliant auditing system (HCAS) for medical images in imaging systems such as PACS and discusses the development of a security monitoring (SM) toolkit based on some of the partial components in HCAS.
ER  - 

TY  - CHAP
AU  - Rittinghouse, John W.
AU  - Hancock, William M.
T1  - 17 - Security Service Level Agreements
A2  - Rittinghouse, John W.  
A2  - Hancock, William M. 
BT  - Cybersecurity Operations Handbook
PB  - Digital Press
CY  - Burlington
PY  - 2004///
SP  - 581
EP  - 600
SN  - 978-1-55558-306-4
DO  - http://dx.doi.org/10.1016/B978-155558306-4/50022-6
UR  - http://www.sciencedirect.com/science/article/pii/B9781555583064500226
AB  - Publisher Summary
Service-level agreement (SLA) data must be negotiated, fair, measurable, and have some level of accountability for nonperformance. It must be subject to periodic review and must go through an approval process. One should never accept an SLA as valid or binding if the items defined in it cannot be benchmarked against some standard and subsequently measured against that benchmark to evaluate performance. If that is the situation, it is definitely time to sit down and revise that SLA to something more reasonable. Many times, SLAs are written without the proper level of participation or negotiation from the support groups, and unreasonable expectations are set. The support group may try to uphold such expectations in good faith, but they are doomed to failure because the expectations are set higher than are reasonable. The support team, working as hard as it possibly can, will bear the brunt of much criticism that will be unwarranted because expectations were not reasonable.
ER  - 

TY  - JOUR
T1  - Digital provenance: Enabling secure data forensics in cloud computing
JO  - Future Generation Computer Systems
VL  - 37
IS  - 0
SP  - 259
EP  - 266
PY  - 2014/7//
T2  - Special Section: Innovative Methods and Algorithms for Advanced Data-Intensive Computing Special Section: Semantics, Intelligent processing and services for big data Special Section: Advances in Data-Intensive Modelling and Simulation Special Section: Hybrid Intelligence for Growing Internet and its Applications
AU  - Li, Jin
AU  - Chen, Xiaofeng
AU  - Huang, Qiong
AU  - Wong, Duncan S.
SN  - 0167-739X
DO  - http://dx.doi.org/10.1016/j.future.2013.10.006
UR  - http://www.sciencedirect.com/science/article/pii/S0167739X13002161
KW  - Provenance
KW  - Cloud computing
KW  - Privacy
KW  - Attribute-based signature
AB  - Abstract
Secure provenance that records the ownership and process history of data objects is vital to the success of data forensics in cloud computing. In this paper, we propose a new secure provenance scheme based on group signature and attribute-based signature techniques. The proposed provenance scheme provides confidentiality on sensitive documents stored in a cloud, unforgeability of the provenance record, anonymous authentication to cloud servers, fine-grained access control on documents, and provenance tracking on disputed documents. Furthermore, it is assumed that the cloud server has huge computation capacity, while users are regarded as devices with low computation capability. Aiming at this, we show how to utilize the cloud server to outsource and decrease the user’s computational overhead during the process of provenance. With provable security techniques, we formally demonstrate the security of the proposed scheme under standard assumptions.
ER  - 

TY  - CHAP
AU  - Rittinghouse, John W.
AU  - Hancock, William M.
T1  - 12 - Security Diligence
A2  - Rittinghouse, John W.  
A2  - Hancock, William M. 
BT  - Cybersecurity Operations Handbook
PB  - Digital Press
CY  - Burlington
PY  - 2004///
SP  - 427
EP  - 440
SN  - 978-1-55558-306-4
DO  - http://dx.doi.org/10.1016/B978-155558306-4/50017-2
UR  - http://www.sciencedirect.com/science/article/pii/B9781555583064500172
AB  - Publisher Summary
This chapter discusses basic diligence efforts needed to keep a security program healthy and to mitigate risks to an organization. The objective of a security audit is to assess the quantity of risk and the effectiveness of the organization's risk-management processes as they relate to the security measures instituted to ensure confidentiality, integrity, and availability of information and to instill accountability for actions taken on the organization's systems. Organizations often inaccurately perceive information security as the state or condition of various controls at a given time. Security is an ongoing process whereby the condition of security controls is just one indicator of the overall security posture. Other indicators include the ability of the institution to assess its posture continually and react appropriately in the face of rapidly changing threats, technologies, and business conditions. This requires an organization to continuously integrate processes, people, and technology to mitigate risk in accordance with risk assessment and acceptable risk tolerance.
ER  - 

TY  - CHAP
AU  - Fischer-Hbner, Simone
AU  - Berthold, Stefan
T1  - Chapter 43 - Privacy-Enhancing Technologies1
A2  - Vacca, John R. 
BT  - Computer and Information Security Handbook (Second Edition)
PB  - Morgan Kaufmann
CY  - Boston
PY  - 2013///
SP  - 755
EP  - 772
SN  - 978-0-12-394397-2
DO  - http://dx.doi.org/10.1016/B978-0-12-394397-2.00043-X
UR  - http://www.sciencedirect.com/science/article/pii/B978012394397200043X
KW  - privacy
KW  - privacy-enhancing technologies
KW  - personal privacy
KW  - legal privacy
KW  - legitimacy
KW  - purpose specification
KW  - purpose limitation
KW  - data minimization
KW  - transparency
KW  - data subjects
AB  - In our modern information age, recent technical developments and trends, such as mobile and pervasive computing, cloud computing, and Web 2.0 applications, increasingly pose privacy dilemmas. Due to the low costs and technical advances of storage technologies, masses of personal data can easily be stored. Once disclosed, these data may be retained forever, often without the knowledge of the individuals concerned, and be removed with difficulty. Hence, it has become hard for individuals to manage and control their personal spheres. Both legal and technical means are needed to protect privacy and to (re)establish the individuals’ control. This chapter provides an overview to the area of privacy-enhancing technologies (PETs), which help to protect privacy by technically enforcing legal privacy principles. It will start with defining the legal foundations of PETs and will present a classification of PETs as well as a definition of traditional privacy properties that PETs are addressing and metrics for measuring the level of privacy that PETs are providing. Then, a selection of the most relevant PETs is presented.
ER  - 

TY  - CHAP
AU  - Yee, George
AU  - Korba, Larry
T1  - Chapter 29 - Personal Privacy Policies1
A2  - Vacca, John R. 
BT  - Computer and Information Security Handbook
PB  - Morgan Kaufmann
CY  - Boston
PY  - 2009///
SP  - 487
EP  - 505
SN  - 978-0-12-374354-1
DO  - http://dx.doi.org/10.1016/B978-0-12-374354-1.00029-7
UR  - http://www.sciencedirect.com/science/article/pii/B9780123743541000297
AB  - Publisher Summary
This chapter illustrates the use of personal privacy policies to protect privacy. The rapid growth of the Internet has been accompanied by a similar growth in the availability of Internet e-services. This proliferation of e-services has in turn fueled the need to protect the personal privacy of e-service users or consumers. E-services are available for banking, shopping, stock investing, and health care, to name a few areas. However, each of these services requires a consumer's personal information in one form or another. This leads to concerns over privacy. The protection of personal privacy is paramount if e-services are to be successful. A personal privacy policy approach to privacy protection seems best. However, for this approach to work, consumers must be able to derive their personal privacy policies easily. An effective and flexible way of protecting privacy is to manage it using privacy policies. In this approach, each provider of an e-service has a privacy policy specifying the private information required for that e-service.
ER  - 

TY  - JOUR
T1  - Human resource information systems: Information security concerns for organizations
JO  - Human Resource Management Review
VL  - 23
IS  - 1
SP  - 105
EP  - 113
PY  - 2013/3//
T2  - Emerging Issues in Theory and Research on Electronic Human Resource Management (eHRM)
AU  - Zafar, Humayun
SN  - 1053-4822
DO  - http://dx.doi.org/10.1016/j.hrmr.2012.06.010
UR  - http://www.sciencedirect.com/science/article/pii/S1053482212000538
KW  - Human resource information system
KW  - Information security
KW  - Information privacy
KW  - Security policies
KW  - Security legislation
KW  - Security architecture
KW  - Security training
KW  - Risk analysis
AB  - We explore HRIS and e-HR security by presenting information security fundamentals and how they pertain to organizations. With increasing use of enterprise systems such as HRIS and e-HR, security of such systems is an area that is worthy of further exploration. Even then, there is surprisingly little research in this area, albeit that extensive work is present in regard to HRIS privacy. While focusing on HRIS and e-HR security, we introduce aspects of HRIS and e-HR security and how it can be enhanced in organizations. A research model is also presented along with propositions that can guide future research.
ER  - 

TY  - CHAP
AU  - Metheny, Matthew
T1  - Chapter 10 - Security Assessment and Authorization: Governance, Preparation, and Execution
A2  - Metheny, Matthew 
BT  - Federal Cloud Computing
PB  - Syngress
CY  - Boston
PY  - 2013///
SP  - 329
EP  - 348
SN  - 978-1-59749-737-4
DO  - http://dx.doi.org/10.1016/B978-1-59-749737-4.00010-1
UR  - http://www.sciencedirect.com/science/article/pii/B9781597497374000101
AB  - Abstract

This chapter focused on the governance, preparation, and execution of the assessment and authorization processes. An introduction to the security assessment process provides a basic understanding of security assessments as it relates to the integration of security testing within system development life cycle (SDLC) and in support of information system authorization. The roles and responsibilities of the security assessment customer and provider are discussed across the various aspects of security assessment activities to include governance, preparation, and execution.

Keywords

SDLC, Security assessment process, RMF, FedRAMP, System security plan, Authorization, Security assessment plan
ER  - 

TY  - JOUR
T1  - Security in cloud computing: Opportunities and challenges
JO  - Information Sciences
VL  - 305
IS  - 0
SP  - 357
EP  - 383
PY  - 2015/6/1/
T2  - 
AU  - Ali, Mazhar
AU  - Khan, Samee U.
AU  - Vasilakos, Athanasios V.
SN  - 0020-0255
DO  - http://dx.doi.org/10.1016/j.ins.2015.01.025
UR  - http://www.sciencedirect.com/science/article/pii/S0020025515000638
KW  - Cloud computing
KW  - Multi-tenancy
KW  - Security
KW  - Virtualization
KW  - Web services
AB  - Abstract
The cloud computing exhibits, remarkable potential to provide cost effective, easy to manage, elastic, and powerful resources on the fly, over the Internet. The cloud computing, upsurges the capabilities of the hardware resources by optimal and shared utilization. The above mentioned features encourage the organizations and individual users to shift their applications and services to the cloud. Even the critical infrastructure, for example, power generation and distribution plants are being migrated to the cloud computing paradigm. However, the services provided by third-party cloud service providers entail additional security threats. The migration of user’s assets (data, applications, etc.) outside the administrative control in a shared environment where numerous users are collocated escalates the security concerns. This survey details the security issues that arise due to the very nature of cloud computing. Moreover, the survey presents the recent solutions presented in the literature to counter the security issues. Furthermore, a brief view of security vulnerabilities in the mobile cloud computing are also highlighted. In the end, the discussion on the open issues and future research directions is also presented.
ER  - 

TY  - JOUR
T1  - Challenges and solutions for secure information centric networks: A case study of the NetInf architecture
JO  - Journal of Network and Computer Applications
VL  - 50
IS  - 0
SP  - 64
EP  - 72
PY  - 2015/4//
T2  - 
AU  - Loo, Jonathan
AU  - Aiash, Mahdi
SN  - 1084-8045
DO  - http://dx.doi.org/10.1016/j.jnca.2014.06.003
UR  - http://www.sciencedirect.com/science/article/pii/S1084804514001337
KW  - Network of information
KW  - Information centric networks
KW  - X.805 standard
AB  - Abstract
A large number of emerging Internet applications require information dissemination across different organizational boundaries, heterogeneous platforms, and a large, dynamic population of publishers and subscribers. A new information-centric network architecture called Network of Information (NetInf) has been developed in the context of the FP7 EU-funded 4WARD project. This architecture can significantly improve large scale information distribution. Furthermore, it supports future mobile networks in situations with intermittent and heterogeneous connectivity and connects the digital with the physical world to enable better user experience. However, NetInf is still in an early stage of implementation and its security is yet to be evaluated. The security concern of NetInf is a major factor for its wide-scale adoption. Therefore, this paper uses the X.805 security standard to analyse the security of the NetInf architecture. The analysis highlights the main source of threats and potential security services to tackle them. The paper also defines a threat model in the form of possible attacks against the NetInf architecture.
ER  - 

TY  - JOUR
T1  - XBRL and open data for global financial ecosystems: A linked data approach
JO  - International Journal of Accounting Information Systems
VL  - 13
IS  - 2
SP  - 141
EP  - 162
PY  - 2012/6//
T2  - XBRL: Research Implications and Future Directions
AU  - O'Riain, Seán
AU  - Curry, Edward
AU  - Harth, Andreas
SN  - 1467-0895
DO  - http://dx.doi.org/10.1016/j.accinf.2012.02.002
UR  - http://www.sciencedirect.com/science/article/pii/S1467089512000140
KW  - Internet
KW  - World Wide Web
KW  - Metadata
KW  - Financial ecosystem
KW  - eXtensible Business Markup Language, XBRL
KW  - Resource Description Framework RDF, Open Data, Linked Data, Linked Open Data
KW  - LOD Financial Mashup
AB  - Information professionals performing business activity related investigative analysis must routinely associate data from a diverse range of Web based general-interest business and financial information sources. XBRL has become an integral part of the financial data landscape. At the same time, Open Data initiatives have contributed relevant financial, economic, and business data to the pool of publicly available information on the Web but the use of XBRL in combination with Open Data remains at an early state of realisation. In this paper we argue that Linked Data technology, created for Web scale information integration, can accommodate XBRL data and make it easier to combine it with open datasets. This can provide the foundations for a global data ecosystem of interlinked and interoperable financial and business information with the potential to leverage XBRL beyond its current regulatory and disclosure role. We outline the uses of Linked Data technologies to facilitate XBRL consumption in conjunction with non-XBRL Open Data, report on current activities and highlight remaining challenges in terms of information consolidation faced by both XBRL and Web technologies.
ER  - 

TY  - JOUR
T1  - Secure cloud storage based on cryptographic techniques
JO  - The Journal of China Universities of Posts and Telecommunications
VL  - 19, Supplement 2
IS  - 0
SP  - 182
EP  - 189
PY  - 2012/10//
T2  - 
AU  - PENG, Yong
AU  - ZHAO, Wei
AU  - XIE, Feng
AU  - DAI, Zhong-hua
AU  - GAO, Yang
AU  - CHEN, Dong-qing
SN  - 1005-8885
DO  - http://dx.doi.org/10.1016/S1005-8885(11)60424-X
UR  - http://www.sciencedirect.com/science/article/pii/S100588851160424X
KW  - cloud computing
KW  - cloud storage
KW  - security
KW  - privacy
KW  - cryptographic technique
AB  - Cloud computing is the delivery of computing and storage capacity as a service to users. Cloud storage, as a subservice of infrastructure as a service (IaaS) in cloud computing, is a model of networked online storage where data is stored in virtualized pools of storage. As fast development and application of cloud computing and cloud storage, users concern more and more about security and privacy issues involved in these techniques. From industrial and academic viewpoints currently, cryptography is considered as a key technology to solve security and privacy problems. In this paper, we mainly give a review on research results of secure cloud storage in which cryptographic techniques have been used to their designs. We start form reviewing the definition of cloud storage, and subsequently review the existing secure cloud storage based on cryptographic techniques. Moreover, we analyze and indicate what type of cryptographic techniques is mainly adopted in existing cloud storages and what role the cryptographic techniques play. Through this work, we can better catch what the relationship between secure cloud storage and cryptographic techniques, and how about the application mechanism of cryptographic techniques in cloud storage. We hope this review can give some help for future research, and more secure cloud storages by using cryptographic techniques can be proposed in the future.
ER  - 

TY  - CHAP
AU  - Wheeler, Evan
T1  - Chapter 6 - Risk Exposure Factors
A2  - Wheeler, Evan 
BT  - Security Risk Management
PB  - Syngress
CY  - Boston
PY  - 2011///
SP  - 105
EP  - 125
SN  - 978-1-59749-615-5
DO  - http://dx.doi.org/10.1016/B978-1-59749-615-5.00006-2
UR  - http://www.sciencedirect.com/science/article/pii/B9781597496155000062
KW  - qualitative
KW  - quantitative
KW  - risk scale
KW  - risk analysis
KW  - severity
KW  - likelihood
KW  - sensitivity
KW  - risk measure
KW  - risk assessment
KW  - risk exposure
KW  - impact
KW  - consequences
KW  - threat
KW  - FAIR
AB  - Publisher Summary
This chapter discusses the risk exposure factors. The two risk exposure factors are: qualitative risk management and risk assessment. The goal of qualitative risk management is to implement a model that goes beyond just labeling a situation or issue as “risky.” A well-designed qualitative model can help one to identify the most likely impact to the organization by clearly defining the sensitivity of the resource, the severity of the vulnerability, and the likelihood of the threat. Risk assessment includes several activities, including identifying threats and vulnerabilities and then rating each combination to produce a final risk exposure value. If one uses a qualitative model to assess risks, then it is certainly important to have well-documented scales with clear differences between each level, but it is also critical to understand how these scales were developed so that one can properly apply them to risks. When one thinks about risk exposure as only including three variables, it seems simple. But in reality many factors go into assessing just likelihood or severity.
 
The most controversial topic in risk management by far is how to rate the risks. This chapter focuses on simple and proven models for both qualitative and quantitative risk analysis. The majority of the chapter is spent framing out a qualitative risk measure that accounts for the sensitivity of the resource, the severity of the vulnerability, and the likelihood the threat will exploit the vulnerability. We start with a simple qualitative approach and then expand it to cover more complex scenarios. The final qualitative model in this chapter is used throughout the book to assess risk examples and case studies. The chapter wraps up with a brief review of quantitative measures, highlighting several implementation challenges and a loss expectancy analysis method.
ER  - 

TY  - JOUR
T1  - Catching the malicious insider
JO  - Information Security Technical Report
VL  - 13
IS  - 4
SP  - 220
EP  - 224
PY  - 2008/11//
T2  - 
AU  - Jones, Andy
SN  - 1363-4127
DO  - http://dx.doi.org/10.1016/j.istr.2008.10.008
UR  - http://www.sciencedirect.com/science/article/pii/S1363412708000526
KW  - Insider
KW  - Defence
KW  - Detection holistic
AB  - This paper looks at the issue of the malicious insider and at a range of the environmental and technical issues that have led to the current situation. The paper also examines why the threat from the malicious insider is changing and looks at a range of measures that can be taken in order to minimise the likelihood of an attack and to enhance the probability of detection in the case of an attack.
ER  - 

TY  - JOUR
T1  - Sarbanes-Oxley: maybe a blessing, maybe a curse
JO  - Computer Fraud & Security
VL  - 2005
IS  - 9
SP  - 4
EP  - 7
PY  - 2005/9//
T2  - 
AU  - Power, Richard
AU  - Forte, Dario
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(05)70250-5
UR  - http://www.sciencedirect.com/science/article/pii/S1361372305702505
AB  - Sarbanes-Oxley can bring benefits and heartache to IT security managers. This article demonstrates the advantages and the headaches that the legislation can cause.
ER  - 

TY  - JOUR
T1  - Single password authentication
JO  - Computer Networks
VL  - 57
IS  - 13
SP  - 2597
EP  - 2614
PY  - 2013/9/9/
T2  - 
AU  - Acar, Tolga
AU  - Belenkiy, Mira
AU  - Küpçü, Alptekin
SN  - 1389-1286
DO  - http://dx.doi.org/10.1016/j.comnet.2013.05.007
UR  - http://www.sciencedirect.com/science/article/pii/S1389128613001667
KW  - Password-based authentication
KW  - Dictionary attacks
KW  - Malware
KW  - Honeypots
KW  - Privacy
KW  - Mobile
AB  - Abstract
Users frequently reuse their passwords when authenticating to various online services. Combined with the use of weak passwords or honeypot/phishing attacks, this brings high risks to the security of the user’s account information. In this paper, we propose several protocols that can allow a user to use a single password to authenticate to multiple services securely. All our constructions provably protect the user from dictionary attacks on the password, and cross-site impersonation or honeypot attacks by the online service providers.

Our solutions assume the user has access to either an untrusted online cloud storage service (as per Boyen [16]), or a mobile storage device that is trusted until stolen. In the cloud storage scenario, we consider schemes that optimize for either storage server or online service performance, as well as anonymity and unlinkability of the user’s actions. In the mobile storage scenario, we minimize the assumptions we make about the capabilities of the mobile device: we do not assume synchronization, tamper resistance, special or expensive hardware, or extensive cryptographic capabilities. Most importantly, the user’s password remains secure even after the mobile device is stolen. Our protocols provide another layer of security against malware and phishing. To the best of our knowledge, we are the first to propose such various and provably secure password-based authentication schemes. Lastly, we argue that our constructions are relatively easy to deploy, especially if a few single sign-on services (e.g., Microsoft, Google, and Facebook) adopt our proposal.
ER  - 

TY  - JOUR
T1  - Securing medical networks
JO  - Network Security
VL  - 2007
IS  - 6
SP  - 13
EP  - 16
PY  - 2007/6//
T2  - 
AU  - Dantu, Ram
AU  - Oosterwijk, Herman
AU  - Kolan, Prakash
AU  - Husna, Husain
SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/S1353-4858(07)70055-7
UR  - http://www.sciencedirect.com/science/article/pii/S1353485807700557
AB  - The Health Information Portability and Accountability Act of 1996 (HIPAA) imposes strict regulations on healthcare institutions and commercial vendors to indemnify clinical data against unscrupulous users. Security vulnerabilities concerning hospital information systems not only negatively impact patient healthcare, but may also represent a potential federal violation. For a comprehensive understanding of the security of a radiology communication network, a detailed survey of the Picture Archiving and Communication Systems (PACS) was compiled. In this paper, we present survey results and a set of recommendations for implementing PACS security.
ER  - 

TY  - JOUR
T1  - Analytical models for risk-based intrusion response
JO  - Computer Networks
VL  - 57
IS  - 10
SP  - 2181
EP  - 2192
PY  - 2013/7/5/
T2  - Towards a Science of Cyber Security Security and Identity Architecture for the Future Internet
AU  - Caskurlu, Bugra
AU  - Gehani, Ashish
AU  - Bilgin, Cemal Cagatay
AU  - Subramani, K.
SN  - 1389-1286
DO  - http://dx.doi.org/10.1016/j.comnet.2013.03.012
UR  - http://www.sciencedirect.com/science/article/pii/S1389128613000881
KW  - Risk analysis
KW  - Intrusion response
KW  - Partial vertex cover
KW  - Approximation algorithms
AB  - Abstract
Risk analysis has been used to manage the security of systems for several decades. However, its use has been limited to offline risk computation and manual response. In contrast, we use risk computation to drive changes in an operating system’s security configuration. This allows risk management to occur in real time and reduces the window of exposure to attack. We posit that it is possible to protect a system by reducing its functionality temporarily when it is under siege. Our goal is to minimize the tension between security and usability by trading them dynamically. Instead of statically configuring a system, we aim to monitor the risk level, using it to drive the tradeoff between security and utility. The advantage of this approach is that it provides users with the maximum possible functionality for any predefined level of risk tolerance.

Risk management can be framed as an exercise in managing the constraints on edge and vertex weights of a tripartite graph, with the partitions corresponding to the threats, vulnerabilities, and assets in the system. If a threat requires a specific permission and affects a particular asset, an edge is added between the threat and the permission that mediates access to the vulnerable resource. Another edge is added between the permission and the asset. The presence of a path from a threat, through a permission check, to an asset contributes an element of risk. Risk can be reduced by denying access to a resource that contains a vulnerability or activating data protection measures. We first show that algorithmic underpinnings of optimal risk management can be formulated as the Partial Vertex Cover (PVC) problem in bipartite graphs. We then experimentally compare several heuristics and a 1 + 2 2 + ∊ -approximation algorithm we designed for the problem.
ER  - 

TY  - CHAP
AU  - Shaul, Josh
AU  - Ingram, Aaron
T1  - Chapter 1 - Oracle Security: The Big Picture
A2  - Shaul, Josh  
A2  - Ingram, Aaron 
BT  - Practical Oracle Security
PB  - Syngress
CY  - Burlington
PY  - 2007///
SP  - 1
EP  - 31
SN  - 978-1-59749-198-3
DO  - http://dx.doi.org/10.1016/B978-159749198-3.50003-2
UR  - http://www.sciencedirect.com/science/article/pii/B9781597491983500032
ER  - 

TY  - CHAP
AU  - Shaul, Josh
AU  - Ingram, Aaron
T1  - Chapter 8 - Database Activity Monitoring
A2  - Shaul, Josh  
A2  - Ingram, Aaron 
BT  - Practical Oracle Security
PB  - Syngress
CY  - Burlington
PY  - 2007///
SP  - 201
EP  - 224
SN  - 978-1-59749-198-3
DO  - http://dx.doi.org/10.1016/B978-159749198-3.50010-X
UR  - http://www.sciencedirect.com/science/article/pii/B978159749198350010X
ER  - 

TY  - JOUR
T1  - Quality standards for DNA sequence variation databases to improve clinical management under development in Australia
JO  - Applied & Translational Genomics
VL  - 3
IS  - 3
SP  - 54
EP  - 57
PY  - 2014/9/1/
T2  - Inaugural Issue
AU  - Bennetts, B.
AU  - Caramins, M.
AU  - Hsu, A.
AU  - Lau, C.
AU  - Mead, S.
AU  - Meldrum, C.
AU  - Smith, T.D.
AU  - Suthers, G.
AU  - Taylor, G.R.
AU  - Cotton, R.G.H.
AU  - Tyrrell, V.
SN  - 2212-0661
DO  - http://dx.doi.org/10.1016/j.atg.2014.07.002
UR  - http://www.sciencedirect.com/science/article/pii/S2212066114000209
KW  - Genetic variation databases
KW  - Data quality
KW  - Standards
KW  - Global knowledge sharing
AB  - Abstract
Despite the routine nature of comparing sequence variations identified during clinical testing to database records, few databases meet quality requirements for clinical diagnostics. To address this issue, The Royal College of Pathologists of Australasia (RCPA) in collaboration with the Human Genetics Society of Australasia (HGSA), and the Human Variome Project (HVP) is developing standards for DNA sequence variation databases intended for use in the Australian clinical environment. The outputs of this project will be promoted to other health systems and accreditation bodies by the Human Variome Project to support the development of similar frameworks in other jurisdictions.
ER  - 

TY  - JOUR
T1  - Towards an Enhanced Knowledge-based Decision Support System (DSS) for Integrated Water Resource Management (IWRM)
JO  - Procedia Engineering
VL  - 89
IS  - 0
SP  - 1097
EP  - 1104
PY  - 2014///
T2  - 16th Water Distribution System Analysis Conference, WDSA2014 Urban Water Hydroinformatics and Strategic Planning
AU  - Anzaldi, G.
AU  - Rubion, E.
AU  - Corchero, A.
AU  - Sanfeliu, R.
AU  - Domingo, X.
AU  - Pijuan, J.
AU  - Tersa, F.
SN  - 1877-7058
DO  - http://dx.doi.org/10.1016/j.proeng.2014.11.230
UR  - http://www.sciencedirect.com/science/article/pii/S1877705814023455
KW  - Knowledge Base
KW  - Decision Support System
KW  - Rule-Based Reasoning
KW  - Case-Based reasoning
KW  - Water Management Ontology
KW  - Intelligent System.
AB  - Abstract
Currently, management tools are designed to solve specific water issues, they cannot interoperate and are losing decisional dependencies along the water supply distribution chain. This article reports a DSS, which uses decisional knowledge to improve daily operations. A specially designed Water Management Ontology (WMO) is used to encompass the water cycle from water management perspective and combined with two inference engines which provide recommendations to the water manager. This knowledge characterization permits to semantically represent human and natural paths interactions in order to discover hidden knowledge, hence founding new IWRM strategies to improve resource management and energy efficiency.
ER  - 

TY  - JOUR
T1  - Measures of retaining digital evidence to prosecute computer-based cyber-crimes
JO  - Computer Standards & Interfaces
VL  - 29
IS  - 2
SP  - 216
EP  - 223
PY  - 2007/2//
T2  - 
AU  - Wang, Shiuh-Jeng
SN  - 0920-5489
DO  - http://dx.doi.org/10.1016/j.csi.2006.03.008
UR  - http://www.sciencedirect.com/science/article/pii/S0920548906000456
KW  - Digital evidence
KW  - Investigation
KW  - Computer forensics
KW  - Cyber-crime
AB  - With the rapid growth of computer and network systems in recent years, there has also been a corresponding increase in cyber-crime. Cyber-crime takes many forms and has garnered much attention in the media, making information security a more urgent and important priority. In order to fight cyber-crime, criminal evidence must be gathered from these computer-based systems. This is quite different from the collection of conventional criminal evidence and can confuse investigators attempting to deal with the forensics of cyber-crime, highlighting the importance of computer forensics. In this paper, we offer solutions to guard against cyber-crime through the implementation of software toolkits for computer-based systems. In this way, those who engage in criminal acts in cyber-space can be more easily apprehended.
ER  - 

TY  - JOUR
T1  - Security in grid computing: A review and synthesis
JO  - Decision Support Systems
VL  - 44
IS  - 4
SP  - 749
EP  - 764
PY  - 2008/3//
T2  - 
AU  - Cody, Erin
AU  - Sharman, Raj
AU  - Rao, Raghav H.
AU  - Upadhyaya, Shambhu
SN  - 0167-9236
DO  - http://dx.doi.org/10.1016/j.dss.2007.09.007
UR  - http://www.sciencedirect.com/science/article/pii/S0167923607001728
KW  - Grid computing
KW  - Information assurance
KW  - Survey and synthesis
KW  - Security
AB  - This paper provides an extensive survey of the different methods of addressing security issues in the grid computing environment, and specifically contributes to the research environment by developing a comprehensive framework for classification of these research endeavors. The framework presented classifies security literature into System Solutions, Behavioral Solutions, Hybrid Solutions and Related Technologies. Each one of these categories is explained in detail in the paper to provide insight as to their unique methods of accomplishing grid security, the types of grid and security situations they apply best to, and the pros and cons for each type of solution. Further, several areas of research were identified in the course of the literature survey where more study is warranted. These avenues for future research are also discussed in this paper. Several types of grid systems exist currently, and the security needs and solutions to address those needs for each type vary as much as the types of systems themselves. This research framework will aid in future research efforts to define, analyze, and address grid security problems for the many varied types of grid setups, as well as the many security situations that each grid may face.
ER  - 

TY  - JOUR
T1  - Declarative secure distributed information systems
JO  - Computer Languages, Systems & Structures
VL  - 39
IS  - 1
SP  - 1
EP  - 24
PY  - 2013/4//
T2  - 
AU  - Zhou, Wenchao
AU  - Tao, Tao
AU  - Loo, Boon Thau
AU  - Mao, Yun
SN  - 1477-8424
DO  - http://dx.doi.org/10.1016/j.cl.2012.09.002
UR  - http://www.sciencedirect.com/science/article/pii/S1477842412000358
KW  - Declarative networking
KW  - Secure query processing
KW  - Secure distributed information systems
KW  - Distributed trust management
AB  - We present a unified declarative platform for specifying, implementing, and analyzing secure networked information systems. Our work builds upon techniques from logic-based trust management systems and declarative networking. We make the following contributions. First, we propose the Secure Network Datalog (SeNDlog) language that unifies Binder, a logic-based language for access control in distributed systems, and Network Datalog, a distributed recursive query language for declarative networks. SeNDlog enables network routing, information systems, and their security policies to be specified and implemented within a common declarative framework. Second, we extend existing distributed recursive query processing techniques to execute SeNDlogprograms that incorporate secure communication via authentication and encryption among untrusted nodes. Third, we demonstrate the use of user-defined cryptographic functions for customizing the authentication and encryption mechanisms used for securing protocols. Finally, using a local cluster and the PlanetLab testbed, we perform a detailed performance study of a variety of secure networked systems implemented using our platform.
ER  - 

TY  - JOUR
T1  - A survey on Advanced Metering Infrastructure
JO  - International Journal of Electrical Power & Energy Systems
VL  - 63
IS  - 0
SP  - 473
EP  - 484
PY  - 2014/12//
T2  - 
AU  - Rashed Mohassel, Ramyar
AU  - Fung, Alan
AU  - Mohammadi, Farah
AU  - Raahemifar, Kaamran
SN  - 0142-0615
DO  - http://dx.doi.org/10.1016/j.ijepes.2014.06.025
UR  - http://www.sciencedirect.com/science/article/pii/S0142061514003743
KW  - Advanced Metering Infrastructure
KW  - Smart metering
KW  - Smart Grid
AB  - Abstract
This survey paper is an excerpt of a more comprehensive study on Smart Grid (SG) and the role of Advanced Metering Infrastructure (AMI) in SG. The survey was carried out as part of a feasibility study for creation of a Net-Zero community in a city in Ontario, Canada. SG is not a single technology; rather it is a combination of different areas of engineering, communication and management. This paper introduces AMI technology and its current status, as the foundation of SG, which is responsible for collecting all the data and information from loads and consumers. AMI is also responsible for implementing control signals and commands to perform necessary control actions as well as Demand Side Management (DSM). In this paper we introduce SG and its features, establish the relation between SG and AMI, explain the three main subsystems of AMI and discuss related security issues.
ER  - 

TY  - JOUR
T1  - On the long-term retention of geometry-centric digital engineering artifacts
JO  - Computer-Aided Design
VL  - 43
IS  - 7
SP  - 820
EP  - 837
PY  - 2011/7//
T2  - The 2009 SIAM/ACM Joint Conference on Geometric and Physical Modeling
AU  - Regli, William C.
AU  - Kopena, Joseph B.
AU  - Grauer, Michael
SN  - 0010-4485
DO  - http://dx.doi.org/10.1016/j.cad.2010.11.012
UR  - http://www.sciencedirect.com/science/article/pii/S0010448510002228
KW  - Digital preservation
KW  - Long-term knowledge retention
KW  - Representation
KW  - Standards
KW  - Knowledge capture
AB  - This paper discusses the challenges of long-term preservation of digital geometric models and the engineering processes associated with them. For engineering, design, manufacturing, and physics-based simulation data this requires formats that are accessible potentially indefinitely into the future. One of the fundamental challenges is the development of digital geometry-centric engineering representations that are self describing and assured to be interpretable over the long lifespans required by archival applications. Additionally, future users may have needs that require other information, going beyond geometry, be also accessible to fully interpret the model. These problems are highly interdisciplinary and not exclusively algorithmic or technical. To provide context, the paper introduces a case study illustrating an overall portrait of the problem. Based on observations from this case study, we present a framework for enhancing the preservation of geometry-centric engineering knowledge. This framework is currently being used on a number of projects in engineering education.
ER  - 

TY  - CHAP
AU  - Faircloth, Jeremy
T1  - Chapter 5 - Information Security
A2  - Faircloth, Jeremy 
BT  - Enterprise Applications Administration
PB  - Morgan Kaufmann
CY  - Boston
PY  - 2014///
SP  - 175
EP  - 220
SN  - 978-0-12-407773-7
DO  - http://dx.doi.org/10.1016/B978-0-12-407773-7.00005-3
UR  - http://www.sciencedirect.com/science/article/pii/B9780124077737000053
KW  - Security
KW  - confidentiality
KW  - integrity
KW  - availability
KW  - access control
KW  - authentication
KW  - authorization
KW  - audit
KW  - controls
KW  - SSO
KW  - SAML
KW  - Kerberos
KW  - RBAC
KW  - SQL injection
KW  - XSS
KW  - brute force
KW  - overflows
KW  - DoS
AB  - Chapter 5 focuses its intent on getting you thinking about various information security topics and how they apply to enterprise applications. In this chapter, we discuss some of the basic concepts and principles associated with information security including confidentiality, integrity, and availability. Authentication, authorization, audit, and administration of access control are also discussed in detail. These four topics tie together to form the access control mechanisms used by most enterprise applications today. Finally, we talk about defense in depth and applying the appropriate level of security controls at each layer of the enterprise application. The best approach is to always secure an enterprise application in the most holistic manner possible through the implementation of appropriate controls at all levels, and this is a common thread throughout the chapter.
ER  - 

TY  - JOUR
T1  - Privacy and consent in pervasive networks
JO  - Information Security Technical Report
VL  - 14
IS  - 3
SP  - 138
EP  - 142
PY  - 2009/8//
T2  - The Changing Shape of Privacy and Consent
AU  - Malik, Nazir A.
AU  - Tomlinson, Allan
SN  - 1363-4127
DO  - http://dx.doi.org/10.1016/j.istr.2009.10.002
UR  - http://www.sciencedirect.com/science/article/pii/S1363412709000375
AB  - Pervasive networks and location based systems have the potential to provide many new services. However the user of these services often has to provide personal information to allow the service to operate effectively. This article considers the problem of protecting personal information in this environment, and reports on the legislative and technical efforts being made to protect user privacy.
ER  - 

TY  - CHAP
AU  - Rocha, Roberto A.
AU  - Maviglia, Saverio M.
AU  - Sordo, Margarita
AU  - Rocha, Beatriz H.
T1  - Chapter 28 - A Clinical Knowledge Management Program
A2  - Greenes, Robert A. 
BT  - Clinical Decision Support (Second Edition)
PB  - Academic Press
CY  - Oxford
PY  - 2014///
SP  - 773
EP  - 817
SN  - 978-0-12-398476-0
DO  - http://dx.doi.org/10.1016/B978-0-12-398476-0.00028-2
UR  - http://www.sciencedirect.com/science/article/pii/B9780123984760000282
KW  - Knowledge Management
KW  - Knowledge Engineering
KW  - Knowledge Curation
KW  - Knowledge lifecycle
KW  - Clinical Decision Support
KW  - Personalized Decision Support
KW  - Clinical Decision Support Standards
KW  - Software Infrastructure
KW  - Collaborative Activities
AB  - This chapter presents a complete program for clinical knowledge management from an institutional perspective, including motivation, requirements, and implementation strategies. The knowledge engineering process and the knowledge asset lifecycle are also discussed, taking into account extensible modeling approaches that ensure consistent knowledge representation and effective collaboration. Special emphasis is given to requirements for a comprehensive software infrastructure to support the knowledge management program and enable effective integration with clinical information systems. New opportunities and challenges related to personalized clinical decision support interventions and advanced curation tools are also outlined, taking into account inter-institutional collaborations leading to a sustainable exchange of knowledge assets. The chapter concludes with a brief overview of the ongoing program implementation efforts at Partners HealthCare.
ER  - 

TY  - CHAP

T1  - Index
A2  - Bourne, Kelly C. 
BT  - Application Administrators Handbook
PB  - Morgan Kaufmann
CY  - Boston
PY  - 2014///
SP  - 591
EP  - 606
SN  - 978-0-12-398545-3
DO  - http://dx.doi.org/10.1016/B978-0-12-398545-3.09990-8
UR  - http://www.sciencedirect.com/science/article/pii/B9780123985453099908
ER  - 

TY  - JOUR
T1  - Security of Electronic Medical Information and Patient Privacy: What You Need to Know
JO  - Journal of the American College of Radiology
VL  - 11
IS  - 12, Part B
SP  - 1212
EP  - 1216
PY  - 2014/12//
T2  - Special Bonus Issue for 2014: ACR Imaging IT Reference Guide
AU  - Andriole, Katherine P.
SN  - 1546-1440
DO  - http://dx.doi.org/10.1016/j.jacr.2014.09.011
UR  - http://www.sciencedirect.com/science/article/pii/S1546144014005468
KW  - Data security
KW  - patient privacy
KW  - HIPAA
KW  - PHI
AB  - The responsibility that physicians have to protect their patients from harm extends to protecting the privacy and confidentiality of patient health information including that contained within radiological images. The intent of HIPAA and subsequent HIPAA Privacy and Security Rules is to keep patients’ private information confidential while allowing providers access to and maintaining the integrity of relevant information needed to provide care. Failure to comply with electronic protected health information (ePHI) regulations could result in financial or criminal penalties or both. Protected health information refers to anything that can reasonably be used to identify a patient (eg, name, age, date of birth, social security number, radiology examination accession number). The basic tools and techniques used to maintain medical information security and patient privacy described in this article include physical safeguards such as computer device isolation and data backup, technical safeguards such as firewalls and secure transmission modes, and administrative safeguards including documentation of security policies, training of staff, and audit tracking through system logs. Other important concepts related to privacy and security are explained, including user authentication, authorization, availability, confidentiality, data integrity, and nonrepudiation. Patient privacy and security of medical information are critical elements in today’s electronic health care environment. Radiology has led the way in adopting digital systems to make possible the availability of medical information anywhere anytime, and in identifying and working to eliminate any risks to patients.
ER  - 

TY  - CHAP
AU  - Fernandes, Diogo A.B.
AU  - Soares, Liliana F.B.
AU  - Gomes, João V.
AU  - Freire, Mário M.
AU  - Inácio, Pedro R.M.
T1  - Chapter 25 - A Quick Perspective on the Current State in Cybersecurity
A2  - Akhgar, Babak  
A2  - Arabnia, Hamid R. 
BT  - Emerging Trends in ICT Security
PB  - Morgan Kaufmann
CY  - Boston
PY  - 2014///
SP  - 423
EP  - 442
SN  - 978-0-12-411474-6
DO  - http://dx.doi.org/10.1016/B978-0-12-411474-6.00025-6
UR  - http://www.sciencedirect.com/science/article/pii/B9780124114746000256
KW  - cybersecurity
KW  - cyber-crime cyber-warfare
KW  - threats
KW  - trends
AB  - Nowadays, cybersecurity makes headlines across the media and in companies, blogs, social networks, among other places. The Internet is a wild cyberspace, an arena for commercialization, consumerism, business, and leisure, to name a few activities. Networks, populations, and nations around the world, now interconnected through the Internet, rely on it for their daily lives. But some Internet users have learned to take advantage of vulnerable systems and of Internet technologies for their own good, sending out spam, phishing, data breaches, botnets, and other threats. An underground criminal network has emerged, creating complex malware kits for several purposes. “Hacktivism” has become a popular term with many supporters worldwide, but cyberwarfare is now on the rise, gaining more and more attention from nation-states. This chapter provides a quick overview of these topics, discussing them in a timely manner, referencing key events from the past while focusing on the present day.
ER  - 

TY  - CHAP
AU  - White, John M.
T1  - Chapter 11 - Security Technology Assessment
A2  - White, John M. 
BT  - Security Risk Assessment
PB  - Butterworth-Heinemann
CY  - Boston
PY  - 2014///
SP  - 135
EP  - 147
SN  - 978-0-12-800221-6
DO  - http://dx.doi.org/10.1016/B978-0-12-800221-6.00011-4
UR  - http://www.sciencedirect.com/science/article/pii/B9780128002216000114
KW  - Mass notification
KW  - Metal detectors
KW  - Preventative maintenance
KW  - Security alarms
KW  - Security cameras
KW  - Security technology
KW  - Specialize security equipment
KW  - Weapons
AB  - Abstract
Do you know how many security cameras you need? Do you need cameras at all? When a security vendor gives you a quote for security technology, do you know for sure that you need such equipment or services? Knowing the answers to these questions is critical because you want to ensure that you have the proper technology in use, yet not be taken advantage of. When you conduct a security risk assessment, you need to know what to watch out for, how to measure the effectiveness of the technology, and how to determine if your organization needs to add to or upgrade your equipment. This chapter will explain the processes and how to determine the appropriate balance of technology and contracted security services.
ER  - 

TY  - JOUR
T1  - ACC/AHA/SCAI 2014 Health Policy Statement on Structured Reporting for the Cardiac Catheterization Laboratory: A Report of the American College of Cardiology Clinical Quality Committee
JO  - Journal of the American College of Cardiology
VL  - 63
IS  - 23
SP  - 2591
EP  - 2623
PY  - 2014/6/17/
T2  - 
AU  - Sanborn, Timothy A.
AU  - Tcheng, James E.
AU  - Anderson, H. Vernon
AU  - Chambers, Charles E.
AU  - Cheatham, Sharon L.
AU  - DeCaro, Matthew V.
AU  - Durack, Jeremy C.
AU  - Everett, Allen D.
AU  - Gordon, John B.
AU  - Hammond, William E.
AU  - Hijazi, Ziyad M.
AU  - Kashyap, Vikram S.
AU  - Knudtson, Merrill
AU  - Landzberg, Michael J.
AU  - Martinez-Rios, Marco A.
AU  - Riggs, Lisa A.
AU  - Sim, Kui Hian
AU  - Slotwiner, David J.
AU  - Solomon, Harry
AU  - Szeto, Wilson Y.
AU  - Weiner, Bonnie H.
AU  - Weintraub, William S.
AU  - Windle, John R.
SN  - 0735-1097
DO  - http://dx.doi.org/10.1016/j.jacc.2014.03.020
UR  - http://www.sciencedirect.com/science/article/pii/S073510971401729X
KW  - ACC Health Policy Statements
KW  - best practice model
KW  - cardiac catheterization
KW  - cardiac catheterization laboratory workflow
KW  - clinical data interchange
KW  - clinical document architecture
KW  - electronic health records
KW  - percutaneous coronary interventions
KW  - structured reporting
ER  - 

TY  - JOUR
T1  - A typology of different development and testing options for symbolic regression modelling of measured and calculated datasets
JO  - Environmental Modelling & Software
VL  - 47
IS  - 0
SP  - 29
EP  - 41
PY  - 2013/9//
T2  - 
AU  - Beriro, Darren J.
AU  - Abrahart, Robert J.
AU  - Nathanail, C. Paul
AU  - Moreno, Jimmy
AU  - Bawazir, A. Salim
SN  - 1364-8152
DO  - http://dx.doi.org/10.1016/j.envsoft.2013.03.020
UR  - http://www.sciencedirect.com/science/article/pii/S1364815213000911
KW  - Simulation
KW  - Emulation
KW  - Pan evaporation
KW  - Gene expression programming
KW  - Data driven modelling
KW  - Emulation simulation typology
KW  - Symbolic regression
AB  - Abstract
Data-driven modelling is used to develop two alternative types of predictive environmental model: a simulator, a model of a real-world process developed from either a conceptual understanding of physical relations and/or using measured records, and an emulator, an imitator of some other model developed on predicted outputs calculated by that source model. A simple four-way typology called Emulation Simulation Typology (EST) is proposed that distinguishes between (i) model type and (ii) different uses of model development period and model test period datasets. To address the question of to what extent simulator and emulator solutions might be considered interchangeable i.e. provide similar levels of output accuracy when tested on data different from that used in their development, a pair of counterpart pan evaporation models was created using symbolic regression. Each model type delivered similar levels of predictive skill to that other of published solutions. Input–output sensitivity analysis of the two different model types likewise confirmed two very similar underlying response functions. This study demonstrates that the type and quality of data on which a model is tested, has a greater influence on model accuracy assessment, than the type and quality of data on which a model is developed, providing that the development record is sufficiently representative of the conceptual underpinnings of the system being examined. Thus, previously reported substantial disparities occurring in goodness-of-fit statistics for pan evaporation models are most likely explained by the use of either measured or calculated data to test particular models, where lower scores do not necessarily represent major deficiencies in the solution itself.
ER  - 

TY  - JOUR
T1  - Authentication of lossy data in body-sensor networks for cloud-based healthcare monitoring
JO  - Future Generation Computer Systems
VL  - 35
IS  - 0
SP  - 80
EP  - 90
PY  - 2014/6//
T2  - Special Section: Integration of Cloud Computing and Body Sensor Networks; Guest Editors: Giancarlo Fortino and Mukaddim Pathan
AU  - Ali, Syed Taha
AU  - Sivaraman, Vijay
AU  - Ostry, Diethelm
SN  - 0167-739X
DO  - http://dx.doi.org/10.1016/j.future.2013.09.007
UR  - http://www.sciencedirect.com/science/article/pii/S0167739X13001866
KW  - Body sensor networks
KW  - Data authentication
KW  - Hash trees
KW  - Network coding
AB  - Abstract
Growing pressure on healthcare costs is spurring development of lightweight bodyworn sensors for real-time and continuous physiological monitoring. Data from these sensors is streamed wirelessly to a handheld device such as a smartphone or tablet, and then archived in the cloud by personal health record services. Authenticating the data these devices generate is vital to ensure proper diagnosis, traceability, and validation of claims. Digital signatures at the packet-level are too resource-intensive for bodyworn devices, while block-level signatures are not robust to loss. In this paper we propose, analyse, and validate a practical, lightweight robust authentication scheme suitable for health-monitoring. We make three specific contributions: (a) we develop an authentication scheme that is both low-cost (using a Merkle hash tree to amortise digital signature costs), and loss-resilient (using network coding to recover strategic nodes within the tree). (b) We design a framework for optimizing placement of network coding within the tree to maximise data verifiability for a given overhead and loss environment. (c) We validate our scheme using experimental traces of typical operating conditions to show that it achieves high success (over 99% of the medical data can be authenticated) at very low overheads (as low as 5% extra transmissions) and at very low cost (the bodyworn device has to perform a digital signature operation no more than once per hour). We believe our novel authentication scheme can be a key step in the integration of wearable medical monitoring devices into current cloud-based healthcare systems.
ER  - 

TY  - JOUR
T1  - The Next Generation of Grand Challenges for Systems Engineering Research
JO  - Procedia Computer Science
VL  - 16
IS  - 0
SP  - 834
EP  - 843
PY  - 2013///
T2  - 2013 Conference on Systems Engineering Research
AU  - Kalawsky, Roy S.
SN  - 1877-0509
DO  - http://dx.doi.org/10.1016/j.procs.2013.01.087
UR  - http://www.sciencedirect.com/science/article/pii/S1877050913000884
KW  - Systems engineering
KW  - Research
KW  - Grand challenges
AB  - Over the past century significant achievements in technological development have transformed human life across a diverse number of sectors (for example Aerospace &amp; Defence, Automotive, Construction, Energy, Transportation, Consumer Electronics, IT, Pharmaceutical &amp; Healthcare and Telecommunications). More recently, the associated systems have grown in terms of their complexity and inter-connectedness. The sheer rate of change and interdependence of these systems is now putting at risk our ability to fully understand and predict their behavior. Exploiting systems engineering and the associated tools/techniques will be crucial if we are to manage future complex systems. Systems engineering is used here, to indicate the broad area of complexity science, systems science and systems engineering, and has deliberately steered clear of a precise definition. This paper builds on an earlier proposition for establishing a set of indicative grand challenges for systems engineering research with the objective of inspiring a research agenda for the systems engineering academic/industry community. In this respect a grand challenge is recognised as being one or two decades in advance; their achievement being regarded as a major milestone or breakthrough in the advancement of knowledge or technology.
ER  - 

TY  - CHAP
AU  - Hale, Alan N.
T1  - 6 - Examples of Automated Genetic Analysis Developments
A2  - Alister G. Craig and Jörg D. Hoheisel
BT  - Methods in Microbiology
PB  - Academic Press
PY  - 1999///
VL  - Volume 28
SP  - 131
EP  - 153
T2  - Automation Genomic and Functional Analyses
SN  - 0580-9517
DO  - http://dx.doi.org/10.1016/S0580-9517(08)70203-4
UR  - http://www.sciencedirect.com/science/article/pii/S0580951708702034
ER  - 

TY  - CHAP
AU  - Nicoletti, Pete
T1  - Chapter e66 - Content Filtering
A2  - Vacca, John R. 
BT  - Computer and Information Security Handbook (Second Edition)
PB  - Morgan Kaufmann
CY  - Boston
PY  - 2013///
SP  - e101
EP  - e122
SN  - 978-0-12-394397-2
DO  - http://dx.doi.org/10.1016/B978-0-12-394397-2.00093-3
UR  - http://www.sciencedirect.com/science/article/pii/B9780123943972000933
KW  - content filtering
KW  - web surfacing
KW  - commercial business
KW  - financial organizations
KW  - healthcare organizations
KW  - internet service partners
KW  - threat
KW  - internet filters
KW  - content-control software
KW  - parents
AB  - Content filtering is a powerful tool that, properly deployed, can offer parents, companies, and local, state, and federal governments protection by classifying Internet-based content. It is disparaged as Orwellian and simultaneously embraced as a positive ROI project, depending on who you are and how it affects your online behavior. In this chapter we examine the many benefits and justifications of Web-based content filtering, such as legal liability, risk reduction, productivity gains, and bandwidth usage. We’ll explore the downside and unintended consequences and risks that improperly deployed or misconfigured systems create. We’ll also look into methods to subvert and bypass these systems and the reasons behind them. It is important for people who are considering content filtering to be aware of all the legal implications, and we’ll also review these. Content filtering is straightforward to deploy, and license costs are so reasonable they can offer extremely fast return on investment while providing a very effective risk reduction strategy. We’ll make sure that your project turns out successfully, since we’ll look at all the angles: Executives will be happy with the project results and employees won’t key your car in the parking lot!
ER  - 

TY  - CHAP

T1  - Appendix D - An Example Roles and Responsibilities RACI Matrix
A2  - Kane, Greg  
A2  - Koppel, Lorna 
BT  - Information Security
PB  - Elsevier
CY  - Boston
PY  - 2013///
SP  - 49
EP  - 57
SN  - 978-0-12-417232-6
DO  - http://dx.doi.org/10.1016/B978-0-12-417232-6.00019-9
UR  - http://www.sciencedirect.com/science/article/pii/B9780124172326000199
ER  - 

TY  - CHAP
AU  - Rittinghouse, John W.
AU  - Hancock, William M.
T1  - 2 - Network Security Management Basics
A2  - Rittinghouse, John W.  
A2  - Hancock, William M. 
BT  - Cybersecurity Operations Handbook
PB  - Digital Press
CY  - Burlington
PY  - 2004///
SP  - 25
EP  - 61
SN  - 978-1-55558-306-4
DO  - http://dx.doi.org/10.1016/B978-155558306-4/50007-X
UR  - http://www.sciencedirect.com/science/article/pii/B978155558306450007X
AB  - Publisher Summary
This chapter focuses on the fundamental issues surrounding security management in an organization. An information infrastructure comprises communications networks, computers, databases, management, applications, and consumer electronics and can exist at the global, national, or local levels. Password management and privacy topics are covered in depth. The defense-in-depth strategy concentrates mainly on the technology aspect. By layering defenses in these three areas, the strategy asserts that a successful attack against one of these three aspects does not result in the compromise of the entire information infrastructure. The common criteria model is used as a standardized framework for the evaluation of the security elements of IT products and systems. The chapter also discusses how information warfare may play an ever-growing role in the cyberworld.
ER  - 

TY  - JOUR
T1  - Design and evaluation of a learning environment to effectively provide network security skills
JO  - Computers & Education
VL  - 69
IS  - 0
SP  - 225
EP  - 236
PY  - 2013/11//
T2  - 
AU  - Marsa-Maestre, Ivan
AU  - de la Hoz, Enrique
AU  - Gimenez-Guzman, Jose Manuel
AU  - Lopez-Carmona, Miguel A.
SN  - 0360-1315
DO  - http://dx.doi.org/10.1016/j.compedu.2013.07.022
UR  - http://www.sciencedirect.com/science/article/pii/S0360131513001899
KW  - Architectures for educational technology systems
KW  - Distributed learning environments
KW  - Interactive learning environments
KW  - Simulations
KW  - Teaching/learning strategies
AB  - Abstract
Information system security and network security are topics of increasing importance in the information society. They are also topics where the adequate education of professionals requires the use of specific laboratory environments where the practical aspects of the discipline may be addressed. However, most approaches currently used are excessively static and lack the flexibility that the education requirements of security professionals demand. In this paper we present NEMESIS, a scenario generation framework for education on system and network security, which is based on virtualization technologies and has been designed to be open, distributed, modular, scalable and flexible. Finally, an example scenario is described and some results validating the benefits of its use in undergraduate computer security courses are shown.
ER  - 

TY  - CHAP
AU  - Broad, James
T1  - Chapter 14 - RMF Phase 6: Monitoring Security Controls
A2  - Broad, James 
BT  - Risk Management Framework
PB  - Syngress
CY  - Boston
PY  - 2013///
SP  - 157
EP  - 167
SN  - 978-1-59749-995-8
DO  - http://dx.doi.org/10.1016/B978-1-59749-995-8.00014-4
UR  - http://www.sciencedirect.com/science/article/pii/B9781597499958000144
KW  - operations and maintenance
KW  - O&amp;M
KW  - security changes
KW  - ongoing monitoring
KW  - continuous monitoring
KW  - system disposal
KW  - change management
AB  - Abstract
This chapter introduces the sixth phase of the RMF, which is when a system enters operation, is continually monitored, and is eventually disposed of.
ER  - 

TY  - JOUR
T1  - Flexible composition and execution of large scale applications on distributed e-infrastructures
JO  - Journal of Computational Science
VL  - 5
IS  - 1
SP  - 51
EP  - 62
PY  - 2014/1//
T2  - 
AU  - Zasada, Stefan J.
AU  - Chang, David C.W.
AU  - Haidar, Ali N.
AU  - Coveney, Peter V.
SN  - 1877-7503
DO  - http://dx.doi.org/10.1016/j.jocs.2013.10.009
UR  - http://www.sciencedirect.com/science/article/pii/S1877750313001269
KW  - E-infrastructure
KW  - High performance computing
KW  - Application virtualization
KW  - Usability
AB  - Abstract
Computer simulation is finding a role in an increasing number of scientific disciplines, concomitant with the rise in available computing power. Marshalling this power facilitates new, more effective and different research than has been hitherto possible. Realizing this inevitably requires access to computational power beyond the desktop, making use of clusters, supercomputers, data repositories, networks and distributed aggregations of these resources. The use of diverse e-infrastructure brings with it the ability to perform distributed multiscale simulations. Accessing one such resource entails a number of usability and security problems; when multiple geographically distributed resources are involved, the difficulty is compounded. In this paper we present a solution, the Application Hosting Environment,33
AHE is available to download under the LGPL license from: https://sourceforge.net/projects/ahe3/.
 which provides a Software as a Service layer on top of distributed e-infrastructure resources. We describe the performance and usability enhancements present in AHE version 3, and show how these have led to a high performance, easy to use gateway for computational scientists working in diverse application domains, from computational physics and chemistry, materials science to biology and biomedicine.
ER  - 

TY  - JOUR
T1  - Identification and application of security measures for petrochemical industrial control systems
JO  - Journal of Loss Prevention in the Process Industries
VL  - 26
IS  - 6
SP  - 982
EP  - 993
PY  - 2013/11//
T2  - 
AU  - Leith, H.M.
AU  - Piper, John W.
SN  - 0950-4230
DO  - http://dx.doi.org/10.1016/j.jlp.2013.10.009
UR  - http://www.sciencedirect.com/science/article/pii/S0950423013002015
KW  - Industrial control systems
KW  - Security controls
KW  - Cyber risk assessment
KW  - Security threats
KW  - Configuration management
AB  - Abstract
The financial success of the chemical and petrochemical industry will increasingly depend upon the security of process control systems. This paper presents recommendations and insights gleaned from over 100 security risk assessment (SRA) and process control analyses, using requirements baselines extracted from the National Institute of Standards and Technology (NIST) special publication 800-53 (and Appendix A), the Recommended Security Controls for Federal Information Systems and Organizations, in conjunction with NIST special publication 800-82, Guide to Industrial Control Systems(ICS) Security, to provide the bridge in application of 800-53 controls to IC/SCADA.

The paper identifies how current and projected malevolent threats posed by insiders, outsiders, collusion, and system-induced threats can erode system performance in terms of shut downs, sabotage, production disruption, and contamination. The issue is not whether there are clear and present cyber threats, nor whether there are business prudent practices that can be implemented to counter those threats; but rather that there is such a diverse compendium, at times conflicting and often technically obtuse guidance, that clarity is needed to narrow the focus of this guidance to assist those responsible for implementing effective process control security.

The paper focuses on application of business-prudent controls and discusses how disparities in implementation of controls can exacerbate system vulnerabilities. Topics include issues of processes control system management, systems documentation, use of contractors and remote contractor access, system authorities that exceed user needs, misalignment of staff perception of information asset values, exposures related to use of USB ports, lack of encryption, and background surety gaps for individuals and contractor companies with access to process control systems.

The paper examines the dynamics of communicating information from process control systems to business IT systems and the pressure from business operations to capture process data and make it available in near real-time through administrative networks. Such pressures may influence systems administrators to overlook or ignore firewall and systems engineering architecture, increasing potentials for two-way interface between business and process control that significantly increases exploit exposures. Despite the availability of excellent guidelines for physical and technical security of IT related assets, these practices are too often unheeded in favor of expediency or expanded access. The paper includes a discussion of Risk Management Framework models that should be considered to enhance the correspondences and relationships between multiple organizational domains, thereby promoting more effective cyber security for current and future process control systems.

The paper summarizes the process for establishing security for industrial control systems (ICS), and addresses cyber security baseline requirements and expectations, within a risk management framework that provides a decision basis, threat dynamics, common vulnerabilities, and prudent mitigation measures. Much of this summary has been derived from The Information Technology Laboratory at the National Institute of Standards and Technology (NIST) Special Publication (SP) 800-53, Recommended Security Controls for Federal Information Systems and NIST SP 800-82, Guide to Industrial Control Systems (ICS) Security. NIST has also published Applying NIST SP 800-53 to Industrial Control Systems which demonstrates the relationship of 800-53 to ICS security and the application of more than 20 control families and over 625 control elements to ICS security. Although originally designed for Federal systems, portions of these publications also provide a solid foundation for critical commercial and industrial information control systems in terms of addressing the basic questions that companies in the process industry should consider when selecting security controls, including:

•
What controls are actually needed to protect process systems, while supporting operations and safeguarding critical assets?
•
Can the selected controls suggested for Federal systems effectively be implemented for systems in the process industry?
•
Once selected and implemented, will these controls really be effective in protecting the processes?


NIST SP 800-53, Recommended Security Controls for Federal Information Systems, helps answer questions to strengthen commercial processes information security programs. The security controls articulated in NIST SP 800-53 provide guidance and recommend practices applicable to security systems in process industries, to provide a foundation for understanding the fundamental concepts of security controls. The introductory material presents the concept of security controls and their use within well-defined information security programs. Some of the issues discussed include the structural components of controls, how the controls are organized into families, and the use of controls to support information security programs. The guide outlines the essential steps that should be followed to determine needed controls, to assure the effectiveness of controls, and to maintain the effectiveness of installed controls.

The appendices in NIST SP 800-53 provide additional resources including general references, definitions, explanation of acronyms, a breakdown of security controls for graduated levels of security requirements, a catalog of security controls, and information relating security controls to other standards and control sets. The controls are organized into classes of operational, management, and technical controls, and then into families within each class. To maintain parity and applicability with advances in technology, NIST also plans to review and to update the controls in the catalog as technology changes and new safeguards and new information security countermeasures are identified. NIST SP 800-53 and related documents are available at http://csrc.nist.gov/publications/nistpubs/index.html. The extensive reference list in SP 800-53 includes standards, guidelines, and recommendations that process industry companies can use as the foundation for comprehensive security planning and lifecycle management processes. Additionally, a significant effort of broad commercial and government cooperation, the Consensus Audit Guideline (CAG) provides a 20-element cyber security controls roster supporting a common commercial framework for cyber security, correlating to the NIST 800-53 Control Library.
ER  - 

TY  - CHAP
AU  - Mallery, John
T1  - Chapter 1 - Building a Secure Organization
A2  - Vacca, John R. 
BT  - Computer and Information Security Handbook (Second Edition)
PB  - Morgan Kaufmann
CY  - Boston
PY  - 2013///
SP  - 3
EP  - 24
SN  - 978-0-12-394397-2
DO  - http://dx.doi.org/10.1016/B978-0-12-394397-2.00001-5
UR  - http://www.sciencedirect.com/science/article/pii/B9780123943972000015
KW  - security
KW  - hacker
KW  - cybercrime
KW  - probable threats
KW  - describable threats
KW  - risks
KW  - threats
KW  - infrastructure model
KW  - denial-of-service (DoS) attacks
KW  - training
AB  - This chapter provides guidelines for building effective security assessment plans and a comprehensive set of procedures for assessing the effectiveness of security controls employed in information systems. Today’s information systems are complex assemblages of technology (hardware, software, and firmware), processes, and people, working together to provide organizations with the capability to process, store, and transmit information in a timely manner to support various missions and business functions. The degree to which organizations have come to depend on these information systems to conduct routine, important, and critical missions and business functions means that protection of the underlying systems is paramount to the success of the organization. The selection of appropriate security controls for an information system is an important task that can have major implications for the operations and assets of an organization, as well as, the welfare of individuals. Security controls are the management, operational, and technical safeguards or countermeasures prescribed for an information system to protect the confidentiality, integrity (including nonrepudiation and authenticity), and availability of the system and its information. Once employed within an information system, security controls are assessed to provide the information necessary to determine their overall effectiveness—that is, the extent to which the controls are implemented correctly, operating as intended, and producing the desired outcome with respect to meeting the security requirements for the system. Understanding the overall effectiveness of the security controls implemented in the information system and its environment of operation is essential in determining the risk to the organization’s operations and assets, to individuals, to other organizations, and to the nation resulting from use of the system.
ER  - 

TY  - JOUR
T1  - Standardisation of test requesting and reporting for the electronic health record
JO  - Clinica Chimica Acta
VL  - 432
IS  - 0
SP  - 148
EP  - 156
PY  - 2014/5/15/
T2  - Harmonization of Laboratory Testing - A global activity
AU  - Legg, Michael
SN  - 0009-8981
DO  - http://dx.doi.org/10.1016/j.cca.2013.12.007
UR  - http://www.sciencedirect.com/science/article/pii/S0009898113004919
KW  - Pathology
KW  - Interoperability
KW  - Request
KW  - Report
KW  - Standardisation
KW  - Electronic health record
AB  - Abstract
This paper is a review of the standardisation required to achieve interoperability for pathology test requesting and reporting. Interoperability is the ability of two parties, either human or machine, to exchange data or information in a manner that preserves shared meaning. This is needed to make healthcare safer, more efficient and more effective. Interoperability requires standardisation around: transmission of data; identification policies; information structures; common terminology; common understanding; and behavioural agreement. It is dependent on consensus. Each of these aspects is considered from the perspective of pathology requesting and reporting concluding that while much has been done, much remains to be done.
ER  - 

TY  - JOUR
T1  - Towards a unified taxonomy and architecture of cloud frameworks
JO  - Future Generation Computer Systems
VL  - 29
IS  - 5
SP  - 1196
EP  - 1210
PY  - 2013/7//
T2  - Special section: Hybrid Cloud Computing
AU  - Dukaric, Robert
AU  - Juric, Matjaz B.
SN  - 0167-739X
DO  - http://dx.doi.org/10.1016/j.future.2012.09.006
UR  - http://www.sciencedirect.com/science/article/pii/S0167739X12001793
KW  - Cloud Computing
KW  - Infrastructure as a service
KW  - Taxonomy
KW  - Architectural framework
AB  - Infrastructure as a Service (IaaS) is one of the most important layers of Cloud Computing. However, there is an evident deficiency of mechanisms for analysis, comparison and evaluation of IaaS cloud implementations, since no unified taxonomy or reference architecture is available. In this article, we propose a unified taxonomy and an IaaS architectural framework. The taxonomy is structured around seven layers: core service layer, support layer, value-added services, control layer, management layer, security layer and resource abstraction. We survey various IaaS systems and map them onto our taxonomy to evaluate the classification. We then introduce an IaaS architectural framework that relies on the unified taxonomy. We provide a detailed description of each layer and define dependencies between the layers and components. Finally, we evaluate the proposed IaaS architectural framework on several real-world projects, while performing a comprehensive analysis of the most important commercial and open-source IaaS products. The evaluation results show notable distinction of feature support and capabilities between commercial and open-source IaaS platforms, significant deficiency of important architectural components in terms of fulfilling true promise of infrastructure clouds, and real-world usability of the proposed taxonomy and architectural framework.
ER  - 

TY  - JOUR
T1  - A configurable cryptography subsystem in a middleware framework for embedded systems
JO  - Computer Networks
VL  - 46
IS  - 6
SP  - 771
EP  - 795
PY  - 2004/12/20/
T2  - 
AU  - McKinnon, A. David
AU  - Bakken, David E.
AU  - Shovic, John C.
SN  - 1389-1286
DO  - http://dx.doi.org/10.1016/j.comnet.2004.06.020
UR  - http://www.sciencedirect.com/science/article/pii/S1389128604001732
KW  - Middleware
KW  - Security
KW  - CORBA
KW  - Embedded systems
AB  - Computer and network security is becoming increasingly important as both large systems and, increasingly small, embedded systems are networked. Middleware frameworks aid the system developer who must interconnect individual systems into larger interconnected, distributed systems. However, there exist very few middleware frameworks that have been designed for use with embedded systems, which constitute the vast majority of CPUs produced each year, and none offer the range of security mechanisms required by the wide range of embedded system applications. This paper describes MicroQoSCORBA, a highly configurable middleware framework for embedded systems, and its security subsystem. It first presents an analysis of security requirements for embedded applications and what can and should be done in middleware. It then presents the design of MicroQoSCORBA’s security subsystem and the wide range of mechanisms it supports. Experimental results for these mechanisms are presented for two different embedded systems and one desktop computer that collectively represent a wide range of computational capabilities.
ER  - 

TY  - JOUR
T1  - PalmCIS: A wireless handheld application for satisfying clinician information needs
JO  - Journal of the American Medical Informatics Association
VL  - 11
IS  - 1
SP  - 19
EP  - 28
PY  - 2004/1//
Y2  - 2004/2//
T2  - 
AU  - Chen, Elizabeth S
AU  - Mendonça, Eneida A
AU  - McKnight, Lawrence K
AU  - Stetson, Peter D
AU  - Lei, Jianbo
AU  - Cimino, James J
SN  - 1067-5027
DO  - http://dx.doi.org/10.1197/jamia.M1387
UR  - http://www.sciencedirect.com/science/article/pii/S1067502703002020
AB  - Wireless handheld technology provides new ways to deliver and present information. As with any technology, its unique features must be taken into consideration and its applications designed accordingly. In the clinical setting, availability of needed information can be crucial during the decision-making process. Preliminary studies performed at New York Presbyterian Hospital (NYPH) determined that there are inadequate access to information and ineffective communication among clinicians (potential proximal causes of medical errors). In response to these findings, the authors have been developing extensions to their Web-based clinical information system including PalmCIS, an application that provides access to needed patient information via a wireless personal digital assistant (PDA). The focus was on achieving end-to-end security and developing a highly usable system. This report discusses the motivation behind PalmCIS, design and development of the system, and future directions.
ER  - 

TY  - CHAP

T1  - Index
A2  - Chuvakin, Anton
A2  - Schmidt, Kevin 
A2  - Phillips, Chris 
BT  - Logging and Log Management
PB  - Syngress
CY  - Boston
PY  - 2013///
SP  - 413
EP  - 434
SN  - 978-1-59749-635-3
DO  - http://dx.doi.org/10.1016/B978-1-59-749635-3.00032-4
UR  - http://www.sciencedirect.com/science/article/pii/B9781597496353000324
ER  - 

TY  - CHAP
AU  - Broad, James
T1  - Chapter 9 - RMF Phase 1: Categorize the Information System
A2  - Broad, James 
BT  - Risk Management Framework
PB  - Syngress
CY  - Boston
PY  - 2013///
SP  - 75
EP  - 102
SN  - 978-1-59749-995-8
DO  - http://dx.doi.org/10.1016/B978-1-59749-995-8.00009-0
UR  - http://www.sciencedirect.com/science/article/pii/B9781597499958000090
KW  - categorization
KW  - information type
KW  - system registration
KW  - NIST 800-60
KW  - systems description
AB  - Abstract
This chapter introduces phase one of the RMF and the process used to categorize the information system.
ER  - 

TY  - CHAP
AU  - Chuvakin, Anton
AU  - Schmidt, Kevin
AU  - Phillips, Chris
T1  - Chapter 15 - Tools for Log Analysis and Collection
A2  - Chuvakin, Anton
A2  - Schmidt, Kevin 
A2  - Phillips, Chris 
BT  - Logging and Log Management
PB  - Syngress
CY  - Boston
PY  - 2013///
SP  - 243
EP  - 266
SN  - 978-1-59749-635-3
DO  - http://dx.doi.org/10.1016/B978-1-59-749635-3.00015-4
UR  - http://www.sciencedirect.com/science/article/pii/B9781597496353000154
AB  - Abstract

This chapter provides a review of open source and commercial toolsets available for the analysis and collection of log data. The review will provide the reader many options to choose from when picking a toolset to manage log data on a daily basis. Examples of using the tools for log analysis are interspersed within the contents of the chapter with real-world examples of using the tools to review common logging tasks and scenarios. The chapter will help the reader review the set of tools available today and hopefully find the right tool for the job to help the reader get started on analyzing logs in their organization today.

Keywords

Open source log analysis tools, Commercial log analysis tools, OSSEC, Snare, Syslog-ng, Rsyslog, Grep, Awk, Sed, tail, head, Logwatch, Logsurfer, Lire, Sec, Ossim, LogHound, Log2timeline, LogZilla, Splunk, NetIQ Sentinel, IBM q 1Labs, Microsoft log parser, Loggly
ER  - 

TY  - CHAP
AU  - Chuvakin, Anton
AU  - Schmidt, Kevin
AU  - Phillips, Chris
T1  - Chapter 1 - Logs, Trees, Forest: The Big Picture
A2  - Chuvakin, Anton
A2  - Schmidt, Kevin 
A2  - Phillips, Chris 
BT  - Logging and Log Management
PB  - Syngress
CY  - Boston
PY  - 2013///
SP  - 1
EP  - 27
SN  - 978-1-59749-635-3
DO  - http://dx.doi.org/10.1016/B978-1-59-749635-3.00001-4
UR  - http://www.sciencedirect.com/science/article/pii/B9781597496353000014
AB  - Abstract

This chapter provides the reader with an introduction to logs and why they are important to the art of log management and analysis. Content covered in the chapter provides a basic foundation for the reader. Introductory topics presented in this chapter will be covered in more detail in later chapters.

Keywords

Logs, Logging, Log data, Firewall, IDS, IPS, HIDS, Syslog, Unix, Windows, Process, Standards, Methodologies, Generation, Analysis, Logs, Log messages, UDP, PCAP, LEA, SDEE, E-Streamer, Cloud, Log collector, UDP, TCP, Correlation, API, Windows Event Log, Normalization, Logging system, Log analysis, Agents, ICMP, SNMP, Event, MSSP, SIEM
ER  - 

TY  - JOUR
T1  - The Analytic Information Warehouse (AIW): A platform for analytics using electronic health record data
JO  - Journal of Biomedical Informatics
VL  - 46
IS  - 3
SP  - 410
EP  - 424
PY  - 2013/6//
T2  - 
AU  - Post, Andrew R.
AU  - Kurc, Tahsin
AU  - Cholleti, Sharath
AU  - Gao, Jingjing
AU  - Lin, Xia
AU  - Bornstein, William
AU  - Cantrell, Dedra
AU  - Levine, David
AU  - Hohmann, Sam
AU  - Saltz, Joel H.
SN  - 1532-0464
DO  - http://dx.doi.org/10.1016/j.jbi.2013.01.005
UR  - http://www.sciencedirect.com/science/article/pii/S153204641300018X
KW  - Healthcare analytics
KW  - Clinical data warehousing
KW  - Temporal abstraction
KW  - Quality improvement
KW  - Comparative effectiveness
AB  - Objective
To create an analytics platform for specifying and detecting clinical phenotypes and other derived variables in electronic health record (EHR) data for quality improvement investigations.
Materials and methods
We have developed an architecture for an Analytic Information Warehouse (AIW). It supports transforming data represented in different physical schemas into a common data model, specifying derived variables in terms of the common model to enable their reuse, computing derived variables while enforcing invariants and ensuring correctness and consistency of data transformations, long-term curation of derived data, and export of derived data into standard analysis tools. It includes software that implements these features and a computing environment that enables secure high-performance access to and processing of large datasets extracted from EHRs.
Results
We have implemented and deployed the architecture in production locally. The software is available as open source. We have used it as part of hospital operations in a project to reduce rates of hospital readmission within 30 days. The project examined the association of over 100 derived variables representing disease and co-morbidity phenotypes with readmissions in 5 years of data from our institution’s clinical data warehouse and the UHC Clinical Database (CDB). The CDB contains administrative data from over 200 hospitals that are in academic medical centers or affiliated with such centers.
Discussion and conclusion
A widely available platform for managing and detecting phenotypes in EHR data could accelerate the use of such data in quality improvement and comparative effectiveness studies.
ER  - 

TY  - CHAP

T1  - Index
A2  - Wheeler, Evan 
BT  - Security Risk Management
PB  - Syngress
CY  - Boston
PY  - 2011///
SP  - 331
EP  - 340
SN  - 978-1-59749-615-5
DO  - http://dx.doi.org/10.1016/B978-1-59749-615-5.00031-1
UR  - http://www.sciencedirect.com/science/article/pii/B9781597496155000311
ER  - 

TY  - CHAP

T1  - Index
A2  - Conrad, Eric
A2  - Misenar, Seth 
A2  - Feldman, Joshua 
BT  - CISSP Study Guide (Second Edition)
PB  - Syngress
CY  - Boston
PY  - 2012///
SP  - 549
EP  - 577
SN  - 978-1-59749-961-3
DO  - http://dx.doi.org/10.1016/B978-1-59749-961-3.09984-2
UR  - http://www.sciencedirect.com/science/article/pii/B9781597499613099842
ER  - 

TY  - JOUR
T1  - Taxonomy of attacks and defense mechanisms in P2P reputation systems—Lessons for reputation system designers
JO  - Computer Science Review
VL  - 6
IS  - 2–3
SP  - 47
EP  - 70
PY  - 2012/5//
T2  - 
AU  - Koutrouli, Eleni
AU  - Tsalgatidou, Aphrodite
SN  - 1574-0137
DO  - http://dx.doi.org/10.1016/j.cosrev.2012.01.002
UR  - http://www.sciencedirect.com/science/article/pii/S1574013712000093
KW  - Reputation systems
KW  - Reputation attacks
KW  - Threat analysis
KW  - Credibility
KW  - Reputation attacks taxonomy
KW  - Peer-to-Peer
AB  - Robust and credible reputation systems are essential for the functionality of Peer-to-Peer (P2P) applications. However, they themselves are susceptible to various types of attacks. Since most current efforts lack an exploration of a comprehensive adversary model, we try to fill in this gap by providing a thorough view of the various credibility threats against a decentralized reputation system and the respective defense mechanisms. Therefore, we explore and classify the types of potential attacks against reputation systems for P2P applications. We also study and classify the defense mechanisms which have been proposed for each type of attack and identify conflicts between defense mechanisms and/or desirable characteristics of credible reputations systems. We finally propose a roadmap for reputation system designers on how to use the results of our survey for the design of robust reputation systems for P2P applications.
ER  - 

TY  - JOUR
T1  - 3D architecture viewpoints on service automation
JO  - Journal of Systems and Software
VL  - 86
IS  - 5
SP  - 1307
EP  - 1322
PY  - 2013/5//
T2  - 
AU  - Gu, Qing
AU  - Cuadrado, Félix
AU  - Lago, Patricia
AU  - Dueñas, Juan C.
SN  - 0164-1212
DO  - http://dx.doi.org/10.1016/j.jss.2012.12.035
UR  - http://www.sciencedirect.com/science/article/pii/S0164121212003445
KW  - Architecture viewpoint
KW  - Service-oriented architecture
KW  - Automation
AB  - Service-oriented architecture is an emerging paradigm for the execution of business-oriented as well as technical infrastructure processes by means of services. Automating the execution of services is of paramount importance in order to fulfill the needs of companies. However we have found that automation – although important – is seldom addressed explicitly as a concern when stating requirements or designing the software architecture of the service-based applications (SBAs). In this paper we define three architectural viewpoints framing the concerns about service automation. These three viewpoints, called 3D (Decisions, Degree, Data), respectively: express architectural decisions about automation; help identifying the level (degree) of automation required, and represent the specific data required to support automation in services. They have been applied to three industrial case studies and one academic experiment. Results show that they successfully support both technical and non-technical stakeholders in understanding how, and communicating upon, their concerns related to service automation have been addressed. The application of the 3D service automation viewpoints to different domains exhibits promising reusability.
ER  - 

TY  - CHAP
AU  - Cope, Bill
AU  - Kalantzis, Mary
T1  - 5 - Books and journal articles: the textual practices of academic knowledge
A2  - Cope, Bill
A2  - Kalantzis, Mary 
A2  - Magee, Liam 
BT  - Towards a Semantic Web
PB  - Chandos Publishing
CY  - 
PY  - 2011///
SP  - 123
EP  - 144
SN  - 978-1-84334-601-2
DO  - http://dx.doi.org/10.1016/B978-1-84334-601-2.50005-2
UR  - http://www.sciencedirect.com/science/article/pii/B9781843346012500052
ER  - 

TY  - JOUR
T1  - The trouble with ischemia
JO  - American Heart Journal
VL  - 164
IS  - 2
SP  - 133
EP  - 134
PY  - 2012/8//
T2  - 
AU  - Califf, Robert M.
SN  - 0002-8703
DO  - http://dx.doi.org/10.1016/j.ahj.2012.06.007
UR  - http://www.sciencedirect.com/science/article/pii/S0002870312004383
ER  - 

TY  - JOUR
T1  - Avoiding the five pitfalls of privileged accounts
JO  - Network Security
VL  - 2013
IS  - 5
SP  - 12
EP  - 14
PY  - 2013/5//
T2  - 
AU  - Grafton, Jane
SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/S1353-4858(13)70060-6
UR  - http://www.sciencedirect.com/science/article/pii/S1353485813700606
AB  - It is a rather human truth that when we hand out privileges they often get abused. Whether you are operating in the high reaches of government or the most basic market the sad fact is – where we find privilege we also find the abuse of privilege. In the world of IT, privileged accounts are identities that have elevated permission to access potentially sensitive data, run programs or change configuration settings. To put it simply, privileged accounts are the keys to the kingdom of IT.

When we hand out privileges they often get abused. In the world of IT, privileged accounts – identities that have elevated permission to access sensitive data, run programs or change settings – are found on every server, workstation and appliance.

In recent years we have witnessed more and more organisations fail to adequately secure such accounts, with catastrophic results. There are common practices that have lead to a number of security breaches and failed IT compliance audits, some of them very high-profile. Jane Grafton of Lieberman Software looks at the five most common errors with privileged accounts – and how to avoid them.
ER  - 

TY  - JOUR
T1  - A survey of security in multi-agent systems
JO  - Expert Systems with Applications
VL  - 39
IS  - 5
SP  - 4835
EP  - 4846
PY  - 2012/4//
T2  - 
AU  - Cavalcante, Rodolfo Carneiro
AU  - Bittencourt, Ig Ibert
AU  - da Silva, Alan Pedro
AU  - Silva, Marlos
AU  - Costa, Evandro
AU  - Santos, Robério
SN  - 0957-4174
DO  - http://dx.doi.org/10.1016/j.eswa.2011.09.130
UR  - http://www.sciencedirect.com/science/article/pii/S0957417411014539
KW  - Agents
KW  - Multi-agent systems
KW  - Security
KW  - Security in MAS
KW  - Multi-agents
AB  - Multi-agent systems (MAS) are a relatively new software paradigm that is being widely accepted in several application domains to address large and complex tasks. However, with the use of MAS in open, distributed and heterogeneous applications, the security issues may endanger the success of the application. The goal of this research is to identify the security issues faced by MAS and to survey the current state of the art of this field of knowledge. In order to do it, this paper examines the basic concepts of security in computing, and some characteristics of agents and multi-agent systems that introduce new threats and ways to attack. After this, some models and architectures proposed in the literature are presented and analyzed.
ER  - 

TY  - JOUR
T1  - Who's in control: a six-step strategy for secure IT
JO  - Network Security
VL  - 2011
IS  - 11
SP  - 18
EP  - 20
PY  - 2011/11//
T2  - 
AU  - Facey, Stuart
SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/S1353-4858(11)70121-0
UR  - http://www.sciencedirect.com/science/article/pii/S1353485811701210
AB  - As more and more organisations encourage flexible and remote working policies that allow employees to work outside the office, the complexity surrounding remote access and support mechanisms for the IT helpdesk has also increased. There is a growing and unregulated market for solutions that can ‘fix’ IT issues quickly and efficiently no matter where workers are located: however, as with many solutions, remote support and access products have their own inherent security risks that should not be underestimated.
ER  - 

TY  - CHAP
AU  - Sitaram, Dinkar
AU  - Manjunath, Geetha
T1  - Chapter 7 - Designing Cloud Security
A2  - Sitaram, Dinkar  
A2  - Manjunath, Geetha 
BT  - Moving To The Cloud
PB  - Syngress
CY  - Boston
PY  - 2012///
SP  - 307
EP  - 328
SN  - 978-1-59749-725-1
DO  - http://dx.doi.org/10.1016/B978-1-59749-725-1.00007-X
UR  - http://www.sciencedirect.com/science/article/pii/B978159749725100007X
KW  - Cloud security
KW  - cloud security standards
KW  - Cloud Risk Management
KW  - security controls
KW  - security architecture
AB  - Publisher Summary
This chapter focuses on processes and practices to be followed in order to ensure a robust security architecture. It starts with the requirements for the cloud security infrastructure. This can be divided into two parts—ensuring the security of the physical infrastructure, and best practices for security processes and technology. Subsequently, the concept of risk management is described. Risk management is the process of evaluating the possible security threats to the system, identifying the major risks, and putting in place security controls to handle them. The FIPS 200 standard for identifying the impact of a risk and the NIST 80053 standard for security controls are described. Subsequently, security design patterns and principles that should be followed to design the security infrastructure for a cloud are detailed. Following this, a high-level security design for a PaaS system based upon these design patterns is discussed. The PaaS security design illustrates the design patterns discussed earlier that can be put into practice. Finally, various security architectures which can be leveraged to implement cloud security are discussed. The chapter also focuses on security concerns arising out of the use of public clouds. The first set of issues arises from the fact that, legally, a cloud service provider is a subcontractor, and it is the responsibility of the business to ensure that they are in compliance with all legal and regulatory issues. Then, issues arising out of the fact that a cloud service provider is a “third party” in any litigation are discussed.
 
One of the main hurdles for adoption of Cloud computing by public and enterprises is that of security and privacy of data hosted on the cloud. While the technical solution for the same can actually use existing security protocols for web services, the key problems to be addressed are in processes and governance. This chapter describes the processes and best practices that need to be implemented for a secure cloud. Additionally, it also describes the security issues introduced by a public cloud service provider. This is an abridged version of the book Securing the Cloud by Vic (J.R.) Winkler.
ER  - 

TY  - CHAP

T1  - Index
A2  - Sitaram, Dinkar  
A2  - Manjunath, Geetha 
BT  - Moving To The Cloud
PB  - Syngress
CY  - Boston
PY  - 2012///
SP  - 427
EP  - 448
SN  - 978-1-59749-725-1
DO  - http://dx.doi.org/10.1016/B978-1-59749-725-1.00022-6
UR  - http://www.sciencedirect.com/science/article/pii/B9781597497251000226
ER  - 

TY  - JOUR
T1  - Analysis of recommended cloud security controls to validate OpenPMF “policy as a service”
JO  - Information Security Technical Report
VL  - 16
IS  - 3–4
SP  - 131
EP  - 141
PY  - 2011/8//
Y2  - 2011/11//
T2  - Cloud Security
AU  - Lang, Ulrich
AU  - Schreiner, Rudolf
SN  - 1363-4127
DO  - http://dx.doi.org/10.1016/j.istr.2011.08.001
UR  - http://www.sciencedirect.com/science/article/pii/S136341271100046X
KW  - Cloud
KW  - Security
KW  - Policy
KW  - Authorization management
KW  - Access policy
KW  - Compliance
KW  - Model-driven security
KW  - Accreditation
KW  - Audit policy
KW  - Application security
KW  - XACML
KW  - OpenPMF
KW  - NIST 800-53
KW  - NIST 800-147
KW  - NIST IR 7628
KW  - PCI-DSS
KW  - HIPAA
AB  - This paper describes some of the findings of a cloud research project the authors carried out in Q2/2011. As part of the project, the authors first identified security concerns related to cloud computing, and gaps in cloud-related standards/regulations. The authors then identified several hard-to-implement, but highly cloud-relevant, security requirements in numerous cloud (and non-cloud) regulations and guidance documents, especially related to “least privilege”, “information flow control”, and “incident monitoring/auditing/analysis”. Further study revealed that there are significant cloud technology gaps in cloud (and non-cloud) platforms, which make it difficult to effectively implement those security policy requirements. The project concluded that model-driven security policy automation offered as a cloud service and tied into the protected cloud platform is ideally suited to achieve correct, consistent, low-effort/cost policy implementation for cloud applications.
ER  - 

TY  - JOUR
T1  - Crossing Borders: The Right Side of Wrong?
JO  - Infosecurity
VL  - 8
IS  - 5
SP  - 22
EP  - 25
PY  - 2011/9//
Y2  - 2011/10//
T2  - 
AU  - Grossman, Wendy M.
SN  - 1754-4548
DO  - http://dx.doi.org/10.1016/S1754-4548(11)70065-1
UR  - http://www.sciencedirect.com/science/article/pii/S1754454811700651
AB  - Most nations consider travel data to be crucial to protecting national security. How that data is collected, stored, and secured however seems to be a closely guarded secret. Wendy M. Grossman investigates
ER  - 

TY  - JOUR
T1  - Economics and the cyber challenge
JO  - Information Security Technical Report
VL  - 17
IS  - 1–2
SP  - 9
EP  - 18
PY  - 2012/2//
T2  - Human Factors and Bio-metrics
AU  - Walker, Simon
SN  - 1363-4127
DO  - http://dx.doi.org/10.1016/j.istr.2011.12.003
UR  - http://www.sciencedirect.com/science/article/pii/S1363412711000860
AB  - Economics can be used as a tool to explain, describe, and to a certain extent predict many forms of human behaviour. However, there is only a limited body of work on its application to information security, much of which is acknowledged as partial or incomplete. As a consequence, there is a paucity of robust explanatory or predictive models that are tuned for the peculiarities of the “cyber” challenge, either to organisations, or, at a higher level, the nation state.

The effect of this is that the base arguments for information security business cases are often weak or flawed; as a result, there is an argument that both organisations and nation states will therefore tend to underinvest in information security. To improve this position, there would be benefits for information security, as a profession adopting economic models used in other areas of endeavour that historically have suffered similar problems. One potential model is full-cost accounting.

However, there are a number of further implications. These include an underlining of the importance of information security professional “speaking business language”. Also highlighted is the potential value of building a common knowledge base of the true cost of security failures, akin to the actuarial bodies of knowledge used in the insurance industry, rather than the partial and imperfect measures in use today.
ER  - 

TY  - JOUR
T1  - IMENSE: An e-infrastructure environment for patient specific multiscale data integration, modelling and clinical treatment
JO  - Journal of Computational Science
VL  - 3
IS  - 5
SP  - 314
EP  - 327
PY  - 2012/9//
T2  - Advanced Computing Solutions for Health Care and Medicine
AU  - Zasada, Stefan J.
AU  - Wang, Tao
AU  - Haidar, Ali
AU  - Liu, Enjie
AU  - Graf, Norbert
AU  - Clapworthy, Gordon
AU  - Manos, Steven
AU  - Coveney, Peter V.
SN  - 1877-7503
DO  - http://dx.doi.org/10.1016/j.jocs.2011.07.001
UR  - http://www.sciencedirect.com/science/article/pii/S1877750311000639
KW  - Clinical decision support
KW  - Electronic health records
KW  - Virtual Physiological Human
KW  - Medical data management
AB  - Secure access to patient data and analysis tools to run on that data will revolutionize the treatment of a wide range of diseases, by using advanced simulation techniques to underpin the clinical decision making process. To achieve these goals, suitable e-Science infrastructures are required to allow clinicians and researchers to trivially access data and launch simulations. In this paper we describe the open source Individualized MEdiciNe Simulation Environment (IMENSE), which provides a platform to securely manage clinical data, and to perform wide ranging analysis on that data, ultimately with the intention of enhancing clinical decision making with direct impact on patient health care. We motivate the design decisions taken in the development of the IMENSE system by considering the needs of researchers in the ContraCancrum project, which provides a paradigmatic case in which clinicians and researchers require coordinated access to data and simulation tools. We show how the modular nature of the IMENSE system makes it applicable to a wide range of biomedical computing scenarios, from within a single hospital to major international research projects.
ER  - 

TY  - JOUR
T1  - Information security obedience: a definition
JO  - Computers & Security
VL  - 24
IS  - 1
SP  - 69
EP  - 75
PY  - 2005/2//
T2  - 
AU  - Thomson, Kerry-Lynn
AU  - von Solms, Rossouw
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/j.cose.2004.10.005
UR  - http://www.sciencedirect.com/science/article/pii/S0167404804002627
KW  - Corporate governance
KW  - Information security
KW  - Corporate culture
KW  - Information security obedience
AB  - Information is a fundamental asset within any organisation and the protection of this asset, through a process of information security, is of equal importance. This paper examines the relationships that exist between the fields of corporate governance, information security and corporate culture. It highlights the role that senior management should play in cultivating an information security conscious culture in their organisation, for the benefit of the organisation, senior management and the users of information.
ER  - 

TY  - CHAP
AU  - Winkler, Vic (J.R.)
T1  - Chapter 8 - Security Criteria: Selecting an External Cloud Provider
A2  - Winkler, Vic (J.R.) 
BT  - Securing the Cloud
PB  - Syngress
CY  - Boston
PY  - 2011///
SP  - 211
EP  - 232
SN  - 978-1-59749-592-9
DO  - http://dx.doi.org/10.1016/B978-1-59749-592-9.00008-7
UR  - http://www.sciencedirect.com/science/article/pii/B9781597495929000087
KW  - Federal Risk and Authorization Management Program
KW  - Information security management system
KW  - ISO 27000 series
KW  - ISO 27000 series accreditation
KW  - Operational risk
KW  - Policy and legal risk
KW  - Recovery point objective
KW  - Recovery time objective
KW  - Technical risk
KW  - Transparency
AB  - Publisher Summary
This chapter examines a number of security criteria that prospective cloud service customers should consider. These include criteria that clearly fall into the realm of security, as well as others that are security relevant—such as SLAs, data center location, and cloud elasticity. The goal of this chapter is to present an overview of how to select a CSP based on security requirements. From a cloud consumer's perspective, the data is only as secure as the cloud in which it exists. CSP claims are easy to come by, but hard to verify without evidence. Achieving third-party certification or accreditation gives the CSP credibility that appropriate processes and procedures are in place to meet the security needs of customers. When the CSP is transparent and reveals information, customers can make an informed selection. But more is required by a customer to assure security on an ongoing basis.
 
Customers of public cloud services do not need to concern themselves with the intricate details of their underlying cloud structure. In fact, a thorough understanding of the cloud service provider's architecture, infrastructure, policies, and procedures may not even be possible. However, customers do need to ensure that the cloud service provider's security claims are independently verified. Just as important as verification is the cloud service provider's willingness to be transparent about its security practices because customers also need to understand issues concerning security and governance of the service they are considering. The goal of this chapter is to present an overview of how to select a cloud service provider based on security requirements. Toward that end, the chapter examines security criteria prospective cloud service customers should consider. Some fall squarely in the realm of security, while others, such as service level agreements, data center location, and cloud elasticity, are security relevant.
ER  - 

TY  - CHAP

T1  - Index
A2  - Rountree, Derrick 
BT  - Security for Microsoft Windows System Administrators
PB  - Syngress
CY  - Boston
PY  - 2011///
SP  - 193
EP  - 198
SN  - 978-1-59749-594-3
DO  - http://dx.doi.org/10.1016/B978-1-59749-594-3.00015-6
UR  - http://www.sciencedirect.com/science/article/pii/B9781597495943000156
ER  - 

TY  - JOUR
T1  - Applying digital rights management systems to privacy rights management
JO  - Computers & Security
VL  - 21
IS  - 7
SP  - 648
EP  - 664
PY  - 2002/11//
T2  - 
AU  - Kenny, Steve
AU  - Korba, Larry
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/S0167-4048(02)01117-3
UR  - http://www.sciencedirect.com/science/article/pii/S0167404802011173
AB  - Disclaimer

The views expressed by the authors of this article are their own and may not necessarily be taken to be those of either the Dutch Data Protection Authority or the National Research Council of Canada.

Abstract

While there are growing concerns about how to manage citizen privacy, currently there are no established technology solutions that meet the privacy needs required in some cases by legislation. In this paper we examine the prospect of adapting systems developed for Digital Rights Management to meet the challenges of Privacy Rights Management. In particular, the goal of this work is the adaptation of DRM technology to produce a privacy management architecture that reflects the requirements of Directive 95/46/EC for the protection of personal data. This paper first outlines the requirements for management of the personal data within the European Community it then describes the changes that would be required to transform a digital rights management system into a system to manage the handling of personal data. The paper concludes with a thorough discussion of the issues and potential of this approach.
ER  - 

TY  - CHAP

T1  - Index
A2  - Photopoulos, Constantine 
BT  - Managing Catastrophic Loss of Sensitive Data
PB  - Syngress
CY  - Burlington
PY  - 2008///
SP  - 285
EP  - 293
SN  - 978-1-59749-239-3
DO  - http://dx.doi.org/10.1016/B978-1-59749-239-3.00011-0
UR  - http://www.sciencedirect.com/science/article/pii/B9781597492393000110
ER  - 

TY  - CHAP
AU  - Jakeman, Anthony J.
AU  - Letcher, Rebecca A.
AU  - Norton, John P.
T1  - Chapter 15 - Outstanding research issues in integration and participation for water resource planning and management
A2  - Castelletti, Andrea  
A2  - Sessa, Rodolfo Soncini 
BT  - Topics on System Analysis and Integrated Water Resources Management
PB  - Elsevier
CY  - Oxford
PY  - 2007///
SP  - 273
EP  - 289
SN  - 978-0-08-044967-8
DO  - http://dx.doi.org/10.1016/B978-008044967-8/50015-4
UR  - http://www.sciencedirect.com/science/article/pii/B9780080449678500154
AB  - Publisher Summary
Substantial efforts are being made worldwide to address sustainability issues in catchments, but the approach is everywhere too fragmented and would benefit substantially from more coordination of research, management, and their interaction. Typically, research and policy have focused on one part of a catchment or one issue in isolation from others, or on a subset of impacts and related disciplines. Often, there has been too little emphasis on socioeconomic aspects, and engagement with managers and the community has been patchy. Continuity has been lacking in many cases. These limitations have led to ineffective and sometimes conflicting attempts to solve environmental problems and to considerable waste of money. Integrated assessment (IA) provides a vehicle for addressing all key issues affecting the sustainability of terrestrial, aquifer, and riverine systems. It integrates knowledge and understanding from research areas including social science, economics, ecology, and hydrology, as well as from the community and managers, to address real-world management issues. A crucial aspect of complex, integrated models for environmental management is gauging of model uncertainty and sensitivity. The crux of model-aided decision-making is to determine whether one alternative is superior to another, by comparison of model outputs for the scenarios being considered.
ER  - 

TY  - CHAP

T1  - Index
A2  - Payne, Thomas 
BT  - Practical Guide to Clinical Computing Systems
PB  - Academic Press
CY  - New York
PY  - 2008///
SP  - 227
EP  - 235
SN  - 978-0-12-374002-1
DO  - http://dx.doi.org/10.1016/B978-0-12-374002-1.00016-0
UR  - http://www.sciencedirect.com/science/article/pii/B9780123740021000160
ER  - 

TY  - JOUR
T1  - Database-managed Grid-enabled analysis of neuroimaging data: The CNARI framework
JO  - International Journal of Psychophysiology
VL  - 73
IS  - 1
SP  - 62
EP  - 72
PY  - 2009/7//
T2  - fMRI: Advances, problems, and the future
AU  - Small, Steven L.
AU  - Wilde, Michael
AU  - Kenny, Sarah
AU  - Andric, Michael
AU  - Hasson, Uri
SN  - 0167-8760
DO  - http://dx.doi.org/10.1016/j.ijpsycho.2009.01.010
UR  - http://www.sciencedirect.com/science/article/pii/S0167876009000282
KW  - fMRI
KW  - Imaging
KW  - Brain
KW  - Image analysis
KW  - Data analysis
KW  - Functional MRI
KW  - High-performance computing
KW  - Grid computing
KW  - Grid
KW  - Cluster computing
KW  - Database
KW  - Relational database
KW  - Data storage
KW  - Cortex
KW  - Language
KW  - Functional imaging
KW  - Infrastructure
KW  - Cognition
KW  - Cognitive neuroscience
KW  - Neuroscience
KW  - Neurology
AB  - Functional magnetic resonance imaging (fMRI) has led to an enormous growth in the study of cognitive neuroanatomy, and combined with advances in high-field electrophysiology (and other methods), has led to a fast-growing field of human neuroscience. Technological advances in both hardware and software will lead to an ever more promising future for fMRI. We have developed a new computational framework that facilitates fMRI experimentation and analysis, and which has led to some rethinking of the nature of experimental design and analysis. The Computational Neuroscience Applications Research Infrastructure (CNARI) incorporates novel methods for maintaining, serving, and analyzing massive amounts of fMRI data. By using CNARI, it is possible to perform naturalistic, network-based, statistically valid experiments in systems neuroscience on a very large scale, with ease of data manipulation and analysis, within reasonable computational time scales. In this article, we describe this infrastructure and then illustrate its use on a number of actual examples in both cognitive neuroscience and neurological research. We believe that these advanced computational approaches will fundamentally change the future shape of cognitive brain imaging with fMRI.
ER  - 

TY  - CHAP
AU  - Photopoulos, Constantine
T1  - Chapter 3 - Controls and Safeguards
A2  - Photopoulos, Constantine 
BT  - Managing Catastrophic Loss of Sensitive Data
PB  - Syngress
CY  - Burlington
PY  - 2008///
SP  - 47
EP  - 92
SN  - 978-1-59749-239-3
DO  - http://dx.doi.org/10.1016/B978-1-59749-239-3.00003-1
UR  - http://www.sciencedirect.com/science/article/pii/B9781597492393000031
AB  - Publisher Summary
A data security program allows the management of data security risks and limits the organization's vulnerability to data compromise. The data security program includes data classification, risk assessment, risk mitigation strategy, controls to protect the data, monitoring and testing of the controls to verify that they are effective, and a process to continuously gather and analyze new threats and vulnerabilities. Information security controls are the technical, physical, administrative, and policy safeguards designed to protect sensitive data. As part of a defense in depth strategy, a variety of controls are necessary for a comprehensive and robust security framework. Lost or stolen laptops represent a significant source of data compromise. If sensitive data must be stored on a laptop, full-disk encryption should be used to prevent unauthorized parties from retrieving the data. The organization should evaluate all transfers of physical media containing sensitive information to discontinue unnecessary or redundant transfers. Sensitive data in transport should be encrypted. A variety of technical safeguards should be used for data security, including firewalls, intrusion detection systems, and vulnerability scanning. Sensitive data transmission should be performed only over a trusted path or medium with cryptographic controls. The organization should implement policies and processes governing the conditions under which remote access is granted and terminated, and all communications should be through a virtual private network that can provide a secure communications channel across a public network. All servers and workstations should be configured with antivirus software that is automatically updated on a daily basis with new virus definitions.
ER  - 

TY  - CHAP
AU  - Taylor, Laura
AU  - Shepherd, Matthew
T1  - Chapter 8 - Performing and preparing the self-assessment
A2  - Taylor, Laura  
A2  - Shepherd, Matthew 
BT  - FISMA Certification and Accreditation Handbook
PB  - Syngress
CY  - Burlington
PY  - 2007///
SP  - 107
EP  - 138
SN  - 978-1-59749-116-7
DO  - http://dx.doi.org/10.1016/B978-159749116-7/50013-5
UR  - http://www.sciencedirect.com/science/article/pii/B9781597491167500135
AB  - Most agencies require a security self-assessment only in the off years when C&amp;A packages are not required for submission. Performing a security self-assessment is a process by which an agency or organization determines the current security posture of their information systems and infrastructure. A self-assessment helps give you a level of assurance as to how well the management, operational, and technical security controls are working. One of the best guides in how to perform a security self-assessment is Special Publication 800-26, Security Self-Assessment Guide for Information Technology Systems, November 2001, by the National Institute of Standards.
ER  - 

TY  - JOUR
T1  - A security framework for a workflow-based grid development platform
JO  - Computer Standards & Interfaces
VL  - 32
IS  - 5–6
SP  - 230
EP  - 245
PY  - 2010/10//
T2  - Information and communications security, privacy and trust: Standards and Regulations
AU  - Vivas, José L.
AU  - Fernández-Gago, Carmen
AU  - Lopez, Javier
AU  - Benjumea, Andrés
SN  - 0920-5489
DO  - http://dx.doi.org/10.1016/j.csi.2009.04.001
UR  - http://www.sciencedirect.com/science/article/pii/S0920548909000270
KW  - Grid
KW  - Security framework
KW  - Security services
KW  - Virtual organizations
AB  - This paper describes the security framework that is to be developed for the generic grid platform created for the project GREDIA. This platform is composed of several components that need to be secured. The platform uses the OGSA standards, so that the security framework will follow GSI, the portion of Globus that implements security. Thus, we will show the security features that GSI already provides and we will outline which others need to be created or enhanced.
ER  - 

TY  - CHAP
AU  - Taylor, Laura
AU  - Shepherd, Matthew
T1  - Chapter 21 - Evaluating Certification Package for Accreditation
A2  - Taylor, Laura  
A2  - Shepherd, Matthew 
BT  - FISMA Certification and Accreditation Handbook
PB  - Syngress
CY  - Burlington
PY  - 2007///
SP  - 365
EP  - 410
SN  - 978-1-59749-116-7
DO  - http://dx.doi.org/10.1016/B978-159749116-7/50026-3
UR  - http://www.sciencedirect.com/science/article/pii/B9781597491167500263
AB  - Once a final C&amp;A package has been submitted, the evaluation team begins the review process. The person or team of people who evaluate the C&amp;A package should not be the same person or group of people who prepared it. Something that the OIG and GAO will be looking for are instances of the fox guarding the hen house. There needs to be a separation of duties between the folks who prepare the C&amp;A documents and the folks who evaluate them.
ER  - 

TY  - CHAP

T1  - Appendix B - Definitions
A2  - Wilhelm, Thomas 
BT  - Professional Penetration Testing
PB  - Syngress
CY  - Boston
PY  - 2010///
SP  - 489
EP  - 493
SN  - 978-1-59749-425-0
DO  - http://dx.doi.org/10.1016/B978-1-59749-425-0.00026-9
UR  - http://www.sciencedirect.com/science/article/pii/B9781597494250000269
ER  - 

TY  - CHAP
AU  - Lahti, Christian B.
AU  - Peterson, Roderick
T1  - Chapter 6 - What's First?
A2  - Lahti, Christian B.  
A2  - Peterson, Roderick 
BT  - Sarbanes-Oxley IT Compliance Using Open Source Tools (Second Edition)
PB  - Syngress
CY  - Burlington
PY  - 2007///
SP  - 167
EP  - 213
SN  - 978-1-59749-216-4
DO  - http://dx.doi.org/10.1016/B978-1-59749-216-4.00006-9
UR  - http://www.sciencedirect.com/science/article/pii/B9781597492164000069
ER  - 

TY  - CHAP
AU  - Contos, Brian T.
T1  - Chapter 14 - Addressing Both Insider Threats and Sarbanes-Oxley with ESM
A2  - Contos, Brian T. 
BT  - Enemy at the Water Cooler
PB  - Syngress
CY  - Burlington
PY  - 2007///
SP  - 179
EP  - 185
SN  - 978-1-59749-129-7
DO  - http://dx.doi.org/10.1016/B978-159749129-7/50019-9
UR  - http://www.sciencedirect.com/science/article/pii/B9781597491297500199
AB  - Publisher Summary
Currently, there are many regulations and control frameworks in existence that can be partially addressed with technology such as ESM. Security does not equal compliance and compliance does not equal security, but the two have enough overlap within ESM to address both in tandem. This is not true only for Sarbanes-Oxley, but extends to other forms of regulatory compliance. In addition, control frameworks such as COSO and COBIT can help define a security strategy before implementing those controls with the ESM environment. This would then lessen the time required to roll out a solution that addresses an organization's compliance needs, monitors insider threats, and provides general security.
ER  - 

TY  - CHAP
AU  - Laahs, Kevin
AU  - McKenna, Emer
AU  - Vanamo, Veli-Matti
T1  - 5 - Enterprise Content Management
A2  - Laahs, Kevin
A2  - McKenna, Emer 
A2  - Vanamo, Veli-Matti 
BT  - Microsoft SharePoint 2007 Technologies
PB  - Digital Press
CY  - Burlington
PY  - 2008///
SP  - 89
EP  - 113
SN  - 978-0-12-373616-1
DO  - http://dx.doi.org/10.1016/B978-012373616-1.50007-9
UR  - http://www.sciencedirect.com/science/article/pii/B9780123736161500079
AB  - Publisher Summary
This chapter explains Enterprise Content Management (ECM), one of the solution areas targeted by SharePoint. Although ECM is a fairly commonly used term, it’s generally quite difficult to come up with an elevator pitch that adequately covers everyone’s particular views of ECM. ECM is not a single application but a set of tools and technologies that allows one to manage content to achieve the business goals. These tools and technologies should be capable of managing the complete lifecycle of content, from its entry into the digital world to its exit, as well as maximizing the value of one’s content to the business. In SharePoint Server 2007 ECM tools and technologies primarily surface in four functional areas: document management, records management, Web content management, and electronic forms management. And all these functional areas leverage common services, such as storage, workflow, security, and search. This chapter also discusses some of those functional areas and highlights the ECM features that can help one manage the enterprise content appropriately. SharePoint Server 2007 provides a collection of features that address common scenarios that can be used to help support one’s specific compliance requirements. Whether these features are sufficient to meet the needs depends upon individual circumstances. However, SharePoint is a very flexible and extensible platform. Therefore, while it may not meet 100% of one’s compliance needs, it will more than likely go a long way toward achieving them. Understanding the available features is key in helping one map to whatever regulations are appropriate for the business.
ER  - 

TY  - CHAP
AU  - Dubrawsky, Ido
T1  - Chapter 9 - Risk Assessment and Risk Mitigation
A2  - Dubrawsky, Ido 
BT  - Eleventh Hour Security+
PB  - Syngress
CY  - Boston
PY  - 2010///
SP  - 127
EP  - 134
SN  - 978-1-59749-427-4
DO  - http://dx.doi.org/10.1016/B978-1-59749-427-4.00009-5
UR  - http://www.sciencedirect.com/science/article/pii/B9781597494274000095
AB  - Publisher Summary
This chapter focuses on the risk assessment, a tool that ensures clients and users internal needs are met in regards to security. Risk assessment and mitigation are important parts of the overall security process in an organization or enterprise. Risk assessments are typically carried out using a variety of tools such as vulnerability scanners, password crackers, and protocol analyzers. Password cracker run test the strength of user and administrator passwords in a network environment. Password crackers range from the old L0phtCrack to the open source John the Ripper to Elcomsoft's Password Auditor to rainbow tables. Protocol analyzers are a vital part of a network administrator's and security administrator's tool kit. Protocol analyzers can monitor the traffic on a network and expose data and protocols that are being passed along the wire. In addition, the results of the risk assessments should be reviewed in order to be able to identify critical risks as well as mitigation techniques that could reduce the overall exposure of the organization. Further, logging and auditing play critical roles in securing an organization's environment. System, performance, and access logs all provide key pieces of information regarding the day-to-day security of a network. Periodic auditing of logs such as these is critical to identify any flaws or the need to update standards and technical configurations.
ER  - 

TY  - CHAP
AU  - Tyson, Dave
T1  - 25 - A model for moving forward
A2  - Tyson, Dave 
BT  - Security Convergence
PB  - Butterworth-Heinemann
CY  - Burlington
PY  - 2007///
SP  - 201
EP  - 204
SN  - 978-0-7506-8425-5
DO  - http://dx.doi.org/10.1016/B978-075068425-5/50028-8
UR  - http://www.sciencedirect.com/science/article/pii/B9780750684255500288
AB  - Publisher Summary
There are many models which can be utilized in order to move ahead with a convergence vision. The creeping incrementalism approach optimizes benefits and change management with organizational culture. The opportunistic approach can be adopted that focuses on enhancing training and building infrastructure, and then engaging convergence projects only when the timing is right. The ability of security groups to fully leverage existing technology infrastructure for organizational benefits and cost savings is a key defining factor in an organization's strategic orientation. The initial integration points for security convergence exist at the strategic, tactical, policy, and operational levels. At the strategic level, focus is on integrated security strategy development and cost saving opportunities. This can come in the form of one enterprise security strategy that provides leadership for enterprise issues. At the tactical level, opportunities exist in merging risk assessment and investigation methodologies, as well as in developing specialty integrated training programs for more effective staff performance. At the policy level, the policy duplication can be minimized by integrating similar physical and IT security policy subjects into a single policy development.
ER  - 

TY  - JOUR
T1  - Privacy, trust and policy-making: Challenges and responses
JO  - Computer Law & Security Review
VL  - 25
IS  - 1
SP  - 69
EP  - 83
PY  - 2009///
T2  - 
AU  - Wright, David
AU  - Gutwirth, Serge
AU  - Friedewald, Michael
AU  - De Hert, Paul
AU  - Langheinrich, Marc
AU  - Moscibroda, Anna
SN  - 0267-3649
DO  - http://dx.doi.org/10.1016/j.clsr.2008.11.004
UR  - http://www.sciencedirect.com/science/article/pii/S0267364908001672
KW  - Ambient intelligence
KW  - Profiling
KW  - RFID
KW  - Data Protection
KW  - Privacy
KW  - Information Society Policies
AB  - The authors contend that the emerging ubiquitous Information Society (aka ambient intelligence, pervasive computing, ubiquitous networking and so on) will raise many privacy and trust issues that are context dependent. These issues will pose many challenges for policy-makers and stakeholders because people's notions of privacy and trust are different and shifting. People's attitudes towards privacy and protecting their personal data can vary significantly according to differing circumstances. In addition, notions of privacy and trust are changing over time. The authors provide numerous examples of the challenges facing policy-makers and identify some possible responses, but they see a need for improvements in the policy-making process in order to deal more effectively with varying contexts. They also identify some useful policy-making tools. They conclude that the broad brush policies of the past are not likely to be adequate to deal with the new challenges and that we are probably entering an era that will require development of “micro-policies”. While the new technologies will pose many challenges, perhaps the biggest challenge of all will be to ensure coherence of these micro-policies.
ER  - 

TY  - CHAP
AU  - Contos, Brian T.
AU  - Crowell, William P.
AU  - DeRodeff, Colby
AU  - Dunkel, Dan
AU  - Cole, Eric
AU  - McKenna, Regis
T1  - Chapter 8 - The New Security Model: The Trusted Enterprise
A2  - Contos, Brian T.
A2  - Crowell, William P.
A2  - DeRodeff, Colby
A2  - Dunkel, Dan
A2  - Cole, Eric 
A2  - McKenna, Regis 
BT  - Physical and Logical Security Convergence
PB  - Syngress
CY  - Burlington
PY  - 2007///
SP  - 225
EP  - 253
SN  - 978-1-59749-122-8
DO  - http://dx.doi.org/10.1016/B978-159749122-8.50012-0
UR  - http://www.sciencedirect.com/science/article/pii/B9781597491228500120
ER  - 

TY  - JOUR
T1  - Patient Privacy and Security of Electronic Medical Information for Radiologists: The Basics
JO  - Journal of the American College of Radiology
VL  - 7
IS  - 6
SP  - 397
EP  - 399
PY  - 2010/6//
T2  - 
AU  - Andriole, Katherine P.
AU  - Khorasani, Ramin
SN  - 1546-1440
DO  - http://dx.doi.org/10.1016/j.jacr.2010.03.001
UR  - http://www.sciencedirect.com/science/article/pii/S154614401000102X
ER  - 

TY  - CHAP
AU  - Joshi, James
AU  - Bagchi, Saurabh
AU  - Davie, Bruce S.
AU  - Farrel, Adrian
AU  - Foo, Bingrui
AU  - Garg, Vijay K.
AU  - Glause, Matthew W.
AU  - Modelo-Howard, Gaspar
AU  - Krishnamurthy, Prashant
AU  - Loshin, Pete
AU  - McCabe, James D.
AU  - Ni, Lionel M.
AU  - Peterson, Larry L.
AU  - Ramaswami, Rajiv
AU  - Sivarajan, Kumar N.
AU  - Spafford, Eugene H.
AU  - Varghese, George
AU  - Wu, Yu-Sung
AU  - Zheng, Pei
T1  - Chapter 8 - Mobile Security and Privacy
A2  - Joshi, James
A2  - Bagchi, Saurabh
A2  - Davie, Bruce S.
A2  - Farrel, Adrian
A2  - Foo, Bingrui
A2  - Garg, Vijay K.
A2  - Glause, Matthew W.
A2  - Modelo-Howard, Gaspar
A2  - Krishnamurthy, Prashant
A2  - Loshin, Pete
A2  - McCabe, James D.
A2  - Ni, Lionel M.
A2  - Peterson, Larry L.
A2  - Ramaswami, Rajiv
A2  - Sivarajan, Kumar N.
A2  - Spafford, Eugene H.
A2  - Varghese, George
A2  - Wu, Yu-Sung 
A2  - Zheng, Pei 
BT  - Network Security: Know It All
PB  - Morgan Kaufmann
CY  - Burlington
PY  - 2008///
SP  - 211
EP  - 261
SN  - 978-0-12-374463-0
DO  - http://dx.doi.org/10.1016/B978-0-12-374463-0.00008-4
UR  - http://www.sciencedirect.com/science/article/pii/B9780123744630000084
AB  - Publisher Summary
Mobile security and privacy are an interrelated issue that is addressed as a whole. This chapter explores a wide range of mobile security and privacy issues that present a big picture of this broad area, and offers some insight into the fundamental security problems surrounding the design of secured mobile wireless systems and applications. It explains a security primer summarizing a set of basic network security concepts and security schemes, followed by an in-depth coverage of security issues in cellular networks, wireless LAN, Bluetooth, and other emerging mobile wireless systems. A mobile wireless system must take security and privacy into account at the very beginning of the design phase and utilize appropriate security service building blocks to provide data confidentiality, integrity, authentication, and no repudiation, as well as efficient access control. Different mobile wireless systems and applications employ a set of security mechanisms at different layers, due to the intrinsic restrictions of the underlying network and mobile devices.
ER  - 

TY  - CHAP
AU  - Wright, Craig
T1  - Chapter 20 - Risk Management, Security Compliance, and Audit Controls
A2  - Wright, Craig 
BT  - The IT Regulatory and Standards Compliance Handbook
PB  - Syngress
CY  - Burlington
PY  - 2008///
SP  - 577
EP  - 607
SN  - 978-1-59749-266-9
DO  - http://dx.doi.org/10.1016/B978-1-59749-266-9.00020-5
UR  - http://www.sciencedirect.com/science/article/pii/B9781597492669000205
AB  - Publisher Summary
This chapter deals with the risk management, security compliance, and audit controls. Major methods of risk measurement and audit are discussed. One must understand the risk management process as a whole and how controls may be implemented to eliminate or mitigate the risk of individual events. Risk assessment is fundamental to the security of any organization. It is essential in ensuring that controls and expenditure are fully commensurate with the risks to which the organization is exposed. The risk analysis process is outlined that allows the organization to determine risk based on threats and vulnerabilities. The process of risk analysis has following steps: threat analysis, vulnerability analysis, business impact analysis, and likelihood analysis. The auditor with the help of risk analysis will be able to classify the severity of the risk and assign importance to each risk.
ER  - 

TY  - CHAP
AU  - Li, Feng
AU  - Williams, Howard
T1  - Chapter 9 - Interorganizational innovations through interorganizational information systems
A2  - Barnes, Stuart 
BT  - E-Commerce and V-Business (Second Edition)
PB  - Butterworth-Heinemann
CY  - Oxford
PY  - 2007///
SP  - 201
EP  - 218
SN  - 978-0-7506-6493-6
DO  - http://dx.doi.org/10.1016/B978-0-7506-6493-6.50012-5
UR  - http://www.sciencedirect.com/science/article/pii/B9780750664936500125
ER  - 

TY  - JOUR
T1  - Data protection: why are organisations still missing the point?
JO  - Computer Fraud & Security
VL  - 2008
IS  - 6
SP  - 5
EP  - 8
PY  - 2008/6//
T2  - 
AU  - Gorge, Mathieu
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(08)70095-2
UR  - http://www.sciencedirect.com/science/article/pii/S1361372308700952
AB  - Mathieu Gorge argues that data protection should be second nature for companies.
ER  - 

TY  - CHAP
AU  - Shinder, Thomas W.
T1  - Chapter 3 - Smart View Tracker
A2  - Shinder, Thomas W. 
BT  - The Best Damn Firewall Book Period (Second Edition)
PB  - Syngress
CY  - Burlington
PY  - 2007///
SP  - 61
EP  - 81
SN  - 978-1-59749-218-8
DO  - http://dx.doi.org/10.1016/B978-1-59749-218-8.00003-X
UR  - http://www.sciencedirect.com/science/article/pii/B978159749218800003X
AB  - Publisher Summary
SmartView Tracker is a tool that helps collect information about the traffic profiles, about the management changes, and shows a glimpse of what is currently happening on the firewall. SmartView Tracker also lets one parse the log data by providing baseline queries, executing custom queries, and applying the filter settings. This chapter discusses the need for tracking and presents a few configuration examples. SmartView Tracker enables the security administrator to visualize traffic that is passing by. Tracking users, connections, and administration through the logs is completely dependent on the gateway's configuration. To be able to audit rules, investigate misfeasors, or simply gather statistics, it is needed to enable logging for the items one cares to examine. The most utilized tab in SmartView Tracker is the Log tab. Here one can invoke predefined queries or custom queries to quickly reveal records one is looking for. These records may be pseudo-real-time or latent records one is searching through for connection information. The Active tab gives insight into what connections currently exist on the firewall. One great aspect of this tool is that one can block intruders and deny the connections based on several options. The downside of this tab is that it carries a potential performance hit, but in the case of an active attack, it can prove invaluable. The Audit tab reveals important accounting information regarding the management of the firewall. One can use this tool to investigate management incidents or audit the processes to verify one is meeting organizational policy guidelines and goals. SmartView Tracker is a comprehensive tool that not only displays our log files in an easily understood format, but also provides sufficient tools to further manipulate the information into productive views.
ER  - 

TY  - CHAP
AU  - Contos, Brian T.
T1  - Chapter 1 - Cyber Crime and Cyber Criminals 101
A2  - Contos, Brian T. 
BT  - Enemy at the Water Cooler
PB  - Syngress
CY  - Burlington
PY  - 2007///
SP  - 3
EP  - 47
SN  - 978-1-59749-129-7
DO  - http://dx.doi.org/10.1016/B978-159749129-7/50006-0
UR  - http://www.sciencedirect.com/science/article/pii/B9781597491297500060
AB  - Publisher Summary
This chapter provides background on the motives, markets, perpetrators, and techniques related to cyber crime. This chapter is a refresher on cyber criminals and their means of profit. The security threatscape has changed significantly. While the Internet was once a playground for government organizations, large businesses, and academic institutions, it has rapidly become an integral part of daily life for millions around the world. This has brought an increased number of characters to the cyber world—from spammers and identity thieves to online extortionists and exploitation-writers for hire. The specific reasons for cyber attacks differ but the motivations are the same as in traditional criminal activity.. Tracking cyber criminals as they interact in on-line black markets is difficult because the criminal can be virtually anywhere. In addition, the criminals operate anonymously and can turn their operations on and off rapidly. The number of threats is growing at an increasing rate. Techniques used by criminals are becoming more sophisticated, faster, harder to detect, and can be much more damaging than those of the past. A brief discussion of all the combinations of exploits, techniques and threats from port scans, Trojans horses, viruses and worms through buffer overflows, packet sniffing, and man-in-the-middle attacks is explained in the chapter.
ER  - 

TY  - CHAP

T1  - Index
A2  - Dubrawsky, Ido 
BT  - Security+ (Second Edition)
PB  - Syngress
CY  - Burlington
PY  - 2007///
SP  - 829
EP  - 842
SN  - 978-1-59749-153-2
DO  - http://dx.doi.org/10.1016/B978-159749153-2/50017-7
UR  - http://www.sciencedirect.com/science/article/pii/B9781597491532500177
ER  - 

TY  - JOUR
T1  - How secure is the next generation of IP-based emergency services architecture?
JO  - International Journal of Critical Infrastructure Protection
VL  - 3
IS  - 1
SP  - 41
EP  - 50
PY  - 2010/5//
T2  - 
AU  - Tschofenig, Hannes
AU  - Arumaithurai, Mayutan
AU  - Schulzrinne, Henning
AU  - Aboba, Bernard
SN  - 1874-5482
DO  - http://dx.doi.org/10.1016/j.ijcip.2010.02.001
UR  - http://www.sciencedirect.com/science/article/pii/S1874548210000028
KW  - Emergency services architecture
KW  - Ecrit
AB  - For some location-based applications, such as emergency calling or roadside assistance, it appears that the identity of the requester is less important than accurate and trustworthy location information for accomplishing the main function. Accurate and genuine location is important for these applications to avoid misuse.

In this paper we point to some ongoing efforts regarding transition emergency service architectures that could introduce security vulnerabilities unless countermeasures are developed. Furthermore, we summarize the ongoing work in providing cryptographic assertions for location.

We argue that many of the currently proposed ideas are difficult to deploy and to operate. Additionally, when used without ensuring that the underlying assumptions are met these mechanisms do not provide any additional benefit, but costs.

We conclude this article with a suggestion on what the research community and industry should be investigating to avoid potential problems with IP-based emergency services.
ER  - 

TY  - JOUR
T1  - Learning to love SIEM
JO  - Network Security
VL  - 2011
IS  - 4
SP  - 18
EP  - 19
PY  - 2011/4//
T2  - 
AU  - Jenkins, Steve
SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/S1353-4858(11)70041-1
UR  - http://www.sciencedirect.com/science/article/pii/S1353485811700411
AB  - In the 1964 motion picture, Dr Strangelove or: How I Learned to Stop Worrying and Love the Bomb, a paranoid general played by Sterling Hayden is able to hack into a system and initiate a nuclear attack on the Soviet Union without the knowledge of his superiors.
ER  - 

TY  - JOUR
T1  - Model-driven business process security requirement specification
JO  - Journal of Systems Architecture
VL  - 55
IS  - 4
SP  - 211
EP  - 223
PY  - 2009/4//
T2  - Secure Service-Oriented Architectures (Special Issue on Secure SOA)
AU  - Wolter, Christian
AU  - Menzel, Michael
AU  - Schaad, Andreas
AU  - Miseldine, Philip
AU  - Meinel, Christoph
SN  - 1383-7621
DO  - http://dx.doi.org/10.1016/j.sysarc.2008.10.002
UR  - http://www.sciencedirect.com/science/article/pii/S1383762108001471
KW  - Web service security
KW  - Business process
KW  - Model transformation
KW  - Security annotations
KW  - Access control
AB  - Various types of security goals, such as authentication or confidentiality, can be defined as policies for service-oriented architectures, typically in a manual fashion. Therefore, we foster a model-driven transformation approach from modelled security goals in the context of process models to concrete security implementations. We argue that specific types of security goals may be expressed in a graphical fashion at the business process modelling level which in turn can be transformed into corresponding access control and security policies. In this paper we present security policy and policy constraint models. We further discuss a translation of security annotated business processes into platform specific target languages, such as XACML or AXIS2 security configurations. To demonstrate the suitability of this approach an example transformation is presented based on an annotated process.
ER  - 

TY  - JOUR
T1  - Learning relational policies from electronic health record access logs
JO  - Journal of Biomedical Informatics
VL  - 44
IS  - 2
SP  - 333
EP  - 342
PY  - 2011/4//
T2  - 
AU  - Malin, Bradley
AU  - Nyemba, Steve
AU  - Paulett, John
SN  - 1532-0464
DO  - http://dx.doi.org/10.1016/j.jbi.2011.01.007
UR  - http://www.sciencedirect.com/science/article/pii/S1532046411000098
KW  - Electronic health records
KW  - Organizational behavior
KW  - Knowledge discovery
KW  - Access logs
KW  - Auditing
AB  - Modern healthcare organizations (HCOs) are composed of complex dynamic teams to ensure clinical operations are executed in a quick and competent manner. At the same time, the fluid nature of such environments hinders administrators’ efforts to define access control policies that appropriately balance patient privacy and healthcare functions. Manual efforts to define these policies are labor-intensive and error-prone, often resulting in systems that endow certain care providers with overly broad access to patients’ medical records while restricting other providers from legitimate and timely use. In this work, we propose an alternative method to generate these policies by automatically mining usage patterns from electronic health record (EHR) systems. EHR systems are increasingly being integrated into clinical environments and our approach is designed to be generalizable across HCOs, thus assisting in the design and evaluation of local access control policies. Our technique, which is grounded in data mining and social network analysis theory, extracts a statistical model of the organization from the access logs of its EHRs. In doing so, our approach enables the review of predefined policies, as well as the discovery of unknown behaviors. We evaluate our approach with 5 months of access logs from the Vanderbilt University Medical Center and confirm the existence of stable social structures and intuitive business operations. Additionally, we demonstrate that there is significant turnover in the interactions between users in the HCO and that policies learned at the department-level afford greater stability over time.
ER  - 

TY  - JOUR
T1  - Security and performance in service-oriented applications: Trading off competing objectives
JO  - Decision Support Systems
VL  - 50
IS  - 1
SP  - 336
EP  - 346
PY  - 2010/12//
T2  - 
AU  - Zo, Hangjung
AU  - Nazareth, Derek L.
AU  - Jain, Hemant K.
SN  - 0167-9236
DO  - http://dx.doi.org/10.1016/j.dss.2010.09.002
UR  - http://www.sciencedirect.com/science/article/pii/S0167923610001715
KW  - Service-oriented computing
KW  - Application composition
KW  - Performance
KW  - Security
KW  - Multiple criteria decision making
AB  - As service-oriented computing becomes more prevalent, an increasing number of applications will be developed using existing software components with standard interfaces. These components may be developed in-house, may represent purchased software, or may involve vendor located leased services. The use of multiple services, possibly utilizing different technologies and different sources, has significant implications for the performance and security of these applications to support a business process effectively. Estimating performance and security in this distributed environment is a hard problem. This paper examines how performance and security measures can be developed for service-based applications. Business processes are broken down into constituent tasks and a formal mechanism is developed for deriving performance and security measures for the application. Given the competing nature of these two objectives, a tradeoff strategy is utilized wherein managers can trade improved performance for reduced security or vice versa. As the number of alternative services for each task increases, the composition problem becomes combinatorially explosive. A genetic algorithm approach is adopted to find the Pareto optimal set of services that can be assembled to support the business process. An application to a real-world business process illustrates its effectiveness.
ER  - 

TY  - CHAP

T1  - Index
A2  - Contos, Brian T. 
BT  - Enemy at the Water Cooler
PB  - Syngress
CY  - Burlington
PY  - 2007///
SP  - 253
EP  - 262
SN  - 978-1-59749-129-7
DO  - http://dx.doi.org/10.1016/B978-159749129-7/50024-2
UR  - http://www.sciencedirect.com/science/article/pii/B9781597491297500242
ER  - 

TY  - CHAP

T1  - Appendix J - Glossary
A2  - Vacca, John R. 
BT  - Computer and Information Security Handbook
PB  - Morgan Kaufmann
CY  - Boston
PY  - 2009///
SP  - 801
EP  - 815
SN  - 978-0-12-374354-1
DO  - http://dx.doi.org/10.1016/B978-0-12-374354-1.00059-5
UR  - http://www.sciencedirect.com/science/article/pii/B9780123743541000595
ER  - 

TY  - CHAP
AU  - Contos, Brian T.
AU  - Crowell, William P.
AU  - DeRodeff, Colby
AU  - Dunkel, Dan
AU  - Cole, Eric
AU  - McKenna, Regis
T1  - Chapter 15 - Intelligent Video Analytics
A2  - Contos, Brian T.
A2  - Crowell, William P.
A2  - DeRodeff, Colby
A2  - Dunkel, Dan
A2  - Cole, Eric 
A2  - McKenna, Regis 
BT  - Physical and Logical Security Convergence
PB  - Syngress
CY  - Burlington
PY  - 2007///
SP  - 445
EP  - 479
SN  - 978-1-59749-122-8
DO  - http://dx.doi.org/10.1016/B978-159749122-8.50019-3
UR  - http://www.sciencedirect.com/science/article/pii/B9781597491228500193
AB  - This chapter focuses on the benefits of integrating video surveillance and monitoring (VSAM) technology with an Enterprise Security Management (ESM) platform. The use cases we describe in this chapter focus on an attempted compromise of a critical data center server residing behind the walls of a secure campus. For the purposes of detail, we are dealing with a large fabrication facility, known as ArcNet, which produces computer components. Instead of just detecting the perpetrator sneaking into the data center and attempting to compromise a system, we will examine the entire scenario—from the perimeter breach, to a tailgating violation, to the perpertrator gaining access to not only the facility, but also the secure data center. Once inside the data center, the attacker accesses a terminal and proceeds to guess administrator and root passwords on multiple systems. This is known as a brute force login attempt. The attacker continues to guess passwords that seem obvious, hoping to get lucky (granted, this is a simple atrack, but you may be surprised how many systems are left with default user accounts and passwords). the use case focuses on detection, as well as on a closed-loop process that identifies the perpetrator, automatically takes as snapshot, and adds the resulting picture to a case or trouble ticket as part of the workflow process. The integration with VSAM in this use case is bidirectional. Not only does the video analytics software forward alerts to the ESM platform, but also the ESM system communicates back to the analytics application, instructing it to take a snapshot at the time a violation is detected.
ER  - 

TY  - JOUR
T1  - Turning log files into a security asset
JO  - Network Security
VL  - 2008
IS  - 2
SP  - 4
EP  - 7
PY  - 2008/2//
T2  - 
AU  - Casey, Donal
SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/S1353-4858(08)70016-3
UR  - http://www.sciencedirect.com/science/article/pii/S1353485808700163
AB  - Despite most businesses having a range of devices which create log files of user and system activity, few actually analyse the logs for information.

As the internet becomes an integral part of the way we live and work, the number of logs generated and their importance has continued to increase. More homes now have broadband connections. Most businesses at least have email and many have an interactive web presence, or at the very least a static web ‘brochure’. All of the internet traffic these generate has the capacity to create useful information for marketers, and more importantly for security engineers, in the form of log entries.
ER  - 

TY  - CHAP

T1  - Appendix A - Validate Existing Security Infrastructure
A2  - Porter, Thomas  
A2  - Gough, Michael 
BT  - How to Cheat at VoIP Security
PB  - Syngress
CY  - Burlington
PY  - 2007///
SP  - 285
EP  - 322
T2  - How to Cheat
SN  - 978-1-59749-169-3
DO  - http://dx.doi.org/10.1016/B978-159749169-3/50013-X
UR  - http://www.sciencedirect.com/science/article/pii/B978159749169350013X
ER  - 

TY  - CHAP

T1  - Appendix eL - Glossary
A2  - Vacca, John R. 
BT  - Computer and Information Security Handbook (Second Edition)
PB  - Morgan Kaufmann
CY  - Boston
PY  - 2013///
SP  - e245
EP  - e263
SN  - 978-0-12-394397-2
DO  - http://dx.doi.org/10.1016/B978-0-12-394397-2.00080-5
UR  - http://www.sciencedirect.com/science/article/pii/B9780123943972000805
ER  - 

TY  - CHAP

T1  - Concluding Remarks
A2  - Critchlow, Zoé LacroixTerence 
BT  - Bioinformatics
PB  - Morgan Kaufmann
CY  - Burlington
PY  - 2003///
SP  - 393
EP  - 396
T2  - The Morgan Kaufmann Series in Multimedia Information and Systems
SN  - 978-1-55860-829-0
DO  - http://dx.doi.org/10.1016/B978-155860829-0/50016-4
UR  - http://www.sciencedirect.com/science/article/pii/B9781558608290500164
ER  - 

TY  - CHAP
AU  - Jacobs, Josh
AU  - Clemmer, Lee
AU  - Dalton, Michael
AU  - Rogers, Russ
AU  - Posluns, Jeffrey
T1  - Chapter 2 - Access Controls
A2  - Jacobs, Josh
A2  - Clemmer, Lee
A2  - Dalton, Michael 
A2  - Posluns, Russ RogersJeffrey 
BT  - SSCP Study Guide and DVD Training System
PB  - Syngress
CY  - Burlington
PY  - 2003///
SP  - 29
EP  - 100
SN  - 978-1-931836-80-7
DO  - http://dx.doi.org/10.1016/B978-193183680-7/50007-2
UR  - http://www.sciencedirect.com/science/article/pii/B9781931836807500072
AB  - Publisher Summary
This chapter explores the access controls of the Systems Security Certified Practitioner exam. Access control encompasses the security controls, processes, or procedures whereby access to specific objects is either granted or denied based on pre-established policies or rules. Access control is made up of many different parts but at its roots is a very simple concept—that is, to allow objects to be accessed (limiting the manner in which they are accessed) by authorized users while denying access to unauthorized users. Access controls are understood by breaking them into individual parts. First, there are the objects that need to be accessed. These objects are referred to as access control objects because they are objects that need to have controlled access. Objects consist of data, hardware devices, data networks, and buildings. Another part of access control are access control subjects that are the users, programs, and processes that request permission to access control objects. The final part of access control, called the access control systems, is the procedures, processes, and controls in place that verify the authenticity of the request and the identity of the access control subject and determines the levels of access that should be granted to the object. This chapter describes the three parts of access control and how they work together. The chapter also discusses different access control systems, how they are implemented, and how they operate. Finally, the chapter examines the dark side of information security by showing how these controls can be bypassed or overridden by intruders.
ER  - 

TY  - JOUR
T1  - A latent class modeling approach to detect network intrusion
JO  - Computer Communications
VL  - 30
IS  - 1
SP  - 93
EP  - 100
PY  - 2006/12/15/
T2  - 
AU  - Wang, Yun
AU  - Kim, Inyoung
AU  - Mbateng, Gaston
AU  - Ho, Shih-Yieh
SN  - 0140-3664
DO  - http://dx.doi.org/10.1016/j.comcom.2006.07.018
UR  - http://www.sciencedirect.com/science/article/pii/S0140366406002891
KW  - Intrusion detection
KW  - Machine learning
KW  - Classification
KW  - Latent class model
KW  - Computer security
AB  - This study presents a latent class modeling approach to examine network traffic data when labeled abnormal events are absent in training data, or such events are insufficient to fit a conventional regression model. Using six anomaly-associated risk factors identified from previous studies, the latent class model based on an unlabeled sample yielded acceptable classification results compared with a logistic regression model based on a labeled sample (correctly classified: 0.95 vs. 0.98, sensitivity: 0.99 vs. 0.99, and specificity: 0.77 vs. 0.97). The study demonstrates a great potency for using the latent class modeling technique to analyze network traffic data.
ER  - 

TY  - CHAP
AU  - Stephens, Robert
AU  - Stiefel, Barry J.
AU  - Watkins, Stephen
AU  - Desmeules, Simon
AU  - Faskha, Eli
T1  - Chapter 6 - SmartView Tracker
A2  - Stephens, Robert
A2  - Stiefel, Barry J.
A2  - Watkins, Stephen
A2  - Desmeules, Simon 
A2  - Faskha, Eli 
BT  - Configuring Check Point NGX VPN-1/Firewall-1
PB  - Syngress
CY  - Burlington
PY  - 2005///
SP  - 149
EP  - 180
SN  - 978-1-59749-031-3
DO  - http://dx.doi.org/10.1016/B978-159749031-3/50010-7
UR  - http://www.sciencedirect.com/science/article/pii/B9781597490313500107
AB  - Publisher Summary
SmartView Tracker is a tool that helps collect information about one's traffic profiles, about management changes, and shows a glimpse of what is currently happening on the firewall. SmartView Tracker also lets a user parse the log data by providing baseline queries, executing custom queries, and applying filter settings. The most utilized tab in SmartView Tracker is the “Log tab”; it can invoke predefined queries or custom queries to quickly reveal records the user is looking for. These records may be pseudo-real-time or latent records one searches through for connection information. The “Active tab” gives insight into what connections currently exist on the firewall. One great aspect of this tool is that it can block intruders and deny connections based on several options. The downside of this tab is that it carries a potential performance hit, but in the case of an active attack, it can prove invaluable. The “Audit tab” reveals important accounting information regarding the management of the firewall. This tool can also be used to investigate management incidents or audit processes to verify the meeting of organizational policy guidelines and/or goals. A firewall administrator needs to realize the value of logs. The user needs to learn configuration of his to produce meaningful data and then to massage these resultant logs to quickly divulge the information that he seeks.
ER  - 

TY  - CHAP

T1  - Chapter 2 - Securing Solaris with the Bundled Security Tools
A2  - Miles, Wyman
A2  - Mitchell, Ed 
A2  - Lynch, F. William 
BT  - Hack Proofing Sun Solaris 8
PB  - Syngress
CY  - Rockland
PY  - 2001///
SP  - 33
EP  - 65
T2  - The Only Way to Stop a Hacker Is to Think Like One
SN  - 978-1-928994-44-2
DO  - http://dx.doi.org/10.1016/B978-192899444-2/50006-0
UR  - http://www.sciencedirect.com/science/article/pii/B9781928994442500060
AB  - Publisher Summary
This chapter attempts to give a brief overview of security from the perspective of its current implementation in the Solaris 8 Operating Environment (OE). It explains how that implementation fits into the overall history of system security as it has developed. In addition to that overview, it provides a brief description of some of the differences Trusted Solaris 8 OE provides in a much more robust level of system security by default. Finally, the chapter discusses a sampling of supplemental security packages available to augment and enhance security under the Solaris 80E. The classifications of Orange Book standards are familiar to most experienced system administrators on some level.
ER  - 

TY  - CHAP
AU  - Jacobs, Josh
AU  - Clemmer, Lee
AU  - Dalton, Michael
AU  - Rogers, Russ
AU  - Posluns, Jeffrey
T1  - Chapter 3 - Administration
A2  - Jacobs, Josh
A2  - Clemmer, Lee
A2  - Dalton, Michael 
A2  - Posluns, Russ RogersJeffrey 
BT  - SSCP Study Guide and DVD Training System
PB  - Syngress
CY  - Burlington
PY  - 2003///
SP  - 101
EP  - 174
SN  - 978-1-931836-80-7
DO  - http://dx.doi.org/10.1016/B978-193183680-7/50008-4
UR  - http://www.sciencedirect.com/science/article/pii/B9781931836807500084
AB  - Publisher Summary
This chapter deals with Systems Security Certified Practitioner (SSCP) exam administration. The administration area encompasses the security principles, policies, standards, and guidelines used to identify, classify, and ensure the confidentiality, integrity, and availability of an organization's information assets. The administration also includes roles and responsibilities, configuration management, change control, security awareness, and the application of accepted industry practices. The topics covered in this chapter are some of the most common topics within the computer security industry that form the basis for what security professionals do all around the world. Access control, information classification, risk assessment and mitigation, and the change management process are all pieces of the puzzle that are put together in this chapter. In many respects, these topics form the basis for the rest of the SSCP common body of knowledge. This chapter provides information on the risk assessment process and how to develop quality recommendations for risk mitigation that take the organizational constraints; the concepts of confidentiality, integrity, and availability; and other considerations into account. In addition, the chapter introduces some forms of malicious code that have wreaked havoc on organizations connected to the Internet for at least the last 10 years.
ER  - 

TY  - JOUR
T1  - Distributed component architectures security issues
JO  - Computer Standards & Interfaces
VL  - 27
IS  - 3
SP  - 269
EP  - 284
PY  - 2005/3//
T2  - 
AU  - Gousios, Giorgos
AU  - Aivaloglou, Efthimia
AU  - Gritzalis, Stefanos
SN  - 0920-5489
DO  - http://dx.doi.org/10.1016/j.csi.2004.08.003
UR  - http://www.sciencedirect.com/science/article/pii/S0920548904000984
KW  - Components
KW  - Component architectures security
KW  - CORBA
KW  - J2EE
KW  - .NET
AB  - Enterprise information systems and e-commerce applications are tightly integrated in today's modern enterprises. Component architectures are the base for building such multitier distributed applications. This paper examines the security threats those systems must confront and the solutions proposed by major existing component architectures. A comparative evaluation of both security features and implementation issues is carried out to determine each architecture's strong points and drawbacks.
ER  - 

TY  - JOUR
T1  - An Informatics Blueprint for Healthcare Quality Information Systems
JO  - Journal of the American Medical Informatics Association
VL  - 13
IS  - 4
SP  - 402
EP  - 417
PY  - 2006/7//
Y2  - 2006/8//
T2  - 
AU  - Niland, Joyce C.
AU  - Rouse, Layla
AU  - Stahl, Douglas C.
SN  - 1067-5027
DO  - http://dx.doi.org/10.1197/jamia.M2050
UR  - http://www.sciencedirect.com/science/article/pii/S1067502706000624
AB  - There is a critical gap in our nation’s ability to accurately measure and manage the quality of medical care. A robust healthcare quality information system (HQIS) has the potential to address this deficiency through the capture, codification, and analysis of information about patient treatments and related outcomes. Because non-technical issues often present the greatest challenges, this paper provides an overview of these socio-technical issues in building a successful HQIS, including the human, organizational, and knowledge management (KM) perspectives. Through an extensive literature review and direct experience in building a practical HQIS (the National Comprehensive Cancer Network Outcomes Research Database system), we have formulated an “informatics blueprint” to guide the development of such systems. While the blueprint was developed to facilitate healthcare quality information collection, management, analysis, and reporting, the concepts and advice provided may be extensible to the development of other types of clinical research information systems.
ER  - 

TY  - CHAP
AU  - Rogers, Russ
AU  - Miles, Greg
AU  - Fuller, Ed
AU  - Dykstra, Ted
AU  - Hoagberg, Matthew
T1  - Chapter 8 - Managing the Findings
A2  - Rogers, Russ
A2  - Miles, Greg
A2  - Fuller, Ed
A2  - Dykstra, Ted 
A2  - Hoagberg, Matthew 
BT  - Security Assessment
PB  - Syngress
CY  - Burlington
PY  - 2004///
SP  - 269
EP  - 308
SN  - 978-1-932266-96-2
DO  - http://dx.doi.org/10.1016/B978-193226696-2/50026-3
UR  - http://www.sciencedirect.com/science/article/pii/B9781932266962500263
AB  - Publisher Summary
The chapter discusses how findings can be positive or negative. As an assessor, one should always keep eyes open to positive findings and be willing to point out the good things that are going on in an organization. Throughout the process, the assessment team collects information and identifies possible vulnerabilities or weaknesses of the customer's system. After that, validation is done, i.e., validating the information. Validation is not a process of taking the word of every interviewee or believing that what the documentation says is actually occurring within an organization. What is more important is to be able to show proof or hard evidence of what is actually occurring within the organization. To do that, there are two options: demonstration and evaluation. The chapter presents a case study to give an example that how this information fits into the real world of information security assessments.
ER  - 

TY  - JOUR
T1  - Principles and requirements for a secure e-voting system
JO  - Computers & Security
VL  - 21
IS  - 6
SP  - 539
EP  - 556
PY  - 2002/10/1/
T2  - 
AU  - Gritzalis, Dimitris A
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/S0167-4048(02)01014-3
UR  - http://www.sciencedirect.com/science/article/pii/S0167404802010143
KW  - Electronic voting
KW  - Secure voting
KW  - Digital divide
KW  - Functional requirements
KW  - Rational Unified Process
KW  - Use cases
AB  - Electronic voting (e-voting) is considered a means to further enhance and strengthen the democratic processes in modern information societies. E-voting should first comply with the existing legal and regulatory framework. Moreover, e-voting should be technically implemented in such a way that ensures adequate user requirements. As a result, the aim of this paper is twofold. Firstly, to identify the set of generic constitutional requirements, which should be met when designing an e-voting system for general elections. This set will lead to the specific (design) principles of a legally acceptable e-voting system. Second, to identify, using the Rational Unified Process, the requirements of an adequately secure e-voting system. These requirements stem from the design principles identified previously. The paper concludes that an e-voting capability should, for the time being, be considered only as a complementary means to the traditional election processes. This is mainly due to the digital divide, to the inherent distrust in the e-voting procedure, as well as to the inadequacy of the existing technological means to meet certain requirements.
ER  - 

TY  - CHAP
AU  - Jacobs, Josh
AU  - Clemmer, Lee
AU  - Dalton, Michael
AU  - Rogers, Russ
AU  - Posluns, Jeffrey
T1  - Chapter 4 - Audit and Monitoring
A2  - Jacobs, Josh
A2  - Clemmer, Lee
A2  - Dalton, Michael 
A2  - Posluns, Russ RogersJeffrey 
BT  - SSCP Study Guide and DVD Training System
PB  - Syngress
CY  - Burlington
PY  - 2003///
SP  - 175
EP  - 228
SN  - 978-1-931836-80-7
DO  - http://dx.doi.org/10.1016/B978-193183680-7/50009-6
UR  - http://www.sciencedirect.com/science/article/pii/B9781931836807500096
AB  - Publisher Summary
This chapter explores the concept of monitoring of Systems Security Certified Practitioner (SSCP) exam. Auditing is the process by which one can ensure that a specific system, process, mechanism, or function meets a defined list of criteria. As far as monitoring of SSCP is concerned, the monitoring area includes those mechanisms, tools, and facilities used to identify, classify, prioritize, respond, and report on security events and vulnerabilities. The audit function provides the ability to determine if the system is being operated in accordance with accepted industry practices and in compliance with specific organizational policies, standards, and procedures. The logging features provided on most networks and systems involve the logging of known or partially known resource event activities. While these logs are sometimes used for analyzing system problems, they are also useful for those whose duty is to process the log files and check for both valid and invalid system activities. To assist in catching mistakes and reduce the likelihood of fraudulent activities, the activities of a process should be split among several people.
ER  - 

TY  - JOUR
T1  - HIPAA security regulations: Protecting patients' electronic health information
JO  - The Journal of the American Dental Association
VL  - 134
IS  - 5
SP  - 640
EP  - 643
PY  - 2003/5//
T2  - 
AU  - SFIKAS, PETER M.
SN  - 0002-8177
DO  - http://dx.doi.org/10.14219/jada.archive.2003.0234
UR  - http://www.sciencedirect.com/science/article/pii/S000281771464157X
ER  - 

TY  - JOUR
T1  - A Formal Approach to HIPAA Risk Analysis in the Optometric Practice
JO  - Optometry - Journal of the American Optometric Association
VL  - 76
IS  - 1
SP  - 55
EP  - 61
PY  - 2005/1//
T2  - 
AU  - Pieper, Bob
SN  - 1529-1839
DO  - http://dx.doi.org/10.1016/S1529-1839(05)70255-4
UR  - http://www.sciencedirect.com/science/article/pii/S1529183905702554
ER  - 

TY  - CHAP
AU  - Rogers, Russ
AU  - Miles, Greg
AU  - Fuller, Ed
AU  - Dykstra, Ted
AU  - Hoagberg, Matthew
T1  - Chapter 3 - Determining the Organization's Information Criticality
A2  - Rogers, Russ
A2  - Miles, Greg
A2  - Fuller, Ed
A2  - Dykstra, Ted 
A2  - Hoagberg, Matthew 
BT  - Security Assessment
PB  - Syngress
CY  - Burlington
PY  - 2004///
SP  - 81
EP  - 118
SN  - 978-1-932266-96-2
DO  - http://dx.doi.org/10.1016/B978-193226696-2/50021-4
UR  - http://www.sciencedirect.com/science/article/pii/B9781932266962500214
AB  - Publisher Summary
The chapter covers the basic activities that must be accomplished to complete the Organizational Information Criticality Matrix (OICM). The OICM is based on customer decisions about the information types within their own organization that are critical for the completion of their mission and meeting organizational goals. The activities cover in the chapter includes identifying the critical information at the customer organization, identifying the mission of the customer organization, creating impact definitions, creating the OICM, and determining the high-water mark for the OICM. The definition of organizational information criticality is one of the primary milestones in the pre-assessment phase. Defining an organization's information criticality is one of the most important steps in the Assessment Methodology process. This process gives the customer a clear understanding of how their organization operates and what information should be protected. These activities typically represent the first in-depth interaction between the assessment team and the customer.
ER  - 

TY  - JOUR
T1  - Development of Information Security Baselines for Healthcare Information Systems in New Zealand
JO  - Computers & Security
VL  - 21
IS  - 2
SP  - 172
EP  - 192
PY  - 2002/3/31/
T2  - 
AU  - Janczewski, Lech
AU  - Xinli Shi, Frank
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/S0167-4048(02)00212-2
UR  - http://www.sciencedirect.com/science/article/pii/S0167404802002122
KW  - healthcare information systems
KW  - electronic medical records
KW  - information privacy
KW  - information security baselines
KW  - security model
AB  - In 1996 New Zealand had introduced security standard AS/NZCS 4444 based on the British Standard BS 7799, which has recently been accepted as an international standard ISO 17799. This standard is very often referred to as the ‘baseline lane approach’ to the issue of managing information security. On the other hand the health information systems (HIS) are undergoing rapid development both in the number of installed systems as in the law and regulations governing HIS developments and deployment. The project was aimed at reviewing the AS/NZCS 4444 standard from the HIS requirements point of view. In this paper, we began with an overview of healthcare information systems (HIS) infrastructure in New Zealand and associated security issues around privacy and confidentiality, followed by a general review of the security baseline approach. We analyzed each clause of the AS/NZS 4444 with the information collected about technical and non-technical approaches to protecting HIS, consisting of a series of multi-case studies of healthcare organizations that collect, process, store and transmit electronic medical records. Finally, we proposed a new set of information security baselines based on the research to build an information security model for healthcare organizations.
ER  - 

TY  - JOUR
T1  - Security for eBusiness
JO  - Information Security Technical Report
VL  - 6
IS  - 2
SP  - 80
EP  - 94
PY  - 2001/6/1/
T2  - 
AU  - Davidson, Mary Ann
SN  - 1363-4127
DO  - http://dx.doi.org/10.1016/S1363-4127(01)00209-6
UR  - http://www.sciencedirect.com/science/article/pii/S1363412701002096
ER  - 

TY  - CHAP

T1  - C - Organizational Security Management
A2  - Rittinghouse, John W.  
A2  - Ransome, James F. 
BT  - Business Continuity and Disaster Recovery for InfoSec Managers
PB  - Digital Press
CY  - Burlington
PY  - 2006///
SP  - 295
EP  - 322
SN  - 978-1-55558-339-2
DO  - http://dx.doi.org/10.1016/B978-155558339-2/50015-2
UR  - http://www.sciencedirect.com/science/article/pii/B9781555583392500152
ER  - 

TY  - CHAP
AU  - Rittinghouse, John W.
AU  - Hancock, William M.
T1  - 15 - Security-Management Issues
A2  - Rittinghouse, John W.  
A2  - Hancock, William M. 
BT  - Cybersecurity Operations Handbook
PB  - Digital Press
CY  - Burlington
PY  - 2004///
SP  - 493
EP  - 523
SN  - 978-1-55558-306-4
DO  - http://dx.doi.org/10.1016/B978-155558306-4/50020-2
UR  - http://www.sciencedirect.com/science/article/pii/B9781555583064500202
AB  - Publisher Summary
This chapter discusses what is required to put together an effective security function in an organization. Security managers must cope daily with the possibility that electronic information could be lost, corrupted, diverted, or misused. These types of issues represent a real threat to an organization's business performance. Management of a security function requires planning and a deep understanding of the concept of risk management. The interface among the CSO, HR, and legal counsel cannot be emphasized enough. Their partnership is key to successful implementation of a site security plan. The basic precepts of security, such as incident response, forensics, training and awareness, perimeter security measures, intrusion detection, and secure remote access, are discussed in terms of establishing functions devoted to those functional areas. Policy development and the role such policies play in an organization's risk management and site security plans are also covered in the chapter.
ER  - 

TY  - CHAP

T1  - Chapter 7 - IT Infrastructure Security Plan
A2  - Henmi, Anne 
BT  - Firewall Policies and VPN Configurations
PB  - Syngress
CY  - Burlington
PY  - 2006///
SP  - 307
EP  - 383
SN  - 978-1-59749-088-7
DO  - http://dx.doi.org/10.1016/B978-159749088-7/50009-8
UR  - http://www.sciencedirect.com/science/article/pii/B9781597490887500098
ER  - 

TY  - CHAP
AU  - Knipp, Eric
AU  - Browne, Brian
AU  - Weaver, Woody
AU  - Baumrucker, C. Tate
AU  - Chaffin, Larry
AU  - Caesar, Jamie
AU  - Osipov, Vitaly
AU  - Danielyan, Edgar
T1  - Chapter 15 - Looking Ahead: Cisco Wireless Security
A2  - Knipp, Eric
A2  - Browne, Brian
A2  - Weaver, Woody
A2  - Baumrucker, C. Tate
A2  - Chaffin, Larry
A2  - Caesar, Jamie
A2  - Osipov, Vitaly 
A2  - Danielyan, Edgar 
BT  - Managing Cisco Network Security (Second Edition)
PB  - Syngress
CY  - Burlington
PY  - 2002///
SP  - 649
EP  - 720
SN  - 978-1-931836-56-2
DO  - http://dx.doi.org/10.1016/B978-193183656-2/50019-2
UR  - http://www.sciencedirect.com/science/article/pii/B9781931836562500192
ER  - 

TY  - CHAP

T1  - C - Security Management Issues
A2  - Rittinghouse, John W.  
A2  - Ransome, James F. 
BT  - IM Instant Messaging Security
PB  - Digital Press
CY  - Burlington
PY  - 2005///
SP  - 271
EP  - 310
SN  - 978-1-55558-338-5
DO  - http://dx.doi.org/10.1016/B978-155558338-5/50013-8
UR  - http://www.sciencedirect.com/science/article/pii/B9781555583385500138
ER  - 

TY  - CHAP

T1  - Chapter 8 - Cisco Wireless Security
A2  - Ouellet, Eric
A2  - Padjen, Robert 
A2  - Blankenship, Arthur PfundRon FullerTim 
BT  - Building A Cisco Wireless LAN
PB  - Syngress
CY  - Rockland
PY  - 2002///
SP  - 375
EP  - 446
SN  - 978-1-928994-58-9
DO  - http://dx.doi.org/10.1016/B978-192899458-9/50011-2
UR  - http://www.sciencedirect.com/science/article/pii/B9781928994589500112
AB  - Publisher Summary
Security flaws are identified and new methods of exploiting these flaws are published regularly. Utilizing security fundamentals developed over the last few decades, one can review and protect the wireless networks from known and unknown threats. This chapter recapitulates security fundamentals and principles that are the foundation of any good security strategy, addressing a range of issues from authentication and authorization, to controls and audit. Common security standards, alongside the emerging privacy standards and their implications for the wireless exchange of information, are addressed. This chapter also examines how one can maximize the features of existing security standards like wired equivalent protocol (WEP). The effectiveness of media access control (MAC) and protocol filtering as a way of minimizing opportunity is also examined. This chapter presents the security advantages of using virtual private networks (VPNs) on a wireless network and discusses the importance of convincing users of the role they can play as key users of the network. The existing and anticipated threats to wireless networks, and the principles of protection that are fundamental to a wireless security strategy are also discussed.
ER  - 

TY  - JOUR
T1  - Security implications of implementing active network infrastructures using agent technology
JO  - Computer Networks
VL  - 36
IS  - 1
SP  - 87
EP  - 100
PY  - 2001/6//
T2  - Active Networks and Services
AU  - Karnouskos, Stamatis
SN  - 1389-1286
DO  - http://dx.doi.org/10.1016/S1389-1286(01)00155-4
UR  - http://www.sciencedirect.com/science/article/pii/S1389128601001554
KW  - Active networks
KW  - Security
KW  - Active code
KW  - Agent technology
AB  - Active networks (AN) are a rapid evolving area of research and in parallel an area of great industry interest. However, for this technology to make the step out of the labs and penetrate the market, the security problems have to be tackled effectively. This paper demonstrates why and how agent technology research, can and should be applied to active networks, in order to fulfill the new security challenges this infrastructure poses. First, we identify the key elements of AN, analyze the nature of active code, specify the role of agents in active networks and present a multi-execution environment active network architecture. Then, we target the security threats for active code and execution environment, and state the basic as well as the extended security requirements. Subsequently, we try to see how we can apply the security solutions and research done for agents to the context of active networks in order to satisfy their requirements.
ER  - 

TY  - CHAP
AU  - Rittinghouse, John W.
AU  - Ransome, James F.
T1  - 8 - WLAN Policy and Risk Management
A2  - Ransome, John W. RittinghouseJames F. 
BT  - Wireless Operational Security
PB  - Digital Press
CY  - Burlington
PY  - 2004///
SP  - 159
EP  - 190
SN  - 978-1-55558-317-0
DO  - http://dx.doi.org/10.1016/B978-155558317-0/50013-1
UR  - http://www.sciencedirect.com/science/article/pii/B9781555583170500131
AB  - Publisher Summary
This chapter discusses the purpose and goals of wireless local area networks (WLANs) security policies. For all security policies developed in an organization, a standard template should be used to ensure consistency of presentation and format. The chapter provides a generic format that is recommend for the development of the security policies. The template may be modified to suit the organizational needs. The implementation of policies, as part of an overall organizational security plan, can greatly enhance the protective posture of any organization. The chapter presents a basic approach to WLAN security and policy development and reviews each of the security policy steps. The chapter also describes the risks that are faced by security managers when implementing WLANs, especially when connecting WLANs to local area networks (LANs). It presents the necessary steps to secure the WLAN configuration.
ER  - 

TY  - CHAP

T1  - Chapter 5 - Securing Your Files
A2  - Miles, Wyman
A2  - Mitchell, Ed 
A2  - Lynch, F. William 
BT  - Hack Proofing Sun Solaris 8
PB  - Syngress
CY  - Rockland
PY  - 2001///
SP  - 127
EP  - 157
T2  - The Only Way to Stop a Hacker Is to Think Like One
SN  - 978-1-928994-44-2
DO  - http://dx.doi.org/10.1016/B978-192899444-2/50009-6
UR  - http://www.sciencedirect.com/science/article/pii/B9781928994442500096
AB  - Publisher Summary
If the CPU is the brain of the computer, the file system is surely the body. Like own body, this one requires constant care and special treatment. Failure to provide these can lead to the destruction of some or all of the vital data, either through intentional attacks or accidental oversights. The chapter points out some common configuration flaws and oversights and demonstrates, through some real world examples, how these mistakes can lead to disastrous consequences. The goal here is twofold: it wants a really secure system, but we also have to keep in mind the function of the system. In fact, it cannot even start securing a system until one has thoughtfully and carefully evaluated the needs that the system will fulfill. In the strictest sense, the process of securing a system is a simple thing. Simply cut the network cable, remove all the user accounts, place the system in a safe, and weld it shut. Although that statement is obviously an unrealistic exaggeration, too often we see the other end of the spectrum in place in organizations. The chapter starts with the basics of file system security: file permissions and ownership.
ER  - 

TY  - CHAP
AU  - Hurson, A.R.
AU  - Ploskonka, J.
AU  - Jiao, Y.
AU  - Haridas, H.
T1  - Security Issues and Solutions in Distributed Heterogeneous Mobile Database Systems
A2  - 
BT  - Advances in Computers
PB  - Elsevier
PY  - 2004///
VL  - Volume 61
SP  - 107
EP  - 198
T2  - 
SN  - 0065-2458
DO  - http://dx.doi.org/10.1016/S0065-2458(03)61003-X
UR  - http://www.sciencedirect.com/science/article/pii/S006524580361003X
AB  - Security continues to be a fundamental requirement of modern computer systems. As more information is stored electronically, users become more concerned with the security of their information. Designing secure systems is a difficult problem, complicated by the distributed nature of modern systems—communication links are now both wired and wireless and information is shared among autonomous and heterogeneous sources. Once the system is deployed, security threats and attacks must be evaluated and handled throughout its life cycle, as attackers discover and exploit vulnerabilities. This chapter considers the security problem within the context of centralized databases and multidatabases as well as mobile systems. The techniques employed to create a secure centralized database system can be extended to meet the security needs of multidatabase systems. This extension is not trivial, however. Many issues must be considered so that the needs of each component are met. When wireless communication links and mobility are introduced, adapting existing methods may not provide a satisfactory solution. Generally, security is expensive in terms of communication, processing, and storage overhead, which translates into increased power consumption. This overhead is unacceptable in mobile systems because mobile devices are already resource-poor. Therefore, new techniques, which draw from existing technologies and the lessons learned from their deployment, must be designed to handle the constraints introduced by mobile systems.
ER  - 

TY  - CHAP
AU  - Russell, Ryan
AU  - Bidwell, Teri
AU  - Steudler, Oliver
AU  - Walshaw, Robin
AU  - Huston, L. Brent
T1  - Chapter 4 - Designing and Implementing Security Policies
A2  - Russell, Ryan
A2  - Bidwell, Teri
A2  - Steudler, Oliver
A2  - Walshaw, Robin 
A2  - Huston, L. Brent 
BT  - Hack Proofing Your E-Commerce Site
PB  - Syngress
CY  - Rockland
PY  - 2001///
SP  - 219
EP  - 260
T2  - The Only Way to Stop a Hacker is to Think Like One
SN  - 978-1-928994-27-5
DO  - http://dx.doi.org/10.1016/B978-192899427-5/50007-9
UR  - http://www.sciencedirect.com/science/article/pii/B9781928994275500079
AB  - Publisher Summary
The primary and most basic security tool of any organization is its security policy. The security policy is the backbone of the entire operation because it defines the rules of conducting a business. These rules create the expected protocols to be followed by systems, applications, employees, and even clients. The security policy should be considered a “living document” in that it will be constantly revised and amended as new lessons are learned and as the organization evolves. The security policy should also be used as a tool to assist with the creation, implementation, and configuration of technical tools, such as firewalls and Intrusion Detection Systems (IDSs). These technological solutions should all reflect the security policy in their operation. This chapter focuses on creating the business policies that lead to profit by implementing security solutions for the site that are cost effective, increase sales, and reduce loss of revenue.
ER  - 

TY  - CHAP
AU  - XYPRO Technology
T1  - 11 - OSS Gazette a to z
A2  - Technology, XYPRO 
BT  - Securing HP NonStop Servers in an Open Systems World
PB  - Digital Press
CY  - Burlington
PY  - 2006///
SP  - 511
EP  - 805
SN  - 978-1-55558-344-6
DO  - http://dx.doi.org/10.1016/B978-155558344-6/50014-7
UR  - http://www.sciencedirect.com/science/article/pii/B9781555583446500147
AB  - Publisher Summary
The Gazette consists of a section per program, process or subsystem, containing a discussion of the object, security concerns, and best practice recommendations. This chapter highlights OSS commands, programs that are grouped by function, $HOME directory, various user programs, system program , system utility, configuration file, and sub-system applications. It discusses directory locations that are designated as DIRLOC and DIROWN (or the numeric identifier), different variables like the UTILSGE environment variable, the/bin/uudecode program, and umask User Program, and built-in commands that run entirely within the current shell process and do not start a new shell process when they are executed. Several of these commands also have program versions with object files (primarily residing in/bin). The built-in command is always the default. To invoke the program rather than the built-in command, a user must use the full pathname to invoke the object file version. The discussion also includes samba subsystem, SQL/MX subsystem, and SSH subsystem.
ER  - 

TY  - JOUR
T1  - Gearing up for grid computing
JO  - Infosecurity Today
VL  - 2
IS  - 5
SP  - 22
EP  - 25
PY  - 2005/9//
Y2  - 2005/10//
T2  - 
AU  - Myerson, Judith M.
SN  - 1742-6847
DO  - http://dx.doi.org/10.1016/S1742-6847(05)70321-9
UR  - http://www.sciencedirect.com/science/article/pii/S1742684705703219
AB  - As an enterprise's infrastructure breaches the borders of both nations and its own direct control, application security becomes a hot issue, particularly in the new Europe.
ER  - 

TY  - CHAP
AU  - Vaughn, Rayford B
T1  - Advances in the Provision of System and Software Security— Thirty Years of Progress
A2  - 
BT  - Advances in Computers
PB  - Elsevier
PY  - 2003///
VL  - Volume 58
SP  - 287
EP  - 340
T2  - 
SN  - 0065-2458
DO  - http://dx.doi.org/10.1016/S0065-2458(03)58007-X
UR  - http://www.sciencedirect.com/science/article/pii/S006524580358007X
AB  - This chapter addresses systems and software security in computing environments over the past thirty years. It is partially a historical treatment of the subject which outlines initial efforts to bound the security problem beginning early 1970 through today's state of security engineering practice. It includes an introduction to the topic, definitions and explanations necessary for background, design principles that were established by researchers and practiced today by security engineers. Government programs are described as they were initiated over time and their applicability to today's security engineering practice is discussed. Important law and Executive decisions are included which have had a direct impact on the state of computer security today. Lessons learned by practicing security engineers are included as a part of concluding remarks. Much progress has been accomplished in research and practice, yet systems today appear as vulnerable or perhaps more vulnerable than they were in the past. This should not be necessarily interpreted as a lack of progress—but as an indication of the complexity of the problem being addressed and the changing nature of the systems and networks needing the protection.
ER  - 

TY  - CHAP
AU  - Vacca, John R.
AU  - Ellis, Scott R.
T1  - 2 - Type of Firewall Security Policy
A2  - Vacca, John R.  
A2  - Ellis, Scott R. 
BT  - Firewalls
PB  - Digital Press
CY  - Burlington
PY  - 2005///
SP  - 23
EP  - 48
SN  - 978-1-55558-297-5
DO  - http://dx.doi.org/10.1016/B978-155558297-5/50004-9
UR  - http://www.sciencedirect.com/science/article/pii/B9781555582975500049
AB  - Publisher Summary
This chapter illustrates different types of firewall security policies. The main function of a firewall is to centralize access control. A firewall serves as the gatekeeper between the untrusted Internet and the more trusted internal networks. An organization may want to support some services without using strong authentication. For example, an anonymous FTP server may be used to allow all external users to download open information. In this case, such services should be hosted outside the firewall or on a service network that is not connected to corporate networks that contain sensitive data. A method for describing such a policy for a service is described. Connecting to the internet makes a wide range of services available to internal users and a wide range of system accesses available to external users. Driven by the needs of the business or mission side of the organization, policy has to be clearly written to state regarding the services that should be allowed or disallowed to both inside and outside networks.
 
Summary

An organization may want to support some services without using strong authentication. For example, an anonymous FTP server may be used to allow all external users to download open information. In this case, such services should be hosted outside the firewall or on a service network not connected to corporate networks that contain sensitive data. Table 2.4 summarizes a method of describing such a policy for a service such as FTP.
ER  - 

TY  - CHAP
AU  - Anderson, Ross
AU  - Stajano, Frank
AU  - Lee, Jong-Hyeon
T1  - Security policies
A2  - Marvin V. Zelkowitz
BT  - Advances in Computers
PB  - Elsevier
PY  - 2002///
VL  - Volume 55
SP  - 185
EP  - 235
T2  - 
SN  - 0065-2458
DO  - http://dx.doi.org/10.1016/S0065-2458(01)80030-9
UR  - http://www.sciencedirect.com/science/article/pii/S0065245801800309
AB  - A security policy is a high-level specification of the security properties that a given system should possess. It is a means for designers, domain experts and implementers to communicate with each other, and a blueprint that drives a project from design through implementation and validation.

We offer a survey of the most significant security policy models in the literature, showing how “security” may mean very different things in different contexts, and we review some of the mechanisms typically used to implement a given security policy.
ER  - 

TY  - CHAP
AU  - Kalman, Deborah Bayles
T1  - Intranets
A2  - Bidgoli, Hossein 
BT  - Encyclopedia of Information Systems
PB  - Elsevier
CY  - New York
PY  - 2003///
SP  - 683
EP  - 692
SN  - 978-0-12-227240-0
DO  - http://dx.doi.org/10.1016/B0-12-227240-4/00097-6
UR  - http://www.sciencedirect.com/science/article/pii/B0122272404000976
ER  - 

TY  - CHAP

T1  - Chapter 9 - Implementing Intrusion Detection Systems
A2  - Andrés, Steven
A2  - Kenyon, Brian 
A2  - Birkholz, Erik Pace 
BT  - Security Sage's Guide to Hardening the Network Infrastructure
PB  - Syngress
CY  - Burlington
PY  - 2004///
SP  - 369
EP  - 425
SN  - 978-1-931836-01-2
DO  - http://dx.doi.org/10.1016/B978-193183601-2/50014-2
UR  - http://www.sciencedirect.com/science/article/pii/B9781931836012500142
AB  - Publisher Summary
This chapter discusses Intrusion Detection System (IDS) basics and the types of components within IDSs. It presents comparisons of some of the many IDS solutions available. The National Institute for Standards and Technology (NIST) Special Publication SP800-31 aptly describes intrusion detection as “the process of monitoring the events occurring in a computer system or network and analyzing them for signs of intrusions, defined as attempts to compromise the confidentiality, integrity, availability, or to bypass the security mechanisms of a computer or network.” IDSs fall into two basic categories: Host Intrusion Detection Systems (HIDSs), Network Intrusion Detection Systems (NIDSs). HIDSs usually take the form of software agents that install on important hosts to report and prevent unauthorized activity back to a central console. This management station usually has a very large database of known attack signatures and a reporting mechanism to notify the network administrator. NIDSs are often network appliances or hardened servers with special software that attaches to the network and monitors traffic looking for attacks. NIDSs can be further subdivided into passive devices, which simply monitor the traffic that flows past them and inline devices, which actually inspect traffic as it flows through the machine, using two or more network interface cards (NICs).
ER  - 

TY  - JOUR
T1  - Information Systems Audit Trails in Legal Proceedings as Evidence
JO  - Computers & Security
VL  - 20
IS  - 5
SP  - 409
EP  - 421
PY  - 2001/7/1/
T2  - 
AU  - Allinson, Caroline
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/S0167-4048(01)00513-2
UR  - http://www.sciencedirect.com/science/article/pii/S0167404801005132
KW  - law enforcement
KW  - audit trails
KW  - evidence
KW  - investigation, expert witness
KW  - information security
KW  - policy
KW  - court
KW  - survey
KW  - computer
AB  - Australian State and Commonwealth Governments are interested in the collection, storage and presentation of audit trail information, particularly within a legal framework. Law enforcement agencies have a legal obligation to keep audit records of all activity on information systems used within their operations. Little to no research has been identified in relation to the use of internal audit systems for evidentiary purpose.

A brief history of audit trails is given with requirements for such audit trails beyond the year 2000.

The Queensland Police Service (QPS), Australia, is used as a major case study . Information on principles, techniques and processes used, and the reason for the recording, storing and releasing of audit information for evidentiary purposes have been studied.

To assist in determining current practice in the Australian Commonwealth and State Governments the results of an Australia wide survey of all government departments are given and contrasted to the major study for QPS.

Reference is also made to the legal obligations for authorization of audit analysis, expert witnessing and legal precedence in relation to court acceptance or rejection of audit information used in evidence.

It is shown that most organizations studied generate and retain audit trails but the approach is not consistent nor is it comprehensive. It is suggested that these materials would not withstand a serious legal challenge.
ER  - 

TY  - CHAP
AU  - Rittinghouse, John W.
AU  - Ransome, James F.
T1  - 4 - Incident Management
A2  - Ransome, John W. RittinghouseJames F. 
BT  - Wireless Operational Security
PB  - Digital Press
CY  - Burlington
PY  - 2004///
SP  - 69
EP  - 84
SN  - 978-1-55558-317-0
DO  - http://dx.doi.org/10.1016/B978-155558317-0/50008-8
UR  - http://www.sciencedirect.com/science/article/pii/B9781555583170500088
AB  - Publisher Summary
This chapter provides an overview of the process of incident handling. Incident handling includes the process of establishing an incident response function in the organization. A security manager must consider several key issues when establishing an Incident Response Group. The structure of the Incident Response Group that is best suited to the needs of a constituency should be determined. The chapter discusses the types of incidents, such as malicious code attacks, unauthorized access, unauthorized utilization of services, disruption of service, misuse, espionage, and hoaxes. A primary objective of the entire security planning process is preventing security incidents from occurring. Incident Handling Process Planning (IHPP) can help achieve this objective. The chapter also outlines the process of IHPP and discusses the occurrence of an incident and their preventive measures.
ER  - 

TY  - JOUR
T1  - Security views
JO  - Computers & Security
VL  - 22
IS  - 8
SP  - 654
EP  - 663
PY  - 2003/12//
T2  - 
AU  - Schultz, Eugene
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/S0167-4048(03)00002-6
UR  - http://www.sciencedirect.com/science/article/pii/S0167404803000026
ER  - 

TY  - CHAP

T1  - G - NRIC Preventative Best Practices for Cybersecurity
A2  - Rittinghouse, John W.  
A2  - Hancock, William M. 
BT  - Cybersecurity Operations Handbook
PB  - Digital Press
CY  - Burlington
PY  - 2004///
SP  - 855
EP  - 924
SN  - 978-1-55558-306-4
DO  - http://dx.doi.org/10.1016/B978-155558306-4/50032-9
UR  - http://www.sciencedirect.com/science/article/pii/B9781555583064500329
ER  - 

TY  - CHAP
AU  - Zheng, Pei
AU  - Ni, Lionel
T1  - 6 - Mobile Security and Privacy
A2  - Zheng, Pei  
A2  - Ni, Lionel 
BT  - Smart Phone and Next Generation Mobile Computing
PB  - Morgan Kaufmann
CY  - Burlington
PY  - 2006///
SP  - 335
EP  - 405
SN  - 978-0-12-088560-2
DO  - http://dx.doi.org/10.1016/B978-012088560-2/50008-3
UR  - http://www.sciencedirect.com/science/article/pii/B9780120885602500083
AB  - Publisher Summary
This chapter explores a wide range of mobile security and privacy issues and offers some insight into the fundamental security problems surrounding the design of secured mobile wireless systems and applications. The chapter begins with a security primer summarizing a set of basic network security concepts and security schemes, followed by an in depth coverage of security issues in cellular networks, wireless local area network (LAN), Bluetooth, and other emerging mobile wireless systems. Subsequently, a mobile wireless system takes security and privacy into account at the very beginning of the design phase and utilize appropriate security service building blocks to provide data confidentiality, integrity, authentication, and nonrepudiation, as well as efficient access control. The chapter introduces technical aspects of each problem and discusses some proposed approaches for solving the issues. It also outlines some real-world solutions to the security problems. Many of the security and privacy problems addressed in this chapter are closely related to requirements of the underlying mobile applications and services such as location-based services, mobile commerce, and instant messaging.
ER  - 

TY  - JOUR
T1  - User provisioning with SPML
JO  - Information Security Technical Report
VL  - 9
IS  - 1
SP  - 86
EP  - 96
PY  - 2004/1//
Y2  - 2004/3//
T2  - 
AU  - Sodhi, Gavenraj
SN  - 1363-4127
DO  - http://dx.doi.org/10.1016/S1363-4127(04)00018-4
UR  - http://www.sciencedirect.com/science/article/pii/S1363412704000184
ER  - 

TY  - JOUR
T1  - Security Views - Malware Update
JO  - Computers & Security
VL  - 25
IS  - 5
SP  - 317
EP  - 324
PY  - 2006/7//
T2  - 

SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/j.cose.2006.06.003
UR  - http://www.sciencedirect.com/science/article/pii/S0167404806000952
ER  - 

TY  - JOUR
T1  - PCASSO: a design for secure communication of personal health information via the internet
JO  - International Journal of Medical Informatics
VL  - 54
IS  - 2
SP  - 97
EP  - 104
PY  - 1999/5//
T2  - 
AU  - Baker, Dixie B
AU  - Masys, Daniel R
SN  - 1386-5056
DO  - http://dx.doi.org/10.1016/S1386-5056(98)00088-4
UR  - http://www.sciencedirect.com/science/article/pii/S1386505698000884
KW  - Computer communication networks
KW  - Computer security
KW  - Confidentiality
KW  - Medical records systems, computerized
AB  - The Internet holds both promise and peril for the communications of person-identifiable health information. Because of technical features designed to promote accessibility and interoperability rather than security, Internet addressing conventions and transport protocols are vulnerable to compromise by malicious persons and programs. In addition, most commonly used personal computer (PC) operating systems currently lack the hardware-based system software protection and process isolation that are essential for ensuring the integrity of trusted applications. Security approaches designed for electronic commerce, that trade known security weaknesses for limited financial liability, are not sufficient for personal health data, where the personal damage caused by unintentional disclosure may be far more serious. To overcome these obstacles, we are developing and evaluating an Internet-based communications system called PCASSO (Patient-centered access to secure systems online) that applies state of the art security to health information. PCASSO includes role-based access control, multi-level security, strong device and user authentication, session-specific encryption and audit trails. Unlike Internet-based electronic commerce ‘solutions,’ PCASSO secures data end-to-end: in the server; in the data repository; across the network; and on the client. PCASSO is designed to give patients as well as providers access to personal health records via the Internet.
ER  - 

TY  - JOUR
T1  - Logs may be found boring, but they are good: NIST
JO  - Computer Fraud & Security
VL  - 2006
IS  - 5
SP  - 2
EP  - 3
PY  - 2006/5//
T2  - 

SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(06)70351-7
UR  - http://www.sciencedirect.com/science/article/pii/S1361372306703517
AB  - A US standards body has said that the benefits of logs are being thrown away because system administrators find it “boring.” It said that the analysis of logs is often “treated as a low-priority task” by administrators because more urgent tasks like fixing vulnerabilities come first.
ER  - 

TY  - CHAP

T1  - Index
A2  - Hunter, Martin GrasdalLaura E.  
A2  - Shinder, Michael CrossLaura HunterDebra Littlejohn ShinderThomas W. 
BT  - MCSE (Exam 70-293) Study Guide
PB  - Syngress
CY  - Rockland
PY  - 2003///
SP  - 1025
EP  - 1064
SN  - 978-1-931836-93-7
DO  - http://dx.doi.org/10.1016/B978-193183693-7/50018-X
UR  - http://www.sciencedirect.com/science/article/pii/B978193183693750018X
ER  - 

TY  - JOUR
T1  - Implementing enterprise security: a case study3
JO  - Computers & Security
VL  - 22
IS  - 2
SP  - 99
EP  - 114
PY  - 2003/2//
T2  - 
AU  - Doughty, Ken
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/S0167-4048(03)00205-0
UR  - http://www.sciencedirect.com/science/article/pii/S0167404803002050
AB  - Introduction

Information is an essential asset for organizations, because it supports the day-today operations, and facilitates decision-making by the organization’s key stakeholders. The challenge facing organizations is how to provide access to this asset without compromising its integrity. This asset is received and distributed by the organization through various distribution channels, which is connected together by the telecommunications network. These channels include:

•
Email
•
Internet
•
Applications (e.g. Financial, Logistics, Retail, Property and Construction, Energy etc.)
•
DBMS (MS SQL Server, Oracle, DB2, Sybase, etc.)
•
Operating systems (e.g. Unix, NT/Windows 2000, etc.)
ER  - 

TY  - JOUR
T1  - New challenges in public order policing: the professionalisation of environmental protest and the emergence of the militant environmental activist
JO  - International Journal of the Sociology of Law
VL  - 30
IS  - 1
SP  - 17
EP  - 32
PY  - 2002/3//
T2  - 
AU  - Button, Mark
AU  - John, Tim
AU  - Brearley, Nigel
SN  - 0194-6595
DO  - http://dx.doi.org/10.1016/S0194-6595(02)00017-5
UR  - http://www.sciencedirect.com/science/article/pii/S0194659502000175
KW  - Policing
KW  - Environmental protest
KW  - Private security
KW  - Militant environmental activist
AB  - Over the last decade there have been significant changes in the nature of protest which have posed new challenges to policing organisations. This article using the case of environmental protest charts the changing nature of protest and using a new typology identifies a new form of protester that has been termed the Militant Environmental Activist. The characteristics and innovative tactics used by these activists are examined and it is argued that they amount to a ‘professionalisation’ of protest. The article then moves on to highlight the ‘fragmented’ policing structure that has evolved to tackle these activists encompassing both public, ‘hybrid’ and private policing agencies. Finally, the article briefly examines some of the strategies these policing agencies are developing to combat these new challenges.
ER  - 

TY  - CHAP

T1  - Chapter 10 - Validate Existing Security Infrastructure
A2  - Porter, Thomas
A2  - Kanclirz, Jan
A2  - Zmolek, Andy
A2  - Rosela, Antonio
A2  - Cross, Michael
A2  - Chaffin, Larry
A2  - Baskin, Brian 
A2  - Shim, Choon 
BT  - Practical VoIP Security
PB  - Syngress
CY  - Burlington
PY  - 2006///
SP  - 263
EP  - 307
SN  - 978-1-59749-060-3
DO  - http://dx.doi.org/10.1016/B978-159749060-3/50033-3
UR  - http://www.sciencedirect.com/science/article/pii/B9781597490603500333
AB  - Publisher Summary
This chapter discusses the process of securing the Voice over Internet Protocol (VoIP) infrastructure by reviewing and validating the existing security infrastructure. Addition of VoIP components to a preexisting data network is the ideal opportunity to review and bolster existing security policy, architecture, and processes. It discusses many of the ways to reuse portions of existing security infrastructure as preparation to add voice traffic to the mix. After the management made the decision to move to a converged network, and before the new architecture is completed, it is important that one or more representatives of the security group participate in the architectural discussions. A truly dedicated attacker, finding little means of accessing an organization's internal IP network over a public network such as the Internet, often turn to physical penetration to bypass the organization's logical perimeter security controls. Supporting Services describes the functions and security characteristics of VoIP supplementary services. The servers that host these services should be hardened and patched per security policy guidelines. Unified Management details some of the responsibilities when designing the monitoring network for VoIP infrastructure.
ER  - 

TY  - CHAP

T1  - Chapter 1 - Designing a Secure Network Framework
A2  - Khnaser, Elias N.
A2  - Snedaker, Susan
A2  - Peiris, Chris
A2  - Amini, Rob 
A2  - Hunter, Laura E. 
BT  - MCSE (Exam 70-298) Study Guide
PB  - Syngress
CY  - Rockland
PY  - 2004///
SP  - 1
EP  - 48
SN  - 978-1-932266-55-9
DO  - http://dx.doi.org/10.1016/B978-193226655-9/50004-1
UR  - http://www.sciencedirect.com/science/article/pii/B9781932266559500041
AB  - Publisher Summary
This chapter provides an overview of the process of securing a Windows Server 2003 enterprise network. The chapter examines how to analyze a company's business requirements for network and data security. This analysis includes looking at the existing security policies and procedures to either incorporate them into a new security design or determine the kinds of changes they would need to work within a Windows Server 2003 network. After determining the organization's security needs, the next step is to determine what types of attacks the network will be subjected to. The chapter deals with many of the common attacks that an enterprise network faces as large Internet-based attacks spread quickly and multiply at an unbelievable rate. The chapter concludes with a discussion on the interoperability concerns and the challenges created by the integration of down-level or third-party operating systems and services into a Windows Server 2003 network.
ER  - 

TY  - CHAP

T1  - Chapter 10 - General IT Security Plan
A2  - Snedaker, Susan  
A2  - Rogers, Russ 
BT  - Syngress IT Security Project Management Handbook
PB  - Syngress
CY  - Burlington
PY  - 2006///
SP  - 261
EP  - 343
SN  - 978-1-59749-076-4
DO  - http://dx.doi.org/10.1016/B978-159749076-4/50015-1
UR  - http://www.sciencedirect.com/science/article/pii/B9781597490764500151
AB  - Publisher Summary
This chapter provides the framework for creating a general Information Technology (IT) security project as part of an overall corporate IT security project plan strategy. As with all of the individual security area projects (ISAPs) discussed in this book, this is intended to be a template to use as a starting point. One might wonder what a “general” IT security project plan consists of. It discusses the security assessment and auditing function in great detail. Most corporate IT security plans start with a thorough assessment so that the problem statement can be developed. The chapter looks at different ways to view the security assessment as a whole, including looking at the perimeter systems, internal network, server and host systems, applications and databases, and data. It's important to understand the elements that comprise an assessment, which typically fall into the vulnerability scanning, pen testing, and risk assessment, with the caveat that any pen testing should be limited in scope prior to implementing a security project. The audit function includes auditing physical, technological, and administrative policies and procedures. It also looks briefly at access control, authentication, and auditing as part of overall security assessment.
ER  - 

TY  - JOUR
T1  - Issues and risks in performing SysTrust® engagements: implications for research and practice
JO  - International Journal of Accounting Information Systems
VL  - 6
IS  - 1
SP  - 55
EP  - 79
PY  - 2005/3//
T2  - 
AU  - Bedard, Jean C.
AU  - Jackson, Cynthia M.
AU  - Graham, Lynford
SN  - 1467-0895
DO  - http://dx.doi.org/10.1016/j.accinf.2004.10.001
UR  - http://www.sciencedirect.com/science/article/pii/S1467089504000880
KW  - SysTrust
KW  - Trust services
KW  - Assurance services
KW  - Assurance risk
AB  - This paper identifies issues related to the delivery of SysTrust® services by public accounting firms. We first review the professional and academic literatures relating to trust services, analogizing to the literature on financial statement audits. Our review reveals that there are a number of articles in the professional and academic literatures on trust services relating to user issues, most of which address the nature and extent of the potential demand for these services. However, very little attention has been paid to issues relating to engagement issues for service providers. In contrast, the auditing literature provides a large body of conceptual and empirical research devoted to the management of audit engagement risk. We identify several specific issues relating to trust services engagement risk, including sources of potential expectation gaps and barriers to attestation risk reduction. In order to facilitate further research on providing trust services, we use a case situation to illustrate specific aspects of these issues in a complex client environment.
ER  - 

TY  - CHAP
AU  - HSIAO, DAVID K.
AU  - KERR, DOUGLAS S.
AU  - MADNICK, STUART E.
T1  - CHAPTER 3 - OPERATIONAL SECURITY
A2  - MADNICK, DAVID K. HSIAODOUGLAS S. KERRSTUART E. 
BT  - Computer Security
PB  - Academic Press
CY  - 
PY  - 1979///
SP  - 43
EP  - 91
T2  - ACM Monograph Series
SN  - 05724252
DO  - http://dx.doi.org/10.1016/B978-0-12-357650-7.50010-7
UR  - http://www.sciencedirect.com/science/article/pii/B9780123576507500107
AB  - Although many security issues are controlled by legislative ruling and social standards or are constrained by technological limitations, there are many important matters of operational security that are directly or indirectly under management control. In this chapter these issues are identified and possible actions are proposed.
ER  - 

TY  - JOUR
T1  - Accurate and large-scale privacy-preserving data mining using the election paradigm
JO  - Data & Knowledge Engineering
VL  - 68
IS  - 11
SP  - 1224
EP  - 1236
PY  - 2009/11//
T2  - Including Special Section: Conference on Privacy in Statistical Databases (PSD 2008) – Six selected and extended papers on Database Privacy
AU  - Magkos, Emmanouil
AU  - Maragoudakis, Manolis
AU  - Chrissikopoulos, Vassilis
AU  - Gritzalis, Stefanos
SN  - 0169-023X
DO  - http://dx.doi.org/10.1016/j.datak.2009.06.003
UR  - http://www.sciencedirect.com/science/article/pii/S0169023X09000950
KW  - Security and privacy
KW  - Distributed data mining
KW  - Homomorphic encryption
KW  - Random Forests classification
AB  - With the proliferation of the Web and ICT technologies there have been concerns about the handling and use of sensitive information by data mining systems. Recent research has focused on distributed environments where the participants in the system may also be mutually mistrustful. In this paper we discuss the design and security requirements for large-scale privacy-preserving data mining (PPDM) systems in a fully distributed setting, where each client possesses its own records of private data. To this end we argue in favor of using some well-known cryptographic primitives, borrowed from the literature on Internet elections. More specifically, our framework is based on the classical homomorphic election model, and particularly on an extension for supporting multi-candidate elections. We also review a recent scheme [Z. Yang, S. Zhong, R.N. Wright, Privacy-preserving classification of customer data without loss of accuracy, in: SDM’ 2005 SIAM International Conference on Data Mining, 2005] which was the first scheme that used the homomorphic encryption primitive for PPDM in the fully distributed setting. Finally, we show how our approach can be used as a building block to obtain Random Forests classification with enhanced prediction performance.
ER  - 

TY  - JOUR
T1  - Speculations on the science of web user security
JO  - Computer Networks
VL  - 56
IS  - 18
SP  - 3891
EP  - 3895
PY  - 2012/12/17/
T2  - The WEB we live in
AU  - Sandhu, Ravi
SN  - 1389-1286
DO  - http://dx.doi.org/10.1016/j.comnet.2012.10.010
UR  - http://www.sciencedirect.com/science/article/pii/S1389128612003647
KW  - Web user security
KW  - Security science
KW  - Cyber security
AB  - There appears to be consensus among seasoned cyber security researchers that there is substantial disconnect between the research community’s priorities and the real world—notwithstanding numerous intellectual advances in the theory and practice of cyber security over the past four decades. This is in part manifested by recent recurring calls for dramatic shifts in cyber security research paradigms, including so called game-changing approaches that go beyond the typical computer science and engineering perspectives. This article focusses on a specially important piece of cyber security called web user security where the prime concern is security for the ordinary consumer of web application services. The proliferation of web services and their enthusiastic reception by the ordinary citizen attests to the tremendous practical success of these technologies. As such it is prima facie evident that the current web is “secure enough” for mass adoption. Now, one certain prediction about the web is that it will continue to evolve rapidly. This article gives the author’s personal perspective on what web user security science might be developed to address the need to be “secure enough” in light of continued evolution. To this end the article begins by considering what happened in evolution of the web in the past and how much of it, if any, was guided by “science.” The article identifies some security principles that can be abstracted from this short but eventful history. The article then speculates on what directions the science of web user security should take.
ER  - 

TY  - JOUR
T1  - NADIR: An automated system for detecting network intrusion and misuse
JO  - Computers & Security
VL  - 12
IS  - 3
SP  - 235
EP  - 248
PY  - 1993/5//
T2  - 
AU  - Hochberg, Judith
AU  - Jackson, Kathleen
AU  - Stallings, Cathy
AU  - McClary, J.F.
AU  - DuBois, David
AU  - Ford, Josephine
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/0167-4048(93)90110-Q
UR  - http://www.sciencedirect.com/science/article/pii/016740489390110Q
KW  - Computer security
KW  - intrusion detection
KW  - misuse detection
KW  - anomaly detection
KW  - expert system
AB  - This paper describes a misuse detection system for Los Alamos National Laboratory's Integrated Computing Network (ICN). This automated expert system, the Network Anomaly Detection and Intrusion Reporter (NADIR), streamlines and supplements the manual audit record review traditionally performed by security auditors. NADIR compares network activity, as summarized in weekly profiles of individual users and the ICN as a whole, against expert rules that define security policy and improper or suspicious behaviour. NADIR reports suspicious behaviour to security auditors and provides tools to aid in follow-up investigations. This paper describes analysis by NADIR of two types of ICN activity: user authentication and access control, and mass file storage. It highlights system design issues of data handling, exploiting existing auditing systems, and performing audit analysis at the network level.
ER  - 

TY  - JOUR
T1  - Subject index to volume 2
JO  - Computers & Security
VL  - 2
IS  - 3
SP  - 305
EP  - 308
PY  - 1983/11//
T2  - 

SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/0167-4048(83)90023-8
UR  - http://www.sciencedirect.com/science/article/pii/0167404883900238
ER  - 

TY  - JOUR
T1  - Preventing software piracy
JO  - Network Security
VL  - 1994
IS  - 9
SP  - 17
EP  - 19
PY  - 1994/9//
T2  - 
AU  - Schifreen, Robert
SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/1353-4858(94)90179-1
UR  - http://www.sciencedirect.com/science/article/pii/1353485894901791
AB  - It's up to you, as the person in charge of network security, to ensure that users are not storing pirated software on the server or workstations. Regular audits can help you achieve this.
ER  - 

TY  - JOUR
T1  - A transaction flow approach to software security certification for document handling systems
JO  - Computers & Security
VL  - 7
IS  - 5
SP  - 495
EP  - 502
PY  - 1988/10//
T2  - 
AU  - Pfleeger, Charles P.
AU  - Pfleeger, Shari Lawrence
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/0167-4048(88)90203-9
UR  - http://www.sciencedirect.com/science/article/pii/0167404888902039
KW  - Security
KW  - Certification
KW  - Transaction
KW  - Data flow
KW  - Verification
KW  - Validation
KW  - Exposure
KW  - Control
AB  - A security certification method is described for a document handling system for a major government organization. The security evaluation process includes identification of the exposures of the system, determination of the controls that cover those exposures, and evaluation of the appropriateness and effectiveness of the controls. Included are the details of the analysis performed and the types of results expected in that analysis, both of which constitute the basic evaluation of the document handling system. The certification analysis approach can be extended naturally to other types of computing systems.
ER  - 

TY  - JOUR
T1  - Auditing and security
JO  - Computer Audit Update
VL  - 1989
IS  - 5
SP  - 2
EP  - 7
PY  - 1989/3//
Y2  - 1989/4//
T2  - 
AU  - Blatchford, Clive
SN  - 0960-2593
DO  - http://dx.doi.org/10.1016/S0960-2593(89)80027-8
UR  - http://www.sciencedirect.com/science/article/pii/S0960259389800278
AB  - Summary
The purpose of this short paper is to create a generic administrative framework in which the operational management of information systems may be supported by effective auditing process.

Emphasis is placed on agreeing a set of overall control principles which may be mapped from the human world to that of the technologically based information systems. Five intial principles have been recognised. This list may be extended as the practical problems of managing and auditing security information systems, especially in the non-Defence sector, are explored.

The objectives of functionally rich, IT ‘open systems’ solutions are used to illustrate the requirements. Where relevant, specific ‘open’ standards activities (especially ISO, CCITT, ect) are referenced as the basis of future development.
ER  - 

TY  - JOUR
T1  - It security testing, a practical guide — part 5: Security stress/loading testing
JO  - Computer Audit Update
VL  - 1993
IS  - 3
SP  - 7
EP  - 10
PY  - 1993/3//
T2  - 
AU  - Robertson, Bernard
AU  - Pullen, David
SN  - 0960-2593
DO  - http://dx.doi.org/10.1016/0960-2593(93)90041-X
UR  - http://www.sciencedirect.com/science/article/pii/096025939390041X
ER  - 

TY  - JOUR
T1  - An ‘intelligent’ approach to audit trail analysis
JO  - Computer Audit Update
VL  - 1991
IS  - 11
SP  - 13
EP  - 17
PY  - 1991/11//
T2  - 
AU  - Hickman, Frank
SN  - 0960-2593
DO  - http://dx.doi.org/10.1016/0960-2593(91)90050-J
UR  - http://www.sciencedirect.com/science/article/pii/096025939190050J
ER  - 

TY  - JOUR
T1  - Cyber Wars and other threats
JO  - Computers & Security
VL  - 17
IS  - 2
SP  - 115
EP  - 118
PY  - 1998///
T2  - 
AU  - Hinde, Stephen
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/S0167-4048(97)81979-7
UR  - http://www.sciencedirect.com/science/article/pii/S0167404897819797
ER  - 

TY  - JOUR
T1  - Information security in workstation environments
JO  - Computers & Security
VL  - 12
IS  - 2
SP  - 117
EP  - 122
PY  - 1993/3//
T2  - 
AU  - Stahl, Stanley H.
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/0167-4048(93)90090-R
UR  - http://www.sciencedirect.com/science/article/pii/016740489390090R
AB  - This article explores information security issues in a context directed to meeting the needs of distributed workstation environments. It is not intended to be self-contained. Many topics, particularly those that are generic to securing computer and communication systems, are hardly touched on. The intent has been to (i) provide a context for exploring information security issues associated with distributed workstation environments and (ii) explore a few of the most important technology-based counter-measures available in workstation security.
ER  - 

TY  - JOUR
T1  - General controls in computer systems
JO  - Computers & Security
VL  - 4
IS  - 1
SP  - 33
EP  - 45
PY  - 1985/3//
T2  - 
AU  - Cerullo, Michael J.
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/0167-4048(85)90007-0
UR  - http://www.sciencedirect.com/science/article/pii/0167404885900070
KW  - Internal control
KW  - plan of organization
KW  - operations controls
KW  - system development and planning controls
KW  - access controls
KW  - documentation
KW  - personnel controls
KW  - operating system controls
KW  - master planning
KW  - contingency planning
AB  - Because almost all business organizations own or will shortly own computer systems, auditors can no longer treat the computer as a “black box” to be ignored when attesting to the fairness with which financial statements present financial position and results of operations. This paper is primarily written for auditors who are becoming involved in auditing computerized accounting systems. It covers in detail general controls in computer systems, the first and most important category of controls evaluated by an auditor. General controls relate to all EDP activities; they span all jobs processed on the computer system. When they are weak or non-existing, the auditor must expand his testing of the entire computer system, often at considerable additional cost to the client.
ER  - 

TY  - JOUR
T1  - Random bits &amp; bytes
JO  - Computers & Security
VL  - 13
IS  - 8
SP  - 622
EP  - 627
PY  - 1994///
T2  - 
AU  - Highland, HaroldJoseph
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/0167-4048(94)90041-8
UR  - http://www.sciencedirect.com/science/article/pii/0167404894900418
ER  - 

TY  - JOUR
T1  - Guide for authors
JO  - Computers & Security
VL  - 12
IS  - 4
SP  - 419
EP  - 
PY  - 1993/6//
T2  - 

SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/0167-4048(93)90030-9
UR  - http://www.sciencedirect.com/science/article/pii/0167404893900309
ER  - 

TY  - JOUR
T1  - Expert Provisioner: a range management aid
JO  - Knowledge-Based Systems
VL  - 11
IS  - 5–6
SP  - 339
EP  - 344
PY  - 1998/11/23/
T2  - 
AU  - Power, Rhys
AU  - Reynolds, Steve
AU  - Kingston, John
AU  - Harrison, Ian
AU  - Ann Macintosh
AU  - Tonberg, Jon
SN  - 0950-7051
DO  - http://dx.doi.org/10.1016/S0950-7051(98)00062-8
UR  - http://www.sciencedirect.com/science/article/pii/S0950705198000628
KW  - Knowledge engineering
KW  - Knowledge-based systems
KW  - Logistics
AB  - Expert Provisioner is a knowledge-based provisioning system developed by Royal Air Force (RAF) logistics research and AIAI at the University of Edinburgh for use by the RAF logistics command to support the procurement of consumable parts. The starting point for Expert Provisioner is an electronic purchase order form and its end point is a recommendation of whether to buy the item or not, its cost and due delivery date. Purchase recommendations are based on many factors including forecast demand, unit costs, shelf life and existing stock levels. Identified benefits of the system include improved speed and accuracy of the data checking and order quantity calculation processes; automatic recording of provisioning history data for use in financial management/analysis; and finally, the ability to allow trainees to work on real life problems and compare their results with the experts.
ER  - 

TY  - JOUR
T1  - Legal admissibility of evidence held in digital form
JO  - Computer Law & Security Review
VL  - 15
IS  - 3
SP  - 185
EP  - 187
PY  - 1999/5//
Y2  - 1999/6//
T2  - 
AU  - Kearsley, Amanda J.
SN  - 0267-3649
DO  - http://dx.doi.org/10.1016/S0267-3649(99)80037-5
UR  - http://www.sciencedirect.com/science/article/pii/S0267364999800375
AB  - Organizations faced with voluminous paper documents are increasingly turning to technology, scanning the paper documents and then destroying the originals, placing increased reliance on electronic document management systems for subsequent retrieval. One key difficulty these organizations face is the uncertain legal status of the electronic record, and whether, if necessary, the digital copy of the original paper document can be used in evidence in legal proceedings. This first part of a two part article considers the effects of the Civil Evidence Act 1995 and also examines current and recommended practices for ensuring the admissibility of digital evidence.
ER  - 

TY  - JOUR
T1  - A higher level of computer security through active policies
JO  - Computers & Security
VL  - 14
IS  - 2
SP  - 147
EP  - 157
PY  - 1995///
T2  - 
AU  - Abrams, Marshall D.
AU  - Moffett, Jonathan D.
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/0167-4048(95)97048-F
UR  - http://www.sciencedirect.com/science/article/pii/016740489597048F
KW  - Monitor
KW  - Active
KW  - Passive
KW  - Reference Monitor
KW  - Policies
KW  - Distributed systems
KW  - Assurance
AB  - This paper views the Reference Monitor in a new framework that makes it possible to generalize from passive to active monitors. It describes a major trend in the evolution of information systems security. The concepts are a practical reflection of real-world needs, expressed in a theoretical framework. The approach of employing active and passive policies provides a higher level of security than would otherwise be possible. The passive traditional Reference Monitor that interprets security policies and permits or prohibits access requests is supplemented by an active monitor to initiate behavior, such as taking positive actions to maintain integrity, taking recovery actions to restore situations after failures, and regularly monitoring the system. This extension to enforcement of various policies supports distributed systems architectures as the appropriate model for thinking about information technology (IT) security.
ER  - 

TY  - JOUR
T1  - Securing digital signatures for non-repudiation
JO  - Computer Communications
VL  - 22
IS  - 8
SP  - 710
EP  - 716
PY  - 1999/5/25/
T2  - 
AU  - Zhou, J.
AU  - Lam, K.Y.
SN  - 0140-3664
DO  - http://dx.doi.org/10.1016/S0140-3664(99)00031-6
UR  - http://www.sciencedirect.com/science/article/pii/S0140366499000316
KW  - Digital signature
KW  - Non-repudiation
KW  - Secure electronic commerce
AB  - Dispute of transactions is a common problem that could jeopardise business. Hence non-repudiation services are essential in business transactions which provide evidence to enable dispute resolution. To be eligible as non-repudiation evidence, the digital signature on an electronic document should remain valid until its expiry date which is specified by some non-repudiation policy. The conventional approaches are either inefficient or insecure to achieve non-repudiation in electronic commerce. This article presents a practical scheme to secure digital signatures as non-repudiation evidence with an adjustable degree of risk.
ER  - 

TY  - JOUR
T1  - Client server architectures and security
JO  - Computer Audit Update
VL  - 1992
IS  - 9
SP  - 8
EP  - 12
PY  - 1992/9//
T2  - 
AU  - Pullen, David
AU  - Robertson, Bernard
SN  - 0960-2593
DO  - http://dx.doi.org/10.1016/0960-2593(92)90011-B
UR  - http://www.sciencedirect.com/science/article/pii/096025939290011B
ER  - 

TY  - CHAP
AU  - Gorman, Michael M.
T1  - 6 - SYSTEM CONTROL
A2  - Gorman, Michael M. 
BT  - Database Management Systems
PB  - Butterworth-Heinemann
CY  - 
PY  - 1991///
SP  - 311
EP  - 402
SN  - 978-0-7506-0135-1
DO  - http://dx.doi.org/10.1016/B978-0-7506-0135-1.50010-5
UR  - http://www.sciencedirect.com/science/article/pii/B9780750601351500105
AB  - Publisher Summary
This chapter discusses system control. System control facilities provide for the protection of the database, the smooth operation of applications that use the database and the database management system (DBMS), and the effective use of the DBMS. DBMS installation and maintenance facilities enables the generation of special run-time versions that favor certain types of update or retrieval processing. The more resources expended during the update operations in preparation for database recovery, the quicker and less expensive is the recovery. A dynamic relationship DBMS only locks a single data record type and its associated instances as they alone are contained in a single O/S file. If the database application is implemented with a static relationship DBMS, logical database changes always consume more resources to incorporate than do changes to a dynamic relationship DBMS. Physical reorganization is process to bring the physical order of the database back into close relationship with its implied logical order. If a run-unit accesses data from multiple databases, then a host language interface run-unit can produce the consolidated report.
ER  - 

TY  - JOUR
T1  - Security issues in system development
JO  - Computer Audit Update
VL  - 1992
IS  - 9
SP  - 4
EP  - 8
PY  - 1992/9//
T2  - 
AU  - Price, G.R.
SN  - 0960-2593
DO  - http://dx.doi.org/10.1016/0960-2593(92)90010-K
UR  - http://www.sciencedirect.com/science/article/pii/096025939290010K
ER  - 

TY  - JOUR
T1  - Teleservice requirements for management
JO  - Computer Networks and ISDN Systems
VL  - 26, Supplement 4
IS  - 0
SP  - S163
EP  - S178
PY  - 1995///
T2  - 
AU  - da Cruz, Alina
AU  - Lewis, Dave
AU  - Crowcroft, Jon
SN  - 0169-7552
DO  - http://dx.doi.org/10.1016/0169-7552(95)90004-7
UR  - http://www.sciencedirect.com/science/article/pii/0169755295900047
KW  - Teleservice
KW  - OSI
KW  - QoS
AB  - Future advances in conferencing will make it feasible to build servers which collaborate to provide services similar to those of an airline booking system over the networks. Such servers should be able to support real-time queries and modifications to the information stored. We present the management requirements for the services that are providd by these servers.
ER  - 

TY  - JOUR
T1  - Administrative controls for password-based computer access control systems
JO  - Computer Fraud & Security Bulletin
VL  - 8
IS  - 3
SP  - 5
EP  - 13
PY  - 1986/1//
T2  - 
AU  - Wood, CharlesCresson
SN  - 0142-0496
DO  - http://dx.doi.org/10.1016/0142-0496(86)90043-3
UR  - http://www.sciencedirect.com/science/article/pii/0142049686900433
ER  - 

TY  - JOUR
T1  - Developing a Windows NT security policy
JO  - Information Security Technical Report
VL  - 3
IS  - 3
SP  - 31
EP  - 43
PY  - 1998///
T2  - 
AU  - White, Ian
SN  - 1363-4127
DO  - http://dx.doi.org/10.1016/S1363-4127(98)80029-0
UR  - http://www.sciencedirect.com/science/article/pii/S1363412798800290
AB  - The choice of Windows NT as the strategic platform for the desktop and shared application server is becoming more widespread. A challenge for the security team is to provide guidelines on the minimum level of security controls that should be implemented on these systems. This article discusses some of the controls that might be expected to be included within such a Windows NT Security Policy (the policy).
ER  - 

TY  - JOUR
T1  - Mean annual variation of transport of major currents in the tropical Pacific Ocean
JO  - Deep Sea Research Part I: Oceanographic Research Papers
VL  - 43
IS  - 7
SP  - 1105
EP  - 1122
PY  - 1996/7//
T2  - 
AU  - Donguy, Jean-Rene
AU  - Meyers, Gary
SN  - 0967-0637
DO  - http://dx.doi.org/10.1016/0967-0637(96)00047-7
UR  - http://www.sciencedirect.com/science/article/pii/0967063796000477
AB  - Expendable bathythermograph (XBT) data and the climatological temperature/salinity relationship were used to calculate the mean annual cycle of dynamic height and geostrophic transport of major currents relative to 400 db along five shipping tracks covering a large part of the tropical Pacific Ocean. The data were selected in bands centered on the most frequently repeated XBT tracklines for the period 1967–1988. Long-term bimonthly mean temperature was calculated in 1° latitude bins along the tracks. The transport function (vertically integrated dynamic height) was then calculated using the mean temperature/salinity relationship. The mean annual cycle of transport of the North Equatorial Current (NEC), the North Equatorial Countercurrent (NECC) and the South Equatorial Current (SEC) (south of 2.5°S) were determined between the ridges and troughs of the transport function. The stochastic errors in bimonthly mean transports were 1–2 Sverdrups on the most sampled tracks. Mean transports of the NEC and NECC increase regularly with longitude from east to west. The NECC has a large annual cycle with a transport-maximum during northern fall and winter. Seasonal variations of the NEC are small. Seasonal variations of the SEC are slightly larger, and they have considerably different phase from track to track. The variation of thermal structure associated with the currents is described. The results of this study are compared in detail to the results of earlier studies of the transports. The differences between the studies are larger than the expected stochastic errors in the mean transports due to differences in the definition of boundaries of the currents and to differences in the procedure for calculating the mean annual variation. The results of all the studies are summarised to facilitate future comparisons to ocean general circulation models and other applications.
ER  - 

TY  - JOUR
T1  - How to supervise and control I/S Security Officers and Auditors
JO  - Computers & Security
VL  - 6
IS  - 1
SP  - 17
EP  - 21
PY  - 1987/2//
T2  - 
AU  - Moulton, Rolf T.
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/0167-4048(87)90121-0
UR  - http://www.sciencedirect.com/science/article/pii/0167404887901210
KW  - IS security
KW  - IS audit
KW  - Computer audit
KW  - Access control
KW  - IS supervision
KW  - IS management
KW  - IS responsibilities
AB  - Information system Security Officers (ISSO) and Information System Auditors (ISA) can have similar capabilities with respect to asset access. Their supervision and control requires special consideration by management because they may literally have the keys to all of the organization's information and physical assets. The managers of each of those employees must be directly accountable for the individual's supervision. The same suggestions which are described for ISSO supervision apply to ISA supervision. Further, each is in a unique position to audit the activities of the other. Recommendations are provided to accomplish this “audit of the auditors.”
ER  - 


