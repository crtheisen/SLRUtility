TY  - CONF
JO  - Globecom Workshops (GC Wkshps), 2013 IEEE
TI  - Massive distributed and parallel log analysis for organizational security
T2  - Globecom Workshops (GC Wkshps), 2013 IEEE
IS  - 
SN  - 
VO  - 
SP  - 194
EP  - 199
AU  - Xiaokui Shu
AU  - Smiy, J.
AU  - Danfeng Yao
AU  - Heshan Lin
Y1  - 9-13 Dec. 2013
PY  - 2013
KW  - cloud computing
KW  - distributed processing
KW  - security of data
KW  - system monitoring
KW  - Amazon cloud environments
KW  - EC2
KW  - MapReduce
KW  - S3
KW  - cloud-based distributed framework
KW  - dynamic task scheduling
KW  - log data demands
KW  - massive distributed frameworks
KW  - organizational security
KW  - parallel security log analysis framework
KW  - streaming logs
KW  - transaction logs
KW  - Cloud computing
KW  - Conferences
KW  - Data privacy
KW  - Organizations
KW  - Security
VL  - 
JA  - Globecom Workshops (GC Wkshps), 2013 IEEE
DO  - 10.1109/GLOCOMW.2013.6824985
AB  - Security log analysis is extremely useful for uncovering intrusions and anomalies. However, the sheer volume of log data demands new frameworks and techniques of computing and security. We present a lightweight distributed and parallel security log analysis framework that allows organizations to analyze a massive number of system, network, and transaction logs efficiently and scalably. Different from the general distributed frameworks, e.g., MapReduce, our framework is specifically designed for security log analysis. It features a minimum set of necessary properties, such as dynamic task scheduling for streaming logs. For prototyping, we implement our framework in Amazon cloud environments (EC2 and S3) with a basic analysis application. Our evaluation demonstrates the effectiveness of our design and shows the potential of our cloud-based distributed framework in large-scale log analysis scenarios.
ER  - 

TY  - CONF
JO  - Software Engineering (ICSE), 2013 35th International Conference on
TI  - Measuring the forensic-ability of audit logs for nonrepudiation
T2  - Software Engineering (ICSE), 2013 35th International Conference on
IS  - 
SN  - 
VO  - 
SP  - 1419
EP  - 1422
AU  - King, J.
Y1  - 18-26 May 2013
PY  - 2013
KW  - auditing
KW  - digital forensics
KW  - educational computing
KW  - financial data processing
KW  - fraud
KW  - health care
KW  - software metrics
KW  - system monitoring
KW  - activity logging mechanism
KW  - application performance monitoring
KW  - audit log
KW  - compliance checking
KW  - data field
KW  - debugging
KW  - education
KW  - finance
KW  - forensic ability measurement
KW  - forensic analysis
KW  - fraud detection
KW  - grounded theory method
KW  - healthcare
KW  - log file attribute
KW  - software log files
KW  - software security events
KW  - software security metrics
KW  - software system
KW  - system resources
KW  - unique user identifier
KW  - user access tracking
KW  - user behavior profile extraction
KW  - user nonrepudiation
KW  - user privilege revocation
KW  - Forensics
KW  - Measurement
KW  - Medical services
KW  - Security
KW  - Software systems
KW  - Standards
KW  - forensics
KW  - grounded theory
KW  - logging
KW  - metric
KW  - nonrepudiation
KW  - security
KW  - software logs
VL  - 
JA  - Software Engineering (ICSE), 2013 35th International Conference on
DO  - 10.1109/ICSE.2013.6606732
AB  - Forensic analysis of software log files is used to extract user behavior profiles, detect fraud, and check compliance with policies and regulations. Software systems maintain several types of log files for different purposes. For example, a system may maintain logs for debugging, monitoring application performance, and/or tracking user access to system resources. The objective of my research is to develop and validate a minimum set of log file attributes and software security metrics for user nonrepudiation by measuring the degree to which a given audit log file captures the data necessary to allow for meaningful forensic analysis of user behavior within the software system. For a log to enable user nonrepudiation, the log file must record certain data fields, such as a unique user identifier. The log must also record relevant user activity, such as creating, viewing, updating, and deleting system resources, as well as software security events, such as the addition or revocation of user privileges. Using a grounded theory method, I propose a methodology for observing the current state of activity logging mechanisms in healthcare, education, and finance, then I quantify differences between activity logs and logs not specifically intended to capture user activity. I will then propose software security metrics for quantifying the forensic-ability of log files. I will evaluate my work with empirical analysis by comparing the performance of my metrics on several types of log files, including both activity logs and logs not directly intended to record user activity. My research will help software developers strengthen user activity logs for facilitating forensic analysis for user nonrepudiation.
ER  - 

TY  - CONF
JO  - Military Communications Conference (MILCOM), 2014 IEEE
TI  - Using Security Logs for Collecting and Reporting Technical Security Metrics
T2  - Military Communications Conference (MILCOM), 2014 IEEE
IS  - 
SN  - 
VO  - 
SP  - 294
EP  - 299
AU  - Vaarandi, R.
AU  - Pihelgas, M.
Y1  - 6-8 Oct. 2014
PY  - 2014
KW  - Big Data
KW  - computer network security
KW  - big data
KW  - log analysis methods
KW  - log analysis techniques
KW  - open source technology
KW  - security logs
KW  - technical security metric collection
KW  - technical security metric reporting
KW  - Correlation
KW  - Internet
KW  - Measurement
KW  - Monitoring
KW  - Peer-to-peer computing
KW  - Security
KW  - Workstations
KW  - security log analysis
KW  - security metrics
VL  - 
JA  - Military Communications Conference (MILCOM), 2014 IEEE
DO  - 10.1109/MILCOM.2014.53
AB  - During recent years, establishing proper metrics for measuring system security has received increasing attention. Security logs contain vast amounts of information which are essential for creating many security metrics. Unfortunately, security logs are known to be very large, making their analysis a difficult task. Furthermore, recent security metrics research has focused on generic concepts, and the issue of collecting security metrics with log analysis methods has not been well studied. In this paper, we will first focus on using log analysis techniques for collecting technical security metrics from security logs of common types (e.g., Network IDS alarm logs, workstation logs, and Net flow data sets). We will also describe a production framework for collecting and reporting technical security metrics which is based on novel open-source technologies for big data.
ER  - 

TY  - CONF
JO  - Systematic Approaches to Digital Forensic Engineering (SADFE), 2010 Fifth IEEE International Workshop on
TI  - Explorative Visualization of Log Data to Support Forensic Analysis and Signature Development
T2  - Systematic Approaches to Digital Forensic Engineering (SADFE), 2010 Fifth IEEE International Workshop on
IS  - 
SN  - 
VO  - 
SP  - 109
EP  - 118
AU  - Schmerl, S.
AU  - Vogel, M.
AU  - Rietz, R.
AU  - Ko&#x0308;nig, H.
Y1  - 20-20 May 2010
PY  - 2010
KW  - computer forensics
KW  - data visualisation
KW  - digital signatures
KW  - computers security threats
KW  - explorative log data visualization
KW  - forensic analysis
KW  - intrusion detection systems
KW  - signature development
KW  - Character generation
KW  - Communication networks
KW  - Communication system security
KW  - Data security
KW  - Data visualization
KW  - Digital forensics
KW  - Event detection
KW  - Information analysis
KW  - Information security
KW  - Proposals
KW  - Attack Signatures
KW  - Audit Data Analysis
KW  - Computer Forensic
KW  - Computer Security
KW  - Data Visualization
KW  - Intrusion Detection
KW  - Misuse Detection
VL  - 
JA  - Systematic Approaches to Digital Forensic Engineering (SADFE), 2010 Fifth IEEE International Workshop on
DO  - 10.1109/SADFE.2010.10
AB  - Today's growing number of security threats to computers and networks also increase the importance of log inspections to support the detection of possible breaches. The investigation and assessment of security incidents becomes more and more a daily business. Further, the manual log analysis is essentially in the context of developing signatures for intrusion detection systems (IDS), which allow for an automated defense against security attacks or incidents. But the analysis of log data in the context of fo-rensic investigations and IDS signature development is a tedious and time-consuming task, due to the large amount of textual data. Moreover, this task requires a skilled knowledge to differentiate between the important and the non-relevant information. In this paper, we propose an approach for log resp. audit data representation, which aims at simplifying the analysis process for the security officer. For this purpose audit data and existing relations between audit events are represented graphically in a three-dimensional space. We describe a general approach for analyzing and exploring audit or log data in the context of this presentation paradigm. Further, we introduce our tool, which implements this approach and demonstrate the strengths and benefits of this presentation and exploration form.
ER  - 

TY  - CONF
JO  - Network and Parallel Computing Workshops, 2007. NPC Workshops. IFIP International Conference on
TI  - A Log Analysis Audit Model Based on Optimized Clustering Algorithm
T2  - Network and Parallel Computing Workshops, 2007. NPC Workshops. IFIP International Conference on
IS  - 
SN  - 
VO  - 
SP  - 841
EP  - 848
AU  - Hui Yu
AU  - Xingjian Shi
Y1  - 18-21 Sept. 2007
PY  - 2007
KW  - auditing
KW  - pattern clustering
KW  - security of data
KW  - cluster number
KW  - network attack type
KW  - optimized clustering algorithm
KW  - security log analysis audit model
KW  - unknown intrusion detection
KW  - Algorithm design and analysis
KW  - Application software
KW  - Automatic control
KW  - Clustering algorithms
KW  - Computer science
KW  - Computer security
KW  - Data mining
KW  - Intrusion detection
KW  - Parallel processing
KW  - Protection
VL  - 
JA  - Network and Parallel Computing Workshops, 2007. NPC Workshops. IFIP International Conference on
DO  - 10.1109/NPC.2007.116
AB  - In view of the problem how to detect the network unknown attacks, a security log analysis audit model based on optimized clustering algorithm is proposed in this paper. Since the main question which influence the clustering algorithm application in the log analysis is uneasy to determine the network attack type and the cluster number, so we bring forward an optimized cluster algorithm to solve this problem. By means of simulated experiments, this algorithm is proved feasible, efficient and extensible for unknown intrusion detection.
ER  - 

TY  - CONF
JO  - Cloud Computing and Intelligence Systems (CCIS), 2014 IEEE 3rd International Conference on
TI  - A model for website anomaly detection based on log analysis
T2  - Cloud Computing and Intelligence Systems (CCIS), 2014 IEEE 3rd International Conference on
IS  - 
SN  - 
VO  - 
SP  - 604
EP  - 608
AU  - Xu Han
AU  - Tao Lv
AU  - Lin Wei
AU  - Yanyan Wu
AU  - Jianyi Liu
AU  - Cong Wang
Y1  - 27-29 Nov. 2014
PY  - 2014
KW  - Algorithm design and analysis
KW  - Analytical models
KW  - Classification algorithms
KW  - Data models
KW  - Databases
KW  - Feature extraction
KW  - Security
KW  - Anomaly detection
KW  - C4.5 algorithm
KW  - Feature sets
KW  - Log analysis
VL  - 
JA  - Cloud Computing and Intelligence Systems (CCIS), 2014 IEEE 3rd International Conference on
DO  - 10.1109/CCIS.2014.7175806
AB  - To found security events from web logs has become an important aspect of network security. This paper proposes a website anomaly detection model based on security-log-analysis. After creating a anomaly feature sets of the model, C4.5 algorithm was used to improve feature sets, making the abnormal records in feature sets store hierarchically. Compared logs in website with the treated feature stes, the model ultimately achieves the purpose of checking website's security event fast and accurately.
ER  - 

TY  - CONF
JO  - Data and Software Engineering (ICODSE), 2014 International Conference on
TI  - Information system log visualization to monitor anomalous user activity based on time
T2  - Data and Software Engineering (ICODSE), 2014 International Conference on
IS  - 
SN  - 
VO  - 
SP  - 1
EP  - 6
AU  - Hanniel, J.J.
AU  - Widagdo, T.E.
AU  - Asnar, Y.D.W.
Y1  - 26-27 Nov. 2014
PY  - 2014
KW  - Internet
KW  - cognition
KW  - data analysis
KW  - data visualisation
KW  - information systems
KW  - security of data
KW  - Web-based data visualization
KW  - anomalous user activity detection
KW  - anomalous user activity monitoring
KW  - anomaly detection
KW  - cognition
KW  - data variables
KW  - design concept
KW  - dot plot
KW  - focused exploration
KW  - heatmap
KW  - information system log visualization
KW  - log data analysis
KW  - log files analysis
KW  - security
KW  - time-based data visualization method
KW  - Data visualization
KW  - Geology
KW  - Heating
KW  - IP networks
KW  - Information systems
KW  - Java
KW  - Monitoring
KW  - anomalous user activity
KW  - data visualization
KW  - log file
VL  - 
JA  - Data and Software Engineering (ICODSE), 2014 International Conference on
DO  - 10.1109/ICODSE.2014.7062673
AB  - As information systems start to manage the more crucial parts of human lives, their security cannot be neglected. One way to ensure the security is by analyzing their generated log files of anomalous user activity. Data visualization has become a common solution to help get around the problems in log analysis. In this paper, we tried to determine key characteristics of effective data visualization on detecting those anomalous user activity recorded in log files. First we analyzed the log data we have and derived 4 anomalies whose indicators are made into visualization topics. Hence we built 4 data visualizations to detect the 4 anomalies. Next, we transformed our data so that they can be visualized. After that, we analyzed the suitable time-based data visualization method to represent our data and decided on heatmap for its wide application on existing solutions and dot plot for it is able to accommodate all data variables needed on every visualization topic and has the suitable nuance for monitoring purposes. Next we decided on design concept of our data visualizations and implemented them as web-based data visualization. We conducted 2 tests in this paper to determine the key characteristics of effective data visualization. Even though the results are inconclusive, but they hinted that an effective data visualization on this matter should support large amount of perceived information through cognition and support focused exploration.
ER  - 

TY  - JOUR
JO  - Information Forensics and Security, IEEE Transactions on
TI  - Trail of Bytes: New Techniques for Supporting Data Provenance and Limiting Privacy Breaches
T2  - Information Forensics and Security, IEEE Transactions on
IS  - 6
SN  - 1556-6013
VO  - 7
SP  - 1876
EP  - 1889
AU  - Krishnan, S.
AU  - Snow, K.Z.
AU  - Monrose, F.
Y1  - Dec. 2012
PY  - 2012
KW  - computer forensics
KW  - data privacy
KW  - computer systems
KW  - data access
KW  - data exfiltration attempts
KW  - data provenance
KW  - forensic analysis
KW  - forensic layer records
KW  - forensic platform
KW  - hypervisor
KW  - multiple disks
KW  - privacy breaches
KW  - tracking mechanism
KW  - version-based audit log
KW  - virtualized environment
KW  - Couplings
KW  - Forensics
KW  - Monitoring
KW  - Semantics
KW  - Virtual machine monitors
KW  - Virtual machining
KW  - Computer security
KW  - checkpointing
KW  - information security
KW  - intrusion detection
KW  - operating systems
KW  - system recovery
KW  - virtual machine monitors
VL  - 7
JA  - Information Forensics and Security, IEEE Transactions on
DO  - 10.1109/TIFS.2012.2210217
AB  - Forensic analysis of computer systems requires that one first identify suspicious objects or events, and then examine them in enough detail to form a hypothesis as to their cause and effect. Sadly, while our ability to gather vast amounts of data has improved significantly over the past two decades, it is all too often the case that we lack detailed information just when we need it the most. In this paper, we attempt to improve on the state of the art by providing a forensic platform that transparently monitors and records data access events within a virtualized environment using only the abstractions exposed by the hypervisor. Our approach monitors accesses to objects on disk and follows the causal chain of these accesses across processes, even after the objects are copied into memory. Our forensic layer records these transactions in a tamper evident version-based audit log that allows for faithful, and efficient, reconstruction of the recorded events and the changes they induced. To demonstrate the utility of our approach, we provide an extensive empirical evaluation, including a real-world case study demonstrating how our platform can be used to reconstruct valuable information about the what, when, and how, after a compromise has been detected. We also extend our earlier work by providing a tracking mechanism that can monitor data exfiltration attempts across multiple disks and also block attempts to copy data over the network.
ER  - 

TY  - CONF
JO  - Communication Systems and Network Technologies (CSNT), 2014 Fourth International Conference on
TI  - User and Device Tracking in Private Networks by Correlating Logs: A System for Responsive Forensic Analysis
T2  - Communication Systems and Network Technologies (CSNT), 2014 Fourth International Conference on
IS  - 
SN  - 
VO  - 
SP  - 1142
EP  - 1147
AU  - Chaudhari, S.
AU  - Chauhan, H.
AU  - Tomar, S.S.
AU  - Rawat, A.
Y1  - 7-9 April 2014
PY  - 2014
KW  - computer network security
KW  - local area networks
KW  - relational databases
KW  - DHCP
KW  - Flat file based sequential search system
KW  - IP address
KW  - Internet protocol address
KW  - RDBMS based tracking systems
KW  - browsing habits
KW  - device connection time
KW  - device location
KW  - device physical address
KW  - device tracking
KW  - electronic mail
KW  - email server logs
KW  - information context
KW  - mail access transactions
KW  - network access control
KW  - private networks
KW  - relational database management system
KW  - responsive forensic analysis
KW  - user tracking
KW  - Correlation
KW  - Databases
KW  - Electronic mail
KW  - Forensics
KW  - IP networks
KW  - Postal services
KW  - Servers
KW  - Logs; DHCP; NAC; squid; email; security logs; data monitoring and analysis tool
VL  - 
JA  - Communication Systems and Network Technologies (CSNT), 2014 Fourth International Conference on
DO  - 10.1109/CSNT.2014.253
AB  - IP address of a device, from where an offending activity was performed, is of limited value, because it does not specify a physical device/user, but an endpoint in network. It is useful to have information about where a device/user was at the time the offending activity was performed. It would be desirable to correlate different pieces of evidence to discover information, such as IP addresses used by the same device, physical address and location of the device, connection time of the device, browsing habits and mail access transactions carried out by the user using the device. Log data from various sources are required to be correlated together to create contexts of information, which is not visible from one source alone. In large networks, users/devices accessing a private network repeatedly can be tracked by analyzing and correlating DHCP, Network Access Control, WWW, Email server logs. With huge amount of logs, the common approach of manual browsing, correlating of log events, based on timelines is tedious, unresponsive approach. Flat file based sequential search system is not responsive, hence RDBMS based tracking systems are desirable. To build a responsive system requires identifying, consolidating log files, conversion, transmission and storage into relational databases. An automated system has been developed at our organization for forensic analysis of network accesses, with device and user tracking as its goal. We present, our approach to perform log management, correlation, which assists in performing responsive forensic analysis of real network with more than 2500 nodes, aimed at tracking users/devices.
ER  - 

TY  - CONF
JO  - Network Operations and Management Symposium, 2008. NOMS 2008. IEEE
TI  - Mining event logs with SLCT and LogHound
T2  - Network Operations and Management Symposium, 2008. NOMS 2008. IEEE
IS  - 
SN  - 1542-1201
VO  - 
SP  - 1071
EP  - 1074
AU  - Vaarandi, R.
Y1  - 7-11 April 2008
PY  - 2008
KW  - data mining
KW  - security of data
KW  - telecommunication computing
KW  - LogHound
KW  - communication networks
KW  - event log analysis
KW  - event logs mining
KW  - log data
KW  - security events
KW  - system management personnel
KW  - Algorithm design and analysis
KW  - Clustering algorithms
KW  - Communication networks
KW  - Communication system security
KW  - Data analysis
KW  - Data mining
KW  - Data security
KW  - Event detection
KW  - Monitoring
KW  - Personnel
KW  - data mining
KW  - data security
KW  - event log analysis
VL  - 
JA  - Network Operations and Management Symposium, 2008. NOMS 2008. IEEE
DO  - 10.1109/NOMS.2008.4575281
AB  - With the growth of communication networks, event logs are increasing in size at a fast rate. Today, it is not uncommon to have systems that generate tens of gigabytes of log data per day. Log data are likely to contain information that deserves closer attention - such as security events - but the task of reviewing logs manually is beyond the capabilities of a human. This paper discusses data mining tools SLCT and log hound that were designed for assisting system management personnel in extracting knowledge from event logs.
ER  - 

TY  - CONF
JO  - Communication Systems and Network Technologies (CSNT), 2014 Fourth International Conference on
TI  - Data Generation and Analysis for Digital Forensic Application Using Data Mining
T2  - Communication Systems and Network Technologies (CSNT), 2014 Fourth International Conference on
IS  - 
SN  - 
VO  - 
SP  - 458
EP  - 462
AU  - Khobragade, P.K.
AU  - Malik, L.G.
Y1  - 7-9 April 2014
PY  - 2014
KW  - computer crime
KW  - data analysis
KW  - data mining
KW  - digital forensics
KW  - firewalls
KW  - storage management
KW  - FTK 4.0
KW  - Web browser
KW  - cyber crime huge log data
KW  - cyber system
KW  - data analysis
KW  - data collection
KW  - data generation
KW  - data mining
KW  - data storage
KW  - digital forensic application
KW  - firewall logs
KW  - intrusion detection system
KW  - memory forensic analysis
KW  - network attack detection
KW  - network forensic analysis
KW  - network traces
KW  - network traffic
KW  - packet captures
KW  - remote system forensic
KW  - transactional data
KW  - Computers
KW  - Data mining
KW  - Data visualization
KW  - Databases
KW  - Digital forensics
KW  - Security
KW  - Clustering
KW  - Data Collection
KW  - Digital forensic tool
KW  - Log Data collection
VL  - 
JA  - Communication Systems and Network Technologies (CSNT), 2014 Fourth International Conference on
DO  - 10.1109/CSNT.2014.97
AB  - In the cyber crime huge log data, transactional data occurs which tends to plenty of data for storage and analyze them. It is difficult for forensic investigators to play plenty of time to find out clue and analyze those data. In network forensic analysis involves network traces and detection of attacks. The trace involves an Intrusion Detection System and firewall logs, logs generated by network services and applications, packet captures by sniffers. In network lots of data is generated in every event of action, so it is difficult for forensic investigators to find out clue and analyzing those data. In network forensics is deals with analysis, monitoring, capturing, recording, and analysis of network traffic for detecting intrusions and investigating them. This paper focuses on data collection from the cyber system and web browser. The FTK 4.0 is discussing for memory forensic analysis and remote system forensic which is to be used as evidence for aiding investigation.
ER  - 

TY  - CONF
JO  - Computer Software and Applications Conference Workshops (COMPSACW), 2010 IEEE 34th Annual
TI  - ULMS: An Accelerator for the Applications by Shifting Writing Log from Local Disk to Clouds
T2  - Computer Software and Applications Conference Workshops (COMPSACW), 2010 IEEE 34th Annual
IS  - 
SN  - 
VO  - 
SP  - 104
EP  - 108
AU  - Li Zhou
AU  - Yong Zhang
AU  - Chunxiao Xing
Y1  - 19-23 July 2010
PY  - 2010
KW  - Web services
KW  - data analysis
KW  - data mining
KW  - database management systems
KW  - disc storage
KW  - software architecture
KW  - system monitoring
KW  - SaaS method
KW  - ULMS
KW  - accelerator
KW  - database system
KW  - local disk
KW  - local storage efficiency
KW  - log analysis
KW  - log data analysis
KW  - log management
KW  - shift-log-by-ActiveMQ
KW  - shift-log-by-Webservice
KW  - user log mining system
KW  - writing log data shift
KW  - Log
KW  - Log Analysis
KW  - Performance
KW  - SaaS
VL  - 
JA  - Computer Software and Applications Conference Workshops (COMPSACW), 2010 IEEE 34th Annual
DO  - 10.1109/COMPSACW.2010.28
AB  - Log data is critical to applications and the management and analysis of log data is a hot research topic. Existing log managements are normally tightly integrated with applications themselves, which may lead to problems including performance, local storage efficiency, security and non realtime functionality. To solve these problems, we present a SaaS method which shifts writing log data from local disk to clouds, at the same time the log management and analysis functionalities are also done by a cloud. We analyze two architectures to implement this method which are Shift-Log-by-WebService and Shift-Log-by-ActiveMQ. Initial experiments show the efficiency of later one. In the future, we can apply this tool to application systems which are based on web and database systems to improve their performances.
ER  - 

TY  - CONF
JO  - Business Management and Electronic Information (BMEI), 2011 International Conference on
TI  - An improved algorithm with key attributes constraints for mining interesting association rules in network log
T2  - Business Management and Electronic Information (BMEI), 2011 International Conference on
IS  - 
SN  - 
VO  - 3
SP  - 104
EP  - 107
AU  - Jin Kezhong
AU  - Wu Chengwen
Y1  - 13-15 May 2011
PY  - 2011
KW  - computer forensics
KW  - data mining
KW  - pattern classification
KW  - security of data
KW  - association rule mining
KW  - computer forensic analysis
KW  - computer log data source
KW  - intrusion detection analysis
KW  - key attribute constraint
KW  - network access
KW  - network log data
KW  - outlier detection
KW  - user pattern mining
KW  - Algorithm design and analysis
KW  - Association rules
KW  - Computers
KW  - Databases
KW  - Performance evaluation
KW  - Protocols
KW  - association rule
KW  - data mining
KW  - key attribute
KW  - network log
VL  - 3
JA  - Business Management and Electronic Information (BMEI), 2011 International Conference on
DO  - 10.1109/ICBMEI.2011.5920405
AB  - Computer logs are generated by application activities, network accesses and system audit, which are important data sources for user pattern mining, computer forensic analysis, intrusion detection analysis and outlier detection. Algorithms for mining association rule are useful methods to find interesting rules implied in large computer log data. But existing algorithms which based on confidence and support are unfit for mining computer log data, many uninteresting rules will be generated and useful rules will be shadowed. To solve this problem, the concept of key attributes of network log data is introduced, and an algorithm with key attributes constraints for mining interesting association rules in network log data is designed. Experimental result shows that the number of uninteresting rules can be reduced effectively and the validity of rules which mined are improved.
ER  - 

TY  - CONF
JO  - Network Operations and Management Symposium (NOMS), 2012 IEEE
TI  - A distance-based method to detect anomalous attributes in log files
T2  - Network Operations and Management Symposium (NOMS), 2012 IEEE
IS  - 
SN  - 1542-1201
VO  - 
SP  - 498
EP  - 501
AU  - Hommes, S.
AU  - State, R.
AU  - Engel, T.
Y1  - 16-20 April 2012
PY  - 2012
KW  - computer network security
KW  - data analysis
KW  - information theory
KW  - statistical process control
KW  - system monitoring
KW  - ISP-provided firewall logs
KW  - anomalous attribute detection
KW  - automated log analysis
KW  - data volume
KW  - distance-based method
KW  - domain-specific knowledge
KW  - haystack problem
KW  - human expertise
KW  - information theory
KW  - log data analysis
KW  - operational logs
KW  - proverbial needle
KW  - real time analysis
KW  - statistical process control
KW  - suspicious network activity detection
KW  - Control charts
KW  - Correlation
KW  - Humans
KW  - IP networks
KW  - Process control
KW  - Protocols
KW  - Real time systems
VL  - 
JA  - Network Operations and Management Symposium (NOMS), 2012 IEEE
DO  - 10.1109/NOMS.2012.6211940
AB  - Dealing with large volumes of logs is like the proverbial needle in the haystack problem. Finding relevant events that might be associated with an incident, or real time analysis of operational logs is extremely difficult when the underlying data volume is huge and when no explicit misuse model exists. While domain-specific knowledge and human expertise may be useful in analysing log data, automated approaches for detecting anomalies and track incidents are the only viable solutions when confronted with large volumes of data. In this paper we address the issue of automated log analysis and consider more specifically the case of ISP-provided firewall logs. We leverage approaches derived from statistical process control and information theory in order to track potential incidents and detect suspicious network activity.
ER  - 

TY  - CONF
JO  - Computer Software and Applications Conference (COMPSAC), 2012 IEEE 36th Annual
TI  - Domain Independent Event Analysis for Log Data Reduction
T2  - Computer Software and Applications Conference (COMPSAC), 2012 IEEE 36th Annual
IS  - 
SN  - 0730-3157
VO  - 
SP  - 225
EP  - 232
AU  - Kalamatianos, T.
AU  - Kontogiannis, K.
AU  - Matthews, P.
Y1  - 16-20 July 2012
PY  - 2012
KW  - data analysis
KW  - data reduction
KW  - program diagnostics
KW  - security of data
KW  - DARPA Intrusion Detection Evaluation 1999 data sets
KW  - KDD 1999 data sets
KW  - domain independent event analysis
KW  - large software systems
KW  - log analysis technique
KW  - log data reduction
KW  - run time behavior analysis
KW  - similarity score
KW  - Algorithm design and analysis
KW  - Analytical models
KW  - Intrusion detection
KW  - Software
KW  - Standards
KW  - Weight measurement
KW  - Software engineering
KW  - dynamic analysis
KW  - log analysis
KW  - log reduction
KW  - software maintenance
KW  - system understanding
VL  - 
JA  - Computer Software and Applications Conference (COMPSAC), 2012 IEEE 36th Annual
DO  - 10.1109/COMPSAC.2012.33
AB  - Analyzing the run time behavior of large software systems is a difficult and challenging task. Log analysis has been proposed as a possible solution. However, such an analysis poses unique challenges, mostly due to the volume and diversity of the logged data that is collected, thus making this analysis often intractable for practical purposes. In this paper, we present a log analysis technique that aims to compute a smaller, compared to the original, collection of events that relate to a given analysis objective. The technique is based on computing a similarity score between the logged events and a collection of significant events that we refer to as beacons. The major novelties of the proposed technique are that it is domain independent and that it does not require the use of a pre-existing training data set. The technique has been evaluated against the DARPA Intrusion Detection Evaluation 1999 and the KDD 1999 data sets with promising results.
ER  - 

TY  - CONF
JO  - Computer Software and Applications, 2008. COMPSAC '08. 32nd Annual IEEE International
TI  - Pattern and Policy Driven Log Analysis for Software Monitoring
T2  - Computer Software and Applications, 2008. COMPSAC '08. 32nd Annual IEEE International
IS  - 
SN  - 0730-3157
VO  - 
SP  - 108
EP  - 111
AU  - Razavi, A.
AU  - Kontogiannis, Kostas
Y1  - July 28 2008-Aug. 1 2008
PY  - 2008
KW  - object-oriented programming
KW  - program diagnostics
KW  - risk analysis
KW  - security of data
KW  - Viterbi algorithm
KW  - component-based software
KW  - industrial software systems
KW  - pattern driven log analysis
KW  - pattern recognition
KW  - policy driven log analysis
KW  - software monitoring
KW  - system auditing
KW  - system diagnosis
KW  - system maintenance
KW  - system monitoring
KW  - system risk
KW  - system threat profile
KW  - Application software
KW  - Collaborative software
KW  - Computer industry
KW  - Context modeling
KW  - Monitoring
KW  - Pattern analysis
KW  - Pattern matching
KW  - Pattern recognition
KW  - Risk analysis
KW  - Software systems
KW  - Software Auditing
KW  - Software Monitoring
KW  - Trace Analysis
VL  - 
JA  - Computer Software and Applications, 2008. COMPSAC '08. 32nd Annual IEEE International
DO  - 10.1109/COMPSAC.2008.81
AB  - The component-based nature of large industrial software systems that consist of a number of diverse collaborating applications, pose significant challenges with respect to system maintenance, monitoring, auditing, and diagnosing. In this context, a monitoring and diagnostic system interprets log data to recognize patterns of significant events that conform to specific threat models. Threat models have been used by the software industry for analyzing and documenting a systempsilas risks in order to understand a systempsilas threat profile. In this paper, we propose a framework whereby patterns of significant events are represented as expressions of a specialized monitoring language that are used to annotate specific threat models. An approximate matching technique that is based on the Viterbi algorithm is then used to identify whether system generated events, fit the given patterns. The technique has been applied and evaluated considering threat models and monitoring policies in logs that have been obtained from multi-user MS-Windows based systems.
ER  - 

TY  - CONF
JO  - Information Security (AsiaJCIS), 2015 10th Asia Joint Conference on
TI  - iF2: An Interpretable Fuzzy Rule Filter for Web Log Post-Compromised Malicious Activity Monitoring
T2  - Information Security (AsiaJCIS), 2015 10th Asia Joint Conference on
IS  - 
SN  - 
VO  - 
SP  - 130
EP  - 137
AU  - Chih-Hung Hsieh
AU  - Yu-Siang Shen
AU  - Chao-Wen Li
AU  - Jain-Shing Wu
Y1  - 24-26 May 2015
PY  - 2015
KW  - Internet
KW  - data mining
KW  - fuzzy set theory
KW  - learning (artificial intelligence)
KW  - neural nets
KW  - pattern classification
KW  - statistical analysis
KW  - support vector machines
KW  - Internet address
KW  - SVM
KW  - Web log file tracking
KW  - Web log post-compromised malicious activity monitoring
KW  - Web-access log dataset
KW  - decision tree
KW  - expectation maximization based kernel algorithm
KW  - fuzzy rule filter
KW  - iF<sup>2</sup>
KW  - interpretable fuzzy rule filter
KW  - kernel based techniques
KW  - log data analysis
KW  - logic based classifiers
KW  - logic based techniques
KW  - machine learning methods
KW  - malicious activities
KW  - neural network
KW  - parameter optimization problem
KW  - recall rate
KW  - rule-based algorithm
KW  - support vector machine
KW  - Accuracy
KW  - Internet
KW  - Kernel
KW  - Monitoring
KW  - Optimization
KW  - Prediction algorithms
KW  - Support vector machines
KW  - Fuzzy Rule Based Filter
KW  - Machine Learning
KW  - Parameter Optimization
KW  - Pattern Recognition
KW  - Post-Compromised Threat Identification
KW  - Web Log Analysis
VL  - 
JA  - Information Security (AsiaJCIS), 2015 10th Asia Joint Conference on
DO  - 10.1109/AsiaJCIS.2015.19
AB  - To alleviate the loads of tracking web log file by human effort, machine learning methods are now commonly used to analyze log data and to identify the pattern of malicious activities. Traditional kernel based techniques, like the neural network and the support vector machine (SVM), typically can deliver higher prediction accuracy. However, the user of a kernel based techniques normally cannot get an overall picture about the distribution of the data set. On the other hand, logic based techniques, such as the decision tree and the rule-based algorithm, feature the advantage of presenting a good summary about the distinctive characteristics of different classes of data such that they are more suitable to generate interpretable feedbacks to domain experts. In this study, a real web-access log dataset from a certain organization was collected. An efficient interpretable fuzzy rule filter (iF<sup>2</sup>) was proposed as a filter to analyze the data and to detect suspicious internet addresses from the normal ones. The historical information of each internet address recorded in web log file is summarized as multiple statistics. And the design process of iF<sup>2</sup> is elaborately modeled as a parameter optimization problem which simultaneously considers 1) maximizing prediction accuracy, 2) minimizing number of used rules, and 3) minimizing number of selected statistics. Experimental results show that the fuzzy rule filter constructed with the proposed approach is capable of delivering superior prediction accuracy in comparison with the conventional logic based classifiers and the expectation maximization based kernel algorithm. On the other hand, though it cannot match the prediction accuracy delivered by the SVM, however, when facing real web log file where the ratio of positive and negative cases is extremely unbalanced, the proposed iF<sup>2</sup> of having optimization flexibility results in a better recall rate and enjoys one major advantage due to providing th- user with an overall picture of the underlying distributions.
ER  - 

TY  - CONF
JO  - Securecomm and Workshops, 2006
TI  - System Anomaly Detection: Mining Firewall Logs
T2  - Securecomm and Workshops, 2006
IS  - 
SN  - 
VO  - 
SP  - 1
EP  - 5
AU  - Winding, R.
AU  - Wright, T.
AU  - Chapple, M.
Y1  - Aug. 28 2006-Sept. 1 2006
PY  - 2006
KW  - authorisation
KW  - computer networks
KW  - data mining
KW  - learning (artificial intelligence)
KW  - statistical analysis
KW  - telecommunication traffic
KW  - data mining
KW  - firewall audit log mining
KW  - machine learning
KW  - network traffic anomalies
KW  - statistical analysis
KW  - system anomaly detection
KW  - Data mining
KW  - Data security
KW  - Forensics
KW  - Intrusion detection
KW  - Machine learning
KW  - Protection
KW  - Reconnaissance
KW  - Statistical analysis
KW  - Telecommunication traffic
KW  - Traffic control
KW  - Data mining
KW  - Firewall log analysis
KW  - Intrusion Detection
VL  - 
JA  - Securecomm and Workshops, 2006
DO  - 10.1109/SECCOMW.2006.359572
AB  - This paper describes an application of data mining and machine learning to discovering network traffic anomalies in firewall logs. There is a variety of issues and problems that can occur with systems that are protected by firewalls. These systems can be improperly configured, operate unexpected services, or fall victim to intrusion attempts. Firewall logs often generate hundreds of thousands of audit entries per day. It is often easy to use these records for forensics if one knows that something happened and when. However, it can be burdensome to attempt to manually review logs for anomalies. This paper uses data mining techniques to analyze network traffic, based on firewall audit logs, to determine if statistical analysis of the logs can be used to identify anomalies
ER  - 

TY  - CONF
JO  - Complex, Intelligent, and Software Intensive Systems (CISIS), 2013 Seventh International Conference on
TI  - An Integrated Distributed Log Management System with Metadata for Network Operation
T2  - Complex, Intelligent, and Software Intensive Systems (CISIS), 2013 Seventh International Conference on
IS  - 
SN  - 
VO  - 
SP  - 747
EP  - 750
AU  - Ikebe, M.
AU  - Yoshida, K.
Y1  - 3-5 July 2013
PY  - 2013
KW  - Internet
KW  - computer network management
KW  - computer network security
KW  - distributed sensors
KW  - meta data
KW  - system monitoring
KW  - campus networks
KW  - cross-processing system
KW  - integrated distributed log management system
KW  - log analysis
KW  - log collection
KW  - metadata
KW  - network administrators
KW  - network management tasks
KW  - network operation
KW  - sensor network
KW  - server administrators
KW  - server management tasks
KW  - Artificial intelligence
KW  - Distributed databases
KW  - Educational institutions
KW  - IP networks
KW  - Protocols
KW  - Prototypes
KW  - Servers
KW  - Distributed System
KW  - Metadata
KW  - Network Operation
KW  - Server Log
VL  - 
JA  - Complex, Intelligent, and Software Intensive Systems (CISIS), 2013 Seventh International Conference on
DO  - 10.1109/CISIS.2013.134
AB  - An enormous amount of log data is generated by servers and other devices on the network, and server/network administrators analyze the logs to investigate anomalous communications or troubleshoot. However, server/network management tasks increase in volume and complexity, resulting in greater burden on the administrator. In this paper, we propose a integrated management system for a sensor network where log data is output from many different kinds of sensors. We consider a server or network device as one of the sensors. We also propose a cross-processing system for several kinds of log data. In particular, we describe the management and collection of logs in our campus networks.
ER  - 


