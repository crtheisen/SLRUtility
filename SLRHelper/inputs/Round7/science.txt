TY  - JOUR
T1  - Security and privacy issues in implantable medical devices: A comprehensive survey
JO  - Journal of Biomedical Informatics
VL  - 55
IS  - 
SP  - 272
EP  - 289
PY  - 2015/6//
T2  - 
AU  - Camara, Carmen
AU  - Peris-Lopez, Pedro
AU  - Tapiador, Juan E.
SN  - 1532-0464
DO  - http://dx.doi.org/10.1016/j.jbi.2015.04.007
UR  - http://www.sciencedirect.com/science/article/pii/S153204641500074X
KW  - Implantable medical devices
KW  - Security
KW  - Privacy
KW  - m-Health
KW  - Survey
AB  - Abstract
Bioengineering is a field in expansion. New technologies are appearing to provide a more efficient treatment of diseases or human deficiencies. Implantable Medical Devices (IMDs) constitute one example, these being devices with more computing, decision making and communication capabilities. Several research works in the computer security field have identified serious security and privacy risks in IMDs that could compromise the implant and even the health of the patient who carries it. This article surveys the main security goals for the next generation of IMDs and analyzes the most relevant protection mechanisms proposed so far. On the one hand, the security proposals must have into consideration the inherent constraints of these small and implanted devices: energy, storage and computing power. On the other hand, proposed solutions must achieve an adequate balance between the safety of the patient and the security level offered, with the battery lifetime being another critical parameter in the design phase.
ER  - 

TY  - JOUR
T1  - Information security incident management: Current practice as reported in the literature
JO  - Computers & Security
VL  - 45
IS  - 
SP  - 42
EP  - 57
PY  - 2014/9//
T2  - 
AU  - Tøndel, Inger Anne
AU  - Line, Maria B.
AU  - Jaatun, Martin Gilje
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/j.cose.2014.05.003
UR  - http://www.sciencedirect.com/science/article/pii/S0167404814000819
KW  - Information security
KW  - Incident management
KW  - Incident response
KW  - ISO/IEC 27035
KW  - Systematic review
AB  - Abstract
This paper reports results of a systematic literature review on current practice and experiences with incident management, covering a wide variety of organisations. Identified practices are summarised according to the incident management phases of ISO/IEC 27035. The study shows that current practice and experience seem to be in line with the standard. We identify some inspirational examples that will be useful for organisations looking to improve their practices, and highlight which recommended practices generally are challenging to follow. We provide suggestions for addressing the challenges, and present identified research needs within information security incident management.
ER  - 

TY  - JOUR
T1  - A survey of information security incident handling in the cloud
JO  - Computers & Security
VL  - 49
IS  - 
SP  - 45
EP  - 69
PY  - 2015/3//
T2  - 
AU  - Ab Rahman, Nurul Hidayah
AU  - Choo, Kim-Kwang Raymond
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/j.cose.2014.11.006
UR  - http://www.sciencedirect.com/science/article/pii/S0167404814001680
KW  - Capability Maturity Model For Services (CMMI-SVC)
KW  - Cloud computing
KW  - Cloud response
KW  - Incident handling
KW  - Incident management
KW  - Incident response
AB  - Abstract
Incident handling strategy is one key strategy to mitigate risks to the confidentiality, integrity and availability (CIA) of organisation assets, as well as minimising loss (e.g. financial, reputational and legal) particularly as organisations move to the cloud. In this paper, we surveyed existing incident handling and digital forensic literature with the aims of contributing to the knowledge gap(s) in handling incidents in the cloud environment. 139 English language publications between January 2009 and May 2014 were located by searching various sources including the websites of standard bodies (e.g. National Institute of Standards and Technology) and academic databases (e.g. Google Scholar, IEEEXplore, ACM Digital Library, Springer and ScienceDirect). We then propose a conceptual cloud incident handling model that brings together incident handling, digital forensic and the Capability Maturity Model for Services to more effectively handle incidents for organisations using the cloud. A discussion of open research issues concludes this survey.
ER  - 

TY  - JOUR
T1  - Three cyber-security strategies to mitigate the impact of a data breach
JO  - Network Security
VL  - 2015
IS  - 1
SP  - 5
EP  - 8
PY  - 2015/1//
T2  - 
AU  - Densham, Ben
SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/S1353-4858(15)70007-3
UR  - http://www.sciencedirect.com/science/article/pii/S1353485815700073
AB  - We know that our mindsets need to shift these days and we must start by expecting to be hacked. But what then? How do we really manage an effective, secure environment? What steps do we need to consider?

The most common approach to keeping organisations safe is through layers of security. Yet frequent high-profile attacks show that this approach is no longer enough to ward off sophisticated threats.

Organisations must now alter their mindsets to expect a breach. It's better to assume your systems will be compromised. And with this in mind, they must have a ‘response in depth’ plan in order to mitigate the breach when it comes. Ben Densham of Nettitude provides a step-by-step guide to implementing an effective cyber-security strategy to ensure a potential disaster can be turned into a manageable incident.
ER  - 

TY  - JOUR
T1  - A new concentric-circle visualization of multi-dimensional data and its application in network security
JO  - Journal of Visual Languages & Computing
VL  - 21
IS  - 4
SP  - 194
EP  - 208
PY  - 2010/8//
T2  - Part Special Issue on Graph Visualization
AU  - Fu Lu, Liang
AU  - Wan Zhang, Jia
AU  - Lin Huang, Mao
AU  - Fu, Lei
SN  - 1045-926X
DO  - http://dx.doi.org/10.1016/j.jvlc.2010.05.002
UR  - http://www.sciencedirect.com/science/article/pii/S1045926X10000285
KW  - Concentric-circle coordinate
KW  - Multi-dimensional data visualization
KW  - Crossing reduction
KW  - Network visualization
KW  - Security visualization
KW  - Network intrusion detection
KW  - Polycurve
AB  - With the rapid growth of networked data communications in size and complexity, network administrators today are facing more challenges to protect their networked computers and devices from all kinds of attacks. This paper proposes a new concentric-circle visualization method for visualizing multi-dimensional network data. This method can be used to identify the main features of network attacks, such as DDoS attack, by displaying their recognizable visual patterns. To reduce the edge overlaps and crossings, we arrange multiple axes displayed as concentric circles rather than the traditional parallel lines. In our method, we use polycurves to link values (vertexes) rather than polylines used in parallel coordinate approach. Some heuristics are applied in our new method in order to improve the readability of views. We discuss the advantages as well as the limitations of our new method. In comparison with the parallel coordinate visualization, our approach can reduce more than 15% of the edge overlaps and crossings. In the second stage of the method, we have further enhanced the readability of views by increasing the edge crossing angle. Finally, we introduce our prototype system: a visual interactive network scan detection system called CCScanViewer. It is based on our new visualization approach and the experiments have showed that the new approach is effective in detecting attack features from a variety of networking patterns, such as the features of network scans and DDoS attacks.
ER  - 

TY  - JOUR
T1  - DSS for computer security incident response applying CBR and collaborative response
JO  - Expert Systems with Applications
VL  - 37
IS  - 1
SP  - 852
EP  - 870
PY  - 2010/1//
T2  - 
AU  - Kim, Huy Kang
AU  - Im, Kwang Hyuk
AU  - Park, Sang Chan
SN  - 0957-4174
DO  - http://dx.doi.org/10.1016/j.eswa.2009.05.100
UR  - http://www.sciencedirect.com/science/article/pii/S0957417409005223
KW  - Log analysis
KW  - System security
KW  - Misuse detection
KW  - Anomaly detection
KW  - Decision support system
KW  - Expert system
KW  - RFM analysis methodology
KW  - CBR (case based reasoning)
AB  - Recently, as hacking attempts increase dramatically; most enterprises are forced to employ some safeguards for hacking proof. For example, firewall or IPS (Intrusion Prevention System) selectively accepts the incoming packets, and IDS (Intrusion Detection System) detects the attack attempts from network. The latest version of firewall works in cooperation with IDS to immediately response to hacking attempts. However, it may make false alarms that misjudge normal traffic as hacking traffic and cause network problems to block the normal IP address by false alarms. By these false alarms made by IDS, system administrators or CSOs make wrong decisions and important data may be exposed or the availability of network or server system may be exhausted. Therefore, it is important to minimize the false alarms.

As a way of minimizing false alarms and supporting adequate decisions, we suggest the RFM (Recency, Frequency, Monetary) analysis methodology, which analyzes log files with incorporating three criteria of recency, frequency and monetary with statistical process control chart, and thus leads to an intuitive detection of anomaly and misuse events. Moreover, to cope with hacking attempts proactively, we apply CBR (case based reasoning) to find out similarities between already known hacking patterns and new hacking patterns. With the RFM analysis methodology and CBR, we develop DSS which can minimize false alarms and decrease the time to respond to hacking events. In case that RFM analysis module finds out unknown viruses or worms occurred, this CBR system matches the most similar incident case from case-based database. System administrators can easily get information about how to fix and how we fixed in similar cases. And CSOs can build a blacklist of frequently detected IP addresses and users. This blacklist can be used for incident handling.

Finally, we propose collaborative incident response system with DSS, this distributed agent systems interactively exchange the suspicious users and source IP addresses data and decide who is true-anomalous users and which IP addresses is the most riskiest and then deny all connections from that users and IP addresses automatically with less false-positives.
ER  - 

TY  - JOUR
T1  - BYOD security challenges: control and protect your most sensitive data
JO  - Network Security
VL  - 2012
IS  - 12
SP  - 5
EP  - 8
PY  - 2012/12//
T2  - 
AU  - Morrow, Bill
SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/S1353-4858(12)70111-3
UR  - http://www.sciencedirect.com/science/article/pii/S1353485812701113
AB  - Several new trends in information access are impacting organisations' ability to control and secure sensitive corporate data. The increase in web applications, cloud computing and Software as a Service (SaaS) offerings, and the Bring Your Own Device (BYOD) phenomenon, means that employees, business partners and customers are increasingly accessing information using a web browser on a device not owned or managed by the organisation.

The increase in web applications, cloud computing and Software as a Service (SaaS) offerings, and the Bring Your Own Device (BYOD) phenomenon are driving employees, business partners and customers to increasingly access information on devices are not managed by IT departments.

This has resulted in security implications for data leakage, data theft and regulatory compliance. To protect valuable information, organisations must stop making a distinction between devices in the corporate network and devices outside of it, argues Bill Morrow of Quarri Technologies.
ER  - 

TY  - JOUR
T1  - An empirical study of industrial security-engineering practices
JO  - Journal of Systems and Software
VL  - 61
IS  - 3
SP  - 225
EP  - 232
PY  - 2002/4/1/
T2  - 
AU  - Vaughn Jr., Rayford B.
AU  - Henning, Ronda
AU  - Fox, Kevin
SN  - 0164-1212
DO  - http://dx.doi.org/10.1016/S0164-1212(01)00150-9
UR  - http://www.sciencedirect.com/science/article/pii/S0164121201001509
KW  - Computer security
KW  - Information assurance
KW  - Security-engineering
KW  - Risk assessment
AB  - This paper presents lessons learned and observations noted about the state of security-engineering practices by three information security practitioners with different perspectives – two in industry and one in academia. All authors have more than 20-years experience in this field and two were former members of the US National Computer Security Center during the early days of the Trusted Computer System Evaluation Criteria and the strong promotion of trusted operating systems that accompanied the release of that document. In the last 20 years, it has been argued that security-engineering practices have not kept pace with the escalating threats to information systems. Much has occurred since that time – new security paradigms, failure of evaluated products to emerge into common use, new systemic threats, and an increased awareness of the risk faced by information systems. This paper presents an empirical view of lessons learned in security-engineering, experiences in applying the trade, and observations made about the successes and failures of security practices and technology. This work was sponsored in part by NSF Grant.
ER  - 

TY  - JOUR
T1  - An agent based and biological inspired real-time intrusion detection and security model for computer network operations
JO  - Computer Communications
VL  - 30
IS  - 13
SP  - 2649
EP  - 2660
PY  - 2007/9/26/
T2  - Sensor-Actuated NetworksSANETs
AU  - Boukerche, Azzedine
AU  - Machado, Renato B.
AU  - Jucá, Kathia R.L.
AU  - Sobral, João Bosco M.
AU  - Notare, Mirela S.M.A.
SN  - 0140-3664
DO  - http://dx.doi.org/10.1016/j.comcom.2007.03.008
UR  - http://www.sciencedirect.com/science/article/pii/S0140366407001181
KW  - Artificial immune system
KW  - Mobile agent
KW  - Computer networks
KW  - Intrusion detection model
AB  - There is a strong correlation between the human immune system and a computer network security system. The human immune system protects the human body from pathogenic elements in the same way that a computer security system protects the computer from malicious users. This paper presents a novel intrusion detection model based on artificial immune and mobile agent paradigms for network intrusion detection. The construction of the model is based on registries’ signature analysis using both Syslog-ng and Logcheck unix tools. The tasks of monitoring, distributing intrusion detection workload, storing relevant information, and ensuring data persistence and reactivity have been carried out by the mobile agents, which represent the leukocytes of an artificial immune system. Our real-time based intrusion detection and communication model is host-based and adopts the anomaly detection paradigm. We present our intrusion detection model, discuss its implementation, and report on its performance evaluation using real data provided by an Internet Service Provider and a data processing corporation.
ER  - 

TY  - JOUR
T1  - Making sense of log management for security purposes – an approach to best practice log collection, analysis and management
JO  - Computer Fraud & Security
VL  - 2007
IS  - 5
SP  - 5
EP  - 10
PY  - 2007/5//
T2  - 
AU  - Gorge, Mathieu
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(07)70047-7
UR  - http://www.sciencedirect.com/science/article/pii/S1361372307700477
AB  - Every computer action registers a log somewhere – giving a rich source of data that can help businesses identify any trace of corruption within their networks. Log collection is also a strong component of keeping in line with legislation such as Sarbanes-Oxley, HIPAA, GLBA in the US, and the European Data Protection Directive in the EU. Mathieu Gorge looks at what logs organizations need to keep and what standards require their storage.

He recommends proactive monitoring of firewalls, anti-virus, VPNs and IDS logs among other security systems. The main goal is to link a transaction back to an individual user in order to perform a forensic investigation. But it is important to be wary, as some countries do not allow companies to monitor staff usage of IT systems. See page 3 on how the European court ruled that a British college's monitoring of one employee was a breach of human rights. Therefore, linking a log with a person's actions may not stand up in court.

Gorge says logs can give as good an insight into external attacks as well as internally driven ones.

Logs should be analyzed for the following:
				•
User account activity: creation, elevation of privilege, changes, inactivity.
•
Client requests and server response.
•
Operational status: shutdown (planned or unplanned), system failure and automatic restart.
•
Usage information and trends – basic user behaviour analysis.


It is best practice to collect, store and analyze logs with a view to being able to get complete, accurate and verifiable information. This will improve the organization's ability to comply with key standards and legislation as regards e-evidence. It could save an organization from potential liability and repair costs and will give visibility over mission critical and security systems, performance and usage. The main advice is to remain proactive so as to be able to respond to a security incident and comply with legal requests should anything happen.

Mathieu Gorge looks at what logs can do for your business and how governance demands them.
ER  - 

TY  - JOUR
T1  - The changing face of IT security
JO  - Network Security
VL  - 2006
IS  - 11
SP  - 16
EP  - 17
PY  - 2006/11//
T2  - 
AU  - Potter, Bruce
SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/S1353-4858(06)70454-8
UR  - http://www.sciencedirect.com/science/article/pii/S1353485806704548
AB  - Faced with multiple changes in the IT landscape, it is sometimes hard to keep track of where the technology trends are going. Some varieties of attacks occurring today leave traditional security measures standing.
ER  - 

TY  - JOUR
T1  - Enhanced Internet security by a distributed traffic control service based on traffic ownership
JO  - Journal of Network and Computer Applications
VL  - 30
IS  - 3
SP  - 841
EP  - 857
PY  - 2007/8//
T2  - 
AU  - Bossardt, Matthias
AU  - Dübendorfer, Thomas
AU  - Plattner, Bernhard
SN  - 1084-8045
DO  - http://dx.doi.org/10.1016/j.jnca.2005.07.006
UR  - http://www.sciencedirect.com/science/article/pii/S1084804505000470
KW  - Traffic control
KW  - Network management
KW  - Network services
KW  - Mitigation
KW  - Distributed denial of service attack
AB  - The frequency and intensity of Internet attacks are rising at an alarming pace. Several technologies and concepts were proposed for fighting distributed denial of service (DDoS) attacks: traceback, pushback, i3, SOS and Mayday. This paper shows that in the case of DDoS reflector attacks they are either ineffective or even counterproductive. We then propose the novel concept of traffic ownership and describe a system that extends control over network traffic by network users to the Internet using adaptive traffic processing devices. We safely delegate partial network management capabilities from network operators to network users. All network packets with a source or destination address “owned” by a network user can now also be controlled within the Internet instead of only at the network user's Internet uplink. By limiting the traffic control features and by restricting the realm of control to the “owner” of the traffic, we can rule out misuse of this system. Applications of our system are manifold: prevention of source address spoofing, DDoS attack mitigation, distributed firewall-like filtering, new ways of collecting traffic statistics, service-level agreement validation, traceback, distributed network debugging, support for forensic analyses and many more. A use case illustrates how our system enables network users to prevent and react to DDoS attacks.
ER  - 

TY  - JOUR
T1  - Security in network attached storage (NAS) for workgroups
JO  - Network Security
VL  - 2004
IS  - 4
SP  - 8
EP  - 12
PY  - 2004/4//
T2  - 
AU  - Edelson, Eve
SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/S1353-4858(04)00065-0
UR  - http://www.sciencedirect.com/science/article/pii/S1353485804000650
AB  - Network-attached storage (NAS) is a relatively simple and inexpensive way to serve files over a network in a cross-platform environment. NAS devices face the same security challenges as other network components. This article discusses how NAS fits into the world of IP storage, some security features present in (and missing from) NAS devices, and some security considerations in choosing a NAS.
ER  - 

TY  - JOUR
T1  - BitTorrent Sync: First Impressions and Digital Forensic Implications
JO  - Digital Investigation
VL  - 11, Supplement 1
IS  - 
SP  - S77
EP  - S86
PY  - 2014/5//
T2  - Proceedings of the First Annual DFRWS Europe
AU  - Farina, Jason
AU  - Scanlon, Mark
AU  - Kechadi, M-Tahar
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2014.03.010
UR  - http://www.sciencedirect.com/science/article/pii/S1742287614000152
KW  - BitTorrent
KW  - Sync
KW  - Peer-to-Peer
KW  - Synchronisation
KW  - Privacy
KW  - Digital forensics
AB  - Abstract
With professional and home Internet users becoming increasingly concerned with data protection and privacy, the privacy afforded by popular cloud file synchronisation services, such as Dropbox, OneDrive and Google Drive, is coming under scrutiny in the press. A number of these services have recently been reported as sharing information with governmental security agencies without warrants. BitTorrent Sync is seen as an alternative by many and has gathered over two million users by December 2013 (doubling since the previous month). The service is completely decentralised, offers much of the same synchronisation functionality of cloud powered services and utilises encryption for data transmission (and optionally for remote storage). The importance of understanding BitTorrent Sync and its resulting digital investigative implications for law enforcement and forensic investigators will be paramount to future investigations. This paper outlines the client application, its detected network traffic and identifies artefacts that may be of value as evidence for future digital investigations.
ER  - 

TY  - JOUR
T1  - Security Log Management
JO  - Network Security
VL  - 2003
IS  - 11
SP  - 6
EP  - 9
PY  - 2003/11//
T2  - 
AU  - Lobo, Colin
SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/S1353-4858(03)01106-1
UR  - http://www.sciencedirect.com/science/article/pii/S1353485803011061
AB  - Have your systems been configured to write events to the system logs? Most people would answer “Yes”. Are they using the standard default settings? The answer to this will vary. How often are the logs examined? Most people would answer only “when there is a problem”.
ER  - 

TY  - JOUR
T1  - Vulnerabilities and mitigation techniques toning in the cloud: A cost and vulnerabilities coverage optimization approach using Cuckoo search algorithm with Lévy flights
JO  - Computers & Security
VL  - 48
IS  - 
SP  - 1
EP  - 18
PY  - 2015/2//
T2  - 
AU  - Zineddine, Mhamed
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/j.cose.2014.09.002
UR  - http://www.sciencedirect.com/science/article/pii/S0167404814001333
KW  - Cloud computing
KW  - ICT security
KW  - Vulnerabilities mapping
KW  - Cuckoo search algorithm
KW  - Lévy flights algorithm
KW  - Optimization
AB  - Abstract
Information and Communication Technology (ICT) security issues have been a major concern for decades. Today's ICT infrastructure faces sophisticated attacks using combinations of multiple vulnerabilities to penetrate networks with devastating impact. With the recent rise of cloud computing as a new utility computing paradigm, organizations have been considering it as a viable option to outsource major IT services in order to cut costs. Some organizations have opted for a private or hybrid cloud to take advantage of the emerging technologies and services. However, ICT security issues have to be appropriately mitigated. This research proposes a cloud security framework and an approach for vulnerabilities coverage and cost optimization using Cuckoo search algorithm with Lévy flights as random walks. The objective is to mitigate an identified set of vulnerabilities using a selected set of techniques when minimizing cost and maximizing coverage. The results show that Cloud Computing providers and organizations implementing cloud technology within their premises can effectively balance IT security coverage and cost using the proposed approach.
ER  - 

TY  - JOUR
T1  - Secured Temporal Log Management Techniques for Cloud
JO  - Procedia Computer Science
VL  - 46
IS  - 
SP  - 589
EP  - 595
PY  - 2015///
T2  - Proceedings of the International Conference on Information and Communication Technologies, ICICT 2014, 3-5 December 2014 at Bolgatty Palace &amp; Island Resort, Kochi, India
AU  - Muthurajkumar, S.
AU  - Ganapathy, S.
AU  - Vijayalakshmi, M.
AU  - Kannan, A.
SN  - 1877-0509
DO  - http://dx.doi.org/10.1016/j.procs.2015.02.098
UR  - http://www.sciencedirect.com/science/article/pii/S1877050915001623
KW  - Cloud computing
KW  - Log management
KW  - Cloud Storage
KW  - Cloud Security;
AB  - Abstract
Log Management has been an important service in Cloud Computing. In any business, maintaining the log records securely over a particular period of time is absolutely necessary for various reasons such as auditing, forensic analysis, evidence etc. In this work, Integrity and confidentiality of the log records are maintained at every stage of Log Management namely the Log Generation phase, Transmission phase and Storage phase. In addition to this, Log records may often contain sensitive information about the organization which should not be leaked to the outside world. In this paper, Temporal Secured Cloud Log Management Algorithm techniques are implemented to provide security to maintain transaction history in cloud within time period. In this work, security to temporal log management is provided by encrypting the log data before they are stored in the cloud storage. They are also stored in batches for easy retrieval. This work was implemented in Java programming language in the Google drive environment.
ER  - 

TY  - JOUR
T1  - Insider Threat Detection Using Log Analysis and Event Correlation
JO  - Procedia Computer Science
VL  - 45
IS  - 
SP  - 436
EP  - 445
PY  - 2015///
T2  - International Conference on Advanced Computing Technologies and Applications (ICACTA)
AU  - Ambre, Amruta
AU  - Shekokar, Narendra
SN  - 1877-0509
DO  - http://dx.doi.org/10.1016/j.procs.2015.03.175
UR  - http://www.sciencedirect.com/science/article/pii/S1877050915004184
KW  - Security
KW  - log analysis
KW  - event correlation
KW  - Bayesian detection rate
KW  - false alarm rate
AB  - Abstract
Insider threat is one of the most dangerous security threat, and a much more complex issue. These insiders can be a former or a disgruntled employee or any business associate that has or had an authorised access to information for any particular organization. They have control and security measures. Hence continuous monitoring is essential to track each and every activity within the network. Log management is a strong technique which includes both Log analysis with event correlation which provides the root cause of any attack and network can be protected from security violations. Though intrusion detection is complex process, while checking the ability to detect intrusive behaviour within the internal environment, it has to take care of suppressing the false alarm rate. Some strong approach is required on the basis of which decisions can be taken fast. This paper proposes a probabilistic approach which illustrates the frequency of occurrence of event in percentage while still considering the false alarm rate at an acceptable level.
ER  - 

TY  - JOUR
T1  - Control systems/SCADA forensics, what's the difference?
JO  - Digital Investigation
VL  - 11
IS  - 3
SP  - 160
EP  - 174
PY  - 2014/9//
T2  - Special Issue: Embedded Forensics
AU  - van der Knijff, R.M.
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2014.06.007
UR  - http://www.sciencedirect.com/science/article/pii/S1742287614000814
KW  - Forensics
KW  - Control systems
KW  - SCADA
KW  - ICS
KW  - Cyber security
AB  - Abstract
Immature IT security, increasing network connectivity and unwavering media attention is causing an increase in the number of control system cyber security incidents. For forensic examinations in these environments, knowledge and skills are needed in the field of hardware, networks and data analysis. For forensic examiners, this paper is meant to be a crash course on control systems and their forensic opportunities, focussing on the differences compared to regular IT systems. Assistance from experienced field engineers during forensic acquisition of control systems seems inevitable in order to guarantee process safety, business continuity and examination efficiency. For people working in the control system community, this paper may be helpful to get an idea about specific forensic issues about which they would normally not bother, but may be crucial as soon as their systems are under attack or become part of a law enforcement investigation. For analysis of acquired data, existing tools for network security monitoring have useful functionality for forensic applications but are designed for real-time acquisition and often not directly usable for post-mortem analysis of acquired data in a forensically sound way. The constant and predictable way in which control systems normally behave makes forensic application of anomaly-based threat detection an interesting topic for further research.
ER  - 

TY  - JOUR
T1  - Online game bot detection based on party-play log analysis
JO  - Computers & Mathematics with Applications
VL  - 65
IS  - 9
SP  - 1384
EP  - 1395
PY  - 2013/5//
T2  - Advanced Information Security
AU  - Kang, Ah Reum
AU  - Woo, Jiyoung
AU  - Park, Juyong
AU  - Kim, Huy Kang
SN  - 0898-1221
DO  - http://dx.doi.org/10.1016/j.camwa.2012.01.034
UR  - http://www.sciencedirect.com/science/article/pii/S0898122112000442
KW  - Online game security
KW  - Game bot
KW  - User behavior analysis
KW  - MMORPG
AB  - As online games become popular and the boundary between virtual and real economies blurs, cheating in games has proliferated in volume and method. In this paper, we propose a framework for user behavior analysis for bot detection in online games. Specifically, we focus on party play which reflects the social activities among gamers: in a Massively Multi-user Online Role Playing Game (MMORPG), party play is a major activity that game bots exploit to keep their characters safe and facilitate the acquisition of cyber assets in a fashion very different from that of normal humans. Through a comprehensive statistical analysis of user behaviors in game activity logs, we establish threshold levels for the activities that allow us to identify game bots. Based on this, we also build a knowledge base of detection rules, which are generic. We apply our rule reasoner to AION, a popular online game serviced by NCsoft, Inc., a leading online game company based in Korea.
ER  - 

TY  - JOUR
T1  - Acquiring forensic evidence from infrastructure-as-a-service cloud computing: Exploring and evaluating tools, trust, and techniques
JO  - Digital Investigation
VL  - 9, Supplement
IS  - 
SP  - S90
EP  - S98
PY  - 2012/8//
T2  - The Proceedings of the Twelfth Annual DFRWS Conference12th Annual Digital Forensics Research Conference
AU  - Dykstra, Josiah
AU  - Sherman, Alan T.
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2012.05.001
UR  - http://www.sciencedirect.com/science/article/pii/S1742287612000266
KW  - Computer security
KW  - Cloud computing
KW  - Digital forensics
KW  - Cloud forensics
KW  - EnCase
KW  - FTK
KW  - Amazon EC2
AB  - We expose and explore technical and trust issues that arise in acquiring forensic evidence from infrastructure-as-a-service cloud computing and analyze some strategies for addressing these challenges. First, we create a model to show the layers of trust required in the cloud. Second, we present the overarching context for a cloud forensic exam and analyze choices available to an examiner. Third, we provide for the first time an evaluation of popular forensic acquisition tools including Guidance EnCase and AccesData Forensic Toolkit, and show that they can successfully return volatile and non-volatile data from the cloud. We explain, however, that with those techniques judge and jury must accept a great deal of trust in the authenticity and integrity of the data from many layers of the cloud model. In addition, we explore four other solutions for acquisition—Trusted Platform Modules, the management plane, forensics-as-a-service, and legal solutions, which assume less trust but require more cooperation from the cloud service provider. Our work lays a foundation for future development of new acquisition methods for the cloud that will be trustworthy and forensically sound. Our work also helps forensic examiners, law enforcement, and the court evaluate confidence in evidence from the cloud.
ER  - 

TY  - JOUR
T1  - A framework for incident response management in the petroleum industry
JO  - International Journal of Critical Infrastructure Protection
VL  - 2
IS  - 1–2
SP  - 26
EP  - 37
PY  - 2009/5//
T2  - 
AU  - Jaatun, Martin Gilje
AU  - Albrechtsen, Eirik
AU  - Line, Maria B.
AU  - Tøndel, Inger Anne
AU  - Longva, Odd Helge
SN  - 1874-5482
DO  - http://dx.doi.org/10.1016/j.ijcip.2009.02.004
UR  - http://www.sciencedirect.com/science/article/pii/S1874548209000043
KW  - Incident response
KW  - Process control systems
KW  - Learning
KW  - Security culture
AB  - Incident response is the process of responding to and handling security-related incidents involving information and communications technology (ICT) infrastructure and data. Incident response has traditionally been reactive in nature, focusing mainly on technical issues. This paper presents the Incident Response Management (IRMA) method, which combines traditional incident response with proactive learning and socio-technical perspectives. The IRMA method is targeted at integrated operations within the petroleum industry, but it is also applicable to other industries that rely on process control systems.
ER  - 

TY  - JOUR
T1  - NetHost-sensor: Monitoring a target host's application via system calls
JO  - Information Security Technical Report
VL  - 11
IS  - 4
SP  - 166
EP  - 175
PY  - 2006///
T2  - 
AU  - Abimbola, A.A.
AU  - Munoz, J.M.
AU  - Buchanan, W.J.
SN  - 1363-4127
DO  - http://dx.doi.org/10.1016/j.istr.2006.10.003
UR  - http://www.sciencedirect.com/science/article/pii/S1363412706000550
KW  - Intrusion detection
KW  - Network
KW  - Host
KW  - Application security
KW  - Dynamic link libraries
KW  - System calls
AB  - Intrusion detection has emerged as an important approach to network, host and application security. Network security includes analysing network packet payload and other inert network packet profiles for intrusive trends; whereas, host security may employ system logs for intrusion detection. In this paper, we contribute to the research community by tackling application security and attempt to detect intrusion via differentiating normal and abnormal application behaviour. A method for anomaly intrusion detection for applications is proposed based on deterministic system call traces derived from a monitored target application's dynamic link libraries (DLLs). We isolate associated DLLs of a monitored target application; log system call traces of the application in real time and use heuristic method to detect intrusion before the application is fully compromised. Our investigative research experiment methodology and set-up are reported, alongside our experimental procedure and results that show our research effort is effective and efficient, and can be used in practice to monitor a target application in real time.
ER  - 

TY  - JOUR
T1  - Wired and wireless intrusion detection system: Classifications, good characteristics and state-of-the-art
JO  - Computer Standards & Interfaces
VL  - 28
IS  - 6
SP  - 670
EP  - 694
PY  - 2006/9//
T2  - 
AU  - Sobh, Tarek S.
SN  - 0920-5489
DO  - http://dx.doi.org/10.1016/j.csi.2005.07.002
UR  - http://www.sciencedirect.com/science/article/pii/S092054890500098X
KW  - Wireless network
KW  - Network-based security
KW  - Host-based security
KW  - Intrusion detection system
KW  - Intrusion prevention systems
KW  - Sniffering
AB  - In computer and network security, standard approaches to intrusion detection and response attempt to detect and prevent individual attacks. Intrusion Detection System (IDS) and intrusion prevention systems (IPS) are real-time software for risk assessment by monitoring for suspicious activity at the network and system layer. Software scanner allows network administrator to audit the network for vulnerabilities and thus securing potential holes before attackers take advantage of them.

In this paper we try to define the intruder, types of intruders, detection behaviors, detection approaches and detection techniques. This paper presents a structural approach to the IDS by introducing a classification of IDS. It presents important features, advantages and disadvantages of each detection approach and the corresponding detection techniques. Furthermore, this paper introduces the wireless intrusion protection systems.

The goal of this paper is to place some characteristics of good IDS and examine the positioning of intrusion prevention as part of an overall layered security strategy and a review of evaluation criteria for identifying and selecting IDS and IPS. With this, we hope to introduce a good characteristic in order to improve the capabilities for early detection of distributed attacks in the preliminary phases against infrastructure and take a full spectrum of manual and automatic response actions against the source of attacks.
ER  - 

TY  - JOUR
T1  - CareWeb™, a web-based medical record for an integrated health care delivery system
JO  - International Journal of Medical Informatics
VL  - 54
IS  - 1
SP  - 1
EP  - 8
PY  - 1999/4//
T2  - 
AU  - Halamka, John D.
AU  - Osterland, Carsten
AU  - Safran, Charles
SN  - 1386-5056
DO  - http://dx.doi.org/10.1016/S1386-5056(98)00095-1
UR  - http://www.sciencedirect.com/science/article/pii/S1386505698000951
KW  - Patient records
KW  - Internet
KW  - Security
AB  - With the advent of Integrated Health care Delivery Systems, medical records are increasingly distributed across multiple institutions. Timely access to these medical records is a critical need for health care providers. The CareWeb™ project provides an architecture for World Wide Web-based retrieval of electronic medical records from heterogeneous data sources. Using Health Level 7 (HL7), web technologies and readily available software components, we consolidated the electronic records of Boston's Beth Israel and Deaconess Hospitals. We report on the creation of CareWeb™ (freya.bidmc.harvard.edu/careweb.htm) and propose it as a means to electronically link Integrated Health care Delivery Systems and geographically distant information resources.
ER  - 

TY  - JOUR
T1  - Secure Audit Log Management
JO  - Procedia Computer Science
VL  - 22
IS  - 
SP  - 1249
EP  - 1258
PY  - 2013///
T2  - 17th International Conference in Knowledge Based and Intelligent Information and Engineering Systems - KES2013
AU  - Söderström, Olof
AU  - Moradian, Esmiralda
SN  - 1877-0509
DO  - http://dx.doi.org/10.1016/j.procs.2013.09.212
UR  - http://www.sciencedirect.com/science/article/pii/S1877050913010053
KW  - Secure Log Management
KW  - Log Analysis
KW  - Log Server
KW  - Audit Log Event
AB  - Abstract
Log management and analysis is a vital part of organization's network management and system administration. Logs indicate current status of the system and contain information that refers to different security events, which occur within the system. Logs are used for different purposes, such as recording user activities, track authentication attempts, and other security events. Due to increasing number of threats against networks and systems, the number of security logs increases. However, many organizations that work in a distributed environment face following problems: log generation and storage, log protection, and log analysis. Moreover, ensuring that security, system and network administrators analyze log data in an effective way is another issue. In this research, we propose an approach for receiving, storing and administrating audit log events. Furthermore, we present a solution design that in a secure way allows organizations in distributed environments to send audit log transactions from different local networks to one centralized server.
ER  - 

TY  - JOUR
T1  - Get ready for PCI DSS 3.0 with real-time monitoring
JO  - Computer Fraud & Security
VL  - 2015
IS  - 2
SP  - 17
EP  - 18
PY  - 2015/2//
T2  - 
AU  - Fernandes, Joel John
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(15)30009-9
UR  - http://www.sciencedirect.com/science/article/pii/S1361372315300099
AB  - PCI DSS 3.0 compliance has gained worldwide acceptance by card service providers – card issuers, banks, and merchants – that plan to protect their customers' cardholder data from being misused. PCI DSS 3.0 has 12 security requirements concerning the protection of cardholder data. All businesses that accept, store, process or transmit customers card data either online or offline have to adhere to those requirements.
ER  - 

TY  - JOUR
T1  - On Incident Handling and Response: A state-of-the-art approach
JO  - Computers & Security
VL  - 25
IS  - 5
SP  - 351
EP  - 370
PY  - 2006/7//
T2  - 
AU  - Mitropoulos, Sarandis
AU  - Patsos, Dimitrios
AU  - Douligeris, Christos
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/j.cose.2005.09.006
UR  - http://www.sciencedirect.com/science/article/pii/S0167404805001574
KW  - Incident Handling
KW  - Incident Response
KW  - Computer forensics
KW  - Internet forensics
KW  - Software forensics
KW  - Trace-back mechanisms
AB  - Incident Response has always been an important aspect of Information Security but it is often overlooked by security administrators. Responding to an incident is not solely a technical issue but has many management, legal, technical and social aspects that are presented in this paper. We propose a detailed management framework along with a complete structured methodology that contains best practices and recommendations for appropriately handling a security incident. We also present the state-of-the art technology in computer, network and software forensics as well as automated trace-back artifacts, schemas and protocols. Finally, we propose a generic Incident Response process within a corporate environment.
ER  - 

TY  - JOUR
T1  - Identifying threats in real time
JO  - Network Security
VL  - 2013
IS  - 11
SP  - 5
EP  - 8
PY  - 2013/11//
T2  - 
AU  - Macrae, Alistair
SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/S1353-4858(13)70119-3
UR  - http://www.sciencedirect.com/science/article/pii/S1353485813701193
AB  - The technology landscape is constantly evolving, and as it does cyber-criminals have no problem keeping pace by altering their increasingly sophisticated tactics to exploit vulnerabilities in the networks of organisations, individuals and nation-states. As such, the fast and accurate detection of potential cyber-threats has become a critical capability in order to avoid significant damage to reputations, sensitive information and, in some extreme cases, lives.

As cyber-attacks become an increasing risk for every organisation, the detection and remediation of any issues must occur immediately to avoid irreparable damage. And IT security professionals should gather as much intelligence about a breach as possible.

Only through advanced correlative, statistical, behavioural and pattern recognition techniques can threats be identified in real time. Alistair Macrae of LogRhythm explores the techniques that can be used for such real-time identification and analysis of breaches, as well as how to navigate the complicated minefield of cyber-security forensic requirements and investigative procedures.
ER  - 

TY  - JOUR
T1  - Network investigation methodology for BitTorrent Sync: A Peer-to-Peer based file synchronisation service
JO  - Computers & Security
VL  - 
IS  - 
SP  - 
EP  - 
PY  - 
T2  - 
AU  - Scanlon, Mark
AU  - Farina, Jason
AU  - Kechadi, M-Tahar
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/j.cose.2015.05.003
UR  - http://www.sciencedirect.com/science/article/pii/S016740481500067X
KW  - BitTorrent Sync
KW  - Distributed storage
KW  - Peer-to-Peer
KW  - Network traffic analysis
KW  - Remote evidence acquisition
AB  - Abstract
High availability is no longer just a business continuity concern. Users are increasingly dependant on devices that consume and produce data in ever increasing volumes. A popular solution is to have a central repository which each device accesses after centrally managed authentication. This model of use is facilitated by cloud based file synchronisation services such as Dropbox, OneDrive, Google Drive and Apple iCloud. Cloud architecture allows the provisioning of storage space with “always-on” access. Recent concerns over unauthorised access to third party systems and large scale exposure of private data have made an alternative solution desirable. These events have caused users to assess their own security practices and the level of trust placed in third party storage services. One option is BitTorrent Sync, a cloudless synchronisation utility provides data availability and redundancy. This utility replicates files stored in shares to remote peers with access controlled by keys and permissions. While lacking the economies brought about by scale, complete control over data access has made this a popular solution. The ability to replicate data without oversight introduces risk of abuse by users as well as difficulties for forensic investigators. This paper suggests a methodology for investigation and analysis of the protocol to assist in the control of data flow across security perimeters.
ER  - 

TY  - JOUR
T1  - Logging, auditing and filtering for Internet electronic commerce
JO  - Computer Fraud & Security
VL  - 1997
IS  - 8
SP  - 11
EP  - 16
PY  - 1997/8//
T2  - 
AU  - Cresson Wood, Charles
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(97)89846-6
UR  - http://www.sciencedirect.com/science/article/pii/S1361372397898466
AB  - This article provides a survey of the features and capabilities of commercial security products specifically designed for conducting business on the Internet. This article focuses on three not-so-glamorous but nonetheless essential, behind-the-scenes security activities. The author has no marketing, referral, or commission relationship with any of the vendors mentioned below.
ER  - 

TY  - JOUR
T1  - A survey on session detection methods in query logs and a proposal for future evaluation
JO  - Information Sciences
VL  - 179
IS  - 12
SP  - 1822
EP  - 1843
PY  - 2009/5/30/
T2  - Special Section: Web Search
AU  - Gayo-Avello, Daniel
SN  - 0020-0255
DO  - http://dx.doi.org/10.1016/j.ins.2009.01.026
UR  - http://www.sciencedirect.com/science/article/pii/S002002550900053X
KW  - Web searching
KW  - Search engine
KW  - Query log
KW  - Topical session
KW  - Session detection
AB  - Search engine logs provide a highly detailed insight of users’ interactions. Hence, they are both extremely useful and sensitive. The datasets publicly available to scholars are, unfortunately, too few, too dated and too small. There are few because search engine companies are reluctant to release such data; they are dated because they were collected in late 1990s or early 2000s; and they are small because they comprise data for at most one day and just a few hundreds of thousands of users. Even worse, the large query log disclosed by AOL in 2006 caused more harm than good because of a big privacy flaw. In this paper the author provides an overall view of the possible applications of query logs, the privacy concerns researchers must face when working on such datasets, and several ways in which query logs can be easily sanitized. One of such measures consists of segmenting the logs into short topical sessions. Therefore, the author offers a comprehensive survey of session detection methods, as well as a thorough description of a new evaluation framework with performance results for each of the different methods. Additionally, a new, simple, but outperforming session detection method is proposed. It is a heuristic-based technique which works on the basis of a geometric interpretation of both the time gap between queries and the similarity between them in order to flag a topic shift.
ER  - 

TY  - JOUR
T1  - Businesses still unaware of risks of account data compromise
JO  - Computer Fraud & Security
VL  - 2011
IS  - 1
SP  - 17
EP  - 19
PY  - 2011/1//
T2  - 
AU  - Hosack, Benj
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(11)70007-0
UR  - http://www.sciencedirect.com/science/article/pii/S1361372311700070
AB  - PCI DSS and PA-DSS are helping to secure card payment processing and reduce fraud but data compromise remains a significant problem in the financial services sector.

In nearly all cases, a compromised business will have rogue cardholder data that it was not aware existed in its systems. Should a criminal identify a flaw in security and gain access to this unprotected data, the compromised entity would find itself in a very difficult situation with regard to forensic investigations, remediation costs, card schemes fines and fraud losses. Benj Hosack of Foregenix examines the issues and offers guidance.

In 2004, the Payment Card Industry Data Security Standard (PCI DSS) was announced as a set of security controls based on best practice and designed to protect cardholder data against the rising levels of fraud on credit cards. Formed initially by the card schemes (Visa, MasterCard, AMEX, JCB, Diners and Discover), the PCI DSS is now managed, maintained and developed by the PCI Security Standards Council (PCI SSC).
ER  - 

TY  - JOUR
T1  - Network forensic frameworks: Survey and research challenges
JO  - Digital Investigation
VL  - 7
IS  - 1–2
SP  - 14
EP  - 27
PY  - 2010/10//
T2  - 
AU  - Pilli, Emmanuel S.
AU  - Joshi, R.C.
AU  - Niyogi, Rajdeep
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2010.02.003
UR  - http://www.sciencedirect.com/science/article/pii/S1742287610000113
KW  - Network forensics
KW  - NFATs
KW  - Distributed systems
KW  - Soft computing
KW  - Honeypots
KW  - Data fusion
KW  - Attribution
KW  - Traceback
KW  - Incident response
AB  - Network forensics is the science that deals with capture, recording, and analysis of network traffic for detecting intrusions and investigating them. This paper makes an exhaustive survey of various network forensic frameworks proposed till date. A generic process model for network forensics is proposed which is built on various existing models of digital forensics. Definition, categorization and motivation for network forensics are clearly stated. The functionality of various Network Forensic Analysis Tools (NFATs) and network security monitoring tools, available for forensics examiners is discussed. The specific research gaps existing in implementation frameworks, process models and analysis tools are identified and major challenges are highlighted. The significance of this work is that it presents an overview on network forensics covering tools, process models and framework implementations, which will be very much useful for security practitioners and researchers in exploring this upcoming and young discipline.
ER  - 

TY  - JOUR
T1  - A Handset-centric View of Smartphone Application Use
JO  - Procedia Computer Science
VL  - 34
IS  - 
SP  - 368
EP  - 375
PY  - 2014///
T2  - The 9th International Conference on Future Networks and Communications (FNC'14)/The 11th International Conference on Mobile Systems and Pervasive Computing (MobiSPC'14)/Affiliated Workshops
AU  - Rana, Juwel
AU  - Bjelland, Johannes
AU  - Couronne, Thomas
AU  - Sundsøy, Pål
AU  - Wagner, Daniel
AU  - Rice, Andrew
SN  - 1877-0509
DO  - http://dx.doi.org/10.1016/j.procs.2014.07.039
UR  - http://www.sciencedirect.com/science/article/pii/S1877050914008941
KW  - Android
KW  - smartphone
KW  - mobile apps
KW  - app usage analytics
KW  - big data;
AB  - Abstract
Studying the use of applications on smart phones is important for developers, handset designers and network operators. We conducted a study on Android devices by installing an instrumentation application, Device Analyzer, on participants’ handsets. Over a 4 month period we collected 10.9 billion records from 674 different users. In this paper we describe how to use the research study features of Device Analyzer to control participant selection and to access information (with consent) that is withheld for privacy reasons from the main dataset. We describe our data processing architecture and the steps required to preformat and analyse the data. Our data contains 3329 distinct applications (from the Google Play store) but despite this, on average, a user makes use of only 8 unique applications in a week. Almost 100% of our users make use of some email application on their phone. Fewer users (85%) made use of the Facebook application but 4–5 times more frequently than for email with sessions lasting almost twice as long. We also investigated whether different applications have correlated usage using a network analysis and a principal component analysis. We see that application usage tends to correlate by vendor more than by activity. This is potentially due to vendors integrating or cross-promoting services between applications.
ER  - 

TY  - JOUR
T1  - Cloud forensics: Technical challenges, solutions and comparative analysis
JO  - Digital Investigation
VL  - 13
IS  - 
SP  - 38
EP  - 57
PY  - 2015/6//
T2  - 
AU  - Pichan, Ameer
AU  - Lazarescu, Mihai
AU  - Soh, Sie Teng
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2015.03.002
UR  - http://www.sciencedirect.com/science/article/pii/S1742287615000407
KW  - Cloud computing
KW  - Cloud forensics
KW  - Cloud service provider
KW  - Cloud customer
KW  - Digital forensics
KW  - Digital evidence
KW  - Service level agreement
KW  - Amazon EC2
AB  - Abstract
Cloud computing is arguably one of the most significant advances in information technology (IT) services today. Several cloud service providers (CSPs) have offered services that have produced various transformative changes in computing activities and presented numerous promising technological and economic opportunities. However, many cloud customers remain reluctant to move their IT needs to the cloud, mainly due to their concerns on cloud security and the threat of the unknown. The CSPs indirectly escalate their concerns by not letting customers see what is behind virtual wall of their clouds that, among others, hinders digital investigations. In addition, jurisdiction, data duplication and multi-tenancy in cloud platform add to the challenge of locating, identifying and separating the suspected or compromised targets for digital forensics. Unfortunately, the existing approaches to evidence collection and recovery in a non-cloud (traditional) system are not practical as they rely on unrestricted access to the relevant system and user data; something that is not available in the cloud due its decentralized data processing. In this paper we systematically survey the forensic challenges in cloud computing and analyze their most recent solutions and developments. In particular, unlike the existing surveys on the topic, we describe the issues in cloud computing using the phases of traditional digital forensics as the base. For each phase of the digital forensic process, we have included a list of challenges and analysis of their possible solutions. Our description helps identifying the differences between the problems and solutions for non-cloud and cloud digital forensics. Further, the presentation is expected to help the investigators better understand the problems in cloud environment. More importantly, the paper also includes most recent development in cloud forensics produced by researchers, National Institute of Standards and Technology and Amazon.
ER  - 

TY  - JOUR
T1  - A triage framework for digital forensics
JO  - Computer Fraud & Security
VL  - 2015
IS  - 3
SP  - 8
EP  - 18
PY  - 2015/3//
T2  - 
AU  - Bashir, Muhammad Shamraiz
AU  - Khan, Muhammad Naeem Ahmed
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(15)30018-X
UR  - http://www.sciencedirect.com/science/article/pii/S136137231530018X
AB  - A sharp increase in malware and cyber-attacks has been observed in recent years. Analysing cyber-attacks on the affected digital devices falls under the purview of digital forensics. The Internet is the main source of cyber and malware attacks, which sometimes result in serious damage to the digital assets. The motive behind digital crimes varies – such as online banking fraud, information stealing, denial of services, security breaches, deceptive output of running programs and data distortion.

Digital forensics analysts use a variety of tools for data acquisition, evidence analysis and presentation of malicious activities. This leads to device diversity posing serious challenges for investigators.

For this reason, some attack scenarios have to be examined repeatedly, which entails tremendous effort on the part of the examiners when analysing the evidence. To counter this problem, Muhammad Shamraiz Bashir and Muhammad Naeem Ahmed Khan at the Shaheed Zulfikar Ali Bhutto Institute of Science and Technology, Islamabad, Pakistan propose a novel triage framework for digital forensics.
ER  - 

TY  - JOUR
T1  - Intrusion detection system: A comprehensive review
JO  - Journal of Network and Computer Applications
VL  - 36
IS  - 1
SP  - 16
EP  - 24
PY  - 2013/1//
T2  - 
AU  - Liao, Hung-Jen
AU  - Richard Lin, Chun-Hung
AU  - Lin, Ying-Chih
AU  - Tung, Kuang-Yuan
SN  - 1084-8045
DO  - http://dx.doi.org/10.1016/j.jnca.2012.09.004
UR  - http://www.sciencedirect.com/science/article/pii/S1084804512001944
KW  - Intrusion detection
KW  - Anomaly
KW  - Misuse
AB  - With the increasing amount of network throughput and security threat, the study of intrusion detection systems (IDSs) has received a lot of attention throughout the computer science field. Current IDSs pose challenges on not only capricious intrusion categories, but also huge computational power. Though there is a number of existing literatures to IDS issues, we attempt to give a more elaborate image for a comprehensive review. Through the extensive survey and sophisticated organization, we propose the taxonomy to outline modern IDSs. In addition, tables and figures we summarized in the content contribute to easily grasp the overall picture of IDSs.
ER  - 

TY  - JOUR
T1  - Learning relational policies from electronic health record access logs
JO  - Journal of Biomedical Informatics
VL  - 44
IS  - 2
SP  - 333
EP  - 342
PY  - 2011/4//
T2  - 
AU  - Malin, Bradley
AU  - Nyemba, Steve
AU  - Paulett, John
SN  - 1532-0464
DO  - http://dx.doi.org/10.1016/j.jbi.2011.01.007
UR  - http://www.sciencedirect.com/science/article/pii/S1532046411000098
KW  - Electronic health records
KW  - Organizational behavior
KW  - Knowledge discovery
KW  - Access logs
KW  - Auditing
AB  - Modern healthcare organizations (HCOs) are composed of complex dynamic teams to ensure clinical operations are executed in a quick and competent manner. At the same time, the fluid nature of such environments hinders administrators’ efforts to define access control policies that appropriately balance patient privacy and healthcare functions. Manual efforts to define these policies are labor-intensive and error-prone, often resulting in systems that endow certain care providers with overly broad access to patients’ medical records while restricting other providers from legitimate and timely use. In this work, we propose an alternative method to generate these policies by automatically mining usage patterns from electronic health record (EHR) systems. EHR systems are increasingly being integrated into clinical environments and our approach is designed to be generalizable across HCOs, thus assisting in the design and evaluation of local access control policies. Our technique, which is grounded in data mining and social network analysis theory, extracts a statistical model of the organization from the access logs of its EHRs. In doing so, our approach enables the review of predefined policies, as well as the discovery of unknown behaviors. We evaluate our approach with 5 months of access logs from the Vanderbilt University Medical Center and confirm the existence of stable social structures and intuitive business operations. Additionally, we demonstrate that there is significant turnover in the interactions between users in the HCO and that policies learned at the department-level afford greater stability over time.
ER  - 

TY  - JOUR
T1  - Protecting critical control systems
JO  - Network Security
VL  - 2012
IS  - 3
SP  - 7
EP  - 10
PY  - 2012/3//
T2  - 
AU  - Brewer, Ross
SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/S1353-4858(12)70044-2
UR  - http://www.sciencedirect.com/science/article/pii/S1353485812700442
AB  - With cyber-attacks continuing to grow in sophistication and frequency, both public and private organisations have been forced to change their outlook on cyber-security and re-examine their strategies when it comes to protecting their networks. System breaches are no longer considered unlikely, and the mindset has shifted to cyber-attacks being a matter of ‘when’ rather than ‘if’.

With cyber-attacks continuing to grow in sophistication and frequency, both public and private organisations have been forced to change their outlook on cyber-security and re-examine their strategies when it comes to protecting their networks. System breaches are no longer considered unlikely, and the mindset has shifted to cyber-attacks being a matter of ‘when’ rather than ‘if’.

Many critical infrastructure installations are controlled by Supervisory Control And Data Acquisition (SCADA) solutions that were never designed to be secure. However, with the correct strategic focus and resources applied, SCADA systems can be secured. Ross Brewer of LogRhythm suggests that a ‘protective monitoring’ approach can be tailored around networks that support high-value cyber-assets.
ER  - 

TY  - JOUR
T1  - On designing usable and secure recognition-based graphical authentication mechanisms
JO  - Interacting with Computers
VL  - 23
IS  - 6
SP  - 582
EP  - 593
PY  - 2011/11//
T2  - 
AU  - Mihajlov, Martin
AU  - Jerman-Blaži?, Borka
SN  - 0953-5438
DO  - http://dx.doi.org/10.1016/j.intcom.2011.09.001
UR  - http://www.sciencedirect.com/science/article/pii/S0953543811000956
KW  - Graphical authentication
KW  - Graphical passwords
KW  - User evaluation
KW  - System design
AB  - In this article we present the development of a new, web-based, graphical authentication mechanism called ImagePass. The authentication mechanism introduces a novel feature based on one-time passwords that increases the security of the system without compromising its usability. Regarding usability, we explore the users’ perception of recognition-based, graphical authentication mechanisms in a web environment. Specifically, we investigate whether the memorability of recognition-based authentication keys is influenced by image content. We also examine how the frequency of use affects the usability of the system and whether user training via mnemonic instructions improves the graphical password recognition rate. The design and development process of the proposed system began with a study that assessed how the users remember abstract, face or single-object images, and showed that single-object images have a higher memorability rate. We then proceeded with the design and development of a recognition-based graphical authentication mechanism, ImagePass, which uses single-objects as the image content and follows usable security guidelines. To conclude the research, in a follow-up study we evaluated the performance of 151 participants under different conditions. We discovered that the frequency of use had a great impact on users’ performance, while the users’ gender had a limited task-specific effect. In contrast, user training through mnemonic instructions showed no differences in the users’ authentication metrics. However, a post-study, focus-group analysis revealed that these instructions greatly influenced the users’ perception for memorability and the usability of the graphical authentication. In general, the results of these studies suggest that single-object graphical authentication can be a complementary replacement for traditional passwords, especially in ubiquitous environments and mobile devices.
ER  - 

TY  - JOUR
T1  - Towards a forensic-aware database solution: Using a secured database replication protocol and transaction management for digital investigations
JO  - Digital Investigation
VL  - 11
IS  - 4
SP  - 336
EP  - 348
PY  - 2014/12//
T2  - 
AU  - Frühwirt, Peter
AU  - Kieseberg, Peter
AU  - Krombholz, Katharina
AU  - Weippl, Edgar
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2014.09.003
UR  - http://www.sciencedirect.com/science/article/pii/S1742287614001078
KW  - MySQL
KW  - InnoDB
KW  - Digital forensics
KW  - Databases
KW  - Data tempering
KW  - Replication
KW  - Transaction management
AB  - Abstract
Databases contain an enormous amount of structured data. While the use of forensic analysis on the file system level for creating (partial) timelines, recovering deleted data and revealing concealed activities is very popular and multiple forensic toolsets exist, the systematic analysis of database management systems has only recently begun. Databases contain a large amount of temporary data files and metadata which are used by internal mechanisms. These data structures are maintained in order to ensure transaction authenticity, to perform rollbacks, or to set back the database to a predefined earlier state in case of e.g. an inconsistent state or a hardware failure. However, these data structures are intended to be used by the internal system methods only and are in general not human-readable.

In this work we present a novel approach for a forensic-aware database management system using transaction- and replication sources. We use these internal data structures as a vital baseline to reconstruct evidence during a forensic investigation. The overall benefit of our method is that no additional logs (such as administrator logs) are needed. Furthermore, our approach is invariant to retroactive malicious modifications by an attacker. This assures the authenticity of the evidence and strengthens the chain of custody. To evaluate our approach, we present a formal description, a prototype implementation in MySQL alongside and a comprehensive security evaluation with respect to the most relevant attack scenarios.
ER  - 

TY  - JOUR
T1  - An extensible analysable system model
JO  - Information Security Technical Report
VL  - 13
IS  - 4
SP  - 235
EP  - 246
PY  - 2008/11//
T2  - 
AU  - Probst, Christian W.
AU  - Hansen, René Rydhof
SN  - 1363-4127
DO  - http://dx.doi.org/10.1016/j.istr.2008.10.012
UR  - http://www.sciencedirect.com/science/article/pii/S1363412708000502
AB  - Analysing real-world systems for vulnerabilities with respect to security and safety threats is a difficult undertaking, not least due to a lack of availability of formalisations for those systems. While both formalisations and analyses can be found for artificial systems such as software, this does not hold for real physical systems. Approaches such as threat modelling try to target the formalisation of the real-world domain, but still are far from the rigid techniques available in security research. Many currently available approaches to assurance of critical infrastructure security are based on (quite successful) ad-hoc techniques. We believe they can be significantly improved beyond the state-of-the-art by pairing them with static analyses techniques.

In this paper we present an approach to both formalising those real-world systems, as well as providing an underlying semantics, which allows for easy development of analyses for the abstracted systems. We briefly present one application of our approach, namely the analysis of systems for potential insider threats.
ER  - 

TY  - JOUR
T1  - DigLA – A Digsby log analysis tool to identify forensic artifacts
JO  - Digital Investigation
VL  - 9
IS  - 3–4
SP  - 222
EP  - 234
PY  - 2013/2//
T2  - 
AU  - Yasin, Muhammad
AU  - Abulaish, Muhammad
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2012.11.003
UR  - http://www.sciencedirect.com/science/article/pii/S1742287612000837
KW  - Digital forensics
KW  - Digsby log analysis
KW  - Forensic artifacts
KW  - Insider attack
KW  - RAM analysis
AB  - Since the inception of Web 2.0, instant messaging, e-mailing, and social networking have emerged as cheap and efficient means of communication over the Web. As a result, a number of communication platforms like Digsby have been developed by various research groups to facilitate access to multiple e-mail, instant messaging, and social networking sites using a single credential. Although such platforms are advantageous for end-users, they present new challenges to digital forensic examiners because of their illegitimate use by anti-social elements. To identify digital artifacts from Digsby log data, an examiner is assumed to have knowledge of the whereabouts of Digsby traces before starting an investigation process. This paper proposes a design for a user-friendly GUI-based forensic tool, DigLA, which provides a unified platform for analyzing Digsby log data at different levels of granularity. DigLA is also equipped with password decryption methods for both machine-specific and portable installation versions of Digsby. By considering Windows registry and Digsby log files as dynamic sources of evidence, specifically when Digsby has been used to commit a cyber crime, this paper presents a systematic approach to analyzing Digsby log data. It also presents an approach to analyzing RAM and swap files to collect relevant traces, specifically the login credentials of Digsby and IM users. An expected insider attack from a server security perspective is also studied and discussed in this paper.
ER  - 

TY  - JOUR
T1  - Enforcement of entailment constraints in distributed service-based business processes
JO  - Information and Software Technology
VL  - 55
IS  - 11
SP  - 1884
EP  - 1903
PY  - 2013/11//
T2  - 
AU  - Hummer, Waldemar
AU  - Gaubatz, Patrick
AU  - Strembeck, Mark
AU  - Zdun, Uwe
AU  - Dustdar, Schahram
SN  - 0950-5849
DO  - http://dx.doi.org/10.1016/j.infsof.2013.05.001
UR  - http://www.sciencedirect.com/science/article/pii/S0950584913001006
KW  - Identity and access management
KW  - Business process management
KW  - Entailment constraints
KW  - Service-Oriented Architecture (SOA)
KW  - WS-BPEL
AB  - AbstractContext
A distributed business process is executed in a distributed computing environment. The service-oriented architecture (SOA) paradigm is a popular option for the integration of software services and execution of distributed business processes. Entailment constraints, such as mutual exclusion and binding constraints, are important means to control process execution. Mutually exclusive tasks result from the division of powerful rights and responsibilities to prevent fraud and abuse. In contrast, binding constraints define that a subject who performed one task must also perform the corresponding bound task(s).
Objective
We aim to provide a model-driven approach for the specification and enforcement of task-based entailment constraints in distributed service-based business processes.
Method
Based on a generic metamodel, we define a domain-specific language (DSL) that maps the different modeling-level artifacts to the implementation-level. The DSL integrates elements from role-based access control (RBAC) with the tasks that are performed in a business process. Process definitions are annotated using the DSL, and our software platform uses automated model transformations to produce executable WS-BPEL specifications which enforce the entailment constraints. We evaluate the impact of constraint enforcement on runtime performance for five selected service-based processes from existing literature.
Results
Our evaluation demonstrates that the approach correctly enforces task-based entailment constraints at runtime. The performance experiments illustrate that the runtime enforcement operates with an overhead that scales well up to the order of several ten thousand logged invocations. Using our DSL annotations, the user-defined process definition remains declarative and clean of security enforcement code.
Conclusion
Our approach decouples the concerns of (non-technical) domain experts from technical details of entailment constraint enforcement. The developed framework integrates seamlessly with WS-BPEL and the Web services technology stack. Our prototype implementation shows the feasibility of the approach, and the evaluation points to future work and further performance optimizations.
ER  - 

TY  - JOUR
T1  - Botnet Detection with Event-Driven Analysis
JO  - Procedia Computer Science
VL  - 22
IS  - 
SP  - 662
EP  - 671
PY  - 2013///
T2  - 17th International Conference in Knowledge Based and Intelligent Information and Engineering Systems - KES2013
AU  - Ersson, Joakim
AU  - Moradian, Esmiralda
SN  - 1877-0509
DO  - http://dx.doi.org/10.1016/j.procs.2013.09.147
UR  - http://www.sciencedirect.com/science/article/pii/S187705091300940X
KW  - Log analysis
KW  - botnet
KW  - firewall log,network analysis
AB  - Abstract
Due to the huge impact on businesses, botnets are recognized as one of the most serious security threats. Malicious entities use various techniques to conceal and keep themselves undetected durin the proliferation of malware from computer to computer. Detection of a botnet is commonly performed in two ways either by using antivirus software or by analysing logged network data. However antivirus software usually detects malware that is already known and has been analysed, which is a main drawback of such approach due to the constant evolving of malware. The approach of analysis of logged network data do not reveals botnet activities and requires knowledge about botnets and type of data to look for within the collected log. Thus, the significant information can be overlooked and missed. In this paper, we propose event-driven log analysis software that enables detection of botnet activities and indicates whether the end-users machines have become a member of a botnet. Moreover, to optimize software functionality we performed an experiment that demonstrates how botnet communicates between itself and the command and control. Experiment along with the result is presented in this research.
ER  - 

TY  - JOUR
T1  - Advanced Persistent threats and how to monitor and deter them
JO  - Network Security
VL  - 2011
IS  - 8
SP  - 16
EP  - 19
PY  - 2011/8//
T2  - 
AU  - Tankard, Colin
SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/S1353-4858(11)70086-1
UR  - http://www.sciencedirect.com/science/article/pii/S1353485811700861
AB  - Advanced Persistent Threat (APT) is a term coined over the past couple of years for a new breed of insidious threats that use multiple attack techniques and vectors and that are conducted by stealth to avoid detection so that hackers can retain control over target systems unnoticed for long periods of time.

Traditional defences aimed at keeping known threats out of the network are no longer sufficient against the exploits being used to conduct such attacks. The focus should be on developing a defence in depth strategy that aims to constantly monitor networks and security controls for their effectiveness, explains Colin Tankard of Digital Pathways.

The UK Government has recently estimated that cybercrime costs the country some £27bn per year and, according to some estimates, the global cost is $1 trillion every year. This crime wave has been greatly facilitated by the rise of electronic communications, primarily those making use of the Internet. The purpose of electronic communications is to make it more efficient and easier to communicate – but they are also relatively easy to attack or intercept. No-one is immune – such attacks are aimed at individuals, small firms, multinationals and governments.
ER  - 

TY  - JOUR
T1  - Hacking: Myth or menace, Part I
JO  - Computer Fraud & Security
VL  - 1998
IS  - 2
SP  - 16
EP  - 18
PY  - 1998/2//
T2  - 
AU  - Blatchford, Clive
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(00)87011-6
UR  - http://www.sciencedirect.com/science/article/pii/S1361372300870116
AB  - Is hacking about the disposition of the perpetrator or the situation of the insecure computer network. Improving the security of the perceived target may reduce the opportunity for the hacker, but can it ever eradicate the problem? Criminologists increasingly insist that the solution must lie in understanding the perspective of the hacker and address the problem at the political, social and cultural levels. The following two part study aims to offer insights about the actual threat before embarking on more legislation, policing or prohibitively expensive and probably, inadequate target hardening.
ER  - 

TY  - JOUR
T1  - PalmCIS: A wireless handheld application for satisfying clinician information needs
JO  - Journal of the American Medical Informatics Association
VL  - 11
IS  - 1
SP  - 19
EP  - 28
PY  - 2004/1//
Y2  - 2004/2//
T2  - 
AU  - Chen, Elizabeth S
AU  - Mendonça, Eneida A
AU  - McKnight, Lawrence K
AU  - Stetson, Peter D
AU  - Lei, Jianbo
AU  - Cimino, James J
SN  - 1067-5027
DO  - http://dx.doi.org/10.1197/jamia.M1387
UR  - http://www.sciencedirect.com/science/article/pii/S1067502703002020
AB  - Wireless handheld technology provides new ways to deliver and present information. As with any technology, its unique features must be taken into consideration and its applications designed accordingly. In the clinical setting, availability of needed information can be crucial during the decision-making process. Preliminary studies performed at New York Presbyterian Hospital (NYPH) determined that there are inadequate access to information and ineffective communication among clinicians (potential proximal causes of medical errors). In response to these findings, the authors have been developing extensions to their Web-based clinical information system including PalmCIS, an application that provides access to needed patient information via a wireless personal digital assistant (PDA). The focus was on achieving end-to-end security and developing a highly usable system. This report discusses the motivation behind PalmCIS, design and development of the system, and future directions.
ER  - 

TY  - JOUR
T1  - A scalable network forensics mechanism for stealthy self-propagating attacks
JO  - Computer Communications
VL  - 36
IS  - 13
SP  - 1471
EP  - 1484
PY  - 2013/7/15/
T2  - 
AU  - Chen, Li Ming
AU  - Chen, Meng Chang
AU  - Liao, Wanjiun
AU  - Sun, Yeali S.
SN  - 0140-3664
DO  - http://dx.doi.org/10.1016/j.comcom.2013.05.005
UR  - http://www.sciencedirect.com/science/article/pii/S0140366413001254
KW  - Network forensics
KW  - Data reduction
KW  - Stealthy self-propagating attack
KW  - Contact activity
AB  - Abstract
Network forensics supports capabilities such as attacker identification and attack reconstruction, which complement the traditional intrusion detection and perimeter defense techniques in building a robust security mechanism. Attacker identification pinpoints attack origin to deter future attackers, while attack reconstruction reveals attack causality and network vulnerabilities. In this paper, we discuss the problem and feasibility of back tracking the origin of a self-propagating stealth attack when given a network traffic trace for a sufficiently long period of time. We propose a network forensics mechanism that is scalable in computation time and space while maintaining high accuracy in the identification of the attack origin. We further develop a data reduction method to filter out attack-irrelevant data and only retain evidence relevant to potential attacks for a post-mortem investigation. Using real-world trace driven experiments, we evaluate the performance of the proposed mechanism and show that we can trim down up to 97% of attack-irrelevant network traffic and successfully identify attack origin.
ER  - 

TY  - JOUR
T1  - Case study: Network intrusion investigation – lessons in forensic preparation
JO  - Digital Investigation
VL  - 2
IS  - 4
SP  - 254
EP  - 260
PY  - 2005/12//
T2  - 
AU  - Casey, Eoghan
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2005.11.007
UR  - http://www.sciencedirect.com/science/article/pii/S1742287605000940
KW  - Forensic readiness
KW  - Forensic preparation
KW  - Incident response
KW  - Incident handling
KW  - Network forensics
KW  - Computer intrusion
KW  - Rootkit
KW  - Forensic computing
KW  - Tracking intruders
KW  - Attribution
AB  - Investigations of network security breaches are both complex and costly. Even a moderate amount of forensic preparation in an organization can mitigate the impact of a major incident and can enable the organization to obtain restitution. A case study of an intrusion is outlined in which the victim organization worked with law enforcement agencies to apprehend the perpetrator. This case study contains examples of challenges that can arise during this type of investigation, and discusses practical steps that an organization can take to prepare for a major incident. The overlapping roles of System Administrators, Incident Handlers, and Forensic Examiners in a network intrusion are explored, with an emphasis on the need for collaboration and proper evidence handling. This case study also shows how effective case management and methodical reconstruction of events can help create a more complete picture of the crime and help establish links between computer intruders and their illegal activities.
ER  - 

TY  - JOUR
T1  - Feature evaluation for web crawler detection with data mining techniques
JO  - Expert Systems with Applications
VL  - 39
IS  - 10
SP  - 8707
EP  - 8717
PY  - 2012/8//
T2  - 
AU  - Stevanovic, Dusan
AU  - An, Aijun
AU  - Vlajic, Natalija
SN  - 0957-4174
DO  - http://dx.doi.org/10.1016/j.eswa.2012.01.210
UR  - http://www.sciencedirect.com/science/article/pii/S0957417412002382
KW  - Web crawler detection
KW  - Web server access logs
KW  - Data mining
KW  - Classification
KW  - DDoS
KW  - WEKA
AB  - Distributed Denial of Service (DDoS) is one of the most damaging attacks on the Internet security today. Recently, malicious web crawlers have been used to execute automated DDoS attacks on web sites across the WWW. In this study we examine the effect of applying seven well-established data mining classification algorithms on static web server access logs in order to: (1) classify user sessions as belonging to either automated web crawlers or human visitors and (2) identify which of the automated web crawlers sessions exhibit ‘malicious’ behavior and are potentially participants in a DDoS attack. The classification performance is evaluated in terms of classification accuracy, recall, precision and F1 score. Seven out of nine vector (i.e. web-session) features employed in our work are borrowed from earlier studies on classification of user sessions as belonging to web crawlers. However, we also introduce two novel web-session features: the consecutive sequential request ratio and standard deviation of page request depth. The effectiveness of the new features is evaluated in terms of the information gain and gain ratio metrics. The experimental results demonstrate the potential of the new features to improve the accuracy of data mining classifiers in identifying malicious and well-behaved web crawler sessions.
ER  - 

TY  - JOUR
T1  - Collapsar: A VM-based honeyfarm and reverse honeyfarm architecture for network attack capture and detention
JO  - Journal of Parallel and Distributed Computing
VL  - 66
IS  - 9
SP  - 1165
EP  - 1180
PY  - 2006/9//
T2  - Special Issue: Security in grid and distributed systems
AU  - Jiang, Xuxian
AU  - Xu, Dongyan
AU  - Wang, Yi-Min
SN  - 0743-7315
DO  - http://dx.doi.org/10.1016/j.jpdc.2006.04.012
UR  - http://www.sciencedirect.com/science/article/pii/S0743731506000876
KW  - Honeypot
KW  - Honeyfarm
KW  - Reverse honeyfarm
AB  - The honeypot has emerged as an effective tool to provide insights into new attacks and exploitation trends. However, a single honeypot or multiple independently operated honeypots only provide limited local views of network attacks. Coordinated deployment of honeypots in different network domains not only provides broader views, but also create opportunities of early network anomaly detection, attack correlation, and global network status inference. Unfortunately, coordinated honeypot operation require close collaboration and uniform security expertise across participating network domains. The conflict between distributed presence and uniform management poses a major challenge in honeypot deployment and operation.

To address this challenge, we present Collapsar, a virtual machine-based architecture for network attack capture and detention. A Collapsar center hosts and manages a large number of high-interaction virtual honeypots in a local dedicated network. To attackers, these honeypots appear as real systems in their respective production networks. Decentralized logical presence of honeypots provides a wide diverse view of network attacks, while the centralized operation enables dedicated administration and convenient event correlation, eliminating the need for honeypot expertise in every production network domain. Collapsar realizes the traditional honeyfarm vision as well as our new reverse honeyfarm vision, where honeypots act as vulnerable clients exploited by real-world malicious servers. We present the design, implementation, and evaluation of a Collapsar prototype. Our experiments with a number of real-world attacks demonstrate the effectiveness and practicality of Collapsar.
ER  - 

TY  - JOUR
T1  - An efficient technique for enhancing forensic capabilities of Ext2 file system
JO  - Digital Investigation
VL  - 4, Supplement
IS  - 
SP  - 55
EP  - 61
PY  - 2007/9//
T2  - 
AU  - Barik, Mridul Sankar
AU  - Gupta, Gaurav
AU  - Sinha, Shubhro
AU  - Mishra, Alok
AU  - Mazumdar, Chandan
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2007.06.007
UR  - http://www.sciencedirect.com/science/article/pii/S1742287607000515
KW  - Electronic documents
KW  - Modification access and creation
KW  - date and time stamps (MAC DTS)
KW  - Authentic date and time stamps (ADTS)
KW  - Computer Frauds and Cyber Crimes (CFCC)
KW  - Ext2 file system
KW  - Loadable Kernel Module (LKM)
AB  - As electronic documents become more important and valuable in the modern era, attempts are invariably made to take undue-advantage by tampering with them. Tampering with the modification, access and creation date and time stamps (MAC DTS) of digital documents pose a great threat and proves to be a major handicap in digital forensic investigation. Authentic date and time stamps (ADTS) can provide crucial evidence in linking crime to criminal in cases of Computer Fraud and Cyber Crimes (CFCC) through reliable time lining of digital evidence. But the ease with which the MAC DTS of stored digital documents can be changed raises some serious questions about the integrity and admissibility of digital evidence, potentially leading to rejection of acquired digital evidence in the court of Law. MAC DTS procedures of popular operating systems are inherently flawed and were created only for the sake of convenience and not necessarily keeping in mind the security and digital forensic aspects. This paper explores these issues in the context of the Ext2 file system and also proposes one solution to tackle such issues for the scenario where systems have preinstalled plug-ins in the form of Loadable Kernel Modules, which provide the capability to preserve ADTS.
ER  - 

TY  - JOUR
T1  - Data mining and machine learning—Towards reducing false positives in intrusion detection
JO  - Information Security Technical Report
VL  - 10
IS  - 3
SP  - 169
EP  - 183
PY  - 2005///
T2  - 
AU  - Pietraszek, Tadeusz
AU  - Tanner, Axel
SN  - 1363-4127
DO  - http://dx.doi.org/10.1016/j.istr.2005.07.001
UR  - http://www.sciencedirect.com/science/article/pii/S1363412705000361
AB  - Intrusion Detection Systems (IDSs) are used to monitor computer systems for signs of security violations. Having detected such signs, IDSs trigger alerts to report them. These alerts are presented to a human analyst, who evaluates them and initiates an adequate response. In practice, IDSs have been observed to trigger thousands of alerts per day, most of which are mistakenly triggered by benign events (i.e., false positives). This makes it extremely difficult for the analyst to correctly identify alerts related to attacks (i.e., true positives).

In this paper, we present two orthogonal and complementary approaches to reduce the number of false positives in intrusion detection using alert postprocessing by data mining and machine learning. Moreover, these two techniques, because of their complementary nature, can be used together in an alert-management system. These concepts have been verified on a variety of data sets, and achieved a significant reduction in the number of false positives in both simulated and real environments.
ER  - 

TY  - JOUR
T1  - Logs may be found boring, but they are good: NIST
JO  - Computer Fraud & Security
VL  - 2006
IS  - 5
SP  - 2
EP  - 3
PY  - 2006/5//
T2  - 

SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(06)70351-7
UR  - http://www.sciencedirect.com/science/article/pii/S1361372306703517
AB  - A US standards body has said that the benefits of logs are being thrown away because system administrators find it “boring.” It said that the analysis of logs is often “treated as a low-priority task” by administrators because more urgent tasks like fixing vulnerabilities come first.
ER  - 

TY  - JOUR
T1  - Design and implementation of a mediation system enabling secure communication among Critical Infrastructures
JO  - International Journal of Critical Infrastructure Protection
VL  - 5
IS  - 2
SP  - 86
EP  - 97
PY  - 2012/7//
T2  - 
AU  - Castrucci, Marco
AU  - Neri, Alessandro
AU  - Caldeira, Filipe
AU  - Aubert, Jocelyn
AU  - Khadraoui, Djamel
AU  - Aubigny, Matthieu
AU  - Harpes, Carlo
AU  - Simões, Paulo
AU  - Suraci, Vincenzo
AU  - Capodieci, Paolo
SN  - 1874-5482
DO  - http://dx.doi.org/10.1016/j.ijcip.2012.04.001
UR  - http://www.sciencedirect.com/science/article/pii/S1874548212000194
KW  - Critical Infrastructure
KW  - Information sharing
KW  - Web services
KW  - MICIE
KW  - Secure mediation gateway
AB  - Nowadays, the increase of interdependencies among different Critical Infrastructures (CI) makes it more and more difficult to protect without using a systemic approach that considers a single infrastructure as part of a complex system of infrastructures. A strong collaboration among CI owners is required to avoid, or at least to limit the propagation of failures from one infrastructure to another and to put CI in safety mode. The key element enabling this required cooperation is the possibility for them to exchange relevant information related to the status of their infrastructures and to the services provided. In this paper, we present a middleware solution that allows CIs sharing real-time information, enabling the design and implementation of fault mitigation strategies and mechanisms to prevent the cascading phenomena generated by the failure propagation from one infrastructure to another.
ER  - 

TY  - JOUR
T1  - An integrated conceptual digital forensic framework for cloud computing
JO  - Digital Investigation
VL  - 9
IS  - 2
SP  - 71
EP  - 80
PY  - 2012/11//
T2  - 
AU  - Martini, Ben
AU  - Choo, Kim-Kwang Raymond
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2012.07.001
UR  - http://www.sciencedirect.com/science/article/pii/S174228761200059X
KW  - Cloud computing
KW  - Cloud forensics
KW  - Digital forensics
KW  - Forensic computing
KW  - Digital evidence
KW  - Computer forensics
AB  - Increasing interest in and use of cloud computing services presents both opportunities for criminal exploitation and challenges for law enforcement agencies (LEAs). For example, it is becoming easier for criminals to store incriminating files in the cloud computing environment but it may be extremely difficult for LEAs to seize these files as the latter could potentially be stored overseas. Two of the most widely used and accepted forensic frameworks – McKemmish (1999) and NIST (Kent et al., 2006) – are then reviewed to identify the required changes to current forensic practices needed to successfully conduct cloud computing investigations. We propose an integrated (iterative) conceptual digital forensic framework (based on McKemmish and NIST), which emphasises the differences in the preservation of forensic data and the collection of cloud computing data for forensic purposes. Cloud computing digital forensic issues are discussed within the context of this framework. Finally suggestions for future research are made to further examine this field and provide a library of digital forensic methodologies for the various cloud platforms and deployment models.
ER  - 

TY  - JOUR
T1  - Cloud computing and its implications for cybercrime investigations in Australia
JO  - Computer Law & Security Review
VL  - 29
IS  - 2
SP  - 152
EP  - 163
PY  - 2013/4//
T2  - 
AU  - Hooper, Christopher
AU  - Martini, Ben
AU  - Choo, Kim-Kwang Raymond
SN  - 0267-3649
DO  - http://dx.doi.org/10.1016/j.clsr.2013.01.006
UR  - http://www.sciencedirect.com/science/article/pii/S0267364913000241
KW  - Cloud computing
KW  - Cybercrime
KW  - Digital forensics
KW  - Law enforcement investigations
KW  - Mutual legal assistance
KW  - Australian Law
KW  - Council of Europe Convention on Cybercrime
KW  - Jurisdictional issues
AB  - The advent of cloud computing has led to a dispersal of user data across international borders. More than ever before, law enforcement investigations into cybercrime and online criminal activity require cooperation between agencies from multiple countries. This paper examines recent changes to the law in Australia in relation to the power of law enforcement agencies to effectively investigate cybercrime insofar as individuals and organisations make use of cloud infrastructure in connection with criminal activity. It concludes that effective law enforcement operations in this area require harmonious laws across jurisdictions and streamlines procedures for granting assistance between law enforcement agencies. In conjunction with these mechanical developments, this paper posits that law enforcement officers require a systematised understanding of cloud infrastructure and its operation in order to effectively make use of their powers.
ER  - 

TY  - JOUR
T1  - In brief
JO  - Network Security
VL  - 2010
IS  - 6
SP  - 3
EP  - 
PY  - 2010/6//
T2  - 

SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/S1353-4858(10)70079-9
UR  - http://www.sciencedirect.com/science/article/pii/S1353485810700799
ER  - 

TY  - JOUR
T1  - Profiling software applications for forensic analysis
JO  - Computer Fraud & Security
VL  - 2015
IS  - 6
SP  - 13
EP  - 18
PY  - 2015/6//
T2  - 
AU  - Rafique, Mamoona
AU  - Khan, Muhammad Naeem Ahmed
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(15)30058-0
UR  - http://www.sciencedirect.com/science/article/pii/S1361372315300580
AB  - Computers are now a fundamental part of our professional lives. Although advanced technologies are being used to contain digital crimes, alongside these are other technologies that have expanded a criminal community that is constantly searching for new means to commit crimes in more sophisticated ways. Due to the availability of corporate data on the web, coupled with the open access nature of the web, digital miscreants can commit cybercrimes either as legitimate or illegitimate users.

Traditional digital forensics involves static analysis of the data available on permanent storage media, while live analysis allows running systems to be examined to analyse volatile data.

However, live analysis is not without its challenges, not least because each application has different effects on the system. Mamoona Rafique and Muhammad Naeem Ahmed Khan present a model for profiling the behaviour of application programs. This allows investigators to build a behavioural profile of each application in order to understand its effects on the system.
ER  - 

TY  - JOUR
T1  - Compliance complacency: How ‘check-box’ compliancy remains a pitfall for many organizations worldwide
JO  - Information Security Technical Report
VL  - 15
IS  - 4
SP  - 154
EP  - 159
PY  - 2010/11//
T2  - Matchmaking between PCI-DSS and Security
AU  - Andrew Valentine, J.
SN  - 1363-4127
DO  - http://dx.doi.org/10.1016/j.istr.2011.02.002
UR  - http://www.sciencedirect.com/science/article/pii/S1363412711000136
ER  - 

TY  - JOUR
T1  - An Integrated Model for Patient Care and Clinical Trials (IMPACT) to support clinical research visit scheduling workflow for future learning health systems
JO  - Journal of Biomedical Informatics
VL  - 46
IS  - 4
SP  - 642
EP  - 652
PY  - 2013/8//
T2  - 
AU  - Weng, Chunhua
AU  - Li, Yu
AU  - Berhe, Solomon
AU  - Boland, Mary Regina
AU  - Gao, Junfeng
AU  - Hruby, Gregory W.
AU  - Steinman, Richard C.
AU  - Lopez-Jimenez, Carlos
AU  - Busacca, Linda
AU  - Hripcsak, George
AU  - Bakken, Suzanne
AU  - Bigger, J. Thomas
SN  - 1532-0464
DO  - http://dx.doi.org/10.1016/j.jbi.2013.05.001
UR  - http://www.sciencedirect.com/science/article/pii/S1532046413000531
KW  - Workflow
KW  - Software
KW  - Personnel staffing and scheduling
KW  - Health resources
KW  - Clinical research informatics
KW  - Learning health systems
AB  - Abstract
We describe a clinical research visit scheduling system that can potentially coordinate clinical research visits with patient care visits and increase efficiency at clinical sites where clinical and research activities occur simultaneously. Participatory Design methods were applied to support requirements engineering and to create this software called Integrated Model for Patient Care and Clinical Trials (IMPACT). Using a multi-user constraint satisfaction and resource optimization algorithm, IMPACT automatically synthesizes temporal availability of various research resources and recommends the optimal dates and times for pending research visits. We conducted scenario-based evaluations with 10 clinical research coordinators (CRCs) from diverse clinical research settings to assess the usefulness, feasibility, and user acceptance of IMPACT. We obtained qualitative feedback using semi-structured interviews with the CRCs. Most CRCs acknowledged the usefulness of IMPACT features. Support for collaboration within research teams and interoperability with electronic health records and clinical trial management systems were highly requested features. Overall, IMPACT received satisfactory user acceptance and proves to be potentially useful for a variety of clinical research settings. Our future work includes comparing the effectiveness of IMPACT with that of existing scheduling solutions on the market and conducting field tests to formally assess user adoption.
ER  - 

TY  - JOUR
T1  - Investigating digital fingerprints: advanced log analysis
JO  - Network Security
VL  - 2010
IS  - 10
SP  - 17
EP  - 20
PY  - 2010/10//
T2  - 
AU  - Knight, Eric
SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/S1353-4858(10)70127-6
UR  - http://www.sciencedirect.com/science/article/pii/S1353485810701276
AB  - Perhaps your organisation has recently suffered a data breach, which would not be uncommon as last year set records for information theft. For example, in 2009 the Identity Theft Resource Center reported 498 breaches that exposed over 222 million protected records.1 The unreported numbers traditionally have been three or more times higher. More still go undetected or ignored.
ER  - 

TY  - JOUR
T1  - Report highlights
JO  - Information Security Technical Report
VL  - 3
IS  - 4
SP  - 3
EP  - 14
PY  - 1998///
T2  - 

SN  - 1363-4127
DO  - http://dx.doi.org/10.1016/S1363-4127(98)80034-4
UR  - http://www.sciencedirect.com/science/article/pii/S1363412798800344
ER  - 

TY  - JOUR
T1  - Cloud storage forensics: ownCloud as a case study
JO  - Digital Investigation
VL  - 10
IS  - 4
SP  - 287
EP  - 299
PY  - 2013/12//
T2  - 
AU  - Martini, Ben
AU  - Choo, Kim-Kwang Raymond
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2013.08.005
UR  - http://www.sciencedirect.com/science/article/pii/S1742287613000911
KW  - Digital forensics
KW  - Private cloud
KW  - SaaS
KW  - Storage as a service
KW  - StaaS
KW  - Cloud forensics
KW  - Cloud storage forensics
KW  - Open source cloud
AB  - Abstract
The storage as a service (StaaS) cloud computing architecture is showing significant growth as users adopt the capability to store data in the cloud environment across a range of devices. Cloud (storage) forensics has recently emerged as a salient area of inquiry. Using a widely used open source cloud StaaS application – ownCloud – as a case study, we document a series of digital forensic experiments with the aim of providing forensic researchers and practitioners with an in-depth understanding of the artefacts required to undertake cloud storage forensics. Our experiments focus upon client and server artefacts, which are categories of potential evidential data specified before commencement of the experiments. A number of digital forensic artefacts are found as part of these experiments and are used to support the selection of artefact categories and provide a technical summary to practitioners of artefact types. Finally we provide some general guidelines for future forensic analysis on open source StaaS products and recommendations for future work.
ER  - 

TY  - JOUR
T1  - A model-based survey of alert correlation techniques
JO  - Computer Networks
VL  - 57
IS  - 5
SP  - 1289
EP  - 1317
PY  - 2013/4/7/
T2  - 
AU  - Salah, Saeed
AU  - Maciá-Fernández, Gabriel
AU  - Díaz-Verdejo, Jesús E.
SN  - 1389-1286
DO  - http://dx.doi.org/10.1016/j.comnet.2012.10.022
UR  - http://www.sciencedirect.com/science/article/pii/S1389128612004124
KW  - Alert correlation
KW  - Network management systems
KW  - Fault localization
KW  - Intrusion detection systems
KW  - SCADA systems
AB  - As telecommunication networks evolve rapidly in terms of scalability, complexity, and heterogeneity, the efficiency of fault localization procedures and the accuracy in the detection of anomalous behaviors are becoming important factors that largely influence the decision making process in large management companies. For this reason, telecommunication companies are doing a big effort investing in new technologies and projects aimed at finding efficient management solutions. One of the challenging issues for network and system management operators is that of dealing with the huge amount of alerts generated by the managed systems and networks. In order to discover anomalous behaviors and speed up fault localization processes, alert correlation is one of the most popular resources. Although many different alert correlation techniques have been investigated, it is still an active research field. In this paper, a survey of the state of the art in alert correlation techniques is presented. Unlike other authors, we consider that the correlation process is a common problem for different fields in the industry. Thus, we focus on showing the broad influence of this problem. Additionally, we suggest an alert correlation architecture capable of modeling current and prospective proposals. Finally, we also review some of the most important commercial products currently available.
ER  - 

TY  - JOUR
T1  - FORZA – Digital forensics investigation framework that incorporate legal issues
JO  - Digital Investigation
VL  - 3, Supplement
IS  - 
SP  - 29
EP  - 36
PY  - 2006/9//
T2  - The Proceedings of the 6th Annual Digital Forensic Research Workshop (DFRWS '06)
AU  - Ieong, Ricci S.C.
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2006.06.004
UR  - http://www.sciencedirect.com/science/article/pii/S1742287606000661
KW  - Digital forensics investigation framework
KW  - Digital forensics
KW  - FORZA framework
KW  - Forensics principles
KW  - Zachman framework
KW  - Legal aspects
AB  - What is Digital Forensics? Mark Pollitt highlighted in DFRWS 2004 [Politt MM. Six blind men from Indostan. Digital forensics research workshop (DFRWS); 2004] that digital forensics is not an elephant, it is a process and not just one process, but a group of tasks and processes in investigation. In fact, many digital forensics investigation processes and tasks were defined on technical implementation details Investigation procedures developed by traditional forensics scientist focused on the procedures in handling the evidence, while those developed by the technologist focused on the technical details in capturing evidence. As a result, many digital forensics practitioners simply followed technical procedures and forget about the actual purpose and core concept of digital forensics investigation.

With all these technical details and complicated procedures, legal practitioners may have difficulties in applying or even understanding their processes and tasks in digital forensics investigations.

In order to break the technical barrier between information technologists, legal practitioners and investigators, and their corresponding tasks together, a technical-independent framework would be required.

In this paper, we first highlighted the fundamental principle of digital forensics investigations (Reconnaissance, Reliability and Relevancy). Based on this principle, we re-visit the investigation tasks and outlined eight different roles and their responsibilities in a digital forensics investigation.

For each role, we defined the sets of six key questions. They are the What (the data attributes), Why (the motivation), How (the procedures), Who (the people), Where (the location) and When (the time) questions. In fact, among all the investigation processes, there are six main questions that each practitioner would always ask.

By incorporating these sets of six questions into the Zachman's framework, a digital forensics investigation framework – FORZA is composed. We will further explain how this new framework can incorporate legal advisors and prosecutors into a bigger picture of digital forensics investigation framework.

Usability of this framework will be illustrated in a web hacking example.

Finally, the road map that interconnects the framework to automatically zero-knowledge data acquisition tools will be briefly described.
ER  - 

TY  - JOUR
T1  - Mining interesting knowledge from weblogs: a survey
JO  - Data & Knowledge Engineering
VL  - 53
IS  - 3
SP  - 225
EP  - 241
PY  - 2005/6//
T2  - 
AU  - Facca, Federico Michele
AU  - Lanzi, Pier Luca
SN  - 0169-023X
DO  - http://dx.doi.org/10.1016/j.datak.2004.08.001
UR  - http://www.sciencedirect.com/science/article/pii/S0169023X04001387
KW  - Machine learning
KW  - Web Mining
AB  - Web Usage Mining is that area of Web Mining which deals with the extraction of interesting knowledge from logging information produced by Web servers. In this paper we present a survey of the recent developments in this area that is receiving increasing attention from the Data Mining community.
ER  - 

TY  - JOUR
T1  - A social graph based text mining framework for chat log investigation
JO  - Digital Investigation
VL  - 11
IS  - 4
SP  - 349
EP  - 362
PY  - 2014/12//
T2  - 
AU  - Anwar, Tarique
AU  - Abulaish, Muhammad
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2014.10.001
UR  - http://www.sciencedirect.com/science/article/pii/S1742287614001091
KW  - Text mining
KW  - Chat logs mining
KW  - Digital forensics
KW  - Social graph generation
KW  - Cyber crime investigation
AB  - Abstract
This paper presents a unified social graph based text mining framework to identify digital evidences from chat logs data. It considers both users' conversation and interaction data in group-chats to discover overlapping users' interests and their social ties. The proposed framework applies n-gram technique in association with a self-customized hyperlink-induced topic search (HITS) algorithm to identify key-terms representing users' interests, key-users, and key-sessions. We propose a social graph generation technique to model users' interactions, where ties (edges) between a pair of users (nodes) are established only if they participate in at least one common group-chat session, and weights are assigned to the ties based on the degree of overlap in users' interests and interactions. Finally, we present three possible cyber-crime investigation scenarios and a user-group identification method for each of them. We present our experimental results on a data set comprising 1100 chat logs of 11,143 chat sessions continued over a period of 29 months from January 2010 to May 2012. Experimental results suggest that the proposed framework is able to identify key-terms, key-users, key-sessions, and user-groups from chat logs data, all of which are crucial for cyber-crime investigation. Though the chat logs are recovered from a single computer, it is very likely that the logs are collected from multiple computers in real scenario. In this case, logs collected from multiple computers can be combined together to generate more enriched social graph. However, our experiments show that the objectives can be achieved even with logs recovered from a single computer by using group-chats data to draw relationships between every pair of users.
ER  - 

TY  - JOUR
T1  - An empirical study of automatic event reconstruction systems
JO  - Digital Investigation
VL  - 3, Supplement
IS  - 
SP  - 108
EP  - 115
PY  - 2006/9//
T2  - The Proceedings of the 6th Annual Digital Forensic Research Workshop (DFRWS '06)
AU  - Jeyaraman, Sundararaman
AU  - Atallah, Mikhail J.
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2006.06.013
UR  - http://www.sciencedirect.com/science/article/pii/S1742287606000752
KW  - Intrusion analysis
KW  - Digital forensics
KW  - Event reconstruction
KW  - Incident response
AB  - Reconstructing the sequence of computer events that led to a particular event is an essential part of the digital investigation process. The ability to quantify the accuracy of automatic event reconstruction systems is an essential step in standardizing the digital investigation process thereby making it resilient to tactics such as the Trojan horse defense. In this paper, we present findings from an empirical study to measure and compare the accuracy and effectiveness of a suite of such event reconstruction techniques. We quantify (as applicable) the rates of false positives and false negatives, and scalability in terms of both computational burden and memory-usage. Some of our findings are quite surprising in the sense of not matching a priori expectations, and whereas other findings qualitatively match the a priori expectations they were never before quantitatively put to the test to determine the boundaries of their applicability. For example, our results show that automatic event reconstruction systems proposed in literature have very high false-positive rates (up to 96%).
ER  - 

TY  - JOUR
T1  - Leveraging CybOX™ to standardize representation and exchange of digital forensic information
JO  - Digital Investigation
VL  - 12, Supplement 1
IS  - 
SP  - S102
EP  - S110
PY  - 2015/3//
T2  - DFRWS 2015 EuropeProceedings of the Second Annual DFRWS Europe
AU  - Casey, Eoghan
AU  - Back, Greg
AU  - Barnum, Sean
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2015.01.014
UR  - http://www.sciencedirect.com/science/article/pii/S1742287615000158
KW  - Digital forensics
KW  - Standard representation
KW  - Digital forensic ontology
KW  - Digital forensic XML
KW  - CybOX
KW  - DFXML
KW  - DFAX
AB  - Abstract
With the growing number of digital forensic tools and the increasing use of digital forensics in various contexts, including incident response and cyber threat intelligence, there is a pressing need for a widely accepted standard for representing and exchanging digital forensic information. Such a standard representation can support correlation between different data sources, enabling more effective and efficient querying and analysis of digital evidence. This work summarizes the strengths and weaknesses of existing schemas, and proposes the open-source CybOX schema as a foundation for storing and sharing digital forensic information. The suitability of CybOX for representing objects and relationships that are common in forensic investigations is demonstrated with examples involving digital evidence. The capability to represent provenance by leveraging CybOX is also demonstrated, including specifics of the tool used to process digital evidence and the resulting output. An example is provided of an ongoing project that uses CybOX to record the state of a system before and after an event in order to capture cause and effect information that can be useful for digital forensics. An additional open-source schema and associated ontology called Digital Forensic Analysis eXpression (DFAX) is proposed that provides a layer of domain specific information overlaid on CybOX. DFAX extends the capability of CybOX to represent more abstract forensic-relevant actions, including actions performed by subjects and by forensic examiners, which can be useful for sharing knowledge and supporting more advanced forensic analysis. DFAX can be used in combination with other existing schemas for representing identity information (CIQ), and location information (KML). This work also introduces and leverages initial steps of a Unified Cyber Ontology (UCO) effort to abstract and express concepts/constructs that are common across the cyber domain.
ER  - 

TY  - JOUR
T1  - The growing need for on-scene triage of mobile devices
JO  - Digital Investigation
VL  - 6
IS  - 3–4
SP  - 112
EP  - 124
PY  - 2010/5//
T2  - Embedded Systems Forensics: Smart Phones, GPS Devices, and Gaming Consoles
AU  - Mislan, Richard P.
AU  - Casey, Eoghan
AU  - Kessler, Gary C.
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2010.03.001
UR  - http://www.sciencedirect.com/science/article/pii/S1742287610000149
KW  - Mobile device forensics
KW  - Cell phone forensics
KW  - On-scene triage inspection
KW  - Mobile device technician
AB  - The increasing number of mobile devices being submitted to Digital Forensic Laboratories (DFLs) is creating a backlog that can hinder investigations and negatively impact public safety and the criminal justice system. In a military context, delays in extracting intelligence from mobile devices can negatively impact troop and civilian safety as well as the overall mission. To address this problem, there is a need for more effective on-scene triage methods and tools to provide investigators with information in a timely manner, and to reduce the number of devices that are submitted to DFLs for analysis. Existing tools that are promoted for on-scene triage actually attempt to fulfill the needs of both on-scene triage and in-lab forensic examination in a single solution. On-scene triage has unique requirements because it is a precursor to and distinct from the forensic examination process, and may be performed by mobile device technicians rather than forensic analysts. This paper formalizes the on-scene triage process, placing it firmly in the overall forensic handling process and providing guidelines for standardization of on-scene triage. In addition, this paper outlines basic requirements for automated triage tools.
ER  - 

TY  - JOUR
T1  - Usage derived recommendations for a video digital library
JO  - Journal of Network and Computer Applications
VL  - 30
IS  - 3
SP  - 1059
EP  - 1083
PY  - 2007/8//
T2  - 
AU  - Bollen, Johan
AU  - Nelson, Michael L.
AU  - Geisler, Gary
AU  - Araujo, Raquel
SN  - 1084-8045
DO  - http://dx.doi.org/10.1016/j.jnca.2005.12.009
UR  - http://www.sciencedirect.com/science/article/pii/S1084804506000038
KW  - Usage analysis
KW  - Recommender systems
KW  - Video
KW  - Open Video Project
AB  - We describe a minimalist methodology to develop usage-based recommender systems for multimedia digital libraries. A prototype recommender system based on this strategy was implemented for the Open Video Project, a digital library of videos that are freely available for download. Sequential patterns of video retrievals are extracted from the project's web download logs and analyzed to generate a network of video relationships. A spreading activation algorithm locates video recommendations by searching for associative paths connecting query-related videos. We evaluate the performance of the resulting system relative to an item-based collaborative filtering technique operating on user profiles extracted from the same log data.
ER  - 

TY  - JOUR
T1  - From expert-derived user needs to user-perceived ease of use and usefulness: A two-phase mixed-methods evaluation framework
JO  - Journal of Biomedical Informatics
VL  - 52
IS  - 
SP  - 141
EP  - 150
PY  - 2014/12//
T2  - Special Section: Methods in Clinical Research Informatics
AU  - Boland, Mary Regina
AU  - Rusanov, Alexander
AU  - So, Yat
AU  - Lopez-Jimenez, Carlos
AU  - Busacca, Linda
AU  - Steinman, Richard C.
AU  - Bakken, Suzanne
AU  - Bigger, J. Thomas
AU  - Weng, Chunhua
SN  - 1532-0464
DO  - http://dx.doi.org/10.1016/j.jbi.2013.12.004
UR  - http://www.sciencedirect.com/science/article/pii/S1532046413001950
KW  - Evaluation studies
KW  - Clinical trials
KW  - Workflow
KW  - Needs assessment
KW  - Medical informatics
AB  - Abstract
Underspecified user needs and frequent lack of a gold standard reference are typical barriers to technology evaluation. To address this problem, this paper presents a two-phase evaluation framework involving usability experts (phase 1) and end-users (phase 2). In phase 1, a cross-system functionality alignment between expert-derived user needs and system functions was performed to inform the choice of “the best available” comparison system to enable a cognitive walkthrough in phase 1 and a comparative effectiveness evaluation in phase 2. During phase 2, five quantitative and qualitative evaluation methods are mixed to assess usability: time-motion analysis, software log, questionnaires – System Usability Scale and the Unified Theory of Acceptance of Use of Technology, think-aloud protocols, and unstructured interviews. Each method contributes data for a unique measure (e.g., time motion analysis contributes task-completion-time; software log contributes action transition frequency). The measures are triangulated to yield complementary insights regarding user-perceived ease-of-use, functionality integration, anxiety during use, and workflow impact. To illustrate its use, we applied this framework in a formative evaluation of a software called Integrated Model for Patient Care and Clinical Trials (IMPACT). We conclude that this mixed-methods evaluation framework enables an integrated assessment of user needs satisfaction and user-perceived usefulness and usability of a novel design. This evaluation framework effectively bridges the gap between co-evolving user needs and technology designs during iterative prototyping and is particularly useful when it is difficult for users to articulate their needs for technology support due to the lack of a baseline.
ER  - 

TY  - JOUR
T1  - Network intrusion investigation – Preparation and challenges
JO  - Digital Investigation
VL  - 3
IS  - 3
SP  - 118
EP  - 126
PY  - 2006/9//
T2  - 
AU  - Johnston, Andy
AU  - Reust, Jessica
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2006.08.001
UR  - http://www.sciencedirect.com/science/article/pii/S1742287606000922
KW  - Intrusion investigation
KW  - Incident response
KW  - Network forensics
KW  - Digital forensic examination
KW  - Compromise of sensitive information
KW  - Forensic preparedness
AB  - As new legislation is written mandating notification of affected parties following the compromise of confidential data, reliable investigative procedures into unauthorized access of such data assume increasing importance. The increasing costs and penalties associated with exposure of sensitive data can be mitigated through forensic preparation and the ability to employ digital forensics. A case study of the compromise of several systems containing sensitive data is outlined, with particular attention given to the procedures followed during the initial response and their impact on the subsequent digital forensic examination. Practical problems and challenges that arise in intrusion investigations are discussed, along with solutions and methodologies to address these issues. This case study illustrates both the importance of evaluating the evidence analyzed and of corroborating findings and conclusions with multiple independent sources of evidence. An initial response that incorporates forensic procedures provides a solid foundation for a successful and thorough forensic examination.
ER  - 

TY  - JOUR
T1  - AlmaNebula: A Computer Forensics Framework for the Cloud
JO  - Procedia Computer Science
VL  - 19
IS  - 
SP  - 139
EP  - 146
PY  - 2013///
T2  - The 4th International Conference on Ambient Systems, Networks and Technologies (ANT 2013), the 3rd International Conference on Sustainable Energy Information Technology (SEIT-2013)
AU  - Federici, Corrado
SN  - 1877-0509
DO  - http://dx.doi.org/10.1016/j.procs.2013.06.023
UR  - http://www.sciencedirect.com/science/article/pii/S1877050913006315
KW  - Forensics as a service
KW  - Computer forensics framework
KW  - Commodity computing
KW  - Big data
KW  - Web scale
KW  - Distributed processing
AB  - Abstract
Scalability, fault tolerance and collaborative processing across possibly dispersed sites are key enablers of modern computer forensics applications, that must be able to elastically accommodate all kinds of digital investigations, without wasting resources or fail to deliver timely outcomes. Traditional tools running in a standalone or client- server setups may fall short when handling the multi terabyte scale of a case just above average or, conversely, lie mainly underutilized when dealing with few digital evidences. A new category of applications that leverage the opportunities offered by modern Cloud Computing (CC) platforms, where scalable computational power and storage capacity can be engaged and decommissioned on demand, allow one to conveniently master huge amounts of information that otherwise could be impossible to wield. This paper discusses the design goals, technical requirements and architecture of AlmaNebula, a conceptual framework for the analysis of digital evidences built on top of a Cloud infrastructure, which aims to embody the concept of “Forensics as a service”.
ER  - 

TY  - JOUR
T1  - Forensic investigation of cloud computing systems
JO  - Network Security
VL  - 2011
IS  - 3
SP  - 4
EP  - 10
PY  - 2011/3//
T2  - 
AU  - Taylor, Mark
AU  - Haggerty, John
AU  - Gresty, David
AU  - Lamb, David
SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/S1353-4858(11)70024-1
UR  - http://www.sciencedirect.com/science/article/pii/S1353485811700241
AB  - Cloud computing describes a computing concept where software services, and the resources they use, operate as (and on) a virtualised platform across many different host machines, connected by the Internet or an organisation's internal network. From a business or system user's point of view, the cloud provides, via virtualisation, a single platform or service collection in which it can operate.

Cloud computing is a new concept in the distributed processing of data and is likely to make computer forensic evidence acquisition and evidence analysis increasingly complex.

Currently there do not appear to be any published guidelines that specifically address the conduct of computer forensic investigations of cloud computing systems. In order to understand and analyse evidence within this environment, computer forensics examiners will require a broader range of technical knowledge across multiple hardware platforms and operating systems. Dr Mark Taylor et al examine the issues concerning the forensic investigation of cloud systems.
ER  - 

TY  - JOUR
T1  - Handling Distributed Denial-of-Service Attacks
JO  - Information Security Technical Report
VL  - 6
IS  - 3
SP  - 37
EP  - 44
PY  - 2001/9/1/
T2  - 
AU  - Janczewski, Lech J
SN  - 1363-4127
DO  - http://dx.doi.org/10.1016/S1363-4127(01)00306-5
UR  - http://www.sciencedirect.com/science/article/pii/S1363412701003065
ER  - 

TY  - JOUR
T1  - Computer forensics challenges in responding to incidents in real-life settings
JO  - Computer Fraud & Security
VL  - 2007
IS  - 12
SP  - 12
EP  - 16
PY  - 2007/12//
T2  - 
AU  - Schultz, E. Eugene
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(07)70169-0
UR  - http://www.sciencedirect.com/science/article/pii/S1361372307701690
AB  - Dr Eugene Shultz looks at the practice of computer forensics during a real-life incident.
ER  - 

TY  - JOUR
T1  - Understanding users’ behavior with software operation data mining
JO  - Computers in Human Behavior
VL  - 30
IS  - 
SP  - 583
EP  - 594
PY  - 2014/1//
T2  - 
AU  - Pachidi, Stella
AU  - Spruit, Marco
AU  - van de Weerd, Inge
SN  - 0747-5632
DO  - http://dx.doi.org/10.1016/j.chb.2013.07.049
UR  - http://www.sciencedirect.com/science/article/pii/S0747563213002884
KW  - Software usage
KW  - User behavior
KW  - Software operation knowledge
KW  - Software analytics
KW  - Log data
KW  - Data mining
AB  - Abstract
Software usage concerns knowledge about how end-users use the software in the field, and how the software itself responds to their actions. In this paper, we present the Usage Mining Method to guide the analysis of data collected during software operation, in order to extract knowledge about how a software product is used by the end-users. Our method suggests three analysis tasks which employ data mining techniques for extracting usage knowledge from software operation data: users profiling, clickstream analysis and classification analysis. The Usage Mining Method was evaluated through a prototype that was executed in the case of Exact Online, the main online financial management application in the Netherlands. The evaluation confirmed the supportive role of the Usage Mining Method in software product management and development processes, as well as the applicability of the suggested data mining algorithms to carry out the usage analysis tasks.
ER  - 

TY  - JOUR
T1  - A novel mobile device user interface with integrated social networking services
JO  - International Journal of Human-Computer Studies
VL  - 71
IS  - 9
SP  - 919
EP  - 932
PY  - 2013/9//
T2  - Social Networks and Ubiquitous Interactions
AU  - Cui, Yanqing
AU  - Honkala, Mikko
SN  - 1071-5819
DO  - http://dx.doi.org/10.1016/j.ijhcs.2013.03.004
UR  - http://www.sciencedirect.com/science/article/pii/S1071581913000396
KW  - Mobile Web
KW  - Social networking services
KW  - Hypertext navigation
KW  - Automatic filtering
KW  - User experience
AB  - Abstract
Modern mobile devices support accessing Web-based social networking services from the user interface (UI) of Web browsers, applications, and mobile widgets. While effectively accessing these services, people may find it tedious to switch between multiple user interfaces in order to be aware of the latest content. Aiming for an improved user experience, we experimented with integration of these services into mobile devices' main user interface. The integrated content is presented beyond application silos and automatically filtered to highlight the relevant elements. A mobile system called LinkedUI was developed and deployed in one lab test and one field study. Three findings emerge from these studies. Firstly, it is feasible to construct an alternative device UI that supports integration of Web content across applications and services via hyperlinking. Time, publisher (e.g., contacts), content types, and geographical locations are key dimensions for association of content. Secondly, the alternative device UI enables better usability of accessing social networking services than accessing them from individual Web sites on mobile devices. It helps people to be aware of the latest content during microbreaks. Thirdly, automatic filtering, on the basis of one user's data, is one promising approach to identifying relevant content. Given filtered content, most people using the automatic filtering approved the functionality and experienced a better sense of control that is arguably due to the reduced information volume.
ER  - 

TY  - JOUR
T1  - Leaving a trace
JO  - Infosecurity
VL  - 5
IS  - 6
SP  - 30
EP  - 35
PY  - 2008/9//
T2  - 
AU  - Gold, Steve
SN  - 1754-4548
DO  - http://dx.doi.org/10.1016/S1754-4548(08)70102-5
UR  - http://www.sciencedirect.com/science/article/pii/S1754454808701025
AB  - IT forensics is seen by many in the industry as something of a black art. But it's actually a highly professional discipline, with professional software to assist, Steve Gold discovers
ER  - 

TY  - JOUR
T1  - Case study: From embedded system analysis to embedded system based investigator tools
JO  - Digital Investigation
VL  - 11
IS  - 3
SP  - 154
EP  - 159
PY  - 2014/9//
T2  - Special Issue: Embedded Forensics
AU  - Souvignet, T.
AU  - Prüfer, T.
AU  - Frinken, J.
AU  - Kricsanowits, R.
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2014.06.004
UR  - http://www.sciencedirect.com/science/article/pii/S1742287614000784
KW  - Skimming
KW  - Embedded systems
KW  - Payment card fraud
KW  - Forensic tools
KW  - Bluetooth forensics
KW  - Arduino
KW  - Android
AB  - Abstract
Since mid-2012, France and Germany have had to deal with a new form of payment card skimming. This fraud consists of adding a wireless embedded system into a point-of-sale payment terminal with the fraudulent goal of collecting payment card data and personal identification numbers (PIN).

This case study details the strategy adopted to conduct the digital forensic examination of these skimmers. Advanced technologies and analyses were necessary to reveal the skimmed data and provide useful information to investigators for their cross-case analysis.

To go further than a typical digital forensic examination, developments based on embedded systems were made to help investigators find compromised payment terminals and identify criminals.

Finally, this case study provides possible reactive and proactive new roles for forensic experts in combating payment card fraud.
ER  - 

TY  - JOUR
T1  - Forensic collection of cloud storage data: Does the act of collection result in changes to the data or its metadata?
JO  - Digital Investigation
VL  - 10
IS  - 3
SP  - 266
EP  - 277
PY  - 2013/10//
T2  - 
AU  - Quick, Darren
AU  - Choo, Kim-Kwang Raymond
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2013.07.001
UR  - http://www.sciencedirect.com/science/article/pii/S1742287613000741
KW  - Cloud storage
KW  - Data collection
KW  - Cloud forensics
KW  - Preservation
KW  - Dropbox
KW  - Google Drive
KW  - Microsoft SkyDrive
KW  - Computer forensics
KW  - Digital forensics
KW  - Digital evidence
AB  - Abstract
The timely acquisition and preservation of data from cloud storage can be an issue for law enforcement agencies and other digital forensic practitioners. In a jurisdiction which has legal provisions to collect data available to a computer or device, the process may involve accessing an account to collect the data. Using three popular public cloud storage providers (Dropbox, Google Drive, and Microsoft SkyDrive) as case studies, this research explores the process of collecting data from a cloud storage account using a browser and also downloading files using client software. We then compare these with the original files and undertake analysis of the resulting data. We determined that there were no changes to the contents of files during the process of upload, storage, and download to the three cloud storage services. The timestamps of the files were also examined in relation to the files downloaded via a browser and via client software. It was observed that some of the timestamp information remained the same throughout the process of uploading, storing and downloading files. Timestamp information may be a crucial aspect of an investigation, prosecution, or civil action, and therefore it is important to record the information available, and to understand the circumstances relating to a timestamp on a file.
ER  - 

TY  - JOUR
T1  - Closeness Preference – A new interestingness measure for sequential rules mining
JO  - Knowledge-Based Systems
VL  - 44
IS  - 
SP  - 48
EP  - 56
PY  - 2013/5//
T2  - 
AU  - Railean, Ion
AU  - Lenca, Philippe
AU  - Moga, Sorin
AU  - Borda, Monica
SN  - 0950-7051
DO  - http://dx.doi.org/10.1016/j.knosys.2013.01.025
UR  - http://www.sciencedirect.com/science/article/pii/S0950705113000452
KW  - Sequential rules
KW  - Interestingness measures
KW  - User-preference
KW  - Time-interval
KW  - Closeness Preference
AB  - The time-interval between the antecedent and the consequent of a sequential rule can be considered as an important aspect in sequential rules interest. For example, in web logs analysis, the end-user can be interested in predicting the next page that will be visited by an internet surfer based on a history of visited pages. A Closeness Preference measure is proposed to favour the sequential rules with close itemsets based on user time-preference in a post-processing step. We illustrate the interest of the Closeness Preference measure with two real datasets (web logs data and activities of daily living data) for first, a predictive task and second, a descriptive one. Both of them show that Closeness Preference measure is helpful to find small and efficient sets of simple sequential rules.
ER  - 

TY  - JOUR
T1  - Log management for effective incident response
JO  - Network Security
VL  - 2005
IS  - 9
SP  - 4
EP  - 7
PY  - 2005/9//
T2  - 
AU  - Forte, Dario
SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/S1353-4858(05)70279-8
UR  - http://www.sciencedirect.com/science/article/pii/S1353485805702798
AB  - Log file correlation is related to two distinct activities: Intrusion Detection and Network Forensics. It is more important than ever that these two disciplines work together, and in cooperation, to avoid points of failure. This article presents an overview of log analysis and correlation, with special emphasis on the tools and techniques for managing them within a network forensics context.
ER  - 

TY  - JOUR
T1  - Effective web log mining and online navigational pattern prediction
JO  - Knowledge-Based Systems
VL  - 49
IS  - 
SP  - 50
EP  - 62
PY  - 2013/9//
T2  - 
AU  - Guerbas, Abdelghani
AU  - Addam, Omar
AU  - Zaarour, Omar
AU  - Nagi, Mohamad
AU  - Elhajj, Ahmad
AU  - Ridley, Mick
AU  - Alhajj, Reda
SN  - 0950-7051
DO  - http://dx.doi.org/10.1016/j.knosys.2013.04.014
UR  - http://www.sciencedirect.com/science/article/pii/S0950705113001263
KW  - Web mining
KW  - Weblog mining
KW  - Pattern analysis
KW  - Prediction
KW  - Navigation
KW  - Indexing
AB  - Abstract
Accurate web log mining results and efficient online navigational pattern prediction are undeniably crucial for tuning up websites and consequently helping in visitors’ retention. Like any other data mining task, web log mining starts with data cleaning and preparation and it ends up discovering some hidden knowledge which cannot be extracted using conventional methods. In order for this process to yield good results it has to rely on some good quality input data. Therefore, more focus in this process should be on data cleaning and pre-processing. On the other hand, one of the challenges facing online prediction is scalability. As a result any improvement in the efficiency of online prediction solutions is more than necessary. As a response to the aforementioned concerns we are proposing an enhancement to the web log mining process and to the online navigational pattern prediction. Our contribution contains three different components. First, we are proposing a refined time-out based heuristic for session identification. Second, we are suggesting the usage of a specific density based algorithm for navigational pattern discovery. Finally, a new approach for efficient online prediction is also suggested. The conducted experiments demonstrate the applicability and effectiveness of the proposed approach.
ER  - 

TY  - JOUR
T1  - Analyzing multiple logs for forensic evidence
JO  - Digital Investigation
VL  - 4, Supplement
IS  - 
SP  - 82
EP  - 91
PY  - 2007/9//
T2  - 
AU  - Arasteh, Ali Reza
AU  - Debbabi, Mourad
AU  - Sakha, Assaad
AU  - Saleh, Mohamed
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2007.06.013
UR  - http://www.sciencedirect.com/science/article/pii/S1742287607000448
KW  - Forensic analysis
KW  - Log analysis
KW  - Formal methods
KW  - Model checking
KW  - Logging systems
KW  - Log correlation
AB  - Information stored in logs of a computer system is of crucial importance to gather forensic evidence of investigated actions or attacks. Analysis of this information should be rigorous and credible, hence it lends itself to formal methods. We propose a model checking approach to the formalization of the forensic analysis of logs. A set of logs is modeled as a tree whose labels are events extracted from the logs. In order to provide a structure to these events, we express each event as a term of algebra. The signature of the algebra is carefully chosen to include all relevant information necessary to conduct the analysis. Properties of the model, attack scenarios, and event sequences are expressed as formulas of a logic having dynamic, linear, temporal, and modal characteristics. Moreover, we provide a tableau-based proof system for this logic upon which a model checking algorithm can be developed. We use our model in a case study to demonstrate how events leading to an SYN attack can be reconstructed from a number of system logs.
ER  - 

TY  - JOUR
T1  - Local anomaly detection for mobile network monitoring
JO  - Information Sciences
VL  - 178
IS  - 20
SP  - 3840
EP  - 3859
PY  - 2008/10/15/
T2  - Special Issue on Industrial Applications of Neural Networks10th Engineering Applications of Neural Networks 2007
AU  - Kumpulainen, Pekka
AU  - Hätönen, Kimmo
SN  - 0020-0255
DO  - http://dx.doi.org/10.1016/j.ins.2008.05.038
UR  - http://www.sciencedirect.com/science/article/pii/S0020025508001886
KW  - Local anomaly detection
KW  - Outlier
KW  - Mobile networks
KW  - System log
KW  - Self-organizing map
KW  - Adaptive thresholds
AB  - Huge amounts of operation data are constantly collected from various parts of communication networks. These data include measurements from the radio connections and system logs from servers. System operators and developers need robust, easy to use decision support tools based on these data. One of their key applications is to detect anomalous phenomena of the network. In this paper we present an anomaly detection method that describes the normal states of the system with a self-organizing map (SOM) identified from the data. Large deviation in the data samples from the SOM nodes is detected as anomalous behavior. Large deviation has traditionally been detected using global thresholds. If variation of the data occurs in separate parts of the data space, the global thresholds either fail to reveal anomalies or reveal false anomalies. Instead of one global threshold, we can use local thresholds, which depend on the local variation of the data. We also present a method to find an adaptive threshold using the distribution of the deviations. Our anomaly detection method can be used both in exploration of history data or comparison of unforeseen data against a data model derived from history data. It is applicable to wide range of processes that produce multivariate data. In this paper we present examples of this method applied to server log data and radio interface data from mobile networks.
ER  - 

TY  - JOUR
T1  - Performance analysis of Bayesian networks and neural networks in classification of file system activities
JO  - Computers & Security
VL  - 31
IS  - 4
SP  - 391
EP  - 401
PY  - 2012/6//
T2  - 
AU  - Khan, Muhammad Naeem Ahmed
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/j.cose.2012.03.003
UR  - http://www.sciencedirect.com/science/article/pii/S0167404812000533
KW  - Digital forensics
KW  - Computer forensic analysis
KW  - Digital evidence
KW  - Neural networks
KW  - Bayesian learning
KW  - Bayesian decision theory
AB  - Precise comprehension of a file system state at any given time is vital for performing digital forensic analyses. To uncover evidence of the digital crime, the logical representation of file system activities helps reconstruct post-event timeline of the unauthorized or malicious accesses made on a system. This paper describes a comparative performance analysis of the Bayesian networks and neural networks techniques to classify the state of file system activities in terms of execution of applications based on the pattern of manipulation of specific files during certain period of time. In particular, this paper discusses the construction of a Bayesian networks and neural networks from the predetermined knowledge of the manipulation of file system artifacts and their corresponding metadata information by a set of software applications. The variability amongst the execution patterns of various applications indicate that the Bayesian network-based model is a more appropriate tool as compared to neural networks because of its ability to learn and detect patterns even from an incomplete dataset. The focus of this paper is to highlight intrinsic significance of the learning approach of Bayesian network methodology in comparison to the techniques used for supervised learning in ordinary neural networks. The paper also highlights the efficacy of Bayesian network technique to proficiently handle large volumes of datasets.
ER  - 

TY  - JOUR
T1  - Mining web navigations for intelligence
JO  - Decision Support Systems
VL  - 41
IS  - 3
SP  - 574
EP  - 591
PY  - 2006/3//
T2  - Intelligence and security informatics
AU  - Wu, Harris
AU  - Gordon, Michael
AU  - DeMaagd, Kurtis
AU  - Fan, Weiguo
SN  - 0167-9236
DO  - http://dx.doi.org/10.1016/j.dss.2004.06.011
UR  - http://www.sciencedirect.com/science/article/pii/S0167923604001320
KW  - Principal clusters analysis
KW  - Intelligence
KW  - Mining
KW  - Trend analysis
KW  - Navigation analysis
KW  - Information overload
KW  - Web community
AB  - The Internet is one of the fastest growing areas of intelligence gathering. We present a statistical approach, called principal clusters analysis, for analyzing millions of user navigations on the Web. This technique identifies prominent navigation clusters on different topics. Furthermore, it can determine information items that are useful starting points to explore a topic, as well as key documents to explore the topic in greater detail. Trends can be detected by observing navigation prominence over time. We apply this technique on a large popular website. The results show promise in web intelligence mining.
ER  - 

TY  - JOUR
T1  - Investigating around mainframes and other high-end systems: the revenge of big iron
JO  - Digital Investigation
VL  - 1
IS  - 2
SP  - 90
EP  - 93
PY  - 2004/6//
T2  - 
AU  - Pemble, Matthew
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2004.04.006
UR  - http://www.sciencedirect.com/science/article/pii/S1742287604000349
KW  - Mainframe
KW  - Audit
KW  - Investigation
KW  - Forensic
KW  - Log
AB  - Normal computer forensic tools and techniques are ineffective or inefficient when applied to mainframes or other very large systems. Incidents involving mainframes are likely to have a significant business impact, so preparation is essential, both of any specialist tools and of staff with mainframe experience. Proper design of system audit logs can also provide admissible material of evidential weight, without the requirement for forensic treatment.
ER  - 

TY  - JOUR
T1  - The 1999 DARPA off-line intrusion detection evaluation
JO  - Computer Networks
VL  - 34
IS  - 4
SP  - 579
EP  - 595
PY  - 2000/10//
T2  - Recent Advances in Intrusion Detection Systems
AU  - Lippmann, Richard
AU  - Haines, Joshua W
AU  - Fried, David J
AU  - Korba, Jonathan
AU  - Das, Kumar
SN  - 1389-1286
DO  - http://dx.doi.org/10.1016/S1389-1286(00)00139-0
UR  - http://www.sciencedirect.com/science/article/pii/S1389128600001390
KW  - Intrusion detection
KW  - Evaluate
KW  - Attack
KW  - Audit
KW  - Test bed
AB  - Eight sites participated in the second Defense Advanced Research Projects Agency (DARPA) off-line intrusion detection evaluation in 1999. A test bed generated live background traffic similar to that on a government site containing hundreds of users on thousands of hosts. More than 200 instances of 58 attack types were launched against victim UNIX and Windows NT hosts in three weeks of training data and two weeks of test data. False-alarm rates were low (less than 10 per day). The best detection was provided by network-based systems for old probe and old denial-of-service (DoS) attacks and by host-based systems for Solaris user-to-root (U2R) attacks. The best overall performance would have been provided by a combined system that used both host- and network-based intrusion detection. Detection accuracy was poor for previously unseen, new, stealthy and Windows NT attacks. Ten of the 58 attack types were completely missed by all systems. Systems missed attacks because signatures for old attacks did not generalize to new attacks, auditing was not available on all hosts, and protocols and TCP services were not analyzed at all or to the depth required. Promising capabilities were demonstrated by host-based systems, anomaly detection systems and a system that performs forensic analysis on file system data.
ER  - 

TY  - JOUR
T1  - Automated event and social network extraction from digital evidence sources with ontological mapping
JO  - Digital Investigation
VL  - 13
IS  - 
SP  - 94
EP  - 106
PY  - 2015/6//
T2  - 
AU  - Turnbull, Benjamin
AU  - Randhawa, Suneel
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2015.04.004
UR  - http://www.sciencedirect.com/science/article/pii/S1742287615000444
KW  - Artificial intelligence
KW  - Big data
KW  - Digital forensics
KW  - Digital evidence
KW  - Event representation
KW  - Forensic tool development
KW  - Knowledge representation
KW  - Ontology
KW  - Software engineering
KW  - Triage
AB  - Abstract
The sharp rise in consumer computing, electronic and mobile devices and data volumes has resulted in increased workloads for digital forensic investigators and analysts. The number of crimes involving electronic devices is increasing, as is the amount of data for each job. This is becoming unscaleable and alternate methods to reduce the time trained analysts spend on each job are necessary.

This work leverages standardised knowledge representations techniques and automated rule-based systems to encapsulate expert knowledge for forensic data. The implementation of this research can provide high-level analysis based on low-level digital artefacts in a way that allows an understanding of what decisions support the facts. Analysts can quickly make determinations as to which artefacts warrant further investigation and create high level case data without manually creating it from the low-level artefacts. Extraction and understanding of users and social networks and translating the state of file systems to sequences of events are the first uses for this work.

A major goal of this work is to automatically derive ‘events’ from the base forensic artefacts. Events may be system events, representing logins, start-ups, shutdowns, or user events, such as web browsing, sending email. The same information fusion and homogenisation techniques are used to reconstruct social networks. There can be numerous social network data sources on a single computer; internet cache can locate Facebook, LinkedIn, Google Plus caches; email has address books and copies of emails sent and received; instant messenger has friend lists and call histories. Fusing these into a single graph allows a more complete, less fractured view for an investigator.

Both event creation and social network creation are expected to assist investigator-led triage and other fast forensic analysis situations.
ER  - 

TY  - JOUR
T1  - Forensic analysis of logs: Modeling and verification
JO  - Knowledge-Based Systems
VL  - 20
IS  - 7
SP  - 671
EP  - 682
PY  - 2007/10//
T2  - Special Issue on Techniques to Produce Intelligent Secure Software
AU  - Saleh, Mohamed
AU  - Arasteh, Ali Reza
AU  - Sakha, Assaad
AU  - Debbabi, Mourad
SN  - 0950-7051
DO  - http://dx.doi.org/10.1016/j.knosys.2007.05.002
UR  - http://www.sciencedirect.com/science/article/pii/S0950705107000561
KW  - Forensic analysis
KW  - Log analysis
KW  - Formal methods
KW  - Model checking
KW  - Logging systems
AB  - Information stored in logs of a computer system is of crucial importance to gather forensic evidence of investigated actions or attacks against the system. Analysis of this information should be rigorous and credible, hence it lends itself to formal methods. We propose a model checking approach to the formalization of the forensic analysis of logs. The set of logs of a certain system is modeled as a tree whose labels are events extracted from the logs. In order to provide a structure to these events, we express each event as a term of a term algebra. The signature of the algebra is carefully chosen to include all relevant information necessary to conduct the analysis. Properties of the model are expressed as formulas of a logic having dynamic, linear, temporal, and modal characteristics. Moreover, we provide a tableau-based proof system for this logic upon which a model checking algorithm can be developed. In order to illustrate the proposed approach, the Windows auditing system is studied. The properties that we capture in our logic include invariant properties of a system, forensic hypotheses, and generic or specific attack signatures. Moreover, we discuss the admissibility of forensics hypotheses and the underlying verification issues.
ER  - 

TY  - JOUR
T1  - An anticipation model of potential customers’ purchasing behavior based on clustering analysis and association rules analysis
JO  - Expert Systems with Applications
VL  - 32
IS  - 3
SP  - 753
EP  - 764
PY  - 2007/4//
T2  - 
AU  - Chang, Horng-Jinh
AU  - Hung, Lun-Ping
AU  - Ho, Chia-Ling
SN  - 0957-4174
DO  - http://dx.doi.org/10.1016/j.eswa.2006.01.049
UR  - http://www.sciencedirect.com/science/article/pii/S0957417406000327
KW  - Web log
KW  - Association rules
KW  - Clustering analysis
AB  - This paper proposes an anticipation model of potential customers’ purchasing behavior. This model is inferred from past purchasing behavior of loyal customers and the web server log files of loyal and potential customers by means of clustering analysis and association rules analysis. Clustering analysis collects key characteristics of loyal customers’ personal information; these are used to locate other potential customers. Association rules analysis extracts knowledge of loyal customers’ purchasing behavior, which is used to detect potential customers’ near-future interest in a star product. Despite using offline analysis to filter out potential customers based on loyal customers’ personal information and generate rules of loyal customers’ click streams based on loyal customers’ web log data, an online analysis which observes potential customers’ web logs and compares it with loyal customers’ click stream rules can more readily target potential customers who may be interested in the star products in the near future.
ER  - 

TY  - JOUR
T1  - Network forensics based on fuzzy logic and expert system
JO  - Computer Communications
VL  - 32
IS  - 17
SP  - 1881
EP  - 1892
PY  - 2009/11/15/
T2  - 
AU  - Liao, Niandong
AU  - Tian, Shengfeng
AU  - Wang, Tinghua
SN  - 0140-3664
DO  - http://dx.doi.org/10.1016/j.comcom.2009.07.013
UR  - http://www.sciencedirect.com/science/article/pii/S0140366409002060
KW  - Network forensics
KW  - Expert system
KW  - Fuzzy logic
KW  - Intrusion detection system
KW  - Vulnerability scanning
AB  - Network forensics is a research area that finds the malicious users by collecting and analyzing the intrusion or infringement evidence of computer crimes such as hacking. In the past, network forensics was only used by means of investigation. However, nowadays, due to the sharp increase of network traffic, not all the information captured or recorded will be useful for analysis or evidence. The existing methods and tools for network forensics show only simple results. The administrators have difficulty in analyzing the state of the damaged system without expert knowledge. Therefore, we need an effective and automated analyzing system for network forensics. In this paper, we firstly guarantee the evidence reliability as far as possible by collecting different forensic information of detection sensors. Secondly, we propose an approach based on fuzzy logic and expert system for network forensics that can analyze computer crimes in network environment and make digital evidences automatically. At the end of the paper, the experimental comparison results between our proposed method and other popular methods are presented. Experimental results show that the system can classify most kinds of attack types (91.5% correct classification rate on average) and provide analyzable and comprehensible information for forensic experts.
ER  - 

TY  - JOUR
T1  - Stratified analysis of AOL query log
JO  - Information Sciences
VL  - 179
IS  - 12
SP  - 1844
EP  - 1858
PY  - 2009/5/30/
T2  - Special Section: Web Search
AU  - Brenes, David J.
AU  - Gayo-Avello, Daniel
SN  - 0020-0255
DO  - http://dx.doi.org/10.1016/j.ins.2009.01.027
UR  - http://www.sciencedirect.com/science/article/pii/S0020025509000516
KW  - Query log analysis
KW  - User behaviour
KW  - User interactions
KW  - User intent
KW  - User profiling
AB  - Characterizing user’s intent and behaviour while using a retrieval information tool (e.g. a search engine) is a key question on web research, as it hold the keys to know how the users interact, what they are expecting and how we can provide them information in the most beneficial way. Previous research has focused on identifying the average characteristics of user interactions. This paper proposes a stratified method for analyzing query logs that groups queries and sessions according to their hit frequency and analyzes the characteristics of each group in order to find how representative the average values are. Findings show that behaviours typically associated with the average user do not fit in most of the aforementioned groups.
ER  - 

TY  - JOUR
T1  - Evaluation of e-learning systems based on fuzzy clustering models and statistical tools
JO  - Expert Systems with Applications
VL  - 37
IS  - 10
SP  - 6891
EP  - 6903
PY  - 2010/10//
T2  - 
AU  - Hogo, Mofreh A.
SN  - 0957-4174
DO  - http://dx.doi.org/10.1016/j.eswa.2010.03.032
UR  - http://www.sciencedirect.com/science/article/pii/S0957417410002137
KW  - e-Learning
KW  - Learner profile
KW  - Fuzzy C-means clustering
KW  - Kernelized FCM
KW  - Log file analyzer
AB  - This paper introduces a hybridization approach of AI techniques and statistical tools to evaluate and adapt the e-learning systems including e-learners. Learner’s profile plays a crucial role in the evaluation process and the recommendations to improve the e-learning process. This work classifies the learners into specific categories based on the learner’s profiles; the learners’ classes named as regular, workers, casual, bad, and absent. The work extracted the statistical usage patterns that give a clear map describing the data and helping in constructing the e-learning system. The work tries to find the answers of the question how to return the bad students who are away back to be regular ones and find a method to evaluate the e-learners as well as to adapt the content and structure of the e-learning system. The work introduces the application of different fuzzy clustering techniques (FCM and KFCM) to find the learners profiles. Different phases of the work are presented. Analysis of the results and comparison: There is a match with a 78% with the real world behavior and the fuzzy clustering reflects the learners’ behavior perfectly. Comparison between FCM and KFCM proved that the KFCM is much better than FCM.
ER  - 

TY  - JOUR
T1  - A framework for post-event timeline reconstruction using neural networks
JO  - Digital Investigation
VL  - 4
IS  - 3–4
SP  - 146
EP  - 157
PY  - 2007/9//
Y2  - 2007/12//
T2  - 
AU  - Khan, M.N.A.
AU  - Chatwin, C.R.
AU  - Young, R.C.D.
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2007.11.001
UR  - http://www.sciencedirect.com/science/article/pii/S1742287607000837
KW  - Computer forensics
KW  - Digital investigation
KW  - Event reconstruction
KW  - Digital evidence
KW  - Digital forensic analysis
KW  - Neural networks
AB  - Post-event timeline reconstruction plays a critical role in forensic investigation and serves as a means of identifying evidence of the digital crime. We present an artificial neural networks based approach for post-event timeline reconstruction using the file system activities. A variety of digital forensic tools have been developed during the past two decades to assist computer forensic investigators undertaking digital timeline analysis, but most of the tools cannot handle large volumes of data efficiently. This paper looks at the effectiveness of employing neural network methodology for computer forensic analysis by preparing a timeline of relevant events occurring on a computing machine by tracing the previous file system activities. Our approach consists of monitoring the file system manipulations, capturing file system snapshots at discrete intervals of time to characterise the use of different software applications, and then using this captured data to train a neural network to recognise execution patterns of the application programs. The trained version of the network may then be used to generate a post-event timeline of a seized hard disk to verify the execution of different applications at different time intervals to assist in the identification of available evidence.
ER  - 

TY  - JOUR
T1  - Detecting criminal organizations in mobile phone networks
JO  - Expert Systems with Applications
VL  - 41
IS  - 13
SP  - 5733
EP  - 5750
PY  - 2014/10/1/
T2  - 
AU  - Ferrara, Emilio
AU  - De Meo, Pasquale
AU  - Catanese, Salvatore
AU  - Fiumara, Giacomo
SN  - 0957-4174
DO  - http://dx.doi.org/10.1016/j.eswa.2014.03.024
UR  - http://www.sciencedirect.com/science/article/pii/S0957417414001614
KW  - Criminal networks
KW  - Community detection
KW  - Criminal communities
AB  - Abstract
The study of criminal networks using traces from heterogeneous communication media is acquiring increasing importance in nowadays society. The usage of communication media such as mobile phones and online social networks leaves digital traces in the form of metadata that can be used for this type of analysis. The goal of this work is twofold: first we provide a theoretical framework for the problem of detecting and characterizing criminal organizations in networks reconstructed from phone call records. Then, we introduce an expert system to support law enforcement agencies in the task of unveiling the underlying structure of criminal networks hidden in communication data. This platform allows for statistical network analysis, community detection and visual exploration of mobile phone network data. It enables forensic investigators to deeply understand hierarchies within criminal organizations, discovering members who play central role and provide connection among sub-groups. Our work concludes illustrating the adoption of our computational framework for a real-word criminal investigation.
ER  - 

TY  - JOUR
T1  - An analysis of web proxy logs with query distribution pattern approach for search engines
JO  - Computer Standards & Interfaces
VL  - 34
IS  - 1
SP  - 162
EP  - 170
PY  - 2012/1//
T2  - 
AU  - Taghavi, Mona
AU  - Patel, Ahmed
AU  - Schmidt, Nikita
AU  - Wills, Christopher
AU  - Tew, Yiqi
SN  - 0920-5489
DO  - http://dx.doi.org/10.1016/j.csi.2011.07.001
UR  - http://www.sciencedirect.com/science/article/pii/S0920548911000808
KW  - Web search services
KW  - Search engines
KW  - Query analysis
KW  - Distributed search engines
KW  - Proxy server logs
AB  - This study presents an analysis of users' queries directed at different search engines to investigate trends and suggest better search engine capabilities. The query distribution among search engines that includes spawning of queries, number of terms per query and query lengths is discussed to highlight the principal factors affecting a user's choice of search engines and evaluate the reasons of varying the length of queries. The results could be used to develop long to short term business plans for search engine service providers to determine whether or not to opt for more focused topic specific search offerings to gain better market share.
ER  - 

TY  - JOUR
T1  - The first 10 years of the Trojan Horse defence
JO  - Computer Fraud & Security
VL  - 2015
IS  - 1
SP  - 5
EP  - 13
PY  - 2015/1//
T2  - 
AU  - Bowles, Stephen
AU  - Hernandez-Castro, Julio
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(15)70005-9
UR  - http://www.sciencedirect.com/science/article/pii/S1361372315700059
AB  - Apprehended criminals throughout history have always attempted to put the blame on someone else, a strategy popularly known as a SODDI defence (Some Other Dude Did It). When this defence is used, the act of the crime (actus reus) and the guilty mind (mens rea) is blamed on another party. A Trojan Horse Defence (THD) is a type of modern SODDI defence, where the mens rea and actus reus are blamed on a piece of software, known as a trojan.1

It has now become common for people accused of some computer-related crime to claim that the responsibility lies with malware placed on their machine without their knowledge.

This so-called Trojan Horse Defence (THD) was first used a decade ago. In this article, Stephen Bowles and Julio Hernandez-Castro of the University of Kent undertake a timely retrospective with an in-depth and critical literature review plus a detailed look at the peculiarities of many court cases from around the world.
ER  - 

TY  - JOUR
T1  - Ideal log setting for database forensics reconstruction
JO  - Digital Investigation
VL  - 12
IS  - 
SP  - 27
EP  - 40
PY  - 2015/3//
T2  - 
AU  - Adedayo, Oluwasola Mary
AU  - Olivier, Martin S.
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2014.12.002
UR  - http://www.sciencedirect.com/science/article/pii/S1742287614001200
KW  - Database management system
KW  - Database forensics
KW  - Digital forensics
KW  - Reconstruction
KW  - Ideal log setting
AB  - Abstract
The ability to reconstruct the data stored in a database at an earlier time is an important aspect of database forensics. Past research shows that the log file in a database can be useful for reconstruction. However, in many database systems there are various options that control which information is included in the logs. This paper introduces the notion of the ideal log setting necessary for an effective reconstruction process in database forensics. The paper provides a survey of the default logging preferences in some of the popular database management systems and identifies the information that a database log should contain in order to be useful for reconstruction. The challenges that may be encountered in storing the information as well as ways of overcoming the challenges are discussed. Possible logging preferences that may be considered as the ideal log setting for the popular database systems are also proposed. In addition, the paper relates the identified requirements to the three dimensions of reconstruction in database forensics and points out the additional requirements and/or techniques that may be required in the different dimensions.
ER  - 

TY  - JOUR
T1  - Improving accuracy for identifying related PubMed queries by an integrated approach
JO  - Journal of Biomedical Informatics
VL  - 42
IS  - 5
SP  - 831
EP  - 838
PY  - 2009/10//
T2  - Biomedical Natural Language Processing
AU  - Lu, Zhiyong
AU  - Wilbur, W. John
SN  - 1532-0464
DO  - http://dx.doi.org/10.1016/j.jbi.2008.12.006
UR  - http://www.sciencedirect.com/science/article/pii/S153204640800155X
KW  - PubMed distance
KW  - Related query
KW  - PubMed query log
KW  - Session segmentation
KW  - Lexical similarity
KW  - Contextual similarity
AB  - PubMed is the most widely used tool for searching biomedical literature online. As with many other online search tools, a user often types a series of multiple related queries before retrieving satisfactory results to fulfill a single information need. Meanwhile, it is also a common phenomenon to see a user type queries on unrelated topics in a single session. In order to study PubMed users’ search strategies, it is necessary to be able to automatically separate unrelated queries and group together related queries. Here, we report a novel approach combining both lexical and contextual analyses for segmenting PubMed query sessions and identifying related queries and compare its performance with the previous approach based solely on concept mapping.

We experimented with our integrated approach on sample data consisting of 1539 pairs of consecutive user queries in 351 user sessions. The prediction results of 1396 pairs agreed with the gold-standard annotations, achieving an overall accuracy of 90.7%. This demonstrates that our approach is significantly better than the previously published method. By applying this approach to a one day query log of PubMed, we found that a significant proportion of information needs involved more than one PubMed query, and that most of the consecutive queries for the same information need are lexically related. Finally, the proposed PubMed distance is shown to be an accurate and meaningful measure for determining the contextual similarity between biological terms. The integrated approach can play a critical role in handling real-world PubMed query log data as is demonstrated in our experiments.
ER  - 

TY  - JOUR
T1  - Continuous auditing technologies and models: A discussion
JO  - Computers & Security
VL  - 25
IS  - 5
SP  - 325
EP  - 331
PY  - 2006/7//
T2  - 
AU  - Flowerday, S.
AU  - Blundell, A.W.
AU  - Von Solms, R.
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/j.cose.2006.06.004
UR  - http://www.sciencedirect.com/science/article/pii/S0167404806000964
KW  - Continuous auditing
KW  - Real-time assurances
KW  - Information integrity
KW  - Internal controls
KW  - Technology-based prevention
AB  - In the age of real-time accounting and real-time communication current audit practices, while effective, often provide audit results long after fraud and/or errors have occurred. Real-time assurances can assist in preventing intentional or unintentional errors. This can best be achieved through continuous auditing which relies heavily on technology. These technologies are embedded within and are crucial to continuous auditing models.
ER  - 

TY  - JOUR
T1  - How ontologies are made: Studying the hidden social dynamics behind collaborative ontology engineering projects
JO  - Web Semantics: Science, Services and Agents on the World Wide Web
VL  - 20
IS  - 
SP  - 18
EP  - 34
PY  - 2013/5//
T2  - 
AU  - Strohmaier, Markus
AU  - Walk, Simon
AU  - Pöschko, Jan
AU  - Lamprecht, Daniel
AU  - Tudorache, Tania
AU  - Nyulas, Csongor
AU  - Musen, Mark A.
AU  - Noy, Natalya F.
SN  - 1570-8268
DO  - http://dx.doi.org/10.1016/j.websem.2013.04.001
UR  - http://www.sciencedirect.com/science/article/pii/S1570826813000164
KW  - Collaborative ontology engineering
KW  - Evaluation
KW  - Change logs
AB  - Abstract
Traditionally, evaluation methods in the field of semantic technologies have focused on the end result of ontology engineering efforts, mainly, on evaluating ontologies and their corresponding qualities and characteristics. This focus has led to the development of a whole arsenal of ontology-evaluation techniques that investigate the quality of ontologies as a product. In this paper, we aim to shed light on the process of ontology engineering construction by introducing and applying a set of measures to analyze hidden social dynamics. We argue that especially for ontologies which are constructed collaboratively, understanding the social processes that have led to their construction is critical not only in understanding but consequently also in evaluating the ontologies. With the work presented in this paper, we aim to expose the texture of collaborative ontology engineering processes that is otherwise left invisible. Using historical change-log data, we unveil qualitative differences and commonalities between different collaborative ontology engineering projects. Explaining and understanding these differences will help us to better comprehend the role and importance of social factors in collaborative ontology engineering projects. We hope that our analysis will spur a new line of evaluation techniques that view ontologies not as the static result of deliberations among domain experts, but as a dynamic, collaborative and iterative process that needs to be understood, evaluated and managed in itself. We believe that advances in this direction would help our community to expand the existing arsenal of ontology evaluation techniques towards more holistic approaches.
ER  - 

TY  - JOUR
T1  - Integrating intrusion alert information to aid forensic explanation: An analytical intrusion detection framework for distributive IDS
JO  - Information Fusion
VL  - 10
IS  - 4
SP  - 325
EP  - 341
PY  - 2009/10//
T2  - Special Issue on Information Fusion in Computer Security
AU  - Sy, Bon K.
SN  - 1566-2535
DO  - http://dx.doi.org/10.1016/j.inffus.2009.01.001
UR  - http://www.sciencedirect.com/science/article/pii/S1566253509000153
KW  - Probabilistic inference
KW  - Model discovery
KW  - Intrusion detection
KW  - Forensic analysis
AB  - The objective of this research is to show an analytical intrusion detection framework (AIDF) comprised of (i) a probability model discovery approach, and (ii) a probabilistic inference mechanism for generating the most probable forensic explanation based on not only just the observed intrusion detection alerts, but also the unreported signature rules that are revealed in the probability model. The significance of the proposed probabilistic inference is its ability to integrate alert information available from IDS sensors distributed across subnets. We choose the open source Snort to illustrate its feasibility, and demonstrate the inference process applied to the intrusion detection alerts produced by Snort. Through a preliminary experimental study, we illustrate the applicability of AIDF for information integration and the realization of (i) a distributive IDS environment comprised of multiple sensors, and (ii) a mechanism for selecting and integrating the probabilistic inference results from multiple models for composing the most probable forensic explanation.
ER  - 

TY  - JOUR
T1  - Toward a general collection methodology for Android devices
JO  - Digital Investigation
VL  - 8, Supplement
IS  - 
SP  - S14
EP  - S24
PY  - 2011/8//
T2  - The Proceedings of the Eleventh Annual DFRWS Conference11th Annual Digital Forensics Research Conference
AU  - Vidas, Timothy
AU  - Zhang, Chengye
AU  - Christin, Nicolas
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2011.05.003
UR  - http://www.sciencedirect.com/science/article/pii/S1742287611000272
KW  - Android framework
KW  - Mobile devices
KW  - Digital forensics
KW  - Collection
KW  - Acquisition
AB  - The Android platform has been deployed across a wide range of devices, predominately mobile phones, bringing unprecedented common software features to a diverse set of devices independent of carrier and manufacturer. Modern digital forensics processes differentiate collection and analysis, with collection ideally only occurring once and the subsequent analysis relying upon proper collection. After exploring special device boot modes and Android’s partitioning schema we detail the composition of an Android bootable image and discuss the creation of such an image designed for forensic collection. The major contribution of this paper is a general process for data collection of Android devices and related results of experiments carried out on several specific devices.
ER  - 

TY  - JOUR
T1  - MEGA: A tool for Mac OS X operating system and application forensics
JO  - Digital Investigation
VL  - 5, Supplement
IS  - 
SP  - S83
EP  - S90
PY  - 2008/9//
T2  - The Proceedings of the Eighth Annual DFRWS Conference
AU  - Joyce, Robert A.
AU  - Powers, Judson
AU  - Adelstein, Frank
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2008.05.011
UR  - http://www.sciencedirect.com/science/article/pii/S1742287608000376
KW  - Mac OS X
KW  - Computer forensics
KW  - Spotlight
KW  - Disk image analysis
KW  - Application analysis
AB  - Computer forensic tools for Apple Mac hardware have traditionally focused on low-level file system details. Mac OS X and common applications on the Mac platform provide an abundance of information about the user's activities in configuration files, caches, and logs. We are developing MEGA, an extensible tool suite for the analysis of files on Mac OS X disk images. MEGA provides simple access to Spotlight metadata maintained by the operating system, yielding efficient file content search and exposing metadata such as digital camera make and model. It can also help investigators to assess FileVault encrypted home directories. MEGA support tools are under development to interpret files written by common Mac OS applications such as Safari, Mail, and iTunes.
ER  - 

TY  - JOUR
T1  - Temporal information searching behaviour and strategies
JO  - Information Processing & Management
VL  - 
IS  - 
SP  - 
EP  - 
PY  - 
T2  - 
AU  - Joho, Hideo
AU  - Jatowt, Adam
AU  - Blanco, Roi
SN  - 0306-4573
DO  - http://dx.doi.org/10.1016/j.ipm.2015.03.006
UR  - http://www.sciencedirect.com/science/article/pii/S0306457315000448
KW  - Temporal information retrieval
KW  - Information searching behaviour
KW  - Search strategies
KW  - User study
AB  - Abstract
Temporal aspects have been receiving a great deal of interest in Information Retrieval and related fields. Although previous studies have proposed, designed and implemented temporal-aware systems and solutions, understanding of people’s temporal information searching behaviour is still limited. This paper reports the findings of a user study that explored temporal information searching behaviour and strategies in a laboratory setting. Information needs were grouped into three temporal classes (Past, Recency, and Future) to systematically study their characteristics. The main findings of our experiment are as follows. (1) It is intuitive for people to augment topical keywords with temporal expressions such as history, recent, or future as a tactic of temporal search. (2) However, such queries produce mixed results and the success of query reformulations appears to depend on topics to a large extent. (3) Search engine interfaces should detect temporal information needs to trigger the display of temporal search options. (4) Finding a relevant Wikipedia page or similar summary page is a popular starting point of past information needs. (5) Current search engines do a good job for information needs related to recent events, but more work is needed for past and future tasks. (6) Participants found it most difficult to find future information. Searching for domain experts was a key tactic in Future search, and file types of relevant documents are different from other temporal classes. Overall, the comparison of search across temporal classes indicated that Future search was the most difficult and the least successful followed by the search for the Past and then for Recency information. This paper discusses the implications of these findings on the design of future temporal IR systems.
ER  - 

TY  - JOUR
T1  - Failure prediction based on log files using Random Indexing and Support Vector Machines
JO  - Journal of Systems and Software
VL  - 86
IS  - 1
SP  - 2
EP  - 11
PY  - 2013/1//
T2  - 
AU  - Fronza, Ilenia
AU  - Sillitti, Alberto
AU  - Succi, Giancarlo
AU  - Terho, Mikko
AU  - Vlasenko, Jelena
SN  - 0164-1212
DO  - http://dx.doi.org/10.1016/j.jss.2012.06.025
UR  - http://www.sciencedirect.com/science/article/pii/S0164121212001732
KW  - Failure prediction
KW  - Random Indexing
KW  - Support Vector Machine (SVM)
KW  - Event sequence data
KW  - Log files
AB  - Research problem
The impact of failures on software systems can be substantial since the recovery process can require unexpected amounts of time and resources. Accurate failure predictions can help in mitigating the impact of failures. Resources, applications, and services can be scheduled to limit the impact of failures. However, providing accurate predictions sufficiently ahead is challenging. Log files contain messages that represent a change of system state. A sequence or a pattern of messages may be used to predict failures.
Contribution
We describe an approach to predict failures based on log files using Random Indexing (RI) and Support Vector Machines (SVMs).
Method
RI is applied to represent sequences: each operation is characterized in terms of its context. SVMs associate sequences to a class of failures or non-failures. Weighted SVMs are applied to deal with imbalanced datasets and to improve the true positive rate. We apply our approach to log files collected during approximately three months of work in a large European manufacturing company.
Results
According to our results, weighted SVMs sacrifice some specificity to improve sensitivity. Specificity remains higher than 0.80 in four out of six analyzed applications.
Conclusions
Overall, our approach is very reliable in predicting both failures and non-failures.
ER  - 

TY  - JOUR
T1  - SoTE: Strategy of Triple-E on solving Trojan defense in Cyber-crime cases
JO  - Computer Law & Security Review
VL  - 26
IS  - 1
SP  - 52
EP  - 60
PY  - 2010/1//
T2  - 
AU  - Kao, Da-Yu
AU  - Wang, Shiuh-Jeng
AU  - Fu-Yuan Huang, Frank
SN  - 0267-3649
DO  - http://dx.doi.org/10.1016/j.clsr.2009.09.008
UR  - http://www.sciencedirect.com/science/article/pii/S0267364909001575
KW  - Cyber-crime
KW  - Cyber criminology
KW  - Digital evidence
KW  - Trojan defense
KW  - Triple-E strategy
AB  - Cyber activity has become an essential part of the general public's everyday life. The hacking threats of Cyber-crime are becoming more sophisticated as internet communication services are more popular. To further confirm the final finding of Cyber-crime, this study proposes three analytical tools to clarify the Cyber-crime issues by means of Ideal Log, M-N model and MDFA (Multi-faceted Digital Forensics Analysis) strategy, where Ideal Log is identified as a traceable element of digital evidence including four elements of IP Address, Timestamp, Digital Action, and Response Message. M-N model applies a formal method for collating and analyzing data sets of investigation-relevant logs in view of connected time with ISP logs. MDFA strategy attempts to outline the basic elements of Cyber-crime using new procedural investigative steps, and combining universal types of evidential information in terms of Evidence, Scene, Victim, and Suspect. After researchers figure out what has happened in Cyber-crime events, it will be easier to communicate with offenders, victims or related people. SoTE (Strategy of Triple-E) is discussed to observe Cyber-crime from the viewpoints of Education, Enforcement and Engineering. That approach is further analyzed from the fields of criminology, investigation and forensics. Each field has its different focus in dealing with diverse topics, such as: the policy of 6W1H (What, Which, When, Where, Who, Why, and How) questions, the procedure of MDFA strategy, the process of ideal Logs and M-N model. In addition, the case study and proposed suggestion of this paper are presented to counter Cyber-crime.
ER  - 

TY  - JOUR
T1  - Collaborative multi-agent rock facies classification from wireline well log data
JO  - Engineering Applications of Artificial Intelligence
VL  - 23
IS  - 7
SP  - 1158
EP  - 1172
PY  - 2010/10//
T2  - 
AU  - Gifford, Christopher M.
AU  - Agah, Arvin
SN  - 0952-1976
DO  - http://dx.doi.org/10.1016/j.engappai.2010.02.004
UR  - http://www.sciencedirect.com/science/article/pii/S095219761000062X
KW  - Rock classification
KW  - Well logs
KW  - Collaborative learning
KW  - Multi-agent systems
KW  - Applied artificial intelligence
AB  - Gas and oil reservoirs have been the focus of modeling efforts for decades as an attempt to locate zones with high volumes. Certain subsurface layers and layer sequences, such as those containing shale, are known to be impermeable to gas and/or liquid. Oil and natural gas then become trapped by these layers, making it possible to drill wells to reach the supply, and extract for use. The drilling of these wells, however, is costly. In this paper, we utilize multi-agent machine learning and classifier combination to learn rock facies sequences from wireline well log data. The paper focuses on how to construct a successful set of classifiers, which periodically collaborate, to increase the classification accuracy. Utilizing multiple, heterogeneous collaborative learning agents is shown to be successful for this classification problem. Utilizing the Multi-Agent Collaborative Learning Architecture, 84.5% absolute accuracy was obtained, an improvement of about 6.5% over the best results achieved by the Kansas Geological Survey with the same data set. A number of heuristics are presented for constructing teams of multiple collaborative classifiers for predicting rock facies.
ER  - 

TY  - JOUR
T1  - Applicability of Latent Dirichlet Allocation to multi-disk search
JO  - Digital Investigation
VL  - 11
IS  - 1
SP  - 43
EP  - 56
PY  - 2014/3//
T2  - 
AU  - Noel, George E.
AU  - Peterson, Gilbert L.
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2014.02.001
UR  - http://www.sciencedirect.com/science/article/pii/S1742287614000048
KW  - Latent Dirichlet Allocation
KW  - Topic models
KW  - Query by document
KW  - Data mining
KW  - Text mining
KW  - Document search
AB  - Abstract
Digital forensics practitioners face a continual increase in the volume of data they must analyze, which exacerbates the problem of finding relevant information in a noisy domain. Current technologies make use of keyword based search to isolate relevant documents and minimize false positives with respect to investigative goals. Unfortunately, selecting appropriate keywords is a complex and challenging task. Latent Dirichlet Allocation (LDA) offers a possible way to relax keyword selection by returning topically similar documents. This research compares regular expression search techniques and LDA using the Real Data Corpus (RDC). The RDC, a set of over 2400 disks from real users, is first analyzed to craft effective tests. Three tests are executed with the results indicating that, while LDA search should not be used as a replacement to regular expression search, it does offer benefits. First, it is able to locate documents when few, if any, of the keywords exist within them. Second, it improves data browsing and deals with keyword ambiguity by segmenting the documents into topics.
ER  - 

TY  - JOUR
T1  - Use of XML and Java for collaborative petroleum reservoir modeling on the Internet
JO  - Computers & Geosciences
VL  - 31
IS  - 9
SP  - 1151
EP  - 1164
PY  - 2005/11//
T2  - Application of XML in the Geosciences
AU  - Victorine, John
AU  - Watney, W. Lynn
AU  - Bhattacharya, Saibal
SN  - 0098-3004
DO  - http://dx.doi.org/10.1016/j.cageo.2004.12.007
UR  - http://www.sciencedirect.com/science/article/pii/S0098300405001068
KW  - GEMINI
KW  - Petroleum web-based software
KW  - Java
KW  - Web start
KW  - XML
KW  - Reservoir modeling
AB  - The GEMINI (Geo-Engineering Modeling through INternet Informatics) is a public-domain, web-based freeware that is made up of an integrated suite of 14 Java-based software tools to accomplish on-line, real-time geologic and engineering reservoir modeling. GEMINI facilitates distant collaborations for small company and academic clients, negotiating analyses of both single and multiple wells. The system operates on a single server and an enterprise database. External data sets must be uploaded into this database.

Feedback from GEMINI users provided the impetus to develop Stand Alone Web Start Applications of GEMINI modules that reside in and operate from the user's PC. In this version, the GEMINI modules run as applets, which may reside in local user PCs, on the server, or Java Web Start. In this enhanced version, XML-based data handling procedures are used to access data from remote and local databases and save results for later access and analyses. The XML data handling process also integrates different stand-alone GEMINI modules enabling the user(s) to access multiple databases. It provides flexibility to the user to customize analytical approach, database location, and level of collaboration. An example integrated field-study using GEMINI modules and Stand Alone Web Start Applications is provided to demonstrate the versatile applicability of this freeware for cost-effective reservoir modeling.
ER  - 

TY  - JOUR
T1  - Algorithms for anomaly detection of traces in logs of process aware information systems
JO  - Information Systems
VL  - 38
IS  - 1
SP  - 33
EP  - 44
PY  - 2013/3//
T2  - 
AU  - Bezerra, Fábio
AU  - Wainer, Jacques
SN  - 0306-4379
DO  - http://dx.doi.org/10.1016/j.is.2012.04.004
UR  - http://www.sciencedirect.com/science/article/pii/S0306437912000567
KW  - Anomaly detection
KW  - Process mining
KW  - Process-aware systems
AB  - This paper discusses four algorithms for detecting anomalies in logs of process aware systems. One of the algorithms only marks as potential anomalies traces that are infrequent in the log. The other three algorithms: threshold, iterative and sampling are based on mining a process model from the log, or a subset of it. The algorithms were evaluated on a set of 1500 artificial logs, with different profiles on the number of anomalous traces and the number of times each anomalous traces was present in the log. The sampling algorithm proved to be the most effective solution. We also applied the algorithm to a real log, and compared the resulting detected anomalous traces with the ones detected by a different procedure that relies on manual choices.
ER  - 

TY  - JOUR
T1  - Dropbox analysis: Data remnants on user machines
JO  - Digital Investigation
VL  - 10
IS  - 1
SP  - 3
EP  - 18
PY  - 2013/6//
T2  - 
AU  - Quick, Darren
AU  - Choo, Kim-Kwang Raymond
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2013.02.003
UR  - http://www.sciencedirect.com/science/article/pii/S174228761300011X
KW  - Cloud storage
KW  - Cloud forensics
KW  - Dropbox analysis
KW  - Computer forensics
KW  - Digital forensics
KW  - Mobile forensics
AB  - Cloud storage has been identified as an emerging challenge to digital forensic researchers and practitioners in a range of literature. There are various types of cloud storage services with each type having a potentially different use in criminal activity. One area of difficulty is the identification, acquisition, and preservation of evidential data when disparate services can be utilised by criminals. Not knowing if a cloud service is being used, or which cloud service, can potentially impede an investigation. It would take additional time to contact all service providers to determine if data is being stored within their cloud service. Using Dropbox™ as a case study, research was undertaken to determine the data remnants on a Windows 7 computer and an Apple iPhone 3G when a user undertakes a variety of methods to store, upload, and access data in the cloud. By determining the data remnants on client devices, we contribute to a better understanding of the types of terrestrial artifacts that are likely to remain for digital forensics practitioners and examiners. Potential information sources identified during the research include client software files, prefetch files, link files, network traffic capture, and memory captures, with many data remnants available subsequent to the use of Dropbox by a user.
ER  - 

TY  - JOUR
T1  - Distributed filesystem forensics: XtreemFS as a case study
JO  - Digital Investigation
VL  - 11
IS  - 4
SP  - 295
EP  - 313
PY  - 2014/12//
T2  - 
AU  - Martini, Ben
AU  - Choo, Kim-Kwang Raymond
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2014.08.002
UR  - http://www.sciencedirect.com/science/article/pii/S1742287614000942
KW  - Big data
KW  - Digital forensics
KW  - Distributed filesystem
KW  - Infrastructure as a Service (IaaS)
KW  - Storage as a Service (StaaS)
KW  - Distributed filesystem forensics
KW  - Cloud storage forensics
AB  - Abstract
Distributed filesystems provide a cost-effective means of storing high-volume, velocity and variety information in cloud computing, big data and other contemporary systems. These technologies have the potential to be exploited for illegal purposes, which highlights the need for digital forensic investigations. However, there have been few papers published in the area of distributed filesystem forensics. In this paper, we aim to address this gap in knowledge. Using our previously published cloud forensic framework as the underlying basis, we conduct an in-depth forensic experiment on XtreemFS, a Contrail EU-funded project, as a case study for distributed filesystem forensics. We discuss the technical and process issues regarding collection of evidential data from distributed filesystems, particularly when used in cloud computing environments. A number of digital forensic artefacts are also discussed. We then propose a process for the collection of evidential data from distributed filesystems.
ER  - 

TY  - JOUR
T1  - Forensic analysis techniques for fragmented flash memory pages in smartphones
JO  - Digital Investigation
VL  - 9
IS  - 2
SP  - 109
EP  - 118
PY  - 2012/11//
T2  - 
AU  - Park, Jungheum
AU  - Chung, Hyunji
AU  - Lee, Sangjin
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2012.09.003
UR  - http://www.sciencedirect.com/science/article/pii/S1742287612000643
KW  - Digital forensics
KW  - Smartphone forensics
KW  - Flash memory
KW  - Unallocated area
KW  - Fragmented data
AB  - A mobile phone contains important personal information, and therefore, it should be considered in digital forensic investigations. Recently, the number of smartphone owners has increased drastically. Unlike feature phones, smartphones have high-performance operating systems (e.g., Android, iOS), and users can install and utilize various mobile applications on smartphones.

Smartphone forensics has been actively studied because of the importance of smartphone user data acquisition and analysis for digital forensic purposes. In general, there are two logical approaches to smartphone forensics. The first approach is to extract user data using the backup and debugging function of smartphones. The second approach is to get root permission through the rooting or the bootloader method with custom kernel, and acquire an image of the flash memory. In addition, the other way is to acquire an image on a more physical way by using e.g., JTAG or chipoff process. In some cases, it may be possible to reconstruct and analyze the file system. However, existing methods for file system analysis are not suitable for recovering and analyzing data deleted from smartphones depending on the manner in which the flash memory image has to be acquired.

This paper proposes new analysis techniques for fragmented flash memory pages in smartphones. In particular, this paper demonstrates analysis techniques on the image that the reconstruction of file system is impossible because the spare area of flash memory pages does not exist or that it is created from the unallocated area of the undamaged file system.
ER  - 

TY  - JOUR
T1  - Validating ambient intelligence based ubiquitous computing systems by means of artificial societies
JO  - Information Sciences
VL  - 222
IS  - 
SP  - 3
EP  - 24
PY  - 2013/2/10/
T2  - Including Special Section on New Trends in Ambient Intelligence and Bio-inspired Systems
AU  - Serrano, Emilio
AU  - Botia, Juan
SN  - 0020-0255
DO  - http://dx.doi.org/10.1016/j.ins.2010.11.012
UR  - http://www.sciencedirect.com/science/article/pii/S0020025510005578
KW  - Social simulations
KW  - Ambient intelligence
KW  - Ubiquitous computing
KW  - Validation
KW  - Verification
KW  - Forensic analysis
AB  - This paper introduces a new methodology based on the use of Multi-Agent Based Simulations (MABS) for testing and validation of Ambient Intelligence based Ubiquitous Computing (UbiCom) systems. An ambient intelligence based UbiCom is a pervasive system in which services have some intelligence in order to smoothly interact with users immersed in the environment. The motivation for this methodology is its application in UbiCom large-scale systems where large numbers of users are involved and in applications which deal with dangerous environments. In these cases, real tests are impractical and an artificial society is required. MABS allows building cheap and quick prototypes which can describe UbiCom systems. Analyzing these prototypes, if they are sufficiently descriptive, allows requisites violations in functionality of real UbiCom system designs to be discovered. MABSs and particularly the most descriptive ones can present very complex behaviors. Therefore, the MABS analysis obtained with the presented methodology is not trivial. Consequently, this paper also proposes two techniques for the analysis of general complex MABSs: forensic analysis and the use of simpler simulations. Moreover, the methodology proposes to inject elements of the actual UbiCom system in the simulated world to increase the confidence of the validation process. The proposal is illustrated with a detailed case study that considers a building on our campus and an AmI service for evacuation in case of fire.
ER  - 

TY  - JOUR
T1  - Factors affecting the selection of search tactics: Tasks, knowledge, process, and systems
JO  - Information Processing & Management
VL  - 48
IS  - 2
SP  - 254
EP  - 270
PY  - 2012/3//
T2  - 
AU  - Xie, Iris
AU  - Joo, Soohyung
SN  - 0306-4573
DO  - http://dx.doi.org/10.1016/j.ipm.2011.08.009
UR  - http://www.sciencedirect.com/science/article/pii/S030645731100094X
KW  - Search tactics
KW  - Factors
KW  - Tasks
KW  - User knowledge
KW  - Search process
KW  - IR systems
AB  - This study investigated whether and how different factors in relation to task, user-perceived knowledge, search process, and system affect users’ search tactic selection. Thirty-one participants, representing the general public with their own tasks, were recruited for this study. Multiple methods were employed to collect data, including pre-questionnaire, verbal protocols, log analysis, diaries, and post-questionnaires. Statistical analysis revealed that seven factors were significantly associated with tactic selection. These factors consist of work task types, search task types, familiarity with topic, search skills, search session length, search phases, and system types. Moreover, the study also discovered, qualitatively, in what ways these factors influence the selection of search tactics. Based on the findings, the authors discuss practical implications for system design to support users’ application of multiple search tactics for each factor.
ER  - 

TY  - JOUR
T1  - Automatic high-performance reconstruction and recovery
JO  - Computer Networks
VL  - 51
IS  - 5
SP  - 1361
EP  - 1377
PY  - 2007/4/11/
T2  - From Intrusion Detection to Self-Protection
AU  - Goel, Ashvin
AU  - Feng, Wu-chang
AU  - Feng, Wu-chi
AU  - Maier, David
SN  - 1389-1286
DO  - http://dx.doi.org/10.1016/j.comnet.2006.09.013
UR  - http://www.sciencedirect.com/science/article/pii/S1389128606002465
KW  - Self-healing computers
KW  - Computer forensics
KW  - Operating systems
KW  - Auditing
AB  - Self-protecting systems require the ability to instantaneously detect malicious activity at run-time and prevent execution. We argue that it is impossible to perfectly self-protect systems without false positives due to the limited amount of information one might have at run-time and that eventually some undesirable activity will occur that will need to be rolled back. As a consequence of this, it is important that self-protecting systems have the ability to completely and automatically roll back malicious activity which has occurred.

As the cost of human resources currently dominates the cost of CPU, network, and storage resources, we contend that computing systems should be built with automated analysis and recovery as a primary goal. Towards this end, we describe the design, implementation, and evaluation of Forensix: a robust, high-precision analysis and recovery system for supporting self-healing. The Forensix system records all activity of a target computer and allows for efficient, automated reconstruction of activity when needed. Such a system can be used to automatically detect patterns of malicious activity and selectively undo their operations.

Forensix uses three key mechanisms to improve the accuracy and reduce the human overhead of performing analysis and recovery. First, it performs comprehensive monitoring of the execution of a target system at the kernel event level, giving a high-resolution, application-independent view of all activity. Second, it streams the kernel event information, in real-time, to append-only storage on a separate, hardened, logging machine, making the system resilient to a wide variety of attacks. Third, it uses database technology to support high-level querying of the archived log, greatly reducing the human cost of performing analysis and recovery.
ER  - 

TY  - JOUR
T1  - WELFIT: A remote evaluation tool for identifying Web usage patterns through client-side logging
JO  - International Journal of Human-Computer Studies
VL  - 76
IS  - 
SP  - 40
EP  - 49
PY  - 2015/4//
T2  - 
AU  - Santana, Vagner Figueredo de
AU  - Baranauskas, Maria Cecília Calani
SN  - 1071-5819
DO  - http://dx.doi.org/10.1016/j.ijhcs.2014.12.005
UR  - http://www.sciencedirect.com/science/article/pii/S1071581914001682
KW  - User interface evaluation
KW  - Remote evaluation
KW  - User interface events
KW  - Client-side events
KW  - Web usability
KW  - Web accessibility
AB  - Abstract
Although websites evaluation tools use different data sources (e.g., Web pages, server logs, and mouse tracks), few of them support remote evaluation using detailed observational data. Without considering data that represent the user’s real interaction with the interface, usability problems and/or accessibility barriers may remain unknown. This work contributes to the field by providing a tool to identify usage patterns based on client-side event logs and by presenting event stream composition characteristics. The work results from a long-term project and the tool is now available to the community. The system records usage data during real use, identifies usage patterns, and indicates potential user interface design problems. The proposed tool was experimented, counting on 180 participants, during a 15 month period collecting data from website usage. Results obtained are promising regarding the identification of usage patterns and the characterization of event streams based on the types of events that compose them.
ER  - 

TY  - JOUR
T1  - An empirical investigation of Web session workloads: Can self-similarity be explained by deterministic chaos?
JO  - Information Processing & Management
VL  - 50
IS  - 1
SP  - 41
EP  - 53
PY  - 2014/1//
T2  - 
AU  - Dick, Scott
AU  - Yazdanbaksh, Omolbanin
AU  - Tang, Xiuli
AU  - Huynh, Toan
AU  - Miller, James
SN  - 0306-4573
DO  - http://dx.doi.org/10.1016/j.ipm.2013.07.004
UR  - http://www.sciencedirect.com/science/article/pii/S0306457313000769
KW  - Web traffic
KW  - Session workload
KW  - Traffic modeling
KW  - Chaos theory
KW  - Nonlinear time series analysis
AB  - Abstract
Several studies of Web server workloads have hypothesized that these workloads are self-similar. The explanation commonly advanced for this phenomenon is that the distribution of Web server requests may be heavy-tailed. However, there is another possible explanation: self-similarity can also arise from deterministic, chaotic processes. To our knowledge, this possibility has not previously been investigated, and so existing studies on Web workloads lack an adequate comparison against this alternative. We conduct an empirical study of workloads from two different Web sites: one public university, and one private company, using the largest datasets that have been described in the literature. Our study employs methods from nonlinear time series analysis to search for chaotic behavior in the web logs of these two sites. While we do find that the deterministic components (i.e. the well-known “weekend effect”) are significant components in these time series, we do not find evidence of chaotic behavior. Predictive modeling experiments contrasting heavy-tailed with deterministic models showed that both approaches were equally effective in modeling our datasets.
ER  - 

TY  - JOUR
T1  - CFTP: a caching FTP server
JO  - Computer Networks and ISDN Systems
VL  - 30
IS  - 22–23
SP  - 2211
EP  - 2222
PY  - 1998/11/25/
T2  - 
AU  - Russell, Mark
AU  - Hopkins, Tim
SN  - 0169-7552
DO  - http://dx.doi.org/10.1016/S0169-7552(98)00240-2
UR  - http://www.sciencedirect.com/science/article/pii/S0169755298002402
KW  - Mirroring
KW  - Caching
KW  - Log summation
KW  - Usage statistics
KW  - Object oriented
AB  - By analyzing the log files generated by the UK National Web Cache and by a number of origin FTP sites we provide evidence that an FTP proxy cache with knowledge of local (national) mirror sites could significantly reduce the amount of data that needs to be transferred across already overused networks. We then describe the design and implementation of CFTP, a caching FTP server, and report on its usage over the first 10 months of its deployment. Finally we discuss a number of ways in which the software could be further enhanced to improve both its efficiency and its usability.
ER  - 

TY  - JOUR
T1  - Toward alternative metrics of journal impact: A comparison of download and citation data
JO  - Information Processing & Management
VL  - 41
IS  - 6
SP  - 1419
EP  - 1440
PY  - 2005/12//
T2  - Special Issue on Infometrics
AU  - Bollen, Johan
AU  - Van de Sompel, Herbert
AU  - Smith, Joan A.
AU  - Luce, Rick
SN  - 0306-4573
DO  - http://dx.doi.org/10.1016/j.ipm.2005.03.024
UR  - http://www.sciencedirect.com/science/article/pii/S0306457305000324
KW  - Usage analysis
KW  - Digital libraries
KW  - Journal impact ranking
KW  - Impact metrics
KW  - Social network analysis
AB  - We generated networks of journal relationships from citation and download data, and determined journal impact rankings from these networks using a set of social network centrality metrics. The resulting journal impact rankings were compared to the ISI IF. Results indicate that, although social network metrics and ISI IF rankings deviate moderately for citation-based journal networks, they differ considerably for journal networks derived from download data. We believe the results represent a unique aspect of general journal impact that is not captured by the ISI IF. These results furthermore raise questions regarding the validity of the ISI IF as the sole assessment of journal impact, and suggest the possibility of devising impact metrics based on usage information in general.
ER  - 

TY  - JOUR
T1  - A stochastic model of e-customer behavior
JO  - Electronic Commerce Research and Applications
VL  - 2
IS  - 1
SP  - 81
EP  - 94
PY  - 2003///Spring
T2  - Containing Special Section: Five Best Papers selected from the International Conference on Electronic Commerce
AU  - Jenamani, Mamata
AU  - Mohapatra, Pratap K.J.
AU  - Ghose, Sujoy
SN  - 1567-4223
DO  - http://dx.doi.org/10.1016/S1567-4223(03)00010-3
UR  - http://www.sciencedirect.com/science/article/pii/S1567422303000103
KW  - E-customer behavior
KW  - Web usage mining
KW  - Discrete-time semi-Markov process
AB  - Web usage mining techniques are increasingly used today to understand e-customers’ within-site behavior. We propose a data mining model that considers e-customers’ activities as a discrete-time semi-Markov process and explains their behavior. An algorithm is proposed to compute transition probability matrix and holding time mass functions from the site navigation data. Finally, the model is used to explain customer behavior in an example site. A software agent, implemented in the site, collects and stores navigation data in the required form and thus helps to avoid data preprocessing. The model results helped to improve the site design and judge its performance.
ER  - 

TY  - JOUR
T1  - Design and architecture of an interactive eTextbook – The OpenDSA system
JO  - Science of Computer Programming
VL  - 88
IS  - 
SP  - 22
EP  - 40
PY  - 2014/8/1/
T2  - Software Development Concerns in the e-Learning Domain
AU  - Fouh, Eric
AU  - Karavirta, Ville
AU  - Breakiron, Daniel A.
AU  - Hamouda, Sally
AU  - Hall, Simin
AU  - Naps, Thomas L.
AU  - Shaffer, Clifford A.
SN  - 0167-6423
DO  - http://dx.doi.org/10.1016/j.scico.2013.11.040
UR  - http://www.sciencedirect.com/science/article/pii/S016764231300333X
KW  - eLearning
KW  - eTextbook
KW  - Automated assessment
KW  - Algorithm visualization
KW  - Data structures and algorithms
AB  - Abstract
The OpenDSA Project seeks to provide complete instructional materials for data structures and algorithms (DSA) courses. Our vision for a highly interactive eTextbook involves the use of many algorithm visualizations (AVs) and a wide range of interactive exercises with automated assessment. To realize this vision we require a mix of third-party and custom software components that make up a client/server-based web application. The massive amount content development required compels us to adopt an appropriate mix of open-source practices that will encourage broad contribution to the project. In this paper we describe the OpenDSA system architecture and the design goals that led to the present version of the system.
ER  - 

TY  - JOUR
T1  - Usability evaluation methods for the web: A systematic mapping study
JO  - Information and Software Technology
VL  - 53
IS  - 8
SP  - 789
EP  - 817
PY  - 2011/8//
T2  - Advances in functional size measurement and effort estimation - Extended best papers
AU  - Fernandez, Adrian
AU  - Insfran, Emilio
AU  - Abrahão, Silvia
SN  - 0950-5849
DO  - http://dx.doi.org/10.1016/j.infsof.2011.02.007
UR  - http://www.sciencedirect.com/science/article/pii/S0950584911000607
KW  - Usability evaluation methods
KW  - Web development
KW  - Systematic mapping
AB  - Context
In recent years, many usability evaluation methods (UEMs) have been employed to evaluate Web applications. However, many of these applications still do not meet most customers’ usability expectations and many companies have folded as a result of not considering Web usability issues. No studies currently exist with regard to either the use of usability evaluation methods for the Web or the benefits they bring.
Objective
The objective of this paper is to summarize the current knowledge that is available as regards the usability evaluation methods (UEMs) that have been employed to evaluate Web applications over the last 14 years.
Method
A systematic mapping study was performed to assess the UEMs that have been used by researchers to evaluate Web applications and their relation to the Web development process. Systematic mapping studies are useful for categorizing and summarizing the existing information concerning a research question in an unbiased manner.
Results
The results show that around 39% of the papers reviewed reported the use of evaluation methods that had been specifically crafted for the Web. The results also show that the type of method most widely used was that of User Testing. The results identify several research gaps, such as the fact that around 90% of the studies applied evaluations during the implementation phase of the Web application development, which is the most costly phase in which to perform changes. A list of the UEMs that were found is also provided in order to guide novice usability practitioners.
Conclusions
From an initial set of 2703 papers, a total of 206 research papers were selected for the mapping study. The results obtained allowed us to reach conclusions concerning the state-of-the-art of UEMs for evaluating Web applications. This allowed us to identify several research gaps, which subsequently provided us with a framework in which new research activities can be more appropriately positioned, and from which useful information for novice usability practitioners can be extracted.
ER  - 

TY  - JOUR
T1  - Discovering simulation models
JO  - Information Systems
VL  - 34
IS  - 3
SP  - 305
EP  - 327
PY  - 2009/5//
T2  - 
AU  - Rozinat, A.
AU  - Mans, R.S.
AU  - Song, M.
AU  - van der Aalst, W.M.P.
SN  - 0306-4379
DO  - http://dx.doi.org/10.1016/j.is.2008.09.002
UR  - http://www.sciencedirect.com/science/article/pii/S0306437908000690
KW  - Process mining
KW  - Simulation models
KW  - Petri nets
KW  - Colored Petri nets
AB  - Process mining is a tool to extract non-trivial and useful information from process execution logs. These so-called event logs (also called audit trails, or transaction logs) are the starting point for various discovery and analysis techniques that help to gain insight into certain characteristics of the process. In this paper we use a combination of process mining techniques to discover multiple perspectives (namely, the control-flow, data, performance, and resource perspective) of the process from historic data, and we integrate them into a comprehensive simulation model. This simulation model is represented as a colored Petri net (CPN) and can be used to analyze the process, e.g., evaluate the performance of different alternative designs. The discovery of simulation models is explained using a running example. Moreover, the approach has been applied in two case studies; the workflows in two different municipalities in the Netherlands have been analyzed using a combination of process mining and simulation. Furthermore, the quality of the CPN models generated for the running example and the two case studies has been evaluated by comparing the original logs with the logs of the generated models.
ER  - 

TY  - JOUR
T1  - Discovering role-based virtual knowledge flows for organizational knowledge support
JO  - Decision Support Systems
VL  - 55
IS  - 1
SP  - 12
EP  - 30
PY  - 2013/4//
T2  - 
AU  - Liu, Duen-Ren
AU  - Lin, Chih-Wei
AU  - Chen, Hui-Fang
SN  - 0167-9236
DO  - http://dx.doi.org/10.1016/j.dss.2012.11.018
UR  - http://www.sciencedirect.com/science/article/pii/S0167923613000316
KW  - Knowledge flow
KW  - Knowledge flow view
KW  - Knowledge support
KW  - Knowledge management
KW  - Role
KW  - Ontology
AB  - Abstract
In knowledge-intensive work environments, workers need task-relevant knowledge and documents to support the execution of tasks. A knowledge flow (KF) represents an individual's or group's knowledge-needs and referencing behavior of codified knowledge during the performance of organizational tasks. Through knowledge flows, organizations can provide workers with task-relevant knowledge to satisfy their knowledge-needs. In teamwork environments, knowledge workers with different roles and task functions usually have diverse knowledge-needs, but conventional KF models cannot satisfy such needs. In a previous work, we proposed a novel concept and theoretical model called Knowledge Flow View (KFV). Based on workers' diverse knowledge-needs, the KFV model abstracts knowledge nodes of partial KFs and generates virtual knowledge nodes through a knowledge concept generalization procedure. However, the KFV model did not consider the diverse knowledge-needs of workers who play different roles in a team. Therefore, in this work, we propose a role-based KFV model that discovers role-based virtual knowledge flows to satisfy the knowledge-needs of different roles. First, we analyze the level of knowledge required by workers to fulfill various roles. Then, we develop role-based knowledge flow abstraction methods that generate appropriate virtual knowledge nodes to provide sufficient knowledge for each role. The proposed role-based KFV model enhances the efficiency of KF usage, as well as the effectiveness of knowledge sharing and knowledge support in organizations.
ER  - 

TY  - JOUR
T1  - Improving process models by discovering decision points
JO  - Information Systems
VL  - 32
IS  - 7
SP  - 1037
EP  - 1055
PY  - 2007/11//
T2  - Special Issue on Intelligent Information Processing
AU  - Subramaniam, Sharmila
AU  - Kalogeraki, Vana
AU  - Gunopulos, Dimitrios
AU  - Casati, Fabio
AU  - Castellanos, Malu
AU  - Dayal, Umeshwar
AU  - Sayal, Mehmet
SN  - 0306-4379
DO  - http://dx.doi.org/10.1016/j.is.2006.11.001
UR  - http://www.sciencedirect.com/science/article/pii/S0306437906001232
KW  - Workflow graph models
KW  - Classification
KW  - Process mining
AB  - Workflow management systems (WfMS) are widely used by business enterprises as tools for administrating, automating and scheduling the business process activities with the available resources. Since the control flow specifications of workflows are manually designed, they entail assumptions and errors, leading to inaccurate workflow models. Decision points, the XOR nodes in a workflow graph model, determine the path chosen toward completion of any process invocation. In this work, we show that positioning the decision points at their earliest points can improve process efficiency by decreasing their uncertainties and identifying redundant activities. We present novel techniques to discover the earliest positions by analyzing workflow logs and to transform the model graph. The experimental results show that the transformed model is more efficient with respect to its average execution time and uncertainty, when compared to the original model.
ER  - 

TY  - JOUR
T1  - Users’ evaluation of digital libraries (DLs): Their uses, their criteria, and their assessment
JO  - Information Processing & Management
VL  - 44
IS  - 3
SP  - 1346
EP  - 1373
PY  - 2008/5//
T2  - 
AU  - Xie, Hong Iris
SN  - 0306-4573
DO  - http://dx.doi.org/10.1016/j.ipm.2007.10.003
UR  - http://www.sciencedirect.com/science/article/pii/S0306457307002154
KW  - Digital library evaluation
KW  - Digital library evaluation criteria
KW  - User perspectives
KW  - Digital library use
AB  - Millions of dollars have been invested into the development of digital libraries. There are many unanswered questions regarding their evaluation, in particular, from users’ perspectives. This study intends to investigate users’ use, their criteria and their evaluation of the two selected digital libraries. Nineteen subjects were recruited to participate in the study. They were instructed to keep a diary for their use of the two digital libraries, rate the importance of digital library evaluation criteria, and evaluate the two digital libraries by applying their perceived important criteria. The results show patterns of users’ use of digital libraries, their perceived important evaluation criteria, and the positive and negative aspects of digital libraries. Finally, the relationships between perceived importance of digital library evaluation criteria and actual evaluation of digital libraries and the relationships between use of digital libraries and evaluation of digital libraries as well as users’ preference, experience and knowledge structure on digital library evaluation are further discussed.
ER  - 

TY  - JOUR
T1  - InnoDB database forensics: Enhanced reconstruction of data manipulation queries from redo logs
JO  - Information Security Technical Report
VL  - 17
IS  - 4
SP  - 227
EP  - 238
PY  - 2013/5//
T2  - Special Issue: ARES 2012 7th International Conference on Availability, Reliability and Security
AU  - Frühwirt, Peter
AU  - Kieseberg, Peter
AU  - Schrittwieser, Sebastian
AU  - Huber, Markus
AU  - Weippl, Edgar
SN  - 1363-4127
DO  - http://dx.doi.org/10.1016/j.istr.2013.02.003
UR  - http://www.sciencedirect.com/science/article/pii/S1363412713000137
KW  - MySQL
KW  - InnoDB
KW  - Digital forensics
KW  - Databases
KW  - Log files
AB  - Abstract
The InnoDB storage engine is one of the most widely used storage engines for MySQL. This paper discusses possibilities of utilizing the redo logs of InnoDB databases for forensic analysis, as well as the extraction of the information needed from the MySQL definition files, in order to carry out this kind of analysis. Since the redo logs are internal log files of the storage engine and thus cannot easily be changed undetected, this forensic method can be very useful against adversaries with administrator privileges, which could otherwise cover their tracks by manipulating traditional log files intended for audit and control purposes. Based on a prototype implementation, we show methods for recovering Insert, Delete and Update statements issued against a database.
ER  - 

TY  - JOUR
T1  - Workflow simulation for operational decision support
JO  - Data & Knowledge Engineering
VL  - 68
IS  - 9
SP  - 834
EP  - 850
PY  - 2009/9//
T2  - Sixth International Conference on Business Process Management (BPM 2008) – Five selected and extended papers
AU  - Rozinat, A.
AU  - Wynn, M.T.
AU  - van der Aalst, W.M.P.
AU  - ter Hofstede, A.H.M.
AU  - Fidge, C.J.
SN  - 0169-023X
DO  - http://dx.doi.org/10.1016/j.datak.2009.02.014
UR  - http://www.sciencedirect.com/science/article/pii/S0169023X09000354
KW  - Workflow management
KW  - Process mining
KW  - Short-term simulation
AB  - Simulation is widely used as a tool for analyzing business processes but is mostly focused on examining abstract steady-state situations. Such analyses are helpful for the initial design of a business process but are less suitable for operational decision making and continuous improvement. Here we describe a simulation system for operational decision support in the context of workflow management. To do this we exploit not only the workflow’s design, but also use logged data describing the system’s observed historic behavior, and incorporate information extracted about the current state of the workflow. Making use of actual data capturing the current state and historic information allows our simulations to accurately predict potential near-future behaviors for different scenarios. The approach is supported by a practical toolset which combines and extends the workflow management system YAWL and the process mining framework ProM.
ER  - 

TY  - JOUR
T1  - Lost Opportunities
JO  - Computer Fraud & Security
VL  - 2003
IS  - 1
SP  - 4
EP  - 6
PY  - 2003/1//
T2  - 
AU  - Wilding, Edward
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(03)01009-1
UR  - http://www.sciencedirect.com/science/article/pii/S1361372303010091
AB  - A few months ago, a solicitor approached me on behalf of his client. The client suspected that an employee had stolen high value proprietary software, source code, customer databases, methodologies, and marketing and business development plans. The employee had been suspended and placed on what is euphemistically referred to as ‘gardening leave’. He had been ‘gardening’ for six weeks. The solicitor requested advice on how an investigation should proceed in order to prove, or disprove, these suspicions. His client was facing lengthy legal proceedings on the grounds of unfair dismissal. Significantly, neither the solicitor nor his client had any evidence that any information had actually been stolen other than rumours, office gossip and other hearsay.
ER  - 

TY  - JOUR
T1  - File Marshal: Automatic extraction of peer-to-peer data
JO  - Digital Investigation
VL  - 4, Supplement
IS  - 
SP  - 43
EP  - 48
PY  - 2007/9//
T2  - 
AU  - Adelstein, Frank
AU  - Joyce, Robert A.
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2007.06.016
UR  - http://www.sciencedirect.com/science/article/pii/S1742287607000400
KW  - Peer-to-peer
KW  - P2P
KW  - Forensics
KW  - LimeWire
KW  - File sharing
AB  - Digital forensic investigators often find peer-to-peer, or file sharing, software present on the computers, or the images of the disks, that they examine. Investigators must first determine what P2P software is present and where the associated information is stored, retrieve the information from the appropriate directories, and then analyze the results. File Marshal is a tool that will automatically detect and analyze peer-to-peer client use on a disk. The tool automates what is currently a manual and labor intensive process. It will determine what clients currently are or have been installed on a machine, and then extracts per-user usage information, specifically a list of peer servers contacted, and files that were shared and downloaded. The tool was designed to perform its actions in a forensically sound way, including maintaining a detailed audit trail of all actions performed. File Marshal is extensible, using a configuration file to specify details about specific peer-to-peer clients (e.g., location of log files and registry keys indicating installation). This paper describes the general design and features of File Marshal, its current status, and the plans for continued development and release. When complete, File Marshal, a National Institute of Justice funded effort, will be disseminated to law enforcement at no cost.
ER  - 

TY  - JOUR
T1  - A novel data replication mechanism in P2P VoD system
JO  - Future Generation Computer Systems
VL  - 28
IS  - 6
SP  - 930
EP  - 939
PY  - 2012/6//
T2  - Including Special sections SS: Volunteer Computing and Desktop Grids and SS: Mobile Ubiquitous Computing
AU  - Liao, Xiaofei
AU  - Jin, Hai
AU  - Yu, Linchen
SN  - 0167-739X
DO  - http://dx.doi.org/10.1016/j.future.2011.10.006
UR  - http://www.sciencedirect.com/science/article/pii/S0167739X11001919
KW  - Video-on-demand
KW  - Peer-to-peer
KW  - Peer churn
KW  - Data replication
AB  - With the development of the Internet, high-quality streaming services, including Video-on-Demand, are more popular than ever with the help of P2P technologies. But peer-to-peer (P2P) on-demand streaming systems inevitably suffer from peer churn that is the inherent dynamic characteristic of overlay networks. With frequent peer departure and VCR operations, a large amount of media data cached on peer disks turn off-line and unavailable, which becomes the major reason of heavy server load. And the phenomenon has been proved by the system logs of self-developed P2P based Video-on-Demand platform, called GridCast. To address the above issues, a new proactive data replication mechanism is proposed and implemented into GirdCast. Based on the new mechanism, a peer can proactively replicate data chunks to stable cache servers for future sharing, when it has high possibility to leave the overlay. Two key heuristic algorithms are designed for departure prediction and replicating chunks selection. And the cache servers managements are also described in the submission. Trace driven simulations show that the mechanisms greatly decrease bandwidth load of media source server and improve the availability of chunks highly demanded but poorly provisioned by overlay peers.
ER  - 

TY  - JOUR
T1  - Anatomy of an advanced persistent threat
JO  - Network Security
VL  - 2015
IS  - 4
SP  - 13
EP  - 16
PY  - 2015/4//
T2  - 
AU  - Auty, Mike
SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/S1353-4858(15)30028-3
UR  - http://www.sciencedirect.com/science/article/pii/S1353485815300283
AB  - Often when a business is the subject of a hacking attack, emotions run high. The knee jerk reaction is one that comes from an emotional place – ‘how could someone do this to me?’ or ‘who is responsible?’. And ultimately, the organisation wants the infiltrator out. However, once the background to nation-state hacking is understood and how these types of attacks operate, what's needed is a change in the mind-set about how we protect IT to a reasoned and rational approach that sees attacks as part and parcel of doing business.

The rise in well-resourced and sustained cyber-attacks, which are often the work of nation-state actors, calls for a new mind-set when it comes to protecting your organisation.

It's important not to have a knee-jerk reaction. Sometimes it's better not to rid the network of the malware and the attackers straight away, but to monitor and analyse what they are doing. Mike Auty of MWR InfoSecurity explains the best approach with the help of two case studies.
ER  - 

TY  - JOUR
T1  - Keyword index
JO  - Computers & Geosciences
VL  - 18
IS  - 10
SP  - 1413
EP  - 1500
PY  - 1992/12//
T2  - 

SN  - 0098-3004
DO  - http://dx.doi.org/10.1016/0098-3004(92)90033-N
UR  - http://www.sciencedirect.com/science/article/pii/009830049290033N
ER  - 

TY  - JOUR
T1  - Advanced evidence collection and analysis of web browser activity
JO  - Digital Investigation
VL  - 8, Supplement
IS  - 
SP  - S62
EP  - S70
PY  - 2011/8//
T2  - The Proceedings of the Eleventh Annual DFRWS Conference11th Annual Digital Forensics Research Conference
AU  - Oh, Junghoon
AU  - Lee, Seungbong
AU  - Lee, Sangjin
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2011.05.008
UR  - http://www.sciencedirect.com/science/article/pii/S1742287611000326
KW  - Web browser forensics
KW  - Integrated timeline analysis
KW  - Search word analysis
KW  - Restoration of deleted web browser information
KW  - URL decoding
AB  - A Web browser is an essential application program for accessing the Internet. If a suspect uses the Internet as a source of information, the evidence related to the crime would be saved in the log file of the Web browser. Therefore, investigating the Web browser’s log file can help to collect information relevant to the case. After considering existing research and tools, this paper suggests a new evidence collection and analysis methodology and tool to aid this process.
ER  - 


