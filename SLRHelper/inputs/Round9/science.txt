TY  - JOUR
T1  - Cloud forensics: Technical challenges, solutions and comparative analysis
JO  - Digital Investigation
VL  - 13
IS  - 
SP  - 38
EP  - 57
PY  - 2015/6//
T2  - 
AU  - Pichan, Ameer
AU  - Lazarescu, Mihai
AU  - Soh, Sie Teng
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2015.03.002
UR  - http://www.sciencedirect.com/science/article/pii/S1742287615000407
KW  - Cloud computing
KW  - Cloud forensics
KW  - Cloud service provider
KW  - Cloud customer
KW  - Digital forensics
KW  - Digital evidence
KW  - Service level agreement
KW  - Amazon EC2
AB  - Abstract
Cloud computing is arguably one of the most significant advances in information technology (IT) services today. Several cloud service providers (CSPs) have offered services that have produced various transformative changes in computing activities and presented numerous promising technological and economic opportunities. However, many cloud customers remain reluctant to move their IT needs to the cloud, mainly due to their concerns on cloud security and the threat of the unknown. The CSPs indirectly escalate their concerns by not letting customers see what is behind virtual wall of their clouds that, among others, hinders digital investigations. In addition, jurisdiction, data duplication and multi-tenancy in cloud platform add to the challenge of locating, identifying and separating the suspected or compromised targets for digital forensics. Unfortunately, the existing approaches to evidence collection and recovery in a non-cloud (traditional) system are not practical as they rely on unrestricted access to the relevant system and user data; something that is not available in the cloud due its decentralized data processing. In this paper we systematically survey the forensic challenges in cloud computing and analyze their most recent solutions and developments. In particular, unlike the existing surveys on the topic, we describe the issues in cloud computing using the phases of traditional digital forensics as the base. For each phase of the digital forensic process, we have included a list of challenges and analysis of their possible solutions. Our description helps identifying the differences between the problems and solutions for non-cloud and cloud digital forensics. Further, the presentation is expected to help the investigators better understand the problems in cloud environment. More importantly, the paper also includes most recent development in cloud forensics produced by researchers, National Institute of Standards and Technology and Amazon.
ER  - 

TY  - JOUR
T1  - Ideal log setting for database forensics reconstruction
JO  - Digital Investigation
VL  - 12
IS  - 
SP  - 27
EP  - 40
PY  - 2015/3//
T2  - 
AU  - Adedayo, Oluwasola Mary
AU  - Olivier, Martin S.
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2014.12.002
UR  - http://www.sciencedirect.com/science/article/pii/S1742287614001200
KW  - Database management system
KW  - Database forensics
KW  - Digital forensics
KW  - Reconstruction
KW  - Ideal log setting
AB  - Abstract
The ability to reconstruct the data stored in a database at an earlier time is an important aspect of database forensics. Past research shows that the log file in a database can be useful for reconstruction. However, in many database systems there are various options that control which information is included in the logs. This paper introduces the notion of the ideal log setting necessary for an effective reconstruction process in database forensics. The paper provides a survey of the default logging preferences in some of the popular database management systems and identifies the information that a database log should contain in order to be useful for reconstruction. The challenges that may be encountered in storing the information as well as ways of overcoming the challenges are discussed. Possible logging preferences that may be considered as the ideal log setting for the popular database systems are also proposed. In addition, the paper relates the identified requirements to the three dimensions of reconstruction in database forensics and points out the additional requirements and/or techniques that may be required in the different dimensions.
ER  - 

TY  - JOUR
T1  - Hviz: HTTP(S) traffic aggregation and visualization for network forensics
JO  - Digital Investigation
VL  - 12, Supplement 1
IS  - 
SP  - S1
EP  - S11
PY  - 2015/3//
T2  - DFRWS 2015 EuropeProceedings of the Second Annual DFRWS Europe
AU  - Gugelmann, David
AU  - Gasser, Fabian
AU  - Ager, Bernhard
AU  - Lenders, Vincent
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2015.01.005
UR  - http://www.sciencedirect.com/science/article/pii/S1742287615000067
KW  - Network forensics
KW  - HTTP(S)
KW  - Event reconstruction
KW  - Aggregation
KW  - Visualization
KW  - Incident investigation
AB  - Abstract
HTTP and HTTPS traffic recorded at the perimeter of an organization is an exhaustive data source for the forensic investigation of security incidents. However, due to the nested nature of today's Web page structures, it is a huge manual effort to tell apart benign traffic caused by regular user browsing from malicious traffic that relates to malware or insider threats. We present Hviz, an interactive visualization approach to represent the event timeline of HTTP and HTTPS activities of a workstation in a comprehensible manner. Hviz facilitates incident investigation by structuring, aggregating, and correlating HTTP events between workstations in order to reduce the number of events that are exposed to an investigator while preserving the big picture. We have implemented a prototype system and have used it to evaluate its utility using synthetic and real-world HTTP traces from a campus network. Our results show that Hviz is able to significantly reduce the number of user browsing events that need to be exposed to an investigator by distilling the structural properties of HTTP traffic, thus simplifying the examination of malicious activities that arise from malware traffic or insider threats.
ER  - 

TY  - JOUR
T1  - Control systems/SCADA forensics, what's the difference?
JO  - Digital Investigation
VL  - 11
IS  - 3
SP  - 160
EP  - 174
PY  - 2014/9//
T2  - Special Issue: Embedded Forensics
AU  - van der Knijff, R.M.
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2014.06.007
UR  - http://www.sciencedirect.com/science/article/pii/S1742287614000814
KW  - Forensics
KW  - Control systems
KW  - SCADA
KW  - ICS
KW  - Cyber security
AB  - Abstract
Immature IT security, increasing network connectivity and unwavering media attention is causing an increase in the number of control system cyber security incidents. For forensic examinations in these environments, knowledge and skills are needed in the field of hardware, networks and data analysis. For forensic examiners, this paper is meant to be a crash course on control systems and their forensic opportunities, focussing on the differences compared to regular IT systems. Assistance from experienced field engineers during forensic acquisition of control systems seems inevitable in order to guarantee process safety, business continuity and examination efficiency. For people working in the control system community, this paper may be helpful to get an idea about specific forensic issues about which they would normally not bother, but may be crucial as soon as their systems are under attack or become part of a law enforcement investigation. For analysis of acquired data, existing tools for network security monitoring have useful functionality for forensic applications but are designed for real-time acquisition and often not directly usable for post-mortem analysis of acquired data in a forensically sound way. The constant and predictable way in which control systems normally behave makes forensic application of anomaly-based threat detection an interesting topic for further research.
ER  - 

TY  - JOUR
T1  - Distributed filesystem forensics: XtreemFS as a case study
JO  - Digital Investigation
VL  - 11
IS  - 4
SP  - 295
EP  - 313
PY  - 2014/12//
T2  - 
AU  - Martini, Ben
AU  - Choo, Kim-Kwang Raymond
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2014.08.002
UR  - http://www.sciencedirect.com/science/article/pii/S1742287614000942
KW  - Big data
KW  - Digital forensics
KW  - Distributed filesystem
KW  - Infrastructure as a Service (IaaS)
KW  - Storage as a Service (StaaS)
KW  - Distributed filesystem forensics
KW  - Cloud storage forensics
AB  - Abstract
Distributed filesystems provide a cost-effective means of storing high-volume, velocity and variety information in cloud computing, big data and other contemporary systems. These technologies have the potential to be exploited for illegal purposes, which highlights the need for digital forensic investigations. However, there have been few papers published in the area of distributed filesystem forensics. In this paper, we aim to address this gap in knowledge. Using our previously published cloud forensic framework as the underlying basis, we conduct an in-depth forensic experiment on XtreemFS, a Contrail EU-funded project, as a case study for distributed filesystem forensics. We discuss the technical and process issues regarding collection of evidential data from distributed filesystems, particularly when used in cloud computing environments. A number of digital forensic artefacts are also discussed. We then propose a process for the collection of evidential data from distributed filesystems.
ER  - 

TY  - JOUR
T1  - Cloud storage forensics: ownCloud as a case study
JO  - Digital Investigation
VL  - 10
IS  - 4
SP  - 287
EP  - 299
PY  - 2013/12//
T2  - 
AU  - Martini, Ben
AU  - Choo, Kim-Kwang Raymond
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2013.08.005
UR  - http://www.sciencedirect.com/science/article/pii/S1742287613000911
KW  - Digital forensics
KW  - Private cloud
KW  - SaaS
KW  - Storage as a service
KW  - StaaS
KW  - Cloud forensics
KW  - Cloud storage forensics
KW  - Open source cloud
AB  - Abstract
The storage as a service (StaaS) cloud computing architecture is showing significant growth as users adopt the capability to store data in the cloud environment across a range of devices. Cloud (storage) forensics has recently emerged as a salient area of inquiry. Using a widely used open source cloud StaaS application – ownCloud – as a case study, we document a series of digital forensic experiments with the aim of providing forensic researchers and practitioners with an in-depth understanding of the artefacts required to undertake cloud storage forensics. Our experiments focus upon client and server artefacts, which are categories of potential evidential data specified before commencement of the experiments. A number of digital forensic artefacts are found as part of these experiments and are used to support the selection of artefact categories and provide a technical summary to practitioners of artefact types. Finally we provide some general guidelines for future forensic analysis on open source StaaS products and recommendations for future work.
ER  - 

TY  - JOUR
T1  - A scalable network forensics mechanism for stealthy self-propagating attacks
JO  - Computer Communications
VL  - 36
IS  - 13
SP  - 1471
EP  - 1484
PY  - 2013/7/15/
T2  - 
AU  - Chen, Li Ming
AU  - Chen, Meng Chang
AU  - Liao, Wanjiun
AU  - Sun, Yeali S.
SN  - 0140-3664
DO  - http://dx.doi.org/10.1016/j.comcom.2013.05.005
UR  - http://www.sciencedirect.com/science/article/pii/S0140366413001254
KW  - Network forensics
KW  - Data reduction
KW  - Stealthy self-propagating attack
KW  - Contact activity
AB  - Abstract
Network forensics supports capabilities such as attacker identification and attack reconstruction, which complement the traditional intrusion detection and perimeter defense techniques in building a robust security mechanism. Attacker identification pinpoints attack origin to deter future attackers, while attack reconstruction reveals attack causality and network vulnerabilities. In this paper, we discuss the problem and feasibility of back tracking the origin of a self-propagating stealth attack when given a network traffic trace for a sufficiently long period of time. We propose a network forensics mechanism that is scalable in computation time and space while maintaining high accuracy in the identification of the attack origin. We further develop a data reduction method to filter out attack-irrelevant data and only retain evidence relevant to potential attacks for a post-mortem investigation. Using real-world trace driven experiments, we evaluate the performance of the proposed mechanism and show that we can trim down up to 97% of attack-irrelevant network traffic and successfully identify attack origin.
ER  - 

TY  - JOUR
T1  - InnoDB database forensics: Enhanced reconstruction of data manipulation queries from redo logs
JO  - Information Security Technical Report
VL  - 17
IS  - 4
SP  - 227
EP  - 238
PY  - 2013/5//
T2  - Special Issue: ARES 2012 7th International Conference on Availability, Reliability and Security
AU  - Frühwirt, Peter
AU  - Kieseberg, Peter
AU  - Schrittwieser, Sebastian
AU  - Huber, Markus
AU  - Weippl, Edgar
SN  - 1363-4127
DO  - http://dx.doi.org/10.1016/j.istr.2013.02.003
UR  - http://www.sciencedirect.com/science/article/pii/S1363412713000137
KW  - MySQL
KW  - InnoDB
KW  - Digital forensics
KW  - Databases
KW  - Log files
AB  - Abstract
The InnoDB storage engine is one of the most widely used storage engines for MySQL. This paper discusses possibilities of utilizing the redo logs of InnoDB databases for forensic analysis, as well as the extraction of the information needed from the MySQL definition files, in order to carry out this kind of analysis. Since the redo logs are internal log files of the storage engine and thus cannot easily be changed undetected, this forensic method can be very useful against adversaries with administrator privileges, which could otherwise cover their tracks by manipulating traditional log files intended for audit and control purposes. Based on a prototype implementation, we show methods for recovering Insert, Delete and Update statements issued against a database.
ER  - 

TY  - JOUR
T1  - AlmaNebula: A Computer Forensics Framework for the Cloud
JO  - Procedia Computer Science
VL  - 19
IS  - 
SP  - 139
EP  - 146
PY  - 2013///
T2  - The 4th International Conference on Ambient Systems, Networks and Technologies (ANT 2013), the 3rd International Conference on Sustainable Energy Information Technology (SEIT-2013)
AU  - Federici, Corrado
SN  - 1877-0509
DO  - http://dx.doi.org/10.1016/j.procs.2013.06.023
UR  - http://www.sciencedirect.com/science/article/pii/S1877050913006315
KW  - Forensics as a service
KW  - Computer forensics framework
KW  - Commodity computing
KW  - Big data
KW  - Web scale
KW  - Distributed processing
AB  - Abstract
Scalability, fault tolerance and collaborative processing across possibly dispersed sites are key enablers of modern computer forensics applications, that must be able to elastically accommodate all kinds of digital investigations, without wasting resources or fail to deliver timely outcomes. Traditional tools running in a standalone or client- server setups may fall short when handling the multi terabyte scale of a case just above average or, conversely, lie mainly underutilized when dealing with few digital evidences. A new category of applications that leverage the opportunities offered by modern Cloud Computing (CC) platforms, where scalable computational power and storage capacity can be engaged and decommissioned on demand, allow one to conveniently master huge amounts of information that otherwise could be impossible to wield. This paper discusses the design goals, technical requirements and architecture of AlmaNebula, a conceptual framework for the analysis of digital evidences built on top of a Cloud infrastructure, which aims to embody the concept of “Forensics as a service”.
ER  - 

TY  - JOUR
T1  - A triage framework for digital forensics
JO  - Computer Fraud & Security
VL  - 2015
IS  - 3
SP  - 8
EP  - 18
PY  - 2015/3//
T2  - 
AU  - Bashir, Muhammad Shamraiz
AU  - Khan, Muhammad Naeem Ahmed
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(15)30018-X
UR  - http://www.sciencedirect.com/science/article/pii/S136137231530018X
AB  - A sharp increase in malware and cyber-attacks has been observed in recent years. Analysing cyber-attacks on the affected digital devices falls under the purview of digital forensics. The Internet is the main source of cyber and malware attacks, which sometimes result in serious damage to the digital assets. The motive behind digital crimes varies – such as online banking fraud, information stealing, denial of services, security breaches, deceptive output of running programs and data distortion.

Digital forensics analysts use a variety of tools for data acquisition, evidence analysis and presentation of malicious activities. This leads to device diversity posing serious challenges for investigators.

For this reason, some attack scenarios have to be examined repeatedly, which entails tremendous effort on the part of the examiners when analysing the evidence. To counter this problem, Muhammad Shamraiz Bashir and Muhammad Naeem Ahmed Khan at the Shaheed Zulfikar Ali Bhutto Institute of Science and Technology, Islamabad, Pakistan propose a novel triage framework for digital forensics.
ER  - 

TY  - JOUR
T1  - Leveraging CybOX™ to standardize representation and exchange of digital forensic information
JO  - Digital Investigation
VL  - 12, Supplement 1
IS  - 
SP  - S102
EP  - S110
PY  - 2015/3//
T2  - DFRWS 2015 EuropeProceedings of the Second Annual DFRWS Europe
AU  - Casey, Eoghan
AU  - Back, Greg
AU  - Barnum, Sean
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2015.01.014
UR  - http://www.sciencedirect.com/science/article/pii/S1742287615000158
KW  - Digital forensics
KW  - Standard representation
KW  - Digital forensic ontology
KW  - Digital forensic XML
KW  - CybOX
KW  - DFXML
KW  - DFAX
AB  - Abstract
With the growing number of digital forensic tools and the increasing use of digital forensics in various contexts, including incident response and cyber threat intelligence, there is a pressing need for a widely accepted standard for representing and exchanging digital forensic information. Such a standard representation can support correlation between different data sources, enabling more effective and efficient querying and analysis of digital evidence. This work summarizes the strengths and weaknesses of existing schemas, and proposes the open-source CybOX schema as a foundation for storing and sharing digital forensic information. The suitability of CybOX for representing objects and relationships that are common in forensic investigations is demonstrated with examples involving digital evidence. The capability to represent provenance by leveraging CybOX is also demonstrated, including specifics of the tool used to process digital evidence and the resulting output. An example is provided of an ongoing project that uses CybOX to record the state of a system before and after an event in order to capture cause and effect information that can be useful for digital forensics. An additional open-source schema and associated ontology called Digital Forensic Analysis eXpression (DFAX) is proposed that provides a layer of domain specific information overlaid on CybOX. DFAX extends the capability of CybOX to represent more abstract forensic-relevant actions, including actions performed by subjects and by forensic examiners, which can be useful for sharing knowledge and supporting more advanced forensic analysis. DFAX can be used in combination with other existing schemas for representing identity information (CIQ), and location information (KML). This work also introduces and leverages initial steps of a Unified Cyber Ontology (UCO) effort to abstract and express concepts/constructs that are common across the cyber domain.
ER  - 

TY  - JOUR
T1  - Network forensics based on fuzzy logic and expert system
JO  - Computer Communications
VL  - 32
IS  - 17
SP  - 1881
EP  - 1892
PY  - 2009/11/15/
T2  - 
AU  - Liao, Niandong
AU  - Tian, Shengfeng
AU  - Wang, Tinghua
SN  - 0140-3664
DO  - http://dx.doi.org/10.1016/j.comcom.2009.07.013
UR  - http://www.sciencedirect.com/science/article/pii/S0140366409002060
KW  - Network forensics
KW  - Expert system
KW  - Fuzzy logic
KW  - Intrusion detection system
KW  - Vulnerability scanning
AB  - Network forensics is a research area that finds the malicious users by collecting and analyzing the intrusion or infringement evidence of computer crimes such as hacking. In the past, network forensics was only used by means of investigation. However, nowadays, due to the sharp increase of network traffic, not all the information captured or recorded will be useful for analysis or evidence. The existing methods and tools for network forensics show only simple results. The administrators have difficulty in analyzing the state of the damaged system without expert knowledge. Therefore, we need an effective and automated analyzing system for network forensics. In this paper, we firstly guarantee the evidence reliability as far as possible by collecting different forensic information of detection sensors. Secondly, we propose an approach based on fuzzy logic and expert system for network forensics that can analyze computer crimes in network environment and make digital evidences automatically. At the end of the paper, the experimental comparison results between our proposed method and other popular methods are presented. Experimental results show that the system can classify most kinds of attack types (91.5% correct classification rate on average) and provide analyzable and comprehensible information for forensic experts.
ER  - 

TY  - JOUR
T1  - Towards a forensic-aware database solution: Using a secured database replication protocol and transaction management for digital investigations
JO  - Digital Investigation
VL  - 11
IS  - 4
SP  - 336
EP  - 348
PY  - 2014/12//
T2  - 
AU  - Frühwirt, Peter
AU  - Kieseberg, Peter
AU  - Krombholz, Katharina
AU  - Weippl, Edgar
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2014.09.003
UR  - http://www.sciencedirect.com/science/article/pii/S1742287614001078
KW  - MySQL
KW  - InnoDB
KW  - Digital forensics
KW  - Databases
KW  - Data tempering
KW  - Replication
KW  - Transaction management
AB  - Abstract
Databases contain an enormous amount of structured data. While the use of forensic analysis on the file system level for creating (partial) timelines, recovering deleted data and revealing concealed activities is very popular and multiple forensic toolsets exist, the systematic analysis of database management systems has only recently begun. Databases contain a large amount of temporary data files and metadata which are used by internal mechanisms. These data structures are maintained in order to ensure transaction authenticity, to perform rollbacks, or to set back the database to a predefined earlier state in case of e.g. an inconsistent state or a hardware failure. However, these data structures are intended to be used by the internal system methods only and are in general not human-readable.

In this work we present a novel approach for a forensic-aware database management system using transaction- and replication sources. We use these internal data structures as a vital baseline to reconstruct evidence during a forensic investigation. The overall benefit of our method is that no additional logs (such as administrator logs) are needed. Furthermore, our approach is invariant to retroactive malicious modifications by an attacker. This assures the authenticity of the evidence and strengthens the chain of custody. To evaluate our approach, we present a formal description, a prototype implementation in MySQL alongside and a comprehensive security evaluation with respect to the most relevant attack scenarios.
ER  - 

TY  - JOUR
T1  - MEGA: A tool for Mac OS X operating system and application forensics
JO  - Digital Investigation
VL  - 5, Supplement
IS  - 
SP  - S83
EP  - S90
PY  - 2008/9//
T2  - The Proceedings of the Eighth Annual DFRWS Conference
AU  - Joyce, Robert A.
AU  - Powers, Judson
AU  - Adelstein, Frank
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2008.05.011
UR  - http://www.sciencedirect.com/science/article/pii/S1742287608000376
KW  - Mac OS X
KW  - Computer forensics
KW  - Spotlight
KW  - Disk image analysis
KW  - Application analysis
AB  - Computer forensic tools for Apple Mac hardware have traditionally focused on low-level file system details. Mac OS X and common applications on the Mac platform provide an abundance of information about the user's activities in configuration files, caches, and logs. We are developing MEGA, an extensible tool suite for the analysis of files on Mac OS X disk images. MEGA provides simple access to Spotlight metadata maintained by the operating system, yielding efficient file content search and exposing metadata such as digital camera make and model. It can also help investigators to assess FileVault encrypted home directories. MEGA support tools are under development to interpret files written by common Mac OS applications such as Safari, Mail, and iTunes.
ER  - 

TY  - JOUR
T1  - FORZA – Digital forensics investigation framework that incorporate legal issues
JO  - Digital Investigation
VL  - 3, Supplement
IS  - 
SP  - 29
EP  - 36
PY  - 2006/9//
T2  - The Proceedings of the 6th Annual Digital Forensic Research Workshop (DFRWS '06)
AU  - Ieong, Ricci S.C.
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2006.06.004
UR  - http://www.sciencedirect.com/science/article/pii/S1742287606000661
KW  - Digital forensics investigation framework
KW  - Digital forensics
KW  - FORZA framework
KW  - Forensics principles
KW  - Zachman framework
KW  - Legal aspects
AB  - What is Digital Forensics? Mark Pollitt highlighted in DFRWS 2004 [Politt MM. Six blind men from Indostan. Digital forensics research workshop (DFRWS); 2004] that digital forensics is not an elephant, it is a process and not just one process, but a group of tasks and processes in investigation. In fact, many digital forensics investigation processes and tasks were defined on technical implementation details Investigation procedures developed by traditional forensics scientist focused on the procedures in handling the evidence, while those developed by the technologist focused on the technical details in capturing evidence. As a result, many digital forensics practitioners simply followed technical procedures and forget about the actual purpose and core concept of digital forensics investigation.

With all these technical details and complicated procedures, legal practitioners may have difficulties in applying or even understanding their processes and tasks in digital forensics investigations.

In order to break the technical barrier between information technologists, legal practitioners and investigators, and their corresponding tasks together, a technical-independent framework would be required.

In this paper, we first highlighted the fundamental principle of digital forensics investigations (Reconnaissance, Reliability and Relevancy). Based on this principle, we re-visit the investigation tasks and outlined eight different roles and their responsibilities in a digital forensics investigation.

For each role, we defined the sets of six key questions. They are the What (the data attributes), Why (the motivation), How (the procedures), Who (the people), Where (the location) and When (the time) questions. In fact, among all the investigation processes, there are six main questions that each practitioner would always ask.

By incorporating these sets of six questions into the Zachman's framework, a digital forensics investigation framework – FORZA is composed. We will further explain how this new framework can incorporate legal advisors and prosecutors into a bigger picture of digital forensics investigation framework.

Usability of this framework will be illustrated in a web hacking example.

Finally, the road map that interconnects the framework to automatically zero-knowledge data acquisition tools will be briefly described.
ER  - 

TY  - JOUR
T1  - BitTorrent Sync: First Impressions and Digital Forensic Implications
JO  - Digital Investigation
VL  - 11, Supplement 1
IS  - 
SP  - S77
EP  - S86
PY  - 2014/5//
T2  - Proceedings of the First Annual DFRWS Europe
AU  - Farina, Jason
AU  - Scanlon, Mark
AU  - Kechadi, M-Tahar
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2014.03.010
UR  - http://www.sciencedirect.com/science/article/pii/S1742287614000152
KW  - BitTorrent
KW  - Sync
KW  - Peer-to-Peer
KW  - Synchronisation
KW  - Privacy
KW  - Digital forensics
AB  - Abstract
With professional and home Internet users becoming increasingly concerned with data protection and privacy, the privacy afforded by popular cloud file synchronisation services, such as Dropbox, OneDrive and Google Drive, is coming under scrutiny in the press. A number of these services have recently been reported as sharing information with governmental security agencies without warrants. BitTorrent Sync is seen as an alternative by many and has gathered over two million users by December 2013 (doubling since the previous month). The service is completely decentralised, offers much of the same synchronisation functionality of cloud powered services and utilises encryption for data transmission (and optionally for remote storage). The importance of understanding BitTorrent Sync and its resulting digital investigative implications for law enforcement and forensic investigators will be paramount to future investigations. This paper outlines the client application, its detected network traffic and identifies artefacts that may be of value as evidence for future digital investigations.
ER  - 

TY  - JOUR
T1  - On the role of file system metadata in digital forensics
JO  - Digital Investigation
VL  - 1
IS  - 4
SP  - 298
EP  - 309
PY  - 2004/12//
T2  - 
AU  - Buchholz, Florian
AU  - Spafford, Eugene
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2004.10.002
UR  - http://www.sciencedirect.com/science/article/pii/S1742287604000829
KW  - Computer forensics
KW  - Digital forensics
KW  - Audit data
KW  - File systems
KW  - Intrusion analysis
AB  - Most of the effort in today's digital forensics community lies in the retrieval and analysis of existing information from computing systems. Little is being done to increase the quantity and quality of the forensic information on today's computing systems. In this paper we pose the question of what kind of information is desired on a system by a forensic investigator. We give an overview of the information that exists on current systems and discuss its shortcomings. We then examine the role that file system metadata play in digital forensics and analyze what kind of information is desirable for different types of forensic investigations, how feasible it is to obtain it, and discuss issues about storing the information.
ER  - 

TY  - JOUR
T1  - Acquiring forensic evidence from infrastructure-as-a-service cloud computing: Exploring and evaluating tools, trust, and techniques
JO  - Digital Investigation
VL  - 9, Supplement
IS  - 
SP  - S90
EP  - S98
PY  - 2012/8//
T2  - 12th Annual Digital Forensics Research Conference
AU  - Dykstra, Josiah
AU  - Sherman, Alan T.
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2012.05.001
UR  - http://www.sciencedirect.com/science/article/pii/S1742287612000266
KW  - Computer security
KW  - Cloud computing
KW  - Digital forensics
KW  - Cloud forensics
KW  - EnCase
KW  - FTK
KW  - Amazon EC2
AB  - We expose and explore technical and trust issues that arise in acquiring forensic evidence from infrastructure-as-a-service cloud computing and analyze some strategies for addressing these challenges. First, we create a model to show the layers of trust required in the cloud. Second, we present the overarching context for a cloud forensic exam and analyze choices available to an examiner. Third, we provide for the first time an evaluation of popular forensic acquisition tools including Guidance EnCase and AccesData Forensic Toolkit, and show that they can successfully return volatile and non-volatile data from the cloud. We explain, however, that with those techniques judge and jury must accept a great deal of trust in the authenticity and integrity of the data from many layers of the cloud model. In addition, we explore four other solutions for acquisition—Trusted Platform Modules, the management plane, forensics-as-a-service, and legal solutions, which assume less trust but require more cooperation from the cloud service provider. Our work lays a foundation for future development of new acquisition methods for the cloud that will be trustworthy and forensically sound. Our work also helps forensic examiners, law enforcement, and the court evaluate confidence in evidence from the cloud.
ER  - 

TY  - JOUR
T1  - Automated computer forensics training in a virtualized environment
JO  - Digital Investigation
VL  - 5, Supplement
IS  - 
SP  - S105
EP  - S111
PY  - 2008/9//
T2  - The Proceedings of the Eighth Annual DFRWS Conference
AU  - Brueckner, Stephen
AU  - Guaspari, David
AU  - Adelstein, Frank
AU  - Weeks, Joseph
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2008.05.009
UR  - http://www.sciencedirect.com/science/article/pii/S1742287608000406
KW  - Digital forensic training
KW  - Computer training
KW  - Virtualized training
KW  - Automated assessment
KW  - Automated evaluation
AB  - The CYber DEfenSe Trainer (CYDEST) is a virtualized training platform for network defense and computer forensics. It uses virtual machines to provide tactical level exercises for personnel such as network administrators, first responders, and digital forensics investigators. CYDEST incorporates a number of features to reduce instructor workload and to improve training realism, including: (1) automated assessment of trainee performance, (2) automated attacks that respond dynamically to the student's actions, (3) a full fidelity training environment, (4) an unrestricted user interface incorporating real tools, and (5) continuous, remote accessibility via the Web.
ER  - 

TY  - JOUR
T1  - Domain name forensics: a systematic approach to investigating an internet presence
JO  - Digital Investigation
VL  - 1
IS  - 4
SP  - 247
EP  - 255
PY  - 2004/12//
T2  - 
AU  - Nikkel, Bruce J.
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2004.10.001
UR  - http://www.sciencedirect.com/science/article/pii/S1742287604000817
KW  - Digital forensics
KW  - Network forensics
KW  - Domain name investigation
KW  - Domain name forensics
KW  - DNS investigation
KW  - Website investigation
AB  - Over the last few years the typical Internet presence has become a crowded outsourcing arrangement of multiple organizations dividing up the complexity of maintaining various parts of an infrastructure. Finding the parties responsible for the different infrastructure areas has become time consuming and error prone. This paper presents a systematic approach to investigating a complex Internet presence, including collecting, time-stamping, packaging, preserving, and presenting evidence. It is geared towards the network forensics practitioner.
ER  - 

TY  - JOUR
T1  - Design and implementation of FROST: Digital forensic tools for the OpenStack cloud computing platform
JO  - Digital Investigation
VL  - 10, Supplement
IS  - 
SP  - S87
EP  - S95
PY  - 2013/8//
T2  - 13th Annual Digital Forensics Research Conference
AU  - Dykstra, Josiah
AU  - Sherman, Alan T.
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2013.06.010
UR  - http://www.sciencedirect.com/science/article/pii/S174228761300056X
KW  - OpenStack
KW  - Cloud computing
KW  - Digital forensics
KW  - Cloud forensics
KW  - FROST
AB  - Abstract
We describe the design, implementation, and evaluation of FROST—three new forensic tools for the OpenStack cloud platform. Our implementation for the OpenStack cloud platform supports an Infrastructure-as-a-Service (IaaS) cloud and provides trustworthy forensic acquisition of virtual disks, API logs, and guest firewall logs. Unlike traditional acquisition tools, FROST works at the cloud management plane rather than interacting with the operating system inside the guest virtual machines, thereby requiring no trust in the guest machine. We assume trust in the cloud provider, but FROST overcomes non-trivial challenges of remote evidence integrity by storing log data in hash trees and returning evidence with cryptographic hashes. Our tools are user-driven, allowing customers, forensic examiners, and law enforcement to conduct investigations without necessitating interaction with the cloud provider. We demonstrate how FROST's new features enable forensic investigators to obtain forensically-sound data from OpenStack clouds independent of provider interaction. Our preliminary evaluation indicates the ability of our approach to scale in a dynamic cloud environment. The design supports an extensible set of forensic objectives, including the future addition of other data preservation, discovery, real-time monitoring, metrics, auditing, and acquisition capabilities.
ER  - 

TY  - JOUR
T1  - An integrated conceptual digital forensic framework for cloud computing
JO  - Digital Investigation
VL  - 9
IS  - 2
SP  - 71
EP  - 80
PY  - 2012/11//
T2  - 
AU  - Martini, Ben
AU  - Choo, Kim-Kwang Raymond
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2012.07.001
UR  - http://www.sciencedirect.com/science/article/pii/S174228761200059X
KW  - Cloud computing
KW  - Cloud forensics
KW  - Digital forensics
KW  - Forensic computing
KW  - Digital evidence
KW  - Computer forensics
AB  - Increasing interest in and use of cloud computing services presents both opportunities for criminal exploitation and challenges for law enforcement agencies (LEAs). For example, it is becoming easier for criminals to store incriminating files in the cloud computing environment but it may be extremely difficult for LEAs to seize these files as the latter could potentially be stored overseas. Two of the most widely used and accepted forensic frameworks – McKemmish (1999) and NIST (Kent et al., 2006) – are then reviewed to identify the required changes to current forensic practices needed to successfully conduct cloud computing investigations. We propose an integrated (iterative) conceptual digital forensic framework (based on McKemmish and NIST), which emphasises the differences in the preservation of forensic data and the collection of cloud computing data for forensic purposes. Cloud computing digital forensic issues are discussed within the context of this framework. Finally suggestions for future research are made to further examine this field and provide a library of digital forensic methodologies for the various cloud platforms and deployment models.
ER  - 

TY  - JOUR
T1  - DigLA – A Digsby log analysis tool to identify forensic artifacts
JO  - Digital Investigation
VL  - 9
IS  - 3–4
SP  - 222
EP  - 234
PY  - 2013/2//
T2  - 
AU  - Yasin, Muhammad
AU  - Abulaish, Muhammad
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2012.11.003
UR  - http://www.sciencedirect.com/science/article/pii/S1742287612000837
KW  - Digital forensics
KW  - Digsby log analysis
KW  - Forensic artifacts
KW  - Insider attack
KW  - RAM analysis
AB  - Since the inception of Web 2.0, instant messaging, e-mailing, and social networking have emerged as cheap and efficient means of communication over the Web. As a result, a number of communication platforms like Digsby have been developed by various research groups to facilitate access to multiple e-mail, instant messaging, and social networking sites using a single credential. Although such platforms are advantageous for end-users, they present new challenges to digital forensic examiners because of their illegitimate use by anti-social elements. To identify digital artifacts from Digsby log data, an examiner is assumed to have knowledge of the whereabouts of Digsby traces before starting an investigation process. This paper proposes a design for a user-friendly GUI-based forensic tool, DigLA, which provides a unified platform for analyzing Digsby log data at different levels of granularity. DigLA is also equipped with password decryption methods for both machine-specific and portable installation versions of Digsby. By considering Windows registry and Digsby log files as dynamic sources of evidence, specifically when Digsby has been used to commit a cyber crime, this paper presents a systematic approach to analyzing Digsby log data. It also presents an approach to analyzing RAM and swap files to collect relevant traces, specifically the login credentials of Digsby and IM users. An expected insider attack from a server security perspective is also studied and discussed in this paper.
ER  - 

TY  - JOUR
T1  - Forensic analysis techniques for fragmented flash memory pages in smartphones
JO  - Digital Investigation
VL  - 9
IS  - 2
SP  - 109
EP  - 118
PY  - 2012/11//
T2  - 
AU  - Park, Jungheum
AU  - Chung, Hyunji
AU  - Lee, Sangjin
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2012.09.003
UR  - http://www.sciencedirect.com/science/article/pii/S1742287612000643
KW  - Digital forensics
KW  - Smartphone forensics
KW  - Flash memory
KW  - Unallocated area
KW  - Fragmented data
AB  - A mobile phone contains important personal information, and therefore, it should be considered in digital forensic investigations. Recently, the number of smartphone owners has increased drastically. Unlike feature phones, smartphones have high-performance operating systems (e.g., Android, iOS), and users can install and utilize various mobile applications on smartphones.

Smartphone forensics has been actively studied because of the importance of smartphone user data acquisition and analysis for digital forensic purposes. In general, there are two logical approaches to smartphone forensics. The first approach is to extract user data using the backup and debugging function of smartphones. The second approach is to get root permission through the rooting or the bootloader method with custom kernel, and acquire an image of the flash memory. In addition, the other way is to acquire an image on a more physical way by using e.g., JTAG or chipoff process. In some cases, it may be possible to reconstruct and analyze the file system. However, existing methods for file system analysis are not suitable for recovering and analyzing data deleted from smartphones depending on the manner in which the flash memory image has to be acquired.

This paper proposes new analysis techniques for fragmented flash memory pages in smartphones. In particular, this paper demonstrates analysis techniques on the image that the reconstruction of file system is impossible because the spare area of flash memory pages does not exist or that it is created from the unallocated area of the undamaged file system.
ER  - 

TY  - JOUR
T1  - The architecture of a digital forensic readiness management system
JO  - Computers & Security
VL  - 32
IS  - 
SP  - 73
EP  - 89
PY  - 2013/2//
T2  - 
AU  - Reddy, K.
AU  - Venter, H.S.
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/j.cose.2012.09.008
UR  - http://www.sciencedirect.com/science/article/pii/S0167404812001447
KW  - Digital forensic readiness
KW  - Management of digital forensic readiness
KW  - Digital forensic management system
KW  - Forensic readiness
KW  - Management of forensics
KW  - Organisational forensic readiness
AB  - A coordinated approach to digital forensic readiness (DFR) in a large organisation requires the management and monitoring of a wide variety of resources, both human and technical. The resources involved in DFR in large organisations typically include staff from multiple departments and business units, as well as network infrastructure and computing platforms. The state of DFR within large organisations may therefore be adversely affected if the myriad human and technical resources involved are not managed in an optimal manner. This paper contributes to DFR by proposing the novel concept of a digital forensic readiness management system (DFRMS). The purpose of a DFRMS is to assist large organisations in achieving an optimal level of management for DFR. In addition to this, we offer an architecture for a DFRMS. This architecture is based on requirements for DFR that we ascertained from an exhaustive review of the DFR literature. We describe the architecture in detail and show that it meets the requirements set out in the DFR literature. The merits and disadvantages of the architecture are also discussed. Finally, we describe and explain an early prototype of a DFRMS.
ER  - 

TY  - JOUR
T1  - Visual latency-based interactive visualization for digital forensics
JO  - Journal of Computational Science
VL  - 1
IS  - 2
SP  - 115
EP  - 120
PY  - 2010/6//
T2  - 
AU  - Cai, Yang
AU  - Franco, Rafael de M.
AU  - García-Herranz, Manuel
SN  - 1877-7503
DO  - http://dx.doi.org/10.1016/j.jocs.2010.04.005
UR  - http://www.sciencedirect.com/science/article/pii/S1877750310000207
KW  - Interaction
KW  - Visualization
KW  - Network anomaly
KW  - Anomalous event
KW  - Clustering
AB  - In this paper, we present an interactive visualization and clustering algorithm for real-time multi-attribute digital forensic data such as network anomalous events. In the model, glyphs are defined with multiple network attributes and clustered with the recursive optimization algorithm for dimensional reduction. The user's visual latency time is incorporated into the recursive process so that it updates the display and the optimization model according to the human factor and maximizes the capacity of real-time computation. The interactive search interface is developed to enable the display of similar data points according to their similarity of attributes. Finally, typical network anomalous events are analyzed and visualized such as password guessing, etc. This technology is expected to have an impact on real-time visual data mining for network security, sensor networks and many other multivariable real-time monitoring systems. Our usability study shows a decent accuracy of context-independent glyph identification (89.37%) with a high precision for anomaly detection (94.36%). The results indicate that, without any context, users tend to classify unknown patterns as possibly harmful. On the other hand, in the dynamic clustering (context-dependent) experiment, clusters of very extremely unusual glyphs normally contain fewer packets. In this case, the packet identification accuracy is remarkably high (99.42%).
ER  - 

TY  - JOUR
T1  - Network forensic frameworks: Survey and research challenges
JO  - Digital Investigation
VL  - 7
IS  - 1–2
SP  - 14
EP  - 27
PY  - 2010/10//
T2  - 
AU  - Pilli, Emmanuel S.
AU  - Joshi, R.C.
AU  - Niyogi, Rajdeep
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2010.02.003
UR  - http://www.sciencedirect.com/science/article/pii/S1742287610000113
KW  - Network forensics
KW  - NFATs
KW  - Distributed systems
KW  - Soft computing
KW  - Honeypots
KW  - Data fusion
KW  - Attribution
KW  - Traceback
KW  - Incident response
AB  - Network forensics is the science that deals with capture, recording, and analysis of network traffic for detecting intrusions and investigating them. This paper makes an exhaustive survey of various network forensic frameworks proposed till date. A generic process model for network forensics is proposed which is built on various existing models of digital forensics. Definition, categorization and motivation for network forensics are clearly stated. The functionality of various Network Forensic Analysis Tools (NFATs) and network security monitoring tools, available for forensics examiners is discussed. The specific research gaps existing in implementation frameworks, process models and analysis tools are identified and major challenges are highlighted. The significance of this work is that it presents an overview on network forensics covering tools, process models and framework implementations, which will be very much useful for security practitioners and researchers in exploring this upcoming and young discipline.
ER  - 

TY  - JOUR
T1  - A system for the proactive, continuous, and efficient collection of digital forensic evidence
JO  - Digital Investigation
VL  - 8, Supplement
IS  - 
SP  - S3
EP  - S13
PY  - 2011/8//
T2  - 11th Annual Digital Forensics Research Conference
AU  - Shields, Clay
AU  - Frieder, Ophir
AU  - Maloof, Mark
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2011.05.002
UR  - http://www.sciencedirect.com/science/article/pii/S1742287611000260
KW  - Proactive forensics
KW  - Evidence collection
KW  - Deleted file logging
KW  - Search
KW  - File similarity
AB  - The historical focus of forensics research and tools on digital systems that are seized from a suspect misses the fact that in centrally controlled networks it is possible to proactively and continuously collect evidence in advance of any known need. We present a proof-of-concept for PROOFS, the first proposed continuous forensic evidence collection system that applies information retrieval techniques to file system forensics. PROOFS creates and stores signatures for files that are deleted, edited, or copied within such a network. The heart of each signature is one or more fingerprints, generated based on statistical properties of file contents, maintaining semantics while requiring as little as 1.06% of the storage space of the original file. We focus on text documents and show that PROOFS has a high precision of 0.96 and recall of 0.85 with stored fingerprint sizes of less than 375 bytes. The two contributions of this work are that we show that common environments exist where proactive collection of forensic evidence is possible and that we demonstrate an efficient and accurate mechanism for collecting evidence in those environments.
ER  - 

TY  - JOUR
T1  - Computer forensics challenges in responding to incidents in real-life settings
JO  - Computer Fraud & Security
VL  - 2007
IS  - 12
SP  - 12
EP  - 16
PY  - 2007/12//
T2  - 
AU  - Schultz, E. Eugene
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(07)70169-0
UR  - http://www.sciencedirect.com/science/article/pii/S1361372307701690
AB  - Dr Eugene Shultz looks at the practice of computer forensics during a real-life incident.
ER  - 

TY  - JOUR
T1  - The impact of Microsoft Windows pool allocation strategies on memory forensics
JO  - Digital Investigation
VL  - 5, Supplement
IS  - 
SP  - S58
EP  - S64
PY  - 2008/9//
T2  - The Proceedings of the Eighth Annual DFRWS Conference
AU  - Schuster, Andreas
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2008.05.007
UR  - http://www.sciencedirect.com/science/article/pii/S1742287608000339
KW  - Microsoft Windows
KW  - Volatile data
KW  - Pool memory
KW  - Process Persistence
AB  - An image of a computer's physical memory can provide a forensic examiner with a wealth of information. A small area of system memory, the nonpaged pool, contains lots of information about currently and formerly active processes. As this paper shows, more than 90% of such information can be retrieved even 24 h after process termination under optimum conditions.

Great care must be taken as the acquisition process usually affects the memory contents to be acquired. In order minimize the impact on volatile data, this paper for the first time analyzes the pool allocation mechanism of the Microsoft Windows operating system. It describes a test arrangement, which allows to obtain a time series of physical memory images, while it also reduces the effect on the observed operating system.

Using this environment it was found that allocations from the nonpaged pool are reused based on their size and a last in-first out schedule. In addition, a passive memory compaction strategy may apply. So, the creation of a new object is likely to eradicate the evidence of an object of the same class that was destructed just before. The paper concludes with a discussion of the implications for incident response procedures, forensic examinations, and the creation of forensic tools.
ER  - 

TY  - JOUR
T1  - Managing corporate computer forensics
JO  - Computer Fraud & Security
VL  - 2006
IS  - 6
SP  - 14
EP  - 16
PY  - 2006/6//
T2  - 
AU  - Haggerty, J.
AU  - Taylor, M.
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(06)70368-2
UR  - http://www.sciencedirect.com/science/article/pii/S1361372306703682
AB  - Many organizations now have an IT security strategy in place covering the management of IT security facilities and activities within the organization. Computer forensics has previously been an activity limited mainly within the bounds of law enforcement agencies. However, commercial organizations are increasingly making use of computer forensics in areas such as fraud, money laundering, the accessing or distribution of pornography, or harassment. In this article we outline a framework for the management of computer forensic facilities and activities within a corporate setting.
ER  - 

TY  - JOUR
T1  - Profiling software applications for forensic analysis
JO  - Computer Fraud & Security
VL  - 2015
IS  - 6
SP  - 13
EP  - 18
PY  - 2015/6//
T2  - 
AU  - Rafique, Mamoona
AU  - Khan, Muhammad Naeem Ahmed
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(15)30058-0
UR  - http://www.sciencedirect.com/science/article/pii/S1361372315300580
AB  - Computers are now a fundamental part of our professional lives. Although advanced technologies are being used to contain digital crimes, alongside these are other technologies that have expanded a criminal community that is constantly searching for new means to commit crimes in more sophisticated ways. Due to the availability of corporate data on the web, coupled with the open access nature of the web, digital miscreants can commit cybercrimes either as legitimate or illegitimate users.

Traditional digital forensics involves static analysis of the data available on permanent storage media, while live analysis allows running systems to be examined to analyse volatile data.

However, live analysis is not without its challenges, not least because each application has different effects on the system. Mamoona Rafique and Muhammad Naeem Ahmed Khan present a model for profiling the behaviour of application programs. This allows investigators to build a behavioural profile of each application in order to understand its effects on the system.
ER  - 

TY  - JOUR
T1  - ANTI-Forensics – distorting the evidence
JO  - Computer Fraud & Security
VL  - 2006
IS  - 5
SP  - 4
EP  - 6
PY  - 2006/5//
T2  - 
AU  - Sartin, Bryan
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(06)70354-2
UR  - http://www.sciencedirect.com/science/article/pii/S1361372306703542
AB  - Computer forensic investigators rely on high quality evidence to win a case. Logs, authentication information, date and timestamps, file contents and other electronic data all need to be proven to be reliable in court. But what happens when criminals are actively trying to ruin the evidence?

ANTI Forensic techniques are now being used to skew evidence and make it impossible for an examiner to use. According to Brian Sartin at ISACA, ANTI forensics is used in two-thirds of all data compromise investigations carried out by his organization. Examiners need to be on the look out for three methods of distorting evidence: Data Obfuscation, Data Hiding and Zero-footprinting

Almost every case will use some form of data obfuscation that involves a hacker erasing his tracks. But Data hiding draws on the power of cryptography to mask data rather than delete it The use of steganography is another data hiding approach. Examiners need to actively search for evidence of the use of ANTI-forensic techniques.

Computer Forensics (CF), as we know it, is in a volatile state. Newer and more sophisticated investigative challenges, both existing and on the horizon, are forcing CF to evolve as a practice. As such, the processes, the technologies, and the tools of the trade that characterise the conventional CF approach have changed. Simply put, CF today is not what it used to be and there are some very simple reasons why.
ER  - 

TY  - JOUR
T1  - Electronic discovery: digital forensics and beyond
JO  - Computer Fraud & Security
VL  - 2006
IS  - 4
SP  - 8
EP  - 10
PY  - 2006/4//
T2  - 
AU  - Forte, Dario
AU  - Power, Richard
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(06)70332-3
UR  - http://www.sciencedirect.com/science/article/pii/S1361372306703323
AB  - Companies need to be ready to find and produce electronic records at the drop of a hat in the face of a lawsuit. Three-quarters of modern day lawsuits entail e-Discovery. Organisations need to salvage data from every remote corner of their systems. Information from email, office documents, log files, transactions and scanned files must be on standby for extraction and scrutiny.

And it isn't cheap - the average cost of an e-Discovery project is several hundred thousand dollars.

The idea is to provide a trusted copy of original documents requested by lawyers or chosen for presentation by the company.

“Litigation Lifecycle Management” is a very common legal procedure in the United States. This article provides an introduction to the topic with special reference to its complexities.
ER  - 

TY  - JOUR
T1  - Forensic discovery auditing of digital evidence containers
JO  - Digital Investigation
VL  - 4
IS  - 2
SP  - 88
EP  - 97
PY  - 2007/6//
T2  - 
AU  - Richard III, Golden G.
AU  - Roussev, Vassil
AU  - Marziale, Lodovico
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2007.04.002
UR  - http://www.sciencedirect.com/science/article/pii/S1742287607000291
KW  - Digital forensics
KW  - Operating systems internals
KW  - Filesystems
KW  - Digital evidence containers Auditing
AB  - Current digital forensics methods capture, preserve, and analyze digital evidence in general-purpose electronic containers (typically, plain files) with no dedicated support to help establish that the evidence has been properly handled. Auditing of a digital investigation, from identification and seizure of evidence through duplication and investigation is, essentially, ad hoc, recorded in separate log files or in an investigator's case notebook. Auditing performed in this fashion is bound to be incomplete, because different tools provide widely disparate amounts of auditing information – including none at all – and there is ample room for human error. The latter is a particularly pressing concern given the fast growth of the size of forensic targets.

Recently, there has been a serious community effort to develop an open standard for specialized digital evidence containers (DECs). A DEC differs from a general purpose container in that, in addition to the actual evidence, it bundles arbitrary metadata associated with it, such as logs and notes, and provides the basic means to detect evidence-tampering through digital signatures. Current approaches consist of defining a container format and providing a specialized library that can be used to manipulate it. While a big step in the right direction, this approach has some non-trivial shortcomings – it requires the retooling of existing forensic software and, thereby, limits the number of tools available to the investigator. More importantly, however, it does not provide a complete solution since it only records snapshots of the state of the DEC without being able to provide a trusted log of all data operations actually performed on the evidence. Without a trusted log the question of whether a tool worked exactly as advertised cannot be answered with certainty, which opens the door to challenges (both legitimate and frivolous) of the results.

In this paper, we propose a complementary mechanism, called the Forensic Discovery Auditing Module (FDAM), aimed at closing this loophole in the discovery process. FDAM can be thought of as a ‘clean-room’ environment for the manipulation of digital evidence, where evidence from containers is placed for controlled manipulation. It functions as an operating system component, which monitors and logs all access to the evidence and enforces policy restrictions. This allows the immediate, safe, and verifiable use of any tool deemed necessary by the examiner. In addition, the module can provide transparent support for multiple DEC formats, thereby greatly simplifying the adoption of open standards.
ER  - 

TY  - JOUR
T1  - FriendlyRoboCopy: A GUI to RoboCopy for computer forensic investigators
JO  - Digital Investigation
VL  - 4
IS  - 1
SP  - 16
EP  - 23
PY  - 2007/3//
T2  - 
AU  - LaVelle, Claire
AU  - Konrad, Almudena
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2007.01.001
UR  - http://www.sciencedirect.com/science/article/pii/S1742287607000023
KW  - Digital forensics
KW  - Network forensics
KW  - Drive mapping
KW  - RoboCopy application
KW  - Microsoft OS forensics
KW  - Network system administration
KW  - NAS
KW  - Computer cluster
KW  - Graphical User Interface
KW  - Perl
KW  - Open Source application
AB  - One of the most pressing challenges in digital investigations today is the extraction and forensic preservation of a subset of data on computer clusters and other large storage systems. As the number and capacity of computer systems increases, it is no longer feasible to create forensic duplicates of every system in their entirety. Although forensic tools are being developed to cope with such situations, they do not support all file systems. Experienced digital investigators use tools such as RoboCopy to preserve a subset of data on target systems, and take steps to document their process and results. This paper explores the need for these tools in digital investigations, and demonstrates the strengths and weaknesses of using RoboCopy to acquire data on a network share. This paper then introduces FriendlyRoboCopy, which provides an effective, user-friendly interface to RoboCopy that addresses the requirements of forensic preservation.
ER  - 

TY  - JOUR
T1  - Case study: Network intrusion investigation – lessons in forensic preparation
JO  - Digital Investigation
VL  - 2
IS  - 4
SP  - 254
EP  - 260
PY  - 2005/12//
T2  - 
AU  - Casey, Eoghan
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2005.11.007
UR  - http://www.sciencedirect.com/science/article/pii/S1742287605000940
KW  - Forensic readiness
KW  - Forensic preparation
KW  - Incident response
KW  - Incident handling
KW  - Network forensics
KW  - Computer intrusion
KW  - Rootkit
KW  - Forensic computing
KW  - Tracking intruders
KW  - Attribution
AB  - Investigations of network security breaches are both complex and costly. Even a moderate amount of forensic preparation in an organization can mitigate the impact of a major incident and can enable the organization to obtain restitution. A case study of an intrusion is outlined in which the victim organization worked with law enforcement agencies to apprehend the perpetrator. This case study contains examples of challenges that can arise during this type of investigation, and discusses practical steps that an organization can take to prepare for a major incident. The overlapping roles of System Administrators, Incident Handlers, and Forensic Examiners in a network intrusion are explored, with an emphasis on the need for collaboration and proper evidence handling. This case study also shows how effective case management and methodical reconstruction of events can help create a more complete picture of the crime and help establish links between computer intruders and their illegal activities.
ER  - 

TY  - JOUR
T1  - LINCS: Towards building a trustworthy litigation hold enabled cloud storage system
JO  - Digital Investigation
VL  - 14, Supplement 1
IS  - 
SP  - S55
EP  - S67
PY  - 2015/8//
T2  - The Proceedings of the Fifteenth Annual DFRWS Conference
AU  - Zawoad, Shams
AU  - Hasan, Ragib
AU  - Grimes, John
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2015.05.014
UR  - http://www.sciencedirect.com/science/article/pii/S1742287615000596
KW  - Litigation hold
KW  - Cloud security
KW  - Cloud forensics
KW  - Storage security
KW  - Regulatory compliance
AB  - Abstract
Litigation holds are inevitable parts of modern civil lawsuits that mandate an organization to preserve all forms of documents related to a lawsuit. In current data storage models, this includes documents stored in clouds. However, due to the fundamental natures of today's clouds, incorporating a trustworthy litigation hold management system is very challenging. To make the situation more complicated, defendants or plaintiffs may collude with the cloud service provider (CSP) to manipulate the documents under the hold. Serious consequences can follow if a litigant party fails to comply with the litigation hold for evidence stored in the cloud, resulting in legal sanctions for spoliation. This will not only harm the reputation of an organization but also levy of sanctions, such as fines, penalties, etc.

In this paper, we define a model of trustworthy litigation hold management for cloud-based storage systems and identify the key security properties. Based on the model, we propose a trustworthy LIitigation hold eNabled Cloud Storage (LINCS) system. We show that LINCS can provide the required security properties in a strong adversarial scenario, where a plaintiff or defendant colludes with a malicious CSP. Our prototype implementation reveals that the performance overhead of using LINCS is very low (average 1.4% for the user), which suggests that such litigation hold enabled storage system can be integrated with real clouds.
ER  - 

TY  - JOUR
T1  - Examining the state of preparedness of Information Technology management in New Zealand for events that may require forensic analysis
JO  - Digital Investigation
VL  - 2
IS  - 4
SP  - 276
EP  - 280
PY  - 2005/12//
T2  - 
AU  - Quinn, Spike
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2005.10.005
UR  - http://www.sciencedirect.com/science/article/pii/S1742287605000873
KW  - Security policy
KW  - Forensic policy
KW  - IT management
KW  - Forensic readiness
KW  - Statistics
AB  - Computer security is of concern to those in IT (Information Technology) and forensic readiness (being prepared to deal effectively with events that may require forensic investigation) is a growing issue. Data held only on magnetic or other transient media require expert knowledge and special procedures to preserve and present it as valid in a criminal or employment court. Staff required to handle possible forensic evidence should be forensically knowledgeable. Having policies and procedures in place is one inexpensive way to protect the forensic data and can mean the difference between a valid case and no case.

This paper presents the results of a survey of IT managers in New Zealand (NZ) examining the state of awareness of IT management in NZ regarding the field of digital forensics in general and their state of preparation for protection of forensic data in the case of an event requiring forensic analysis.
ER  - 

TY  - JOUR
T1  - Integrating intrusion alert information to aid forensic explanation: An analytical intrusion detection framework for distributive IDS
JO  - Information Fusion
VL  - 10
IS  - 4
SP  - 325
EP  - 341
PY  - 2009/10//
T2  - Special Issue on Information Fusion in Computer Security
AU  - Sy, Bon K.
SN  - 1566-2535
DO  - http://dx.doi.org/10.1016/j.inffus.2009.01.001
UR  - http://www.sciencedirect.com/science/article/pii/S1566253509000153
KW  - Probabilistic inference
KW  - Model discovery
KW  - Intrusion detection
KW  - Forensic analysis
AB  - The objective of this research is to show an analytical intrusion detection framework (AIDF) comprised of (i) a probability model discovery approach, and (ii) a probabilistic inference mechanism for generating the most probable forensic explanation based on not only just the observed intrusion detection alerts, but also the unreported signature rules that are revealed in the probability model. The significance of the proposed probabilistic inference is its ability to integrate alert information available from IDS sensors distributed across subnets. We choose the open source Snort to illustrate its feasibility, and demonstrate the inference process applied to the intrusion detection alerts produced by Snort. Through a preliminary experimental study, we illustrate the applicability of AIDF for information integration and the realization of (i) a distributive IDS environment comprised of multiple sensors, and (ii) a mechanism for selecting and integrating the probabilistic inference results from multiple models for composing the most probable forensic explanation.
ER  - 

TY  - JOUR
T1  - Forensic analysis of logs: Modeling and verification
JO  - Knowledge-Based Systems
VL  - 20
IS  - 7
SP  - 671
EP  - 682
PY  - 2007/10//
T2  - Special Issue on Techniques to Produce Intelligent Secure Software
AU  - Saleh, Mohamed
AU  - Arasteh, Ali Reza
AU  - Sakha, Assaad
AU  - Debbabi, Mourad
SN  - 0950-7051
DO  - http://dx.doi.org/10.1016/j.knosys.2007.05.002
UR  - http://www.sciencedirect.com/science/article/pii/S0950705107000561
KW  - Forensic analysis
KW  - Log analysis
KW  - Formal methods
KW  - Model checking
KW  - Logging systems
AB  - Information stored in logs of a computer system is of crucial importance to gather forensic evidence of investigated actions or attacks against the system. Analysis of this information should be rigorous and credible, hence it lends itself to formal methods. We propose a model checking approach to the formalization of the forensic analysis of logs. The set of logs of a certain system is modeled as a tree whose labels are events extracted from the logs. In order to provide a structure to these events, we express each event as a term of a term algebra. The signature of the algebra is carefully chosen to include all relevant information necessary to conduct the analysis. Properties of the model are expressed as formulas of a logic having dynamic, linear, temporal, and modal characteristics. Moreover, we provide a tableau-based proof system for this logic upon which a model checking algorithm can be developed. In order to illustrate the proposed approach, the Windows auditing system is studied. The properties that we capture in our logic include invariant properties of a system, forensic hypotheses, and generic or specific attack signatures. Moreover, we discuss the admissibility of forensics hypotheses and the underlying verification issues.
ER  - 

TY  - JOUR
T1  - Analyzing multiple logs for forensic evidence
JO  - Digital Investigation
VL  - 4, Supplement
IS  - 
SP  - 82
EP  - 91
PY  - 2007/9//
T2  - 
AU  - Arasteh, Ali Reza
AU  - Debbabi, Mourad
AU  - Sakha, Assaad
AU  - Saleh, Mohamed
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2007.06.013
UR  - http://www.sciencedirect.com/science/article/pii/S1742287607000448
KW  - Forensic analysis
KW  - Log analysis
KW  - Formal methods
KW  - Model checking
KW  - Logging systems
KW  - Log correlation
AB  - Information stored in logs of a computer system is of crucial importance to gather forensic evidence of investigated actions or attacks. Analysis of this information should be rigorous and credible, hence it lends itself to formal methods. We propose a model checking approach to the formalization of the forensic analysis of logs. A set of logs is modeled as a tree whose labels are events extracted from the logs. In order to provide a structure to these events, we express each event as a term of algebra. The signature of the algebra is carefully chosen to include all relevant information necessary to conduct the analysis. Properties of the model, attack scenarios, and event sequences are expressed as formulas of a logic having dynamic, linear, temporal, and modal characteristics. Moreover, we provide a tableau-based proof system for this logic upon which a model checking algorithm can be developed. We use our model in a case study to demonstrate how events leading to an SYN attack can be reconstructed from a number of system logs.
ER  - 

TY  - JOUR
T1  - On-scene triage open source forensic tool chests: Are they effective?
JO  - Digital Investigation
VL  - 10
IS  - 2
SP  - 99
EP  - 115
PY  - 2013/9//
T2  - Triage in Digital Forensics
AU  - Shiaeles, Stavros
AU  - Chryssanthou, Anargyros
AU  - Katos, Vasilios
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2013.04.002
UR  - http://www.sciencedirect.com/science/article/pii/S1742287613000315
KW  - Triage
KW  - ACPO principles
KW  - Open source
KW  - Incident response
KW  - TriageIR
KW  - Kludge
KW  - TR3Secure
AB  - Abstract
Considering that a triage related task may essentially make-or-break a digital investigation and the fact that a number of triage tools are freely available online but there is currently no mature framework for practically testing and evaluating them, in this paper we put three open source triage tools to the test. In an attempt to identify common issues, strengths and limitations we evaluate them both in terms of efficiency and compliance to published forensic principles. Our results show that due to the increased complexity and wide variety of system configurations, the triage tools should be made more adaptable, either dynamically or manually (depending on the case and context) instead of maintaining a monolithic functionality.
ER  - 

TY  - JOUR
T1  - VMI-PL: A monitoring language for virtual platforms using virtual machine introspection
JO  - Digital Investigation
VL  - 11, Supplement 2
IS  - 
SP  - S85
EP  - S94
PY  - 2014/8//
T2  - Fourteenth Annual DFRWS Conference
AU  - Westphal, Florian
AU  - Axelsson, Stefan
AU  - Neuhaus, Christian
AU  - Polze, Andreas
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2014.05.016
UR  - http://www.sciencedirect.com/science/article/pii/S1742287614000590
KW  - Virtualization
KW  - Security
KW  - Monitoring language
KW  - Live forensics
KW  - Introspection
KW  - Classification
AB  - Abstract
With the growth of virtualization and cloud computing, more and more forensic investigations rely on being able to perform live forensics on a virtual machine using virtual machine introspection (VMI). Inspecting a virtual machine through its hypervisor enables investigation without risking contamination of the evidence, crashing the computer, etc.

To further access to these techniques for the investigator/researcher we have developed a new VMI monitoring language. This language is based on a review of the most commonly used VMI-techniques to date, and it enables the user to monitor the virtual machine's memory, events and data streams. A prototype implementation of our monitoring system was implemented in KVM, though implementation on any hypervisor that uses the common x86 virtualization hardware assistance support should be straightforward. Our prototype outperforms the proprietary VMWare VProbes in many cases, with a maximum performance loss of 18% for a realistic test case, which we consider acceptable. Our implementation is freely available under a liberal software distribution license.
ER  - 

TY  - JOUR
T1  - Management strategies for implementing forensic security measures
JO  - Information Security Technical Report
VL  - 8
IS  - 2
SP  - 55
EP  - 64
PY  - 2003/6//
T2  - 

SN  - 1363-4127
DO  - http://dx.doi.org/10.1016/S1363-4127(03)00207-3
UR  - http://www.sciencedirect.com/science/article/pii/S1363412703002073
AB  - We live in the age of electronic information and rapidly evolving technology. Almost every aspect of our lives is touched or somehow controlled by technology-driven processes, procedures and devices. It is therefore important to understand that because of the pervasive electronic influence, the opportunist or criminal element has turned its attention to exploiting weaknesses inherent in many traditional and electronic information systems. With that undisputable fact in mind, we must face the inevitable: a successful criminal or unacceptable incident occurring within the organization’s perimeter of the information and/or computer and network infrastructure.
ER  - 

TY  - JOUR
T1  - ASE: A comprehensive pattern-driven security methodology for distributed systems
JO  - Computer Standards & Interfaces
VL  - 41
IS  - 
SP  - 112
EP  - 137
PY  - 2015/9//
T2  - 
AU  - Uzunov, Anton V.
AU  - Fernandez, Eduardo B.
AU  - Falkner, Katrina
SN  - 0920-5489
DO  - http://dx.doi.org/10.1016/j.csi.2015.02.011
UR  - http://www.sciencedirect.com/science/article/pii/S0920548915000276
KW  - Secure software engineering
KW  - Security methodologies
KW  - Distributed systems security
KW  - Security patterns
KW  - Security solution frames
AB  - Abstract
Incorporating security features is one of the most important and challenging tasks in designing distributed systems. Over the last decade, researchers and practitioners have come to recognize that the incorporation of security features should proceed by means of a structured, systematic approach, combining principles from both software and security engineering. Such systematic approaches, particularly those implying some sort of process aligned with the development life-cycle, are termed security methodologies. There are a number of security methodologies in the literature, of which the most flexible and, according to a recent survey, most satisfactory from an industry-adoption viewpoint are methodologies that encapsulate their security solutions in some fashion, especially via the use of security patterns. While the literature does present several mature pattern-driven security methodologies with either a general or a highly specific system applicability, there are currently no (pattern-driven) security methodologies specifically designed for general distributed systems. Going further, there are also currently no methodologies with mixed specific applicability, e.g. for both general and peer-to-peer distributed systems. In this paper we aim to fill these gaps by presenting a comprehensive pattern-driven security methodology – arrived at by applying a previously devised approach to engineering security methodologies – specifically designed for general distributed systems, which is also capable of taking into account the specifics of peer-to-peer systems as needed. Our methodology takes the principle of encapsulation several steps further, by employing patterns not only for the incorporation of security features (via security solution frames), but also for the modeling of threats, and even as part of its process. We illustrate and evaluate the presented methodology in detail via a realistic example – the development of a distributed system for file sharing and collaborative editing. In both the presentation of the methodology and example our focus is on the early life-cycle phases (analysis and design).
ER  - 

TY  - JOUR
T1  - Forensic investigation of cloud computing systems
JO  - Network Security
VL  - 2011
IS  - 3
SP  - 4
EP  - 10
PY  - 2011/3//
T2  - 
AU  - Taylor, Mark
AU  - Haggerty, John
AU  - Gresty, David
AU  - Lamb, David
SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/S1353-4858(11)70024-1
UR  - http://www.sciencedirect.com/science/article/pii/S1353485811700241
AB  - Cloud computing describes a computing concept where software services, and the resources they use, operate as (and on) a virtualised platform across many different host machines, connected by the Internet or an organisation's internal network. From a business or system user's point of view, the cloud provides, via virtualisation, a single platform or service collection in which it can operate.

Cloud computing is a new concept in the distributed processing of data and is likely to make computer forensic evidence acquisition and evidence analysis increasingly complex.

Currently there do not appear to be any published guidelines that specifically address the conduct of computer forensic investigations of cloud computing systems. In order to understand and analyse evidence within this environment, computer forensics examiners will require a broader range of technical knowledge across multiple hardware platforms and operating systems. Dr Mark Taylor et al examine the issues concerning the forensic investigation of cloud systems.
ER  - 

TY  - JOUR
T1  - Digital Wiretap Warrant: Improving the security of ETSI Lawful Interception
JO  - Digital Investigation
VL  - 14
IS  - 
SP  - 1
EP  - 16
PY  - 2015/9//
T2  - 
AU  - Muñoz, Alfonso
AU  - Urueña, Manuel
AU  - Aparicio, Raquel
AU  - Rodríguez de los Santos, Gerson
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2015.04.005
UR  - http://www.sciencedirect.com/science/article/pii/S1742287615000456
KW  - Digital Wiretap Warrant (DWW)
KW  - Lawful Interception (LI)
KW  - ETSI LI Technical Committee
KW  - Civil rights
KW  - Security
KW  - Privacy
KW  - Digital evidence
KW  - Chain of custody
AB  - Abstract
Lawful Interception (LI) of data communications is an essential tool for Law Enforcement Agencies (LEA) in order to investigate criminal activities carried out or coordinated by means of Internet. However, the ability to secretly monitor the activities of citizens also has a great impact on civil rights. Therefore, democratic societies must prevent abuse and ensure that LI is only employed in specific cases with justifiable grounds or a probable cause. Nowadays, in many countries each interception must be authorized by a wiretap warrant, usually issued by a judge. However, this wiretap warrant is merely an administrative document that should be checked by the network or service operator before enabling the monitoring of its customers, whose communications are later handed over to a LEA in plaintext. This paper proposes the idea of employing a Digital Wiretap Warrant (DWW), which further protects the civil liberties, security and privacy of LI by ensuring that monitoring devices can only be enabled with a valid DWW, and by encrypting the captured data so only the authorized LEA is able to decrypt those communications. Moreover, in the proposed DWW framework all digital evidence is securely time-stamped and signed, thus guaranteeing that it has not been tampered with, and that a proper chain of custody has been met. In particular this paper proposes how to apply the DWW concept to the lawful interception framework defined by the ETSI LI Technical Committee, and evaluates how the additional security mechanisms could impact the performance and storage costs of a LI platform.
ER  - 

TY  - JOUR
T1  - Automated event and social network extraction from digital evidence sources with ontological mapping
JO  - Digital Investigation
VL  - 13
IS  - 
SP  - 94
EP  - 106
PY  - 2015/6//
T2  - 
AU  - Turnbull, Benjamin
AU  - Randhawa, Suneel
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2015.04.004
UR  - http://www.sciencedirect.com/science/article/pii/S1742287615000444
KW  - Artificial intelligence
KW  - Big data
KW  - Digital forensics
KW  - Digital evidence
KW  - Event representation
KW  - Forensic tool development
KW  - Knowledge representation
KW  - Ontology
KW  - Software engineering
KW  - Triage
AB  - Abstract
The sharp rise in consumer computing, electronic and mobile devices and data volumes has resulted in increased workloads for digital forensic investigators and analysts. The number of crimes involving electronic devices is increasing, as is the amount of data for each job. This is becoming unscaleable and alternate methods to reduce the time trained analysts spend on each job are necessary.

This work leverages standardised knowledge representations techniques and automated rule-based systems to encapsulate expert knowledge for forensic data. The implementation of this research can provide high-level analysis based on low-level digital artefacts in a way that allows an understanding of what decisions support the facts. Analysts can quickly make determinations as to which artefacts warrant further investigation and create high level case data without manually creating it from the low-level artefacts. Extraction and understanding of users and social networks and translating the state of file systems to sequences of events are the first uses for this work.

A major goal of this work is to automatically derive ‘events’ from the base forensic artefacts. Events may be system events, representing logins, start-ups, shutdowns, or user events, such as web browsing, sending email. The same information fusion and homogenisation techniques are used to reconstruct social networks. There can be numerous social network data sources on a single computer; internet cache can locate Facebook, LinkedIn, Google Plus caches; email has address books and copies of emails sent and received; instant messenger has friend lists and call histories. Fusing these into a single graph allows a more complete, less fractured view for an investigator.

Both event creation and social network creation are expected to assist investigator-led triage and other fast forensic analysis situations.
ER  - 

TY  - JOUR
T1  - Towards an Analysis of Data Accountability and Auditing for Secure Cloud Data Storage
JO  - Procedia Computer Science
VL  - 50
IS  - 
SP  - 543
EP  - 550
PY  - 2015///
T2  - Big Data, Cloud and Computing Challenges
AU  - Prassanna, J.
AU  - Punitha, K.
AU  - Neelanarayanan, V.
SN  - 1877-0509
DO  - http://dx.doi.org/10.1016/j.procs.2015.04.101
UR  - http://www.sciencedirect.com/science/article/pii/S187705091500602X
AB  - Abstract
Cloud technology, runs off-premise, provides an extensive information technology support and service that can offer as an on-premise version if on demanded as a virtualized solution for customers’ technological requirements. Cloud computing put together the comprehensive solutions by integrating the different technology that provision Software, Platform and Infrastructure as services. Cloud, an integrated technology that delivers a complete, open and flexible solution, leads to a critical concern over its trust. The extensive use of virtualization in cloud especially in data storage capability put forth many security concerns to the customer. It is the time to prove the customer about the strength of the cloud security through enabling their data accountability and auditing mechanism. In this work we going to provide a systematic analysis of the data accountability mechanism and auditing approaches existing now in cloud. To strengthen the customers trust over the cloud technology and service, we provided a clear enhanced view on the cloud data security and also analyze the secured way of utilizing the advantages of cloud computing.
ER  - 

TY  - JOUR
T1  - Security and privacy issues in implantable medical devices: A comprehensive survey
JO  - Journal of Biomedical Informatics
VL  - 55
IS  - 
SP  - 272
EP  - 289
PY  - 2015/6//
T2  - 
AU  - Camara, Carmen
AU  - Peris-Lopez, Pedro
AU  - Tapiador, Juan E.
SN  - 1532-0464
DO  - http://dx.doi.org/10.1016/j.jbi.2015.04.007
UR  - http://www.sciencedirect.com/science/article/pii/S153204641500074X
KW  - Implantable medical devices
KW  - Security
KW  - Privacy
KW  - m-Health
KW  - Survey
AB  - Abstract
Bioengineering is a field in expansion. New technologies are appearing to provide a more efficient treatment of diseases or human deficiencies. Implantable Medical Devices (IMDs) constitute one example, these being devices with more computing, decision making and communication capabilities. Several research works in the computer security field have identified serious security and privacy risks in IMDs that could compromise the implant and even the health of the patient who carries it. This article surveys the main security goals for the next generation of IMDs and analyzes the most relevant protection mechanisms proposed so far. On the one hand, the security proposals must have into consideration the inherent constraints of these small and implanted devices: energy, storage and computing power. On the other hand, proposed solutions must achieve an adequate balance between the safety of the patient and the security level offered, with the battery lifetime being another critical parameter in the design phase.
ER  - 

TY  - JOUR
T1  - Security in cloud computing: Opportunities and challenges
JO  - Information Sciences
VL  - 305
IS  - 
SP  - 357
EP  - 383
PY  - 2015/6/1/
T2  - 
AU  - Ali, Mazhar
AU  - Khan, Samee U.
AU  - Vasilakos, Athanasios V.
SN  - 0020-0255
DO  - http://dx.doi.org/10.1016/j.ins.2015.01.025
UR  - http://www.sciencedirect.com/science/article/pii/S0020025515000638
KW  - Cloud computing
KW  - Multi-tenancy
KW  - Security
KW  - Virtualization
KW  - Web services
AB  - Abstract
The cloud computing exhibits, remarkable potential to provide cost effective, easy to manage, elastic, and powerful resources on the fly, over the Internet. The cloud computing, upsurges the capabilities of the hardware resources by optimal and shared utilization. The above mentioned features encourage the organizations and individual users to shift their applications and services to the cloud. Even the critical infrastructure, for example, power generation and distribution plants are being migrated to the cloud computing paradigm. However, the services provided by third-party cloud service providers entail additional security threats. The migration of user’s assets (data, applications, etc.) outside the administrative control in a shared environment where numerous users are collocated escalates the security concerns. This survey details the security issues that arise due to the very nature of cloud computing. Moreover, the survey presents the recent solutions presented in the literature to counter the security issues. Furthermore, a brief view of security vulnerabilities in the mobile cloud computing are also highlighted. In the end, the discussion on the open issues and future research directions is also presented.
ER  - 

TY  - JOUR
T1  - Case study: From embedded system analysis to embedded system based investigator tools
JO  - Digital Investigation
VL  - 11
IS  - 3
SP  - 154
EP  - 159
PY  - 2014/9//
T2  - Special Issue: Embedded Forensics
AU  - Souvignet, T.
AU  - Prüfer, T.
AU  - Frinken, J.
AU  - Kricsanowits, R.
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2014.06.004
UR  - http://www.sciencedirect.com/science/article/pii/S1742287614000784
KW  - Skimming
KW  - Embedded systems
KW  - Payment card fraud
KW  - Forensic tools
KW  - Bluetooth forensics
KW  - Arduino
KW  - Android
AB  - Abstract
Since mid-2012, France and Germany have had to deal with a new form of payment card skimming. This fraud consists of adding a wireless embedded system into a point-of-sale payment terminal with the fraudulent goal of collecting payment card data and personal identification numbers (PIN).

This case study details the strategy adopted to conduct the digital forensic examination of these skimmers. Advanced technologies and analyses were necessary to reveal the skimmed data and provide useful information to investigators for their cross-case analysis.

To go further than a typical digital forensic examination, developments based on embedded systems were made to help investigators find compromised payment terminals and identify criminals.

Finally, this case study provides possible reactive and proactive new roles for forensic experts in combating payment card fraud.
ER  - 

TY  - JOUR
T1  - An efficient technique for enhancing forensic capabilities of Ext2 file system
JO  - Digital Investigation
VL  - 4, Supplement
IS  - 
SP  - 55
EP  - 61
PY  - 2007/9//
T2  - 
AU  - Barik, Mridul Sankar
AU  - Gupta, Gaurav
AU  - Sinha, Shubhro
AU  - Mishra, Alok
AU  - Mazumdar, Chandan
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2007.06.007
UR  - http://www.sciencedirect.com/science/article/pii/S1742287607000515
KW  - Electronic documents
KW  - Modification access and creation
KW  - date and time stamps (MAC DTS)
KW  - Authentic date and time stamps (ADTS)
KW  - Computer Frauds and Cyber Crimes (CFCC)
KW  - Ext2 file system
KW  - Loadable Kernel Module (LKM)
AB  - As electronic documents become more important and valuable in the modern era, attempts are invariably made to take undue-advantage by tampering with them. Tampering with the modification, access and creation date and time stamps (MAC DTS) of digital documents pose a great threat and proves to be a major handicap in digital forensic investigation. Authentic date and time stamps (ADTS) can provide crucial evidence in linking crime to criminal in cases of Computer Fraud and Cyber Crimes (CFCC) through reliable time lining of digital evidence. But the ease with which the MAC DTS of stored digital documents can be changed raises some serious questions about the integrity and admissibility of digital evidence, potentially leading to rejection of acquired digital evidence in the court of Law. MAC DTS procedures of popular operating systems are inherently flawed and were created only for the sake of convenience and not necessarily keeping in mind the security and digital forensic aspects. This paper explores these issues in the context of the Ext2 file system and also proposes one solution to tackle such issues for the scenario where systems have preinstalled plug-ins in the form of Loadable Kernel Modules, which provide the capability to preserve ADTS.
ER  - 

TY  - JOUR
T1  - A social graph based text mining framework for chat log investigation
JO  - Digital Investigation
VL  - 11
IS  - 4
SP  - 349
EP  - 362
PY  - 2014/12//
T2  - 
AU  - Anwar, Tarique
AU  - Abulaish, Muhammad
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2014.10.001
UR  - http://www.sciencedirect.com/science/article/pii/S1742287614001091
KW  - Text mining
KW  - Chat logs mining
KW  - Digital forensics
KW  - Social graph generation
KW  - Cyber crime investigation
AB  - Abstract
This paper presents a unified social graph based text mining framework to identify digital evidences from chat logs data. It considers both users' conversation and interaction data in group-chats to discover overlapping users' interests and their social ties. The proposed framework applies n-gram technique in association with a self-customized hyperlink-induced topic search (HITS) algorithm to identify key-terms representing users' interests, key-users, and key-sessions. We propose a social graph generation technique to model users' interactions, where ties (edges) between a pair of users (nodes) are established only if they participate in at least one common group-chat session, and weights are assigned to the ties based on the degree of overlap in users' interests and interactions. Finally, we present three possible cyber-crime investigation scenarios and a user-group identification method for each of them. We present our experimental results on a data set comprising 1100 chat logs of 11,143 chat sessions continued over a period of 29 months from January 2010 to May 2012. Experimental results suggest that the proposed framework is able to identify key-terms, key-users, key-sessions, and user-groups from chat logs data, all of which are crucial for cyber-crime investigation. Though the chat logs are recovered from a single computer, it is very likely that the logs are collected from multiple computers in real scenario. In this case, logs collected from multiple computers can be combined together to generate more enriched social graph. However, our experiments show that the objectives can be achieved even with logs recovered from a single computer by using group-chats data to draw relationships between every pair of users.
ER  - 

TY  - JOUR
T1  - Dropbox analysis: Data remnants on user machines
JO  - Digital Investigation
VL  - 10
IS  - 1
SP  - 3
EP  - 18
PY  - 2013/6//
T2  - 
AU  - Quick, Darren
AU  - Choo, Kim-Kwang Raymond
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2013.02.003
UR  - http://www.sciencedirect.com/science/article/pii/S174228761300011X
KW  - Cloud storage
KW  - Cloud forensics
KW  - Dropbox analysis
KW  - Computer forensics
KW  - Digital forensics
KW  - Mobile forensics
AB  - Cloud storage has been identified as an emerging challenge to digital forensic researchers and practitioners in a range of literature. There are various types of cloud storage services with each type having a potentially different use in criminal activity. One area of difficulty is the identification, acquisition, and preservation of evidential data when disparate services can be utilised by criminals. Not knowing if a cloud service is being used, or which cloud service, can potentially impede an investigation. It would take additional time to contact all service providers to determine if data is being stored within their cloud service. Using Dropbox™ as a case study, research was undertaken to determine the data remnants on a Windows 7 computer and an Apple iPhone 3G when a user undertakes a variety of methods to store, upload, and access data in the cloud. By determining the data remnants on client devices, we contribute to a better understanding of the types of terrestrial artifacts that are likely to remain for digital forensics practitioners and examiners. Potential information sources identified during the research include client software files, prefetch files, link files, network traffic capture, and memory captures, with many data remnants available subsequent to the use of Dropbox by a user.
ER  - 

TY  - JOUR
T1  - Managing networks through context: Graph visualization and exploration
JO  - Computer Networks
VL  - 54
IS  - 16
SP  - 2809
EP  - 2824
PY  - 2010/11/15/
T2  - Managing Emerging Computing Environments
AU  - Liao, Qi
AU  - Blaich, Andrew
AU  - VanBruggen, Dirk
AU  - Striegel, Aaron
SN  - 1389-1286
DO  - http://dx.doi.org/10.1016/j.comnet.2010.07.015
UR  - http://www.sciencedirect.com/science/article/pii/S1389128610002446
KW  - Enterprise network management
KW  - Security
KW  - Visualization
KW  - Context
KW  - Graphs
KW  - Visual mining
KW  - Interactive exploration
KW  - Forensics
AB  - With the increasing prevalence of multi-user environments in distributed systems, it has become an increasingly challenging task to precisely identify who is doing what on an enterprise network. Current management systems that rely on inference for user identity and application are not capable of accurately reporting and managing a large-scale network due to the coarseness of the collected data or scaling of the collection mechanism. We propose a system that focuses data collection in the form of local context, i.e. the precise user and application associated with a network connection. Through the use of dynamic correlation and novel graph modeling, we developed a visualization tool called ENAVis (the work appeared in earlier form in [1] and received USENIX best paper award). (Enterprise Network Activities Visualization). ENAVis aids a real-world administrator in allowing them to more efficiently manage and gain insight about the connectivity between hosts, users, applications and data access offering significant streamlining of the management process.
ER  - 

TY  - JOUR
T1  - Information security incident management: Current practice as reported in the literature
JO  - Computers & Security
VL  - 45
IS  - 
SP  - 42
EP  - 57
PY  - 2014/9//
T2  - 
AU  - Tøndel, Inger Anne
AU  - Line, Maria B.
AU  - Jaatun, Martin Gilje
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/j.cose.2014.05.003
UR  - http://www.sciencedirect.com/science/article/pii/S0167404814000819
KW  - Information security
KW  - Incident management
KW  - Incident response
KW  - ISO/IEC 27035
KW  - Systematic review
AB  - Abstract
This paper reports results of a systematic literature review on current practice and experiences with incident management, covering a wide variety of organisations. Identified practices are summarised according to the incident management phases of ISO/IEC 27035. The study shows that current practice and experience seem to be in line with the standard. We identify some inspirational examples that will be useful for organisations looking to improve their practices, and highlight which recommended practices generally are challenging to follow. We provide suggestions for addressing the challenges, and present identified research needs within information security incident management.
ER  - 

TY  - JOUR
T1  - Computer evidence: an overview of forensic procedures, tools and techniques : Edward Wilding, Network Security Management
JO  - Computers & Security
VL  - 15
IS  - 5
SP  - 417
EP  - 
PY  - 1996///
T2  - 

SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/0167-4048(96)82633-2
UR  - http://www.sciencedirect.com/science/article/pii/0167404896826332
ER  - 

TY  - JOUR
T1  - Trust in digital records: An increasingly cloudy legal area
JO  - Computer Law & Security Review
VL  - 28
IS  - 5
SP  - 522
EP  - 531
PY  - 2012/10//
T2  - 
AU  - Duranti, Luciana
AU  - Rogers, Corinne
SN  - 0267-3649
DO  - http://dx.doi.org/10.1016/j.clsr.2012.07.009
UR  - http://www.sciencedirect.com/science/article/pii/S0267364912001458
KW  - Digital records
KW  - Digital forensics
KW  - Cloud computing
KW  - Law of evidence
KW  - Digital documentary evidence
AB  - Trust has been defined in many ways, but at its core it involves acting without the knowledge needed to act. Trust in records depends on four types of knowledge about the creator or custodian of the records: reputation, past performance, competence, and the assurance of confidence in future performance. For over half a century society has been developing and adopting new computer technologies for business and communications in both the public and private realm. Frameworks for establishing trust have developed as technology has progressed. Today, individuals and organizations are increasingly saving and accessing records in cloud computing infrastructures, where we cannot assess our trust in records solely on the four types of knowledge used in the past. Drawing on research conducted at the University of British Columbia into the nature of digital records and their trustworthiness, this article presents the conceptual archival and digital forensic frameworks of trust in records and data, and explores the common law legal framework within which questions of trust in documentary evidence are being tested. Issues and challenges specific to cloud computing are introduced.
ER  - 

TY  - JOUR
T1  - Cloud computing and its implications for cybercrime investigations in Australia
JO  - Computer Law & Security Review
VL  - 29
IS  - 2
SP  - 152
EP  - 163
PY  - 2013/4//
T2  - 
AU  - Hooper, Christopher
AU  - Martini, Ben
AU  - Choo, Kim-Kwang Raymond
SN  - 0267-3649
DO  - http://dx.doi.org/10.1016/j.clsr.2013.01.006
UR  - http://www.sciencedirect.com/science/article/pii/S0267364913000241
KW  - Cloud computing
KW  - Cybercrime
KW  - Digital forensics
KW  - Law enforcement investigations
KW  - Mutual legal assistance
KW  - Australian Law
KW  - Council of Europe Convention on Cybercrime
KW  - Jurisdictional issues
AB  - The advent of cloud computing has led to a dispersal of user data across international borders. More than ever before, law enforcement investigations into cybercrime and online criminal activity require cooperation between agencies from multiple countries. This paper examines recent changes to the law in Australia in relation to the power of law enforcement agencies to effectively investigate cybercrime insofar as individuals and organisations make use of cloud infrastructure in connection with criminal activity. It concludes that effective law enforcement operations in this area require harmonious laws across jurisdictions and streamlines procedures for granting assistance between law enforcement agencies. In conjunction with these mechanical developments, this paper posits that law enforcement officers require a systematised understanding of cloud infrastructure and its operation in order to effectively make use of their powers.
ER  - 

TY  - JOUR
T1  - Toward a general collection methodology for Android devices
JO  - Digital Investigation
VL  - 8, Supplement
IS  - 
SP  - S14
EP  - S24
PY  - 2011/8//
T2  - 11th Annual Digital Forensics Research Conference
AU  - Vidas, Timothy
AU  - Zhang, Chengye
AU  - Christin, Nicolas
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2011.05.003
UR  - http://www.sciencedirect.com/science/article/pii/S1742287611000272
KW  - Android framework
KW  - Mobile devices
KW  - Digital forensics
KW  - Collection
KW  - Acquisition
AB  - The Android platform has been deployed across a wide range of devices, predominately mobile phones, bringing unprecedented common software features to a diverse set of devices independent of carrier and manufacturer. Modern digital forensics processes differentiate collection and analysis, with collection ideally only occurring once and the subsequent analysis relying upon proper collection. After exploring special device boot modes and Android’s partitioning schema we detail the composition of an Android bootable image and discuss the creation of such an image designed for forensic collection. The major contribution of this paper is a general process for data collection of Android devices and related results of experiments carried out on several specific devices.
ER  - 

TY  - JOUR
T1  - Performance analysis of Bayesian networks and neural networks in classification of file system activities
JO  - Computers & Security
VL  - 31
IS  - 4
SP  - 391
EP  - 401
PY  - 2012/6//
T2  - 
AU  - Khan, Muhammad Naeem Ahmed
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/j.cose.2012.03.003
UR  - http://www.sciencedirect.com/science/article/pii/S0167404812000533
KW  - Digital forensics
KW  - Computer forensic analysis
KW  - Digital evidence
KW  - Neural networks
KW  - Bayesian learning
KW  - Bayesian decision theory
AB  - Precise comprehension of a file system state at any given time is vital for performing digital forensic analyses. To uncover evidence of the digital crime, the logical representation of file system activities helps reconstruct post-event timeline of the unauthorized or malicious accesses made on a system. This paper describes a comparative performance analysis of the Bayesian networks and neural networks techniques to classify the state of file system activities in terms of execution of applications based on the pattern of manipulation of specific files during certain period of time. In particular, this paper discusses the construction of a Bayesian networks and neural networks from the predetermined knowledge of the manipulation of file system artifacts and their corresponding metadata information by a set of software applications. The variability amongst the execution patterns of various applications indicate that the Bayesian network-based model is a more appropriate tool as compared to neural networks because of its ability to learn and detect patterns even from an incomplete dataset. The focus of this paper is to highlight intrinsic significance of the learning approach of Bayesian network methodology in comparison to the techniques used for supervised learning in ordinary neural networks. The paper also highlights the efficacy of Bayesian network technique to proficiently handle large volumes of datasets.
ER  - 

TY  - JOUR
T1  - Privacy-preserving network flow recording
JO  - Digital Investigation
VL  - 8, Supplement
IS  - 
SP  - S90
EP  - S100
PY  - 2011/8//
T2  - 11th Annual Digital Forensics Research Conference
AU  - Shebaro, Bilal
AU  - Crandall, Jedidiah R.
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2011.05.011
UR  - http://www.sciencedirect.com/science/article/pii/S1742287611000351
KW  - NetFlow
KW  - Network forensics
KW  - Identity based encryption
KW  - Privacy preserving semantics
KW  - Statistical database
AB  - Network flow recording is an important tool with applications that range from legal compliance and security auditing to network forensics, troubleshooting, and marketing. Unfortunately, current network flow recording technologies do not allow network operators to enforce a privacy policy on the data that is recorded, in particular how this data is stored and used within the organization. Challenges to building such a technology include the public key infrastructure, scalability, and gathering statistics about the data while still preserving privacy.

We present a network flow recording technology that addresses these challenges by using Identity Based Encryption in combination with privacy-preserving semantics for on-the-fly statistics. We argue that our implementation supports a wide range of policies that cover many current applications of network flow recording. We also characterize the performance and scalability of our implementation and find that the encryption and statistics scale well and can easily keep up with the rate at which commodity systems can capture traffic, with a couple of interesting caveats about the size of the subnet that data is being recorded for and how statistics generation is affected by implementation details. We conclude that privacy-preserving network flow recording is possible at 10 gigabit rates for subnets as large as a /20 (4096 hosts).

Because network flow recording is one of the most serious threats to web privacy today, we believe that developing technology to enforce a privacy policy on the recorded data is an important first step before policy makers can make decisions about how network operators can and should store and use network flow data. Our goal in this paper is to explore the tradeoffs of performance and scalability vs. privacy, and the usefulness of the recorded data in forensics vs. privacy.
ER  - 

TY  - JOUR
T1  - Security and privacy in electronic health records: A systematic literature review
JO  - Journal of Biomedical Informatics
VL  - 46
IS  - 3
SP  - 541
EP  - 562
PY  - 2013/6//
T2  - 
AU  - Fernández-Alemán, José Luis
AU  - Señor, Inmaculada Carrión
AU  - Lozoya, Pedro Ángel Oliver
AU  - Toval, Ambrosio
SN  - 1532-0464
DO  - http://dx.doi.org/10.1016/j.jbi.2012.12.003
UR  - http://www.sciencedirect.com/science/article/pii/S1532046412001864
KW  - Electronic health records
KW  - Systematic review
KW  - Privacy
KW  - Confidentiality
KW  - Security
KW  - Standards
AB  - Objective
To report the results of a systematic literature review concerning the security and privacy of electronic health record (EHR) systems.
Data sources
Original articles written in English found in MEDLINE, ACM Digital Library, Wiley InterScience, IEEE Digital Library, Science@Direct, MetaPress, ERIC, CINAHL and Trip Database.
Study selection
Only those articles dealing with the security and privacy of EHR systems.
Data extraction
The extraction of 775 articles using a predefined search string, the outcome of which was reviewed by three authors and checked by a fourth.
Results
A total of 49 articles were selected, of which 26 used standards or regulations related to the privacy and security of EHR data. The most widely used regulations are the Health Insurance Portability and Accountability Act (HIPAA) and the European Data Protection Directive 95/46/EC. We found 23 articles that used symmetric key and/or asymmetric key schemes and 13 articles that employed the pseudo anonymity technique in EHR systems. A total of 11 articles propose the use of a digital signature scheme based on PKI (Public Key Infrastructure) and 13 articles propose a login/password (seven of them combined with a digital certificate or PIN) for authentication. The preferred access control model appears to be Role-Based Access Control (RBAC), since it is used in 27 studies. Ten of these studies discuss who should define the EHR systems’ roles. Eleven studies discuss who should provide access to EHR data: patients or health entities. Sixteen of the articles reviewed indicate that it is necessary to override defined access policies in the case of an emergency. In 25 articles an audit-log of the system is produced. Only four studies mention that system users and/or health staff should be trained in security and privacy.
Conclusions
Recent years have witnessed the design of standards and the promulgation of directives concerning security and privacy in EHR systems. However, more work should be done to adopt these regulations and to deploy secure EHR systems.
ER  - 

TY  - JOUR
T1  - A survey of information security incident handling in the cloud
JO  - Computers & Security
VL  - 49
IS  - 
SP  - 45
EP  - 69
PY  - 2015/3//
T2  - 
AU  - Ab Rahman, Nurul Hidayah
AU  - Choo, Kim-Kwang Raymond
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/j.cose.2014.11.006
UR  - http://www.sciencedirect.com/science/article/pii/S0167404814001680
KW  - Capability Maturity Model For Services (CMMI-SVC)
KW  - Cloud computing
KW  - Cloud response
KW  - Incident handling
KW  - Incident management
KW  - Incident response
AB  - Abstract
Incident handling strategy is one key strategy to mitigate risks to the confidentiality, integrity and availability (CIA) of organisation assets, as well as minimising loss (e.g. financial, reputational and legal) particularly as organisations move to the cloud. In this paper, we surveyed existing incident handling and digital forensic literature with the aims of contributing to the knowledge gap(s) in handling incidents in the cloud environment. 139 English language publications between January 2009 and May 2014 were located by searching various sources including the websites of standard bodies (e.g. National Institute of Standards and Technology) and academic databases (e.g. Google Scholar, IEEEXplore, ACM Digital Library, Springer and ScienceDirect). We then propose a conceptual cloud incident handling model that brings together incident handling, digital forensic and the Capability Maturity Model for Services to more effectively handle incidents for organisations using the cloud. A discussion of open research issues concludes this survey.
ER  - 

TY  - JOUR
T1  - Medical image security in a HIPAA mandated PACS environment
JO  - Computerized Medical Imaging and Graphics
VL  - 27
IS  - 2–3
SP  - 185
EP  - 196
PY  - 2003/3//
Y2  - 2003/6//
T2  - Picture Archiving and Communication Systems 20 Years Later
AU  - Cao, F
AU  - Huang, H.K
AU  - Zhou, X.Q
SN  - 0895-6111
DO  - http://dx.doi.org/10.1016/S0895-6111(02)00073-3
UR  - http://www.sciencedirect.com/science/article/pii/S0895611102000733
KW  - Data encryption
KW  - Picture archiving and communication system security
KW  - Image integrity
KW  - Digital imaging and communication in medicine compliance
KW  - Health insurance portability and accountability act
AB  - Medical image security is an important issue when digital images and their pertinent patient information are transmitted across public networks. Mandates for ensuring health data security have been issued by the federal government such as Health Insurance Portability and Accountability Act (HIPAA), where healthcare institutions are obliged to take appropriate measures to ensure that patient information is only provided to people who have a professional need. Guidelines, such as digital imaging and communication in medicine (DICOM) standards that deal with security issues, continue to be published by organizing bodies in healthcare. However, there are many differences in implementation especially for an integrated system like picture archiving and communication system (PACS), and the infrastructure to deploy these security standards is often lacking. Over the past 6 years, members in the Image Processing and Informatics Laboratory, Childrens Hospital, Los Angeles/University of Southern California, have actively researched image security issues related to PACS and teleradiology. The paper summarizes our previous work and presents an approach to further research on the digital envelope (DE) concept that provides image integrity and security assurance in addition to conventional network security protection. The DE, including the digital signature (DS) of the image as well as encrypted patient information from the DICOM image header, can be embedded in the background area of the image as an invisible permanent watermark. The paper outlines the systematic development, evaluation and deployment of the DE method in a PACS environment. We have also proposed a dedicated PACS security server that will act as an image authority to check and certify the image origin and integrity upon request by a user, and meanwhile act also as a secure DICOM gateway to the outside connections and a PACS operation monitor for HIPAA supporting information.
ER  - 

TY  - JOUR
T1  - The growing need for on-scene triage of mobile devices
JO  - Digital Investigation
VL  - 6
IS  - 3–4
SP  - 112
EP  - 124
PY  - 2010/5//
T2  - Embedded Systems Forensics: Smart Phones, GPS Devices, and Gaming Consoles
AU  - Mislan, Richard P.
AU  - Casey, Eoghan
AU  - Kessler, Gary C.
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2010.03.001
UR  - http://www.sciencedirect.com/science/article/pii/S1742287610000149
KW  - Mobile device forensics
KW  - Cell phone forensics
KW  - On-scene triage inspection
KW  - Mobile device technician
AB  - The increasing number of mobile devices being submitted to Digital Forensic Laboratories (DFLs) is creating a backlog that can hinder investigations and negatively impact public safety and the criminal justice system. In a military context, delays in extracting intelligence from mobile devices can negatively impact troop and civilian safety as well as the overall mission. To address this problem, there is a need for more effective on-scene triage methods and tools to provide investigators with information in a timely manner, and to reduce the number of devices that are submitted to DFLs for analysis. Existing tools that are promoted for on-scene triage actually attempt to fulfill the needs of both on-scene triage and in-lab forensic examination in a single solution. On-scene triage has unique requirements because it is a precursor to and distinct from the forensic examination process, and may be performed by mobile device technicians rather than forensic analysts. This paper formalizes the on-scene triage process, placing it firmly in the overall forensic handling process and providing guidelines for standardization of on-scene triage. In addition, this paper outlines basic requirements for automated triage tools.
ER  - 

TY  - JOUR
T1  - Advanced evidence collection and analysis of web browser activity
JO  - Digital Investigation
VL  - 8, Supplement
IS  - 
SP  - S62
EP  - S70
PY  - 2011/8//
T2  - 11th Annual Digital Forensics Research Conference
AU  - Oh, Junghoon
AU  - Lee, Seungbong
AU  - Lee, Sangjin
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2011.05.008
UR  - http://www.sciencedirect.com/science/article/pii/S1742287611000326
KW  - Web browser forensics
KW  - Integrated timeline analysis
KW  - Search word analysis
KW  - Restoration of deleted web browser information
KW  - URL decoding
AB  - A Web browser is an essential application program for accessing the Internet. If a suspect uses the Internet as a source of information, the evidence related to the crime would be saved in the log file of the Web browser. Therefore, investigating the Web browser’s log file can help to collect information relevant to the case. After considering existing research and tools, this paper suggests a new evidence collection and analysis methodology and tool to aid this process.
ER  - 

TY  - JOUR
T1  - A survey of security in multi-agent systems
JO  - Expert Systems with Applications
VL  - 39
IS  - 5
SP  - 4835
EP  - 4846
PY  - 2012/4//
T2  - 
AU  - Cavalcante, Rodolfo Carneiro
AU  - Bittencourt, Ig Ibert
AU  - da Silva, Alan Pedro
AU  - Silva, Marlos
AU  - Costa, Evandro
AU  - Santos, Robério
SN  - 0957-4174
DO  - http://dx.doi.org/10.1016/j.eswa.2011.09.130
UR  - http://www.sciencedirect.com/science/article/pii/S0957417411014539
KW  - Agents
KW  - Multi-agent systems
KW  - Security
KW  - Security in MAS
KW  - Multi-agents
AB  - Multi-agent systems (MAS) are a relatively new software paradigm that is being widely accepted in several application domains to address large and complex tasks. However, with the use of MAS in open, distributed and heterogeneous applications, the security issues may endanger the success of the application. The goal of this research is to identify the security issues faced by MAS and to survey the current state of the art of this field of knowledge. In order to do it, this paper examines the basic concepts of security in computing, and some characteristics of agents and multi-agent systems that introduce new threats and ways to attack. After this, some models and architectures proposed in the literature are presented and analyzed.
ER  - 

TY  - JOUR
T1  - Measures of retaining digital evidence to prosecute computer-based cyber-crimes
JO  - Computer Standards & Interfaces
VL  - 29
IS  - 2
SP  - 216
EP  - 223
PY  - 2007/2//
T2  - 
AU  - Wang, Shiuh-Jeng
SN  - 0920-5489
DO  - http://dx.doi.org/10.1016/j.csi.2006.03.008
UR  - http://www.sciencedirect.com/science/article/pii/S0920548906000456
KW  - Digital evidence
KW  - Investigation
KW  - Computer forensics
KW  - Cyber-crime
AB  - With the rapid growth of computer and network systems in recent years, there has also been a corresponding increase in cyber-crime. Cyber-crime takes many forms and has garnered much attention in the media, making information security a more urgent and important priority. In order to fight cyber-crime, criminal evidence must be gathered from these computer-based systems. This is quite different from the collection of conventional criminal evidence and can confuse investigators attempting to deal with the forensics of cyber-crime, highlighting the importance of computer forensics. In this paper, we offer solutions to guard against cyber-crime through the implementation of software toolkits for computer-based systems. In this way, those who engage in criminal acts in cyber-space can be more easily apprehended.
ER  - 

TY  - JOUR
T1  - Analysis of recommended cloud security controls to validate OpenPMF “policy as a service”
JO  - Information Security Technical Report
VL  - 16
IS  - 3–4
SP  - 131
EP  - 141
PY  - 2011/8//
Y2  - 2011/11//
T2  - Cloud Security
AU  - Lang, Ulrich
AU  - Schreiner, Rudolf
SN  - 1363-4127
DO  - http://dx.doi.org/10.1016/j.istr.2011.08.001
UR  - http://www.sciencedirect.com/science/article/pii/S136341271100046X
KW  - Cloud
KW  - Security
KW  - Policy
KW  - Authorization management
KW  - Access policy
KW  - Compliance
KW  - Model-driven security
KW  - Accreditation
KW  - Audit policy
KW  - Application security
KW  - XACML
KW  - OpenPMF
KW  - NIST 800-53
KW  - NIST 800-147
KW  - NIST IR 7628
KW  - PCI-DSS
KW  - HIPAA
AB  - This paper describes some of the findings of a cloud research project the authors carried out in Q2/2011. As part of the project, the authors first identified security concerns related to cloud computing, and gaps in cloud-related standards/regulations. The authors then identified several hard-to-implement, but highly cloud-relevant, security requirements in numerous cloud (and non-cloud) regulations and guidance documents, especially related to “least privilege”, “information flow control”, and “incident monitoring/auditing/analysis”. Further study revealed that there are significant cloud technology gaps in cloud (and non-cloud) platforms, which make it difficult to effectively implement those security policy requirements. The project concluded that model-driven security policy automation offered as a cloud service and tied into the protected cloud platform is ideally suited to achieve correct, consistent, low-effort/cost policy implementation for cloud applications.
ER  - 

TY  - JOUR
T1  - Network intrusion investigation – Preparation and challenges
JO  - Digital Investigation
VL  - 3
IS  - 3
SP  - 118
EP  - 126
PY  - 2006/9//
T2  - 
AU  - Johnston, Andy
AU  - Reust, Jessica
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2006.08.001
UR  - http://www.sciencedirect.com/science/article/pii/S1742287606000922
KW  - Intrusion investigation
KW  - Incident response
KW  - Network forensics
KW  - Digital forensic examination
KW  - Compromise of sensitive information
KW  - Forensic preparedness
AB  - As new legislation is written mandating notification of affected parties following the compromise of confidential data, reliable investigative procedures into unauthorized access of such data assume increasing importance. The increasing costs and penalties associated with exposure of sensitive data can be mitigated through forensic preparation and the ability to employ digital forensics. A case study of the compromise of several systems containing sensitive data is outlined, with particular attention given to the procedures followed during the initial response and their impact on the subsequent digital forensic examination. Practical problems and challenges that arise in intrusion investigations are discussed, along with solutions and methodologies to address these issues. This case study illustrates both the importance of evaluating the evidence analyzed and of corroborating findings and conclusions with multiple independent sources of evidence. An initial response that incorporates forensic procedures provides a solid foundation for a successful and thorough forensic examination.
ER  - 

TY  - JOUR
T1  - On Incident Handling and Response: A state-of-the-art approach
JO  - Computers & Security
VL  - 25
IS  - 5
SP  - 351
EP  - 370
PY  - 2006/7//
T2  - 
AU  - Mitropoulos, Sarandis
AU  - Patsos, Dimitrios
AU  - Douligeris, Christos
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/j.cose.2005.09.006
UR  - http://www.sciencedirect.com/science/article/pii/S0167404805001574
KW  - Incident Handling
KW  - Incident Response
KW  - Computer forensics
KW  - Internet forensics
KW  - Software forensics
KW  - Trace-back mechanisms
AB  - Incident Response has always been an important aspect of Information Security but it is often overlooked by security administrators. Responding to an incident is not solely a technical issue but has many management, legal, technical and social aspects that are presented in this paper. We propose a detailed management framework along with a complete structured methodology that contains best practices and recommendations for appropriately handling a security incident. We also present the state-of-the art technology in computer, network and software forensics as well as automated trace-back artifacts, schemas and protocols. Finally, we propose a generic Incident Response process within a corporate environment.
ER  - 

TY  - JOUR
T1  - Enhancing Security in Cloud Using Trusted Monitoring Framework
JO  - Procedia Computer Science
VL  - 48
IS  - 
SP  - 198
EP  - 203
PY  - 2015///
T2  - International Conference on Computer, Communication and Convergence (ICCC 2015)
AU  - Fera, M. Arun
AU  - manikandaprabhu, C.
AU  - Natarajan, Ilakiya
AU  - Brinda, K.
AU  - Darathiprincy, R.
SN  - 1877-0509
DO  - http://dx.doi.org/10.1016/j.procs.2015.04.170
UR  - http://www.sciencedirect.com/science/article/pii/S1877050915006791
AB  - Abstract
Cloud computing is a technology that provides network based services on demand. Cloud computing technology provides advantages to end users and business organizations. Few notable advantages are cost efficiency, increased storage capacity, backup and recovery, continuous resource availability and location independence. Data owners host their private data in the cloud and worry about unauthorized access of their data. They feel uncomfortable about any user misusing their private data. This insecure feeling of data owners holds them back from using cloud services. Any unauthorized users accessing the owner's private data leads to accountability issues. We design a trusted monitoring framework, which provides a chain of trust that excludes the untrusted privileged domain, as well as utilizing the trusted computing technology to ensure the integrity of the monitoring environment. To solve the accountability issue, a mechanism to monitor the actual data usage is proposed. This approach grants access rights to users based on their role and also monitors every access to the owner's data, verifying that the service level agreements have been violated or not.
ER  - 

TY  - JOUR
T1  - A framework for post-event timeline reconstruction using neural networks
JO  - Digital Investigation
VL  - 4
IS  - 3–4
SP  - 146
EP  - 157
PY  - 2007/9//
Y2  - 2007/12//
T2  - 
AU  - Khan, M.N.A.
AU  - Chatwin, C.R.
AU  - Young, R.C.D.
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2007.11.001
UR  - http://www.sciencedirect.com/science/article/pii/S1742287607000837
KW  - Computer forensics
KW  - Digital investigation
KW  - Event reconstruction
KW  - Digital evidence
KW  - Digital forensic analysis
KW  - Neural networks
AB  - Post-event timeline reconstruction plays a critical role in forensic investigation and serves as a means of identifying evidence of the digital crime. We present an artificial neural networks based approach for post-event timeline reconstruction using the file system activities. A variety of digital forensic tools have been developed during the past two decades to assist computer forensic investigators undertaking digital timeline analysis, but most of the tools cannot handle large volumes of data efficiently. This paper looks at the effectiveness of employing neural network methodology for computer forensic analysis by preparing a timeline of relevant events occurring on a computing machine by tracing the previous file system activities. Our approach consists of monitoring the file system manipulations, capturing file system snapshots at discrete intervals of time to characterise the use of different software applications, and then using this captured data to train a neural network to recognise execution patterns of the application programs. The trained version of the network may then be used to generate a post-event timeline of a seized hard disk to verify the execution of different applications at different time intervals to assist in the identification of available evidence.
ER  - 

TY  - JOUR
T1  - A security framework for a workflow-based grid development platform
JO  - Computer Standards & Interfaces
VL  - 32
IS  - 5–6
SP  - 230
EP  - 245
PY  - 2010/10//
T2  - Information and communications security, privacy and trust: Standards and Regulations
AU  - Vivas, José L.
AU  - Fernández-Gago, Carmen
AU  - Lopez, Javier
AU  - Benjumea, Andrés
SN  - 0920-5489
DO  - http://dx.doi.org/10.1016/j.csi.2009.04.001
UR  - http://www.sciencedirect.com/science/article/pii/S0920548909000270
KW  - Grid
KW  - Security framework
KW  - Security services
KW  - Virtual organizations
AB  - This paper describes the security framework that is to be developed for the generic grid platform created for the project GREDIA. This platform is composed of several components that need to be secured. The platform uses the OGSA standards, so that the security framework will follow GSI, the portion of Globus that implements security. Thus, we will show the security features that GSI already provides and we will outline which others need to be created or enhanced.
ER  - 

TY  - JOUR
T1  - Three cyber-security strategies to mitigate the impact of a data breach
JO  - Network Security
VL  - 2015
IS  - 1
SP  - 5
EP  - 8
PY  - 2015/1//
T2  - 
AU  - Densham, Ben
SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/S1353-4858(15)70007-3
UR  - http://www.sciencedirect.com/science/article/pii/S1353485815700073
AB  - We know that our mindsets need to shift these days and we must start by expecting to be hacked. But what then? How do we really manage an effective, secure environment? What steps do we need to consider?

The most common approach to keeping organisations safe is through layers of security. Yet frequent high-profile attacks show that this approach is no longer enough to ward off sophisticated threats.

Organisations must now alter their mindsets to expect a breach. It's better to assume your systems will be compromised. And with this in mind, they must have a ‘response in depth’ plan in order to mitigate the breach when it comes. Ben Densham of Nettitude provides a step-by-step guide to implementing an effective cyber-security strategy to ensure a potential disaster can be turned into a manageable incident.
ER  - 

TY  - JOUR
T1  - Security and performance in service-oriented applications: Trading off competing objectives
JO  - Decision Support Systems
VL  - 50
IS  - 1
SP  - 336
EP  - 346
PY  - 2010/12//
T2  - 
AU  - Zo, Hangjung
AU  - Nazareth, Derek L.
AU  - Jain, Hemant K.
SN  - 0167-9236
DO  - http://dx.doi.org/10.1016/j.dss.2010.09.002
UR  - http://www.sciencedirect.com/science/article/pii/S0167923610001715
KW  - Service-oriented computing
KW  - Application composition
KW  - Performance
KW  - Security
KW  - Multiple criteria decision making
AB  - As service-oriented computing becomes more prevalent, an increasing number of applications will be developed using existing software components with standard interfaces. These components may be developed in-house, may represent purchased software, or may involve vendor located leased services. The use of multiple services, possibly utilizing different technologies and different sources, has significant implications for the performance and security of these applications to support a business process effectively. Estimating performance and security in this distributed environment is a hard problem. This paper examines how performance and security measures can be developed for service-based applications. Business processes are broken down into constituent tasks and a formal mechanism is developed for deriving performance and security measures for the application. Given the competing nature of these two objectives, a tradeoff strategy is utilized wherein managers can trade improved performance for reduced security or vice versa. As the number of alternative services for each task increases, the composition problem becomes combinatorially explosive. A genetic algorithm approach is adopted to find the Pareto optimal set of services that can be assembled to support the business process. An application to a real-world business process illustrates its effectiveness.
ER  - 

TY  - JOUR
T1  - File Marshal: Automatic extraction of peer-to-peer data
JO  - Digital Investigation
VL  - 4, Supplement
IS  - 
SP  - 43
EP  - 48
PY  - 2007/9//
T2  - 
AU  - Adelstein, Frank
AU  - Joyce, Robert A.
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2007.06.016
UR  - http://www.sciencedirect.com/science/article/pii/S1742287607000400
KW  - Peer-to-peer
KW  - P2P
KW  - Forensics
KW  - LimeWire
KW  - File sharing
AB  - Digital forensic investigators often find peer-to-peer, or file sharing, software present on the computers, or the images of the disks, that they examine. Investigators must first determine what P2P software is present and where the associated information is stored, retrieve the information from the appropriate directories, and then analyze the results. File Marshal is a tool that will automatically detect and analyze peer-to-peer client use on a disk. The tool automates what is currently a manual and labor intensive process. It will determine what clients currently are or have been installed on a machine, and then extracts per-user usage information, specifically a list of peer servers contacted, and files that were shared and downloaded. The tool was designed to perform its actions in a forensically sound way, including maintaining a detailed audit trail of all actions performed. File Marshal is extensible, using a configuration file to specify details about specific peer-to-peer clients (e.g., location of log files and registry keys indicating installation). This paper describes the general design and features of File Marshal, its current status, and the plans for continued development and release. When complete, File Marshal, a National Institute of Justice funded effort, will be disseminated to law enforcement at no cost.
ER  - 

TY  - JOUR
T1  - Structure design and test of enterprise security management system with advanced internal security
JO  - Future Generation Computer Systems
VL  - 25
IS  - 3
SP  - 358
EP  - 363
PY  - 2009/3//
T2  - 
AU  - Kim, Seoksoo
AU  - Kim, Soongohn
AU  - Lee, Geuk
SN  - 0167-739X
DO  - http://dx.doi.org/10.1016/j.future.2006.04.019
UR  - http://www.sciencedirect.com/science/article/pii/S0167739X06001026
KW  - IDS
KW  - EMS
KW  - Security
KW  - Firewall
KW  - Internal
KW  - Management
AB  - A security system for a company network is progressing as a ESM (Enterprise Security Management) in an existing security solution foundation. The establishment of the security policy is occupying a very important area in ESM of the security system. We tried to analyze the existing ESM system for this and designed a security solution structure for enhancing the internal security. We applied implementing directly IDS system and tested it. This study examined the structure of security solutions in order to build an enterprise security management system. For this purpose, we analyzed existing enterprise security management systems and, based on the results, proposed a enterprise security management system with reinforced internal security and tested the system. For the test, we used a firewall through log analysis and designed an intra-network using virtual IP system.
ER  - 

TY  - JOUR
T1  - Human factors in information security: The insider threat – Who can you trust these days?
JO  - Information Security Technical Report
VL  - 14
IS  - 4
SP  - 186
EP  - 196
PY  - 2009/11//
T2  - Human Factors in Information Security
AU  - Colwill, Carl
SN  - 1363-4127
DO  - http://dx.doi.org/10.1016/j.istr.2010.04.004
UR  - http://www.sciencedirect.com/science/article/pii/S1363412710000051
KW  - Insider threat
KW  - Human factors
KW  - Security risk management
KW  - Outsourcing
AB  - This paper examines some of the key issues relating to insider threats to information security and the nature of loyalty and betrayal in the context of organisational, cultural factors and changing economic and social factors. It is recognised that insiders pose security risks due to their legitimate access to facilities and information, knowledge of the organisation and the location of valuable assets. Insiders will know how to achieve the greatest impact whilst leaving little evidence. However, organisations may not have employed effective risk management regimes to deal with the speed and scale of change, for example the rise of outsourcing. Outsourcing can lead to the fragmentation of protection barriers and controls and increase the number of people treated as full time employees. Regional and cultural differences will manifest themselves in differing security threat and risk profiles. At the same time, the recession is causing significant individual (and organisational) uncertainty and may prompt an increase in abnormal behaviour in long-term employees and managers – those traditionally most trusted – including members of the security community. In this environment, how can organisations know who to trust and how to maintain this trust?

The paper describes a practitioner’s view of the issue and the approaches used by BT to assess and address insider threats and risks. Proactive measures need to be taken to mitigate against insider attacks rather than reactive measures after the event. A key priority is to include a focus on insiders within security risk assessments and compliance regimes. The application of technology alone will not provide solutions. Security controls need to be workable in a variety of environments and designed, implemented and maintained with people’s behaviour in mind. Solutions need to be agile and build and maintain trust and secure relationships over time. This requires a focus on human factors, education and awareness and greater attention on the security ‘aftercare’ of employees and third parties.
ER  - 

TY  - JOUR
T1  - A new concentric-circle visualization of multi-dimensional data and its application in network security
JO  - Journal of Visual Languages & Computing
VL  - 21
IS  - 4
SP  - 194
EP  - 208
PY  - 2010/8//
T2  - Part Special Issue on Graph Visualization
AU  - Fu Lu, Liang
AU  - Wan Zhang, Jia
AU  - Lin Huang, Mao
AU  - Fu, Lei
SN  - 1045-926X
DO  - http://dx.doi.org/10.1016/j.jvlc.2010.05.002
UR  - http://www.sciencedirect.com/science/article/pii/S1045926X10000285
KW  - Concentric-circle coordinate
KW  - Multi-dimensional data visualization
KW  - Crossing reduction
KW  - Network visualization
KW  - Security visualization
KW  - Network intrusion detection
KW  - Polycurve
AB  - With the rapid growth of networked data communications in size and complexity, network administrators today are facing more challenges to protect their networked computers and devices from all kinds of attacks. This paper proposes a new concentric-circle visualization method for visualizing multi-dimensional network data. This method can be used to identify the main features of network attacks, such as DDoS attack, by displaying their recognizable visual patterns. To reduce the edge overlaps and crossings, we arrange multiple axes displayed as concentric circles rather than the traditional parallel lines. In our method, we use polycurves to link values (vertexes) rather than polylines used in parallel coordinate approach. Some heuristics are applied in our new method in order to improve the readability of views. We discuss the advantages as well as the limitations of our new method. In comparison with the parallel coordinate visualization, our approach can reduce more than 15% of the edge overlaps and crossings. In the second stage of the method, we have further enhanced the readability of views by increasing the edge crossing angle. Finally, we introduce our prototype system: a visual interactive network scan detection system called CCScanViewer. It is based on our new visualization approach and the experiments have showed that the new approach is effective in detecting attack features from a variety of networking patterns, such as the features of network scans and DDoS attacks.
ER  - 

TY  - JOUR
T1  - Model-driven business process security requirement specification
JO  - Journal of Systems Architecture
VL  - 55
IS  - 4
SP  - 211
EP  - 223
PY  - 2009/4//
T2  - Secure Service-Oriented Architectures (Special Issue on Secure SOA)
AU  - Wolter, Christian
AU  - Menzel, Michael
AU  - Schaad, Andreas
AU  - Miseldine, Philip
AU  - Meinel, Christoph
SN  - 1383-7621
DO  - http://dx.doi.org/10.1016/j.sysarc.2008.10.002
UR  - http://www.sciencedirect.com/science/article/pii/S1383762108001471
KW  - Web service security
KW  - Business process
KW  - Model transformation
KW  - Security annotations
KW  - Access control
AB  - Various types of security goals, such as authentication or confidentiality, can be defined as policies for service-oriented architectures, typically in a manual fashion. Therefore, we foster a model-driven transformation approach from modelled security goals in the context of process models to concrete security implementations. We argue that specific types of security goals may be expressed in a graphical fashion at the business process modelling level which in turn can be transformed into corresponding access control and security policies. In this paper we present security policy and policy constraint models. We further discuss a translation of security annotated business processes into platform specific target languages, such as XACML or AXIS2 security configurations. To demonstrate the suitability of this approach an example transformation is presented based on an annotated process.
ER  - 

TY  - JOUR
T1  - Tool review—WinHex
JO  - Digital Investigation
VL  - 1
IS  - 2
SP  - 114
EP  - 128
PY  - 2004/6//
T2  - 
AU  - Casey, Eoghan
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2004.04.001
UR  - http://www.sciencedirect.com/science/article/pii/S1742287604000295
KW  - Digital forensics tool testing
KW  - Digital evidence preservation
KW  - Forensic examination
KW  - File systems
KW  - Data recovery
AB  - This paper presents strengths and shortcomings of WinHex Specialist Edition (version 11.25 SR-7) in the context of the overall digital forensics process, focusing on its ability to preserve and examine data on storage media. No serious problems were found during non-exhaustive testing of the tool's ability to create a forensic image of a disk, and to verify the integrity of an image. Generally accepted data sets were used to test WinHex's ability to reliably and accurately interpret file date–time stamps, recover deleted files, and search for keywords. The results of these tests are summarized in this paper. Certain advanced examination capabilities were also evaluated, including the creation of custom templates to interpret EXT2/EXT3 file systems. Based on this review, several enhancements are proposed. In addition to these results, this paper demonstrates a systematic approach to evaluating similar forensic tools.
ER  - 

TY  - JOUR
T1  - A correlation method for establishing provenance of timestamps in digital evidence
JO  - Digital Investigation
VL  - 3, Supplement
IS  - 
SP  - 98
EP  - 107
PY  - 2006/9//
T2  - The Proceedings of the 6th Annual Digital Forensic Research Workshop (DFRWS '06)
AU  - Schatz, Bradley
AU  - Mohay, George
AU  - Clark, Andrew
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2006.06.009
UR  - http://www.sciencedirect.com/science/article/pii/S1742287606000715
KW  - Digital forensics
KW  - Digital evidence
KW  - Event correlation
KW  - Reverse engineering
KW  - Timestamp interpretation
AB  - Establishing the time at which a particular event happened is a fundamental concern when relating cause and effect in any forensic investigation. Reliance on computer generated timestamps for correlating events is complicated by uncertainty as to clock skew and drift, environmental factors such as location and local time zone offsets, as well as human factors such as clock tampering. Establishing that a particular computer's temporal behaviour was consistent during its operation remains a challenge. The contributions of this paper are both a description of assumptions commonly made regarding the behaviour of clocks in computers, and empirical results demonstrating that real world behaviour diverges from the idealised or assumed behaviour. We present an approach for inferring the temporal behaviour of a particular computer over a range of time by correlating commonly available local machine timestamps with another source of timestamps. We show that a general characterisation of the passage of time may be inferred from an analysis of commonly available browser records.
ER  - 

TY  - JOUR
T1  - DSS for computer security incident response applying CBR and collaborative response
JO  - Expert Systems with Applications
VL  - 37
IS  - 1
SP  - 852
EP  - 870
PY  - 2010/1//
T2  - 
AU  - Kim, Huy Kang
AU  - Im, Kwang Hyuk
AU  - Park, Sang Chan
SN  - 0957-4174
DO  - http://dx.doi.org/10.1016/j.eswa.2009.05.100
UR  - http://www.sciencedirect.com/science/article/pii/S0957417409005223
KW  - Log analysis
KW  - System security
KW  - Misuse detection
KW  - Anomaly detection
KW  - Decision support system
KW  - Expert system
KW  - RFM analysis methodology
KW  - CBR (case based reasoning)
AB  - Recently, as hacking attempts increase dramatically; most enterprises are forced to employ some safeguards for hacking proof. For example, firewall or IPS (Intrusion Prevention System) selectively accepts the incoming packets, and IDS (Intrusion Detection System) detects the attack attempts from network. The latest version of firewall works in cooperation with IDS to immediately response to hacking attempts. However, it may make false alarms that misjudge normal traffic as hacking traffic and cause network problems to block the normal IP address by false alarms. By these false alarms made by IDS, system administrators or CSOs make wrong decisions and important data may be exposed or the availability of network or server system may be exhausted. Therefore, it is important to minimize the false alarms.

As a way of minimizing false alarms and supporting adequate decisions, we suggest the RFM (Recency, Frequency, Monetary) analysis methodology, which analyzes log files with incorporating three criteria of recency, frequency and monetary with statistical process control chart, and thus leads to an intuitive detection of anomaly and misuse events. Moreover, to cope with hacking attempts proactively, we apply CBR (case based reasoning) to find out similarities between already known hacking patterns and new hacking patterns. With the RFM analysis methodology and CBR, we develop DSS which can minimize false alarms and decrease the time to respond to hacking events. In case that RFM analysis module finds out unknown viruses or worms occurred, this CBR system matches the most similar incident case from case-based database. System administrators can easily get information about how to fix and how we fixed in similar cases. And CSOs can build a blacklist of frequently detected IP addresses and users. This blacklist can be used for incident handling.

Finally, we propose collaborative incident response system with DSS, this distributed agent systems interactively exchange the suspicious users and source IP addresses data and decide who is true-anomalous users and which IP addresses is the most riskiest and then deny all connections from that users and IP addresses automatically with less false-positives.
ER  - 

TY  - JOUR
T1  - Inter-organizational future proof EHR systems: A review of the security and privacy related issues
JO  - International Journal of Medical Informatics
VL  - 78
IS  - 3
SP  - 141
EP  - 160
PY  - 2009/3//
T2  - 
AU  - van der Linden, Helma
AU  - Kalra, Dipak
AU  - Hasman, Arie
AU  - Talmon, Jan
SN  - 1386-5056
DO  - http://dx.doi.org/10.1016/j.ijmedinf.2008.06.013
UR  - http://www.sciencedirect.com/science/article/pii/S1386505608001081
KW  - Computerized Medical Records Systems
KW  - Data security
KW  - Access policy
KW  - Standards
KW  - Networked care
AB  - Objectives
Identification and analysis of privacy and security related issues that occur when health information is exchanged between health care organizations.
Methods
Based on a generic scenario questions were formulated to reveal the occurring issues. Possible answers were verified in literature.
Results
Ensuring secure health information exchange across organizations requires a standardization of security measures that goes beyond organizational boundaries, such as global definitions of professional roles, global standards for patient consent and semantic interoperable audit logs.
Conclusion
As to be able to fully address the privacy and security issues in interoperable EHRs and the long-life virtual EHR it is necessary to realize a paradigm shift from storing all incoming information in a local system to retrieving information from external systems whenever that information is deemed necessary for the care of the patient.
ER  - 

TY  - JOUR
T1  - Automatic high-performance reconstruction and recovery
JO  - Computer Networks
VL  - 51
IS  - 5
SP  - 1361
EP  - 1377
PY  - 2007/4/11/
T2  - From Intrusion Detection to Self-Protection
AU  - Goel, Ashvin
AU  - Feng, Wu-chang
AU  - Feng, Wu-chi
AU  - Maier, David
SN  - 1389-1286
DO  - http://dx.doi.org/10.1016/j.comnet.2006.09.013
UR  - http://www.sciencedirect.com/science/article/pii/S1389128606002465
KW  - Self-healing computers
KW  - Computer forensics
KW  - Operating systems
KW  - Auditing
AB  - Self-protecting systems require the ability to instantaneously detect malicious activity at run-time and prevent execution. We argue that it is impossible to perfectly self-protect systems without false positives due to the limited amount of information one might have at run-time and that eventually some undesirable activity will occur that will need to be rolled back. As a consequence of this, it is important that self-protecting systems have the ability to completely and automatically roll back malicious activity which has occurred.

As the cost of human resources currently dominates the cost of CPU, network, and storage resources, we contend that computing systems should be built with automated analysis and recovery as a primary goal. Towards this end, we describe the design, implementation, and evaluation of Forensix: a robust, high-precision analysis and recovery system for supporting self-healing. The Forensix system records all activity of a target computer and allows for efficient, automated reconstruction of activity when needed. Such a system can be used to automatically detect patterns of malicious activity and selectively undo their operations.

Forensix uses three key mechanisms to improve the accuracy and reduce the human overhead of performing analysis and recovery. First, it performs comprehensive monitoring of the execution of a target system at the kernel event level, giving a high-resolution, application-independent view of all activity. Second, it streams the kernel event information, in real-time, to append-only storage on a separate, hardened, logging machine, making the system resilient to a wide variety of attacks. Third, it uses database technology to support high-level querying of the archived log, greatly reducing the human cost of performing analysis and recovery.
ER  - 

TY  - JOUR
T1  - An empirical study of automatic event reconstruction systems
JO  - Digital Investigation
VL  - 3, Supplement
IS  - 
SP  - 108
EP  - 115
PY  - 2006/9//
T2  - The Proceedings of the 6th Annual Digital Forensic Research Workshop (DFRWS '06)
AU  - Jeyaraman, Sundararaman
AU  - Atallah, Mikhail J.
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2006.06.013
UR  - http://www.sciencedirect.com/science/article/pii/S1742287606000752
KW  - Intrusion analysis
KW  - Digital forensics
KW  - Event reconstruction
KW  - Incident response
AB  - Reconstructing the sequence of computer events that led to a particular event is an essential part of the digital investigation process. The ability to quantify the accuracy of automatic event reconstruction systems is an essential step in standardizing the digital investigation process thereby making it resilient to tactics such as the Trojan horse defense. In this paper, we present findings from an empirical study to measure and compare the accuracy and effectiveness of a suite of such event reconstruction techniques. We quantify (as applicable) the rates of false positives and false negatives, and scalability in terms of both computational burden and memory-usage. Some of our findings are quite surprising in the sense of not matching a priori expectations, and whereas other findings qualitatively match the a priori expectations they were never before quantitatively put to the test to determine the boundaries of their applicability. For example, our results show that automatic event reconstruction systems proposed in literature have very high false-positive rates (up to 96%).
ER  - 

TY  - JOUR
T1  - Improving evidence acquisition from live network sources
JO  - Digital Investigation
VL  - 3
IS  - 2
SP  - 89
EP  - 96
PY  - 2006/6//
T2  - 
AU  - Nikkel, Bruce J.
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2006.05.002
UR  - http://www.sciencedirect.com/science/article/pii/S1742287606000363
KW  - Network forensics
KW  - Live network evidence
KW  - Live network acquisition
KW  - Live network forensics
KW  - NFAT
AB  - The pervasiveness of network technology is causing a shift in the location of digital evidence. What was once largely found on individual disks tied to single individuals is now becoming distributed across remote networked machines, under the control of multiple organizations, and scattered over multiple jurisdictions. The network interactions between these machines are also becoming recognized as a source of network evidence. These live network sources of evidence bring additional challenges which need to be addressed. This paper discusses these issues and suggests some improvements in the methods used for the collection of evidence from live network sources.
ER  - 

TY  - JOUR
T1  - Security in grid computing: A review and synthesis
JO  - Decision Support Systems
VL  - 44
IS  - 4
SP  - 749
EP  - 764
PY  - 2008/3//
T2  - 
AU  - Cody, Erin
AU  - Sharman, Raj
AU  - Rao, Raghav H.
AU  - Upadhyaya, Shambhu
SN  - 0167-9236
DO  - http://dx.doi.org/10.1016/j.dss.2007.09.007
UR  - http://www.sciencedirect.com/science/article/pii/S0167923607001728
KW  - Grid computing
KW  - Information assurance
KW  - Survey and synthesis
KW  - Security
AB  - This paper provides an extensive survey of the different methods of addressing security issues in the grid computing environment, and specifically contributes to the research environment by developing a comprehensive framework for classification of these research endeavors. The framework presented classifies security literature into System Solutions, Behavioral Solutions, Hybrid Solutions and Related Technologies. Each one of these categories is explained in detail in the paper to provide insight as to their unique methods of accomplishing grid security, the types of grid and security situations they apply best to, and the pros and cons for each type of solution. Further, several areas of research were identified in the course of the literature survey where more study is warranted. These avenues for future research are also discussed in this paper. Several types of grid systems exist currently, and the security needs and solutions to address those needs for each type vary as much as the types of systems themselves. This research framework will aid in future research efforts to define, analyze, and address grid security problems for the many varied types of grid setups, as well as the many security situations that each grid may face.
ER  - 

TY  - JOUR
T1  - The enemy within: the inherent security risks of temporary staff
JO  - Computer Fraud & Security
VL  - 2014
IS  - 5
SP  - 5
EP  - 7
PY  - 2014/5//
T2  - 
AU  - Liu, Ching
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(14)70489-0
UR  - http://www.sciencedirect.com/science/article/pii/S1361372314704890
AB  - Since the credit crunch and the subsequent tentative recovery, there has been a boom in the use of temporary staff. According to the Chartered Institute of Personnel and Development's (CIPD) Labour Market Outlook Survey last year, employers reported that 29% of new recruits will be employed on this basis.1

There is a growing issue with fraudsters taking positions with companies as temporary staff with the sole intention of learning their systems and procedures so they can later commit fraud.

The risk of fraud has been exacerbated by the growth of ‘Bring Your Own Device’ (BYOD) on many company networks. IT departments need to effectively control BYOD users, especially temporary ones, and look at what privileges temporary staff are given, explains Ching Liu of Control Risks.
ER  - 

TY  - JOUR
T1  - Beyond lightning: A survey on security challenges in cloud computing
JO  - Computers & Electrical Engineering
VL  - 39
IS  - 1
SP  - 47
EP  - 54
PY  - 2013/1//
T2  - Special issue on Recent Advanced Technologies and Theories for Grid and Cloud Computing and Bio-engineering
AU  - Rong, Chunming
AU  - Nguyen, Son T.
AU  - Jaatun, Martin Gilje
SN  - 0045-7906
DO  - http://dx.doi.org/10.1016/j.compeleceng.2012.04.015
UR  - http://www.sciencedirect.com/science/article/pii/S0045790612000870
AB  - Cloud computing is a model to provide convenient, on-demand access to a shared pool configurable computing resources. In cloud computing, IT-related capabilities are provided as services, accessible without requiring detailed knowledge of the underlying technologies, and with minimal management effort. The great savings promised by the cloud are however offset by the perceived security threats feared by users. This paper gives an overview of cloud computing, and discusses related security challenges. We emphasize that although there are many technological approaches that can improve cloud security, there are currently no one-size-fits-all solutions, and future work has to tackle challenges such as service level agreements for security, as well as holistic mechanisms for ensuring accountability in the cloud.
ER  - 

TY  - JOUR
T1  - Information Security – The Fourth Wave
JO  - Computers & Security
VL  - 25
IS  - 3
SP  - 165
EP  - 168
PY  - 2006/5//
T2  - 
AU  - von Solms, Basie
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/j.cose.2006.03.004
UR  - http://www.sciencedirect.com/science/article/pii/S016740480600054X
KW  - Corporate Governance
KW  - Information Security
KW  - Information Security Management
KW  - Information Security Governance
KW  - Risk management
KW  - Sarbanes–Oxley
KW  - Social engineering
AB  - In a previous article [von Solms, 2000], the development of Information Security up to the year 2000 was characterized as consisting of three waves:•
the technical wave,
•
the management wave, and
•
the institutional wave.


This paper continues this development of Information Security by characterizing the Fourth Wave – that of Information Security Governance.
ER  - 

TY  - JOUR
T1  - A model driven approach for the German health telematics architectural framework and security infrastructure
JO  - International Journal of Medical Informatics
VL  - 76
IS  - 2–3
SP  - 169
EP  - 175
PY  - 2007/2//
Y2  - 2007/3//
T2  - Connecting Medical Informatics and Bio-Informatics - MIE 2005
AU  - Blobel, Bernd
AU  - Pharow, Peter
SN  - 1386-5056
DO  - http://dx.doi.org/10.1016/j.ijmedinf.2006.05.044
UR  - http://www.sciencedirect.com/science/article/pii/S1386505606001572
KW  - Health telematics
KW  - Model driven architecture
KW  - Electronic health record
KW  - Smart cards
KW  - Security
KW  - Privacy
AB  - Shared care concepts such as managed care and continuity of care are based on extended communication and cooperation between different health professionals or between them and the patient respectively. Health information systems and their components, which are very different in their structure, behavior, data and their semantics as well as regarding implementation details used in different environments for different purposes, have to provide intelligent interoperability. Therefore, flexibility, portability, and future orientation must be guaranteed using the newest development of model driven architecture. The ongoing work for the German health telematics platform based on an architectural framework and a security infrastructure is described in some detail. This concept of future proof health information networks with virtual electronic health records as core application starts with multifunctional electronic health cards. It fits into developments currently performed by many other developed countries.
ER  - 

TY  - JOUR
T1  - Distributed component architectures security issues
JO  - Computer Standards & Interfaces
VL  - 27
IS  - 3
SP  - 269
EP  - 284
PY  - 2005/3//
T2  - 
AU  - Gousios, Giorgos
AU  - Aivaloglou, Efthimia
AU  - Gritzalis, Stefanos
SN  - 0920-5489
DO  - http://dx.doi.org/10.1016/j.csi.2004.08.003
UR  - http://www.sciencedirect.com/science/article/pii/S0920548904000984
KW  - Components
KW  - Component architectures security
KW  - CORBA
KW  - J2EE
KW  - .NET
AB  - Enterprise information systems and e-commerce applications are tightly integrated in today's modern enterprises. Component architectures are the base for building such multitier distributed applications. This paper examines the security threats those systems must confront and the solutions proposed by major existing component architectures. A comparative evaluation of both security features and implementation issues is carried out to determine each architecture's strong points and drawbacks.
ER  - 

TY  - JOUR
T1  - A multi-resolution accountable logging and its applications
JO  - Computer Networks
VL  - 
IS  - 
SP  - 
EP  - 
PY  - 
T2  - 
AU  - Fu, Bo
AU  - Xiao, Yang
SN  - 1389-1286
DO  - http://dx.doi.org/10.1016/j.comnet.2015.06.011
UR  - http://www.sciencedirect.com/science/article/pii/S1389128615002042
KW  - Flow-net
KW  - Accountability
KW  - Multi-resolution
KW  - Computer networks
AB  - Abstract
Today's computer and network systems were not originally designed for accountability which plays a crucial role in information assurance systems. To assure accountability, each entity in the system should be held responsible for its own behaviors so that the entity is part of larger chains of the system's accountability. To achieve accountable logging, a flow-net methodology was proposed in our previous work. Moreover, the multi-layer feature of computer and network systems brings us the chance to achieve multiple degrees of accountability, which means that we are able to acknowledge the system's behaviors at different levels of accountability. To achieve multiple degrees of accountability, we propose designs of a multi-resolution flow-net scheme in this paper and find out the optimal design under different assumptions. Furthermore, we apply the multi-resolution flow-net on TCP/IP networks that are designed and organized with multiple layers. Finally, evaluation results are presented to verify the better performance provided by multi-resolution flow-net than other schemes.
ER  - 

TY  - JOUR
T1  - Security requirements for e-government services: a methodological approach for developing a common PKI-based security policy
JO  - Computer Communications
VL  - 26
IS  - 16
SP  - 1873
EP  - 1883
PY  - 2003/10/15/
T2  - Securing Computer Communications with Public Key Infrastructure.
AU  - Lambrinoudakis, Costas
AU  - Gritzalis, Stefanos
AU  - Dridi, Fredj
AU  - Pernul, Günther
SN  - 0140-3664
DO  - http://dx.doi.org/10.1016/S0140-3664(03)00082-3
UR  - http://www.sciencedirect.com/science/article/pii/S0140366403000823
KW  - e-Government
KW  - Security requirements
KW  - Public Key Infrastructure
AB  - The concept of one-stop on-line government is not science fiction any more. On the contrary, the high reliability and performance of communication links, combined with architectural models that facilitate transparent access to distributed computational and storage resources, propel the development of integrated e-government platforms that support increased citizen mobility. The price we have to pay is the complexity introduced in the design of the security mechanisms required for protecting several heterogeneous information systems—each one supporting some of the services offered through the e-government integrated environment—and ensuring user privacy.

This paper demonstrates that the security services offered by Public Key Infrastructure (PKI) can be employed for fulfilling most of the identified security requirements for an integrated e-government platform. The list of security requirements has been compiled by adopting an organisational framework that facilitates the classification of e-government services according to the security requirements they exhibit.

The proposed approach has been applied, as a case study, to the e-government system ‘Webocrat’, identifying its security requirements and then designing a PKI-based security architecture for fulfilling them.
ER  - 

TY  - JOUR
T1  - BYOD security challenges: control and protect your most sensitive data
JO  - Network Security
VL  - 2012
IS  - 12
SP  - 5
EP  - 8
PY  - 2012/12//
T2  - 
AU  - Morrow, Bill
SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/S1353-4858(12)70111-3
UR  - http://www.sciencedirect.com/science/article/pii/S1353485812701113
AB  - Several new trends in information access are impacting organisations' ability to control and secure sensitive corporate data. The increase in web applications, cloud computing and Software as a Service (SaaS) offerings, and the Bring Your Own Device (BYOD) phenomenon, means that employees, business partners and customers are increasingly accessing information using a web browser on a device not owned or managed by the organisation.

The increase in web applications, cloud computing and Software as a Service (SaaS) offerings, and the Bring Your Own Device (BYOD) phenomenon are driving employees, business partners and customers to increasingly access information on devices are not managed by IT departments.

This has resulted in security implications for data leakage, data theft and regulatory compliance. To protect valuable information, organisations must stop making a distinction between devices in the corporate network and devices outside of it, argues Bill Morrow of Quarri Technologies.
ER  - 

TY  - JOUR
T1  - The impact of security layering on end-to-end latency and system performance in switched and distributed e-business environments
JO  - Computer Networks
VL  - 39
IS  - 6
SP  - 827
EP  - 840
PY  - 2002/8/21/
T2  - 
AU  - Iheagwara, Charles
AU  - Blyth, Andrew
SN  - 1389-1286
DO  - http://dx.doi.org/10.1016/S1389-1286(02)00250-5
UR  - http://www.sciencedirect.com/science/article/pii/S1389128602002505
KW  - Network security
KW  - Latency
KW  - Authentication
KW  - Retrieval
AB  - Contemporary e-business networks are increasingly implementing the multi-layer security scheme in order to provide a reasonable measure of security for their information systems. The implementation entails formation of a layered architecture (concentric security layers) using packet and application-level filters neither of which provides complimentary functions. The layered architecture provides convenient abstractions and increases the end-to-end latency that results into sub-optimal system performance. In this paper, we present the results of the experiment to quantify the latency introduced by security layering on end-to-end latency and investigate the resulting degree of sub-optimality of system performance in a distributed and switched e-business network.
ER  - 

TY  - JOUR
T1  - An empirical study of industrial security-engineering practices
JO  - Journal of Systems and Software
VL  - 61
IS  - 3
SP  - 225
EP  - 232
PY  - 2002/4/1/
T2  - 
AU  - Vaughn Jr., Rayford B.
AU  - Henning, Ronda
AU  - Fox, Kevin
SN  - 0164-1212
DO  - http://dx.doi.org/10.1016/S0164-1212(01)00150-9
UR  - http://www.sciencedirect.com/science/article/pii/S0164121201001509
KW  - Computer security
KW  - Information assurance
KW  - Security-engineering
KW  - Risk assessment
AB  - This paper presents lessons learned and observations noted about the state of security-engineering practices by three information security practitioners with different perspectives – two in industry and one in academia. All authors have more than 20-years experience in this field and two were former members of the US National Computer Security Center during the early days of the Trusted Computer System Evaluation Criteria and the strong promotion of trusted operating systems that accompanied the release of that document. In the last 20 years, it has been argued that security-engineering practices have not kept pace with the escalating threats to information systems. Much has occurred since that time – new security paradigms, failure of evaluated products to emerge into common use, new systemic threats, and an increased awareness of the risk faced by information systems. This paper presents an empirical view of lessons learned in security-engineering, experiences in applying the trade, and observations made about the successes and failures of security practices and technology. This work was sponsored in part by NSF Grant.
ER  - 

TY  - JOUR
T1  - Development of Information Security Baselines for Healthcare Information Systems in New Zealand
JO  - Computers & Security
VL  - 21
IS  - 2
SP  - 172
EP  - 192
PY  - 2002/3/31/
T2  - 
AU  - Janczewski, Lech
AU  - Xinli Shi, Frank
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/S0167-4048(02)00212-2
UR  - http://www.sciencedirect.com/science/article/pii/S0167404802002122
KW  - healthcare information systems
KW  - electronic medical records
KW  - information privacy
KW  - information security baselines
KW  - security model
AB  - In 1996 New Zealand had introduced security standard AS/NZCS 4444 based on the British Standard BS 7799, which has recently been accepted as an international standard ISO 17799. This standard is very often referred to as the ‘baseline lane approach’ to the issue of managing information security. On the other hand the health information systems (HIS) are undergoing rapid development both in the number of installed systems as in the law and regulations governing HIS developments and deployment. The project was aimed at reviewing the AS/NZCS 4444 standard from the HIS requirements point of view. In this paper, we began with an overview of healthcare information systems (HIS) infrastructure in New Zealand and associated security issues around privacy and confidentiality, followed by a general review of the security baseline approach. We analyzed each clause of the AS/NZS 4444 with the information collected about technical and non-technical approaches to protecting HIS, consisting of a series of multi-case studies of healthcare organizations that collect, process, store and transmit electronic medical records. Finally, we proposed a new set of information security baselines based on the research to build an information security model for healthcare organizations.
ER  - 

TY  - JOUR
T1  - Security implications of implementing active network infrastructures using agent technology
JO  - Computer Networks
VL  - 36
IS  - 1
SP  - 87
EP  - 100
PY  - 2001/6//
T2  - Active Networks and Services
AU  - Karnouskos, Stamatis
SN  - 1389-1286
DO  - http://dx.doi.org/10.1016/S1389-1286(01)00155-4
UR  - http://www.sciencedirect.com/science/article/pii/S1389128601001554
KW  - Active networks
KW  - Security
KW  - Active code
KW  - Agent technology
AB  - Active networks (AN) are a rapid evolving area of research and in parallel an area of great industry interest. However, for this technology to make the step out of the labs and penetrate the market, the security problems have to be tackled effectively. This paper demonstrates why and how agent technology research, can and should be applied to active networks, in order to fulfill the new security challenges this infrastructure poses. First, we identify the key elements of AN, analyze the nature of active code, specify the role of agents in active networks and present a multi-execution environment active network architecture. Then, we target the security threats for active code and execution environment, and state the basic as well as the extended security requirements. Subsequently, we try to see how we can apply the security solutions and research done for agents to the context of active networks in order to satisfy their requirements.
ER  - 

TY  - JOUR
T1  - Validating ambient intelligence based ubiquitous computing systems by means of artificial societies
JO  - Information Sciences
VL  - 222
IS  - 
SP  - 3
EP  - 24
PY  - 2013/2/10/
T2  - Including Special Section on New Trends in Ambient Intelligence and Bio-inspired Systems
AU  - Serrano, Emilio
AU  - Botia, Juan
SN  - 0020-0255
DO  - http://dx.doi.org/10.1016/j.ins.2010.11.012
UR  - http://www.sciencedirect.com/science/article/pii/S0020025510005578
KW  - Social simulations
KW  - Ambient intelligence
KW  - Ubiquitous computing
KW  - Validation
KW  - Verification
KW  - Forensic analysis
AB  - This paper introduces a new methodology based on the use of Multi-Agent Based Simulations (MABS) for testing and validation of Ambient Intelligence based Ubiquitous Computing (UbiCom) systems. An ambient intelligence based UbiCom is a pervasive system in which services have some intelligence in order to smoothly interact with users immersed in the environment. The motivation for this methodology is its application in UbiCom large-scale systems where large numbers of users are involved and in applications which deal with dangerous environments. In these cases, real tests are impractical and an artificial society is required. MABS allows building cheap and quick prototypes which can describe UbiCom systems. Analyzing these prototypes, if they are sufficiently descriptive, allows requisites violations in functionality of real UbiCom system designs to be discovered. MABSs and particularly the most descriptive ones can present very complex behaviors. Therefore, the MABS analysis obtained with the presented methodology is not trivial. Consequently, this paper also proposes two techniques for the analysis of general complex MABSs: forensic analysis and the use of simpler simulations. Moreover, the methodology proposes to inject elements of the actual UbiCom system in the simulated world to increase the confidence of the validation process. The proposal is illustrated with a detailed case study that considers a building on our campus and an AmI service for evacuation in case of fire.
ER  - 

TY  - JOUR
T1  - Assessing insider threats to information security using technical, behavioural and organisational measures
JO  - Information Security Technical Report
VL  - 15
IS  - 3
SP  - 112
EP  - 133
PY  - 2010/8//
T2  - Computer Crime - A 2011 Update
AU  - Roy Sarkar, Kuheli
SN  - 1363-4127
DO  - http://dx.doi.org/10.1016/j.istr.2010.11.002
UR  - http://www.sciencedirect.com/science/article/pii/S1363412710000488
AB  - The UK government took a bruising in the headlines (Sep 2008) after a Home Office contractor lost a USB stick containing unencrypted data on all 84,000 prisoners in England and Wales. As a result, the Home Office terminated the £1.5 million contract with the management consultancy firm.

The world woke up to the largest attempted bank fraud ever when the UK’s National Hi-Tech Crime Unit foiled the world’s largest potential bank robbery in March 2005. With the help of the security supervisor, thieves masquerading as cleaning staff installed hardware keystroke loggers on computers within the London branch of a Japanese bank, to steal £220m.

It is indeed sobering to imagine that any organisation could fall victim to such events and the damage an insider can do. The consulting firm lost the contract worth £1.5 million due to a small mistake by an employee. The London branch of the Japanese Bank would have lost £220 million had not the crime been foiled.

Insider threat is a reality. Insiders commit fraud or steal sensitive information when motivated by money or revenge. Well-meaning employees can compromise the security of an organisation with their overzealousness in getting their job done. Every organisation has a varied mix of employees, consultants, management, partners and complex infrastructure and that makes handling insider threats a daunting challenge. With insider attacks, organisations face potential damage through loss of revenue, loss of reputation, loss of intellectual property or even loss of human life.

The insider threat problem is more elusive and perplexing than any other threat. Assessing the insider threat is the first step to determine the likelihood of any insider attack. Technical solutions do not suffice since insider threats are fundamentally a people issue. Therefore, a three-pronged approach - technological, behavioural and organisational assessment is essential in facilitating the prediction of insider threats and pre-empt any insider attack thus improving the organization’s security, survivability, and resiliency in light of insider threats.
ER  - 

TY  - JOUR
T1  - Making sense of log management for security purposes – an approach to best practice log collection, analysis and management
JO  - Computer Fraud & Security
VL  - 2007
IS  - 5
SP  - 5
EP  - 10
PY  - 2007/5//
T2  - 
AU  - Gorge, Mathieu
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(07)70047-7
UR  - http://www.sciencedirect.com/science/article/pii/S1361372307700477
AB  - Every computer action registers a log somewhere – giving a rich source of data that can help businesses identify any trace of corruption within their networks. Log collection is also a strong component of keeping in line with legislation such as Sarbanes-Oxley, HIPAA, GLBA in the US, and the European Data Protection Directive in the EU. Mathieu Gorge looks at what logs organizations need to keep and what standards require their storage.

He recommends proactive monitoring of firewalls, anti-virus, VPNs and IDS logs among other security systems. The main goal is to link a transaction back to an individual user in order to perform a forensic investigation. But it is important to be wary, as some countries do not allow companies to monitor staff usage of IT systems. See page 3 on how the European court ruled that a British college's monitoring of one employee was a breach of human rights. Therefore, linking a log with a person's actions may not stand up in court.

Gorge says logs can give as good an insight into external attacks as well as internally driven ones.

Logs should be analyzed for the following:
				•
User account activity: creation, elevation of privilege, changes, inactivity.
•
Client requests and server response.
•
Operational status: shutdown (planned or unplanned), system failure and automatic restart.
•
Usage information and trends – basic user behaviour analysis.


It is best practice to collect, store and analyze logs with a view to being able to get complete, accurate and verifiable information. This will improve the organization's ability to comply with key standards and legislation as regards e-evidence. It could save an organization from potential liability and repair costs and will give visibility over mission critical and security systems, performance and usage. The main advice is to remain proactive so as to be able to respond to a security incident and comply with legal requests should anything happen.

Mathieu Gorge looks at what logs can do for your business and how governance demands them.
ER  - 

TY  - JOUR
T1  - Security baselines to give you momentum as you move into the New Year
JO  - Computer Fraud & Security
VL  - 2008
IS  - 1
SP  - 13
EP  - 20
PY  - 2008/1//
T2  - 
AU  - Power, Richard
AU  - Forte, Dario
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(08)70011-3
UR  - http://www.sciencedirect.com/science/article/pii/S1361372308700113
AB  - In our last column, we provided you with security baselines for business process controls, end user controls and mergers, acquisitions and divestitures.

To avoid the mistakes (see page 5) made by some in 2007, see our experts' list of baseline security controls to ensure 2008 gets off to a secure start.

This month, columnists Richard Power and Dario Forte continue on the subject of security baselines for business process controls. Following on from the December issue of Computer Fraud &amp; Security, they provide further insight into security baselines for business process controls, end user controls and mergers, acquisitions and divestitures.


				
It is important to set baseline security for 2008.
ER  - 

TY  - JOUR
T1  - Compliance vs business security
JO  - Network Security
VL  - 2009
IS  - 9
SP  - 16
EP  - 18
PY  - 2009/9//
T2  - 
AU  - Forte, Dario
SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/S1353-4858(09)70101-1
UR  - http://www.sciencedirect.com/science/article/pii/S1353485809701011
AB  - The Italian Authority for the Protection of Personal Information (aka, Guarantor of Privacy) is an internationally recognised reference point for both legislators and academicians. Many of the Authority's regulations (and opinions) have been adopted by other countries and incorporated into their legislative frameworks. One of the recent regulations issued by the Authority that will certainly generate ripples in Europe and beyond regards management of system administrators.
ER  - 

TY  - JOUR
T1  - Security issues for third party games: Technical, business and legal perspectives
JO  - Computer Law & Security Review
VL  - 24
IS  - 2
SP  - 163
EP  - 168
PY  - 2008///
T2  - 
AU  - Davis, Steven B.
AU  - Price, W. Joseph
SN  - 0267-3649
DO  - http://dx.doi.org/10.1016/j.clsr.2008.01.004
UR  - http://www.sciencedirect.com/science/article/pii/S026736490800006X
AB  - There has been a substantial rise in security incidents for the multi-billion dollar, worldwide computer gaming industry. As the industry has grown, games are not just developed and operated by one company, but include multiple support, development, publishing, and other partners. This article will discuss how to include technical, business, and legal security issues in the game development process to manage security risks. This article also will discuss potential “long tail” opportunities, which game publishers and developers have not fully exploited or considered in their games, such as secondary commercial games licensing and other developmental and licensing strategies that would benefit from managing security issues early in the developmental lifecycle.
ER  - 

TY  - JOUR
T1  - Delegation and digital mandates: Legal requirements and security objectives
JO  - Computer Law & Security Review
VL  - 25
IS  - 5
SP  - 415
EP  - 431
PY  - 2009/9//
T2  - 
AU  - Van Alsenoy, Brendan
AU  - De Cock, Danny
AU  - Simoens, Koen
AU  - Dumortier, Jos
AU  - Preneel, Bart
SN  - 0267-3649
DO  - http://dx.doi.org/10.1016/j.clsr.2009.07.007
UR  - http://www.sciencedirect.com/science/article/pii/S0267364909001265
KW  - Delegation
KW  - Agency
KW  - Mandate
KW  - Identity management
KW  - Delegated user and access management
AB  - Now that more and more legal transactions are being performed online, it is increasingly necessary to enable integration of legal mandates within identity and information management systems. The purpose of this article is to outline the legal framework surrounding delegation and to identify basic requirements for any technical application which seeks to provide recognition to legal mandates and delegation processes. Special consideration is also given to the legal implications in situations where a (presumed) mandate holder acts without or outside his authority. Based on these considerations, this article attempts to outline an approach which can significantly reduce the potential risks for both mandate issuers and relying service providers.
ER  - 

TY  - JOUR
T1  - Turning log files into a security asset
JO  - Network Security
VL  - 2008
IS  - 2
SP  - 4
EP  - 7
PY  - 2008/2//
T2  - 
AU  - Casey, Donal
SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/S1353-4858(08)70016-3
UR  - http://www.sciencedirect.com/science/article/pii/S1353485808700163
AB  - Despite most businesses having a range of devices which create log files of user and system activity, few actually analyse the logs for information.

As the internet becomes an integral part of the way we live and work, the number of logs generated and their importance has continued to increase. More homes now have broadband connections. Most businesses at least have email and many have an interactive web presence, or at the very least a static web ‘brochure’. All of the internet traffic these generate has the capacity to create useful information for marketers, and more importantly for security engineers, in the form of log entries.
ER  - 

TY  - JOUR
T1  - Enhanced Internet security by a distributed traffic control service based on traffic ownership
JO  - Journal of Network and Computer Applications
VL  - 30
IS  - 3
SP  - 841
EP  - 857
PY  - 2007/8//
T2  - 
AU  - Bossardt, Matthias
AU  - Dübendorfer, Thomas
AU  - Plattner, Bernhard
SN  - 1084-8045
DO  - http://dx.doi.org/10.1016/j.jnca.2005.07.006
UR  - http://www.sciencedirect.com/science/article/pii/S1084804505000470
KW  - Traffic control
KW  - Network management
KW  - Network services
KW  - Mitigation
KW  - Distributed denial of service attack
AB  - The frequency and intensity of Internet attacks are rising at an alarming pace. Several technologies and concepts were proposed for fighting distributed denial of service (DDoS) attacks: traceback, pushback, i3, SOS and Mayday. This paper shows that in the case of DDoS reflector attacks they are either ineffective or even counterproductive. We then propose the novel concept of traffic ownership and describe a system that extends control over network traffic by network users to the Internet using adaptive traffic processing devices. We safely delegate partial network management capabilities from network operators to network users. All network packets with a source or destination address “owned” by a network user can now also be controlled within the Internet instead of only at the network user's Internet uplink. By limiting the traffic control features and by restricting the realm of control to the “owner” of the traffic, we can rule out misuse of this system. Applications of our system are manifold: prevention of source address spoofing, DDoS attack mitigation, distributed firewall-like filtering, new ways of collecting traffic statistics, service-level agreement validation, traceback, distributed network debugging, support for forensic analyses and many more. A use case illustrates how our system enables network users to prevent and react to DDoS attacks.
ER  - 

TY  - JOUR
T1  - A comprehensive security system— the concepts, agents and protocols
JO  - Computers & Security
VL  - 9
IS  - 7
SP  - 631
EP  - 643
PY  - 1990/11//
T2  - 
AU  - Shepherd, S.J.
AU  - Sanders, P.W.
AU  - Patel, A.
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/0167-4048(90)90062-X
UR  - http://www.sciencedirect.com/science/article/pii/016740489090062X
KW  - Security
KW  - Protocols
KW  - Computer networks
KW  - Security management
KW  - Policy
KW  - Agents
AB  - This paper presents an overview of a comprehensive security architecture for use within, and as a value-added enhancement to, the ISO Open System Interconnection model. The system is arranged basically as an application layer service but can allow all of the ISO-recommended security facilities to be provided at any layer of the model. It is suitable as an “add-on” service to existing arrangements or can be fully integrated into new applications. For large-scale, distributed processing operations, a network of “security management centres” is suggested, that can help to ensure that system misuse is minimized, and that flexible operations are provided in an efficient manner.
ER  - 

TY  - JOUR
T1  - Framework of a methodology for the life cycle of computer security in an organization
JO  - Computers & Security
VL  - 8
IS  - 5
SP  - 433
EP  - 442
PY  - 1989/8//
T2  - 
AU  - Badenhorst, K.P.
AU  - Eloff, Jan H.P.
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/0167-4048(89)90025-4
UR  - http://www.sciencedirect.com/science/article/pii/0167404889900254
KW  - Technological security
KW  - Applications security
KW  - Methodology
KW  - Life cycle
KW  - Management
AB  - The life cycle of computer security is a paradigm to the software development life cycle, a tool that provides structure and foundation for the planning, development and implementation of application software. Current “off-the-shelf” methodologies are mostly for conventional software system development. The objective of this paper is to design a methodology for the introduction, development and maintenance of computer security within major organizations. Both the following issues will be addressed: technological computer security such as physical and logical aspects, and applications computer security referring to the development of software.
ER  - 

TY  - JOUR
T1  - An agent based and biological inspired real-time intrusion detection and security model for computer network operations
JO  - Computer Communications
VL  - 30
IS  - 13
SP  - 2649
EP  - 2660
PY  - 2007/9/26/
T2  - Sensor-Actuated NetworksSANETs
AU  - Boukerche, Azzedine
AU  - Machado, Renato B.
AU  - Jucá, Kathia R.L.
AU  - Sobral, João Bosco M.
AU  - Notare, Mirela S.M.A.
SN  - 0140-3664
DO  - http://dx.doi.org/10.1016/j.comcom.2007.03.008
UR  - http://www.sciencedirect.com/science/article/pii/S0140366407001181
KW  - Artificial immune system
KW  - Mobile agent
KW  - Computer networks
KW  - Intrusion detection model
AB  - There is a strong correlation between the human immune system and a computer network security system. The human immune system protects the human body from pathogenic elements in the same way that a computer security system protects the computer from malicious users. This paper presents a novel intrusion detection model based on artificial immune and mobile agent paradigms for network intrusion detection. The construction of the model is based on registries’ signature analysis using both Syslog-ng and Logcheck unix tools. The tasks of monitoring, distributing intrusion detection workload, storing relevant information, and ensuring data persistence and reactivity have been carried out by the mobile agents, which represent the leukocytes of an artificial immune system. Our real-time based intrusion detection and communication model is host-based and adopts the anomaly detection paradigm. We present our intrusion detection model, discuss its implementation, and report on its performance evaluation using real data provided by an Internet Service Provider and a data processing corporation.
ER  - 

TY  - JOUR
T1  - The changing face of IT security
JO  - Network Security
VL  - 2006
IS  - 11
SP  - 16
EP  - 17
PY  - 2006/11//
T2  - 
AU  - Potter, Bruce
SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/S1353-4858(06)70454-8
UR  - http://www.sciencedirect.com/science/article/pii/S1353485806704548
AB  - Faced with multiple changes in the IT landscape, it is sometimes hard to keep track of where the technology trends are going. Some varieties of attacks occurring today leave traditional security measures standing.
ER  - 

TY  - JOUR
T1  - A transaction flow approach to software security certification for document handling systems
JO  - Computers & Security
VL  - 7
IS  - 5
SP  - 495
EP  - 502
PY  - 1988/10//
T2  - 
AU  - Pfleeger, Charles P.
AU  - Pfleeger, Shari Lawrence
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/0167-4048(88)90203-9
UR  - http://www.sciencedirect.com/science/article/pii/0167404888902039
KW  - Security
KW  - Certification
KW  - Transaction
KW  - Data flow
KW  - Verification
KW  - Validation
KW  - Exposure
KW  - Control
AB  - A security certification method is described for a document handling system for a major government organization. The security evaluation process includes identification of the exposures of the system, determination of the controls that cover those exposures, and evaluation of the appropriateness and effectiveness of the controls. Included are the details of the analysis performed and the types of results expected in that analysis, both of which constitute the basic evaluation of the document handling system. The certification analysis approach can be extended naturally to other types of computing systems.
ER  - 

TY  - JOUR
T1  - Modular design, application architecture, and usage of a self-service model for enterprise data delivery: The Duke Enterprise Data Unified Content Explorer (DEDUCE)
JO  - Journal of Biomedical Informatics
VL  - 52
IS  - 
SP  - 231
EP  - 242
PY  - 2014/12//
T2  - Special Section: Methods in Clinical Research Informatics
AU  - Horvath, Monica M.
AU  - Rusincovitch, Shelley A.
AU  - Brinson, Stephanie
AU  - Shang, Howard C.
AU  - Evans, Steve
AU  - Ferranti, Jeffrey M.
SN  - 1532-0464
DO  - http://dx.doi.org/10.1016/j.jbi.2014.07.006
UR  - http://www.sciencedirect.com/science/article/pii/S1532046414001531
KW  - Cohort definition
KW  - Research query tool
KW  - Medical informatics applications
KW  - Information systems
KW  - Application development
KW  - System design and architecture
AB  - AbstractPurpose
Data generated in the care of patients are widely used to support clinical research and quality improvement, which has hastened the development of self-service query tools. User interface design for such tools, execution of query activity, and underlying application architecture have not been widely reported, and existing tools reflect a wide heterogeneity of methods and technical frameworks. We describe the design, application architecture, and use of a self-service model for enterprise data delivery within Duke Medicine.
Methods
Our query platform, the Duke Enterprise Data Unified Content Explorer (DEDUCE), supports enhanced data exploration, cohort identification, and data extraction from our enterprise data warehouse (EDW) using a series of modular environments that interact with a central keystone module, Cohort Manager (CM). A data-driven application architecture is implemented through three components: an application data dictionary, the concept of “smart dimensions”, and dynamically-generated user interfaces.
Results
DEDUCE CM allows flexible hierarchies of EDW queries within a grid-like workspace. A cohort “join” functionality allows switching between filters based on criteria occurring within or across patient encounters. To date, 674 users have been trained and activated in DEDUCE, and logon activity shows a steady increase, with variability between months. A comparison of filter conditions and export criteria shows that these activities have different patterns of usage across subject areas.
Conclusions
Organizations with sophisticated EDWs may find that users benefit from development of advanced query functionality, complimentary to the user interfaces and infrastructure used in other well-published models. Driven by its EDW context, the DEDUCE application architecture was also designed to be responsive to source data and to allow modification through alterations in metadata rather than programming, allowing an agile response to source system changes.
ER  - 

TY  - JOUR
T1  - Transferring business and support functions: the information security risks of outsourcing and off-shoring: (A beginner's guide to avoiding the abrogation of responsibility)
JO  - Computer Fraud & Security
VL  - 2004
IS  - 12
SP  - 5
EP  - 9
PY  - 2004/12//
T2  - 
AU  - Pemble, Matthew
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(05)70183-4
UR  - http://www.sciencedirect.com/science/article/pii/S1361372305701834
AB  - This article delves into the area of outsourcing where information security impacts are higher, such as IT or core customer contact. It deals with such issues as staff loyalty, contracts and service level agreements, legislation and security architectures.
ER  - 

TY  - JOUR
T1  - How to supervise and control I/S Security Officers and Auditors
JO  - Computers & Security
VL  - 6
IS  - 1
SP  - 17
EP  - 21
PY  - 1987/2//
T2  - 
AU  - Moulton, Rolf T.
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/0167-4048(87)90121-0
UR  - http://www.sciencedirect.com/science/article/pii/0167404887901210
KW  - IS security
KW  - IS audit
KW  - Computer audit
KW  - Access control
KW  - IS supervision
KW  - IS management
KW  - IS responsibilities
AB  - Information system Security Officers (ISSO) and Information System Auditors (ISA) can have similar capabilities with respect to asset access. Their supervision and control requires special consideration by management because they may literally have the keys to all of the organization's information and physical assets. The managers of each of those employees must be directly accountable for the individual's supervision. The same suggestions which are described for ISSO supervision apply to ISA supervision. Further, each is in a unique position to audit the activities of the other. Recommendations are provided to accomplish this “audit of the auditors.”
ER  - 

TY  - JOUR
T1  - Security Views - Malware Update
JO  - Computers & Security
VL  - 25
IS  - 5
SP  - 317
EP  - 324
PY  - 2006/7//
T2  - 

SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/j.cose.2006.06.003
UR  - http://www.sciencedirect.com/science/article/pii/S0167404806000952
ER  - 

TY  - JOUR
T1  - Security in network attached storage (NAS) for workgroups
JO  - Network Security
VL  - 2004
IS  - 4
SP  - 8
EP  - 12
PY  - 2004/4//
T2  - 
AU  - Edelson, Eve
SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/S1353-4858(04)00065-0
UR  - http://www.sciencedirect.com/science/article/pii/S1353485804000650
AB  - Network-attached storage (NAS) is a relatively simple and inexpensive way to serve files over a network in a cross-platform environment. NAS devices face the same security challenges as other network components. This article discusses how NAS fits into the world of IP storage, some security features present in (and missing from) NAS devices, and some security considerations in choosing a NAS.
ER  - 

TY  - JOUR
T1  - Security Views - Malware Update
JO  - Computers & Security
VL  - 25
IS  - 2
SP  - 81
EP  - 88
PY  - 2006/3//
T2  - 

SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/j.cose.2006.01.004
UR  - http://www.sciencedirect.com/science/article/pii/S0167404806000204
ER  - 

TY  - JOUR
T1  - “Adequate” security — what exactly do you mean?
JO  - Computer Law & Security Review
VL  - 19
IS  - 5
SP  - 406
EP  - 410
PY  - 2003/9//
T2  - 
AU  - Buzzard, Keith
SN  - 0267-3649
DO  - http://dx.doi.org/10.1016/S0267-3649(03)00510-7
UR  - http://www.sciencedirect.com/science/article/pii/S0267364903005107
AB  - This article explores the concept of what is ‘adequate’ computer security and concludes that this will vary amongst the various sectors– government, military, banking, commercial etc.
ER  - 

TY  - JOUR
T1  - Introducing the Microsoft Vista event log file format
JO  - Digital Investigation
VL  - 4, Supplement
IS  - 
SP  - 65
EP  - 72
PY  - 2007/9//
T2  - 
AU  - Schuster, Andreas
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2007.06.015
UR  - http://www.sciencedirect.com/science/article/pii/S1742287607000424
KW  - Digital evidence
KW  - Forensic examination
KW  - Microsoft Windows Vista
KW  - Windows event logging
KW  - Log file
KW  - evtx
AB  - Several operating systems provide a central logging service which collects event messages from the kernel and applications, filters them and writes them into log files. Since more than a decade such a system service exists in Microsoft Windows NT. Its file format is well understood and supported by forensic software. Microsoft Vista introduces an event logging service which entirely got newly designed. This confronts forensic examiners and software authors with unfamiliar system behavior and a new, widely undocumented file format.

This article describes the history of Windows system loggers, what has been changed over time and for what reason. It compares Vista log files in their native binary form and in a textual form. Based on the results, this paper for the first time publicly describes the key-elements of the new log file format and the proprietary binary encoding of XML. It discusses the problems that may arise during daily work. Finally it proposes a procedure for how to recover information from log fragments. During a criminal investigation this procedure was successfully applied to recover information from a corrupted event log.
ER  - 

TY  - JOUR
T1  - Investigating around mainframes and other high-end systems: the revenge of big iron
JO  - Digital Investigation
VL  - 1
IS  - 2
SP  - 90
EP  - 93
PY  - 2004/6//
T2  - 
AU  - Pemble, Matthew
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2004.04.006
UR  - http://www.sciencedirect.com/science/article/pii/S1742287604000349
KW  - Mainframe
KW  - Audit
KW  - Investigation
KW  - Forensic
KW  - Log
AB  - Normal computer forensic tools and techniques are ineffective or inefficient when applied to mainframes or other very large systems. Incidents involving mainframes are likely to have a significant business impact, so preparation is essential, both of any specialist tools and of staff with mainframe experience. Proper design of system audit logs can also provide admissible material of evidential weight, without the requirement for forensic treatment.
ER  - 

TY  - JOUR
T1  - Security Log Management
JO  - Network Security
VL  - 2003
IS  - 11
SP  - 6
EP  - 9
PY  - 2003/11//
T2  - 
AU  - Lobo, Colin
SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/S1353-4858(03)01106-1
UR  - http://www.sciencedirect.com/science/article/pii/S1353485803011061
AB  - Have your systems been configured to write events to the system logs? Most people would answer “Yes”. Are they using the standard default settings? The answer to this will vary. How often are the logs examined? Most people would answer only “when there is a problem”.
ER  - 

TY  - JOUR
T1  - Security views
JO  - Computers & Security
VL  - 22
IS  - 8
SP  - 654
EP  - 663
PY  - 2003/12//
T2  - 
AU  - Schultz, Eugene
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/S0167-4048(03)00002-6
UR  - http://www.sciencedirect.com/science/article/pii/S0167404803000026
ER  - 

TY  - JOUR
T1  - IT Security — State of the Nation
JO  - Network Security
VL  - 2002
IS  - 12
SP  - 15
EP  - 17
PY  - 2002/12//
T2  - 
AU  - Wilson, Piers
SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/S1353-4858(02)12010-1
UR  - http://www.sciencedirect.com/science/article/pii/S1353485802120101
AB  - Rather than tackle one particular topic in detail, this article will consider where IT security has got to across several fronts — in some areas considerable progress has been made, systems are “more” secure now than a few years ago. In other areas however, little has changed; the problem has just been moved around or hasn’t really been effectively solved. Certainly the threat landscape has changed, modern attacks are often intriguingly clever and powerful (witness the breadth of distribution mechanisms that the NIMDA virus used, the use of SQL injection against websites etc…). Technology has also presented further opportunity — just look at the flurry of threats that have surrounded wireless networks since their inception.
ER  - 

TY  - JOUR
T1  - Implementing enterprise security: a case study3
JO  - Computers & Security
VL  - 22
IS  - 2
SP  - 99
EP  - 114
PY  - 2003/2//
T2  - 
AU  - Doughty, Ken
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/S0167-4048(03)00205-0
UR  - http://www.sciencedirect.com/science/article/pii/S0167404803002050
AB  - Introduction

Information is an essential asset for organizations, because it supports the day-today operations, and facilitates decision-making by the organization’s key stakeholders. The challenge facing organizations is how to provide access to this asset without compromising its integrity. This asset is received and distributed by the organization through various distribution channels, which is connected together by the telecommunications network. These channels include:

•
Email
•
Internet
•
Applications (e.g. Financial, Logistics, Retail, Property and Construction, Energy etc.)
•
DBMS (MS SQL Server, Oracle, DB2, Sybase, etc.)
•
Operating systems (e.g. Unix, NT/Windows 2000, etc.)
ER  - 

TY  - JOUR
T1  - Who holds the key to IT security?
JO  - Information Security Technical Report
VL  - 7
IS  - 4
SP  - 10
EP  - 22
PY  - 2002/12//
T2  - 
AU  - Flink, Yona
SN  - 1363-4127
DO  - http://dx.doi.org/10.1016/S1363-4127(02)00403-X
UR  - http://www.sciencedirect.com/science/article/pii/S136341270200403X
ER  - 

TY  - JOUR
T1  - Managing network security: What's happening out there
JO  - Network Security
VL  - 1999
IS  - 8
SP  - 8
EP  - 11
PY  - 1999/8//
T2  - 
AU  - Cohen, Fred
SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/S1353-4858(00)80034-3
UR  - http://www.sciencedirect.com/science/article/pii/S1353485800800343
AB  - Computing operates in an almost universally networked environment, but the technical aspects of information protection have not kept up. As a result, the success of information security programs has increasingly become a function of our ability to make prudent management decisions about organizational activities. Managing Network Security takes a management view of protection and seeks to reconcile the need for security with the limitations of technology.
ER  - 

TY  - JOUR
T1  - Security for eBusiness
JO  - Information Security Technical Report
VL  - 6
IS  - 2
SP  - 80
EP  - 94
PY  - 2001/6/1/
T2  - 
AU  - Davidson, Mary Ann
SN  - 1363-4127
DO  - http://dx.doi.org/10.1016/S1363-4127(01)00209-6
UR  - http://www.sciencedirect.com/science/article/pii/S1363412701002096
ER  - 

TY  - JOUR
T1  - Developing a Windows NT security policy
JO  - Information Security Technical Report
VL  - 3
IS  - 3
SP  - 31
EP  - 43
PY  - 1998///
T2  - 
AU  - White, Ian
SN  - 1363-4127
DO  - http://dx.doi.org/10.1016/S1363-4127(98)80029-0
UR  - http://www.sciencedirect.com/science/article/pii/S1363412798800290
AB  - The choice of Windows NT as the strategic platform for the desktop and shared application server is becoming more widespread. A challenge for the security team is to provide guidelines on the minimum level of security controls that should be implemented on these systems. This article discusses some of the controls that might be expected to be included within such a Windows NT Security Policy (the policy).
ER  - 

TY  - JOUR
T1  - Network traffic as a source of evidence: tool strengths, weaknesses, and future needs
JO  - Digital Investigation
VL  - 1
IS  - 1
SP  - 28
EP  - 43
PY  - 2004/2//
T2  - 
AU  - Casey, Eoghan
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2003.12.002
UR  - http://www.sciencedirect.com/science/article/pii/S1742287603000033
KW  - Network traffic
KW  - Network investigations
KW  - Digital evidence
KW  - Forensic examination
KW  - Computer crime
AB  - Digital investigators require specialized knowledge and tools to process network traffic as a source of evidence. Existing open source tools can be used for basic tasks in simple cases but lack the functionality of commercial tools that are specifically designed to process network traffic as evidence. These commercial tools reduce the amount of time and specialized technical knowledge required to examine large quantities of network traffic but even these tools are lacking from a forensic standpoint. This paper discusses the strengths and shortcomings of existing tools in the context of the overall digital investigation process—specifically the collection, documentation, preservation, examination and analysis stages. In addition to highlighting the capabilities of different tools, this paper familiarizes digital investigators with different aspects of network traffic as a source of evidence. Based on this discussion, a set of requirements is proposed for tools used to process network traffic as evidence in the hope that existing developers will enhance the capabilities of their tools to address the weaknesses.
ER  - 

TY  - JOUR
T1  - Security monitoring in heterogeneous globally distributed environments
JO  - Information Security Technical Report
VL  - 3
IS  - 4
SP  - 15
EP  - 31
PY  - 1998///
T2  - 
AU  - Venables, Phil
SN  - 1363-4127
DO  - http://dx.doi.org/10.1016/S1363-4127(98)80035-6
UR  - http://www.sciencedirect.com/science/article/pii/S1363412798800356
AB  - Keeping control of a highly heterogeneous network of distributed systems in any global company is a difficult task. This problem cannot be brought under control with any single approach or technology. We must focus on the objectives and build tools into a common integrated framework to provide a security state and event monitoring concept for the entire organisation.
ER  - 

TY  - JOUR
T1  - Secured Temporal Log Management Techniques for Cloud
JO  - Procedia Computer Science
VL  - 46
IS  - 
SP  - 589
EP  - 595
PY  - 2015///
T2  - Proceedings of the International Conference on Information and Communication Technologies, ICICT 2014, 3-5 December 2014 at Bolgatty Palace &amp; Island Resort, Kochi, India
AU  - Muthurajkumar, S.
AU  - Ganapathy, S.
AU  - Vijayalakshmi, M.
AU  - Kannan, A.
SN  - 1877-0509
DO  - http://dx.doi.org/10.1016/j.procs.2015.02.098
UR  - http://www.sciencedirect.com/science/article/pii/S1877050915001623
KW  - Cloud computing
KW  - Log management
KW  - Cloud Storage
KW  - Cloud Security;
AB  - Abstract
Log Management has been an important service in Cloud Computing. In any business, maintaining the log records securely over a particular period of time is absolutely necessary for various reasons such as auditing, forensic analysis, evidence etc. In this work, Integrity and confidentiality of the log records are maintained at every stage of Log Management namely the Log Generation phase, Transmission phase and Storage phase. In addition to this, Log records may often contain sensitive information about the organization which should not be leaked to the outside world. In this paper, Temporal Secured Cloud Log Management Algorithm techniques are implemented to provide security to maintain transaction history in cloud within time period. In this work, security to temporal log management is provided by encrypting the log data before they are stored in the cloud storage. They are also stored in batches for easy retrieval. This work was implemented in Java programming language in the Google drive environment.
ER  - 

TY  - JOUR
T1  - The future information system : The quality and security debate — Part 2
JO  - Computer Fraud & Security Bulletin
VL  - 1994
IS  - 9
SP  - 7
EP  - 14
PY  - 1994/9//
T2  - 
AU  - Blatchford, CliveW
SN  - 0142-0496
DO  - http://dx.doi.org/10.1016/0142-0496(94)90184-8
UR  - http://www.sciencedirect.com/science/article/pii/0142049694901848
AB  - This is the concluding part of the article in which Clive Blatchford outlines some of the current considerations in the Open Systems Information Security Integration.
ER  - 

TY  - JOUR
T1  - A higher level of computer security through active policies
JO  - Computers & Security
VL  - 14
IS  - 2
SP  - 147
EP  - 157
PY  - 1995///
T2  - 
AU  - Abrams, Marshall D.
AU  - Moffett, Jonathan D.
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/0167-4048(95)97048-F
UR  - http://www.sciencedirect.com/science/article/pii/016740489597048F
KW  - Monitor
KW  - Active
KW  - Passive
KW  - Reference Monitor
KW  - Policies
KW  - Distributed systems
KW  - Assurance
AB  - This paper views the Reference Monitor in a new framework that makes it possible to generalize from passive to active monitors. It describes a major trend in the evolution of information systems security. The concepts are a practical reflection of real-world needs, expressed in a theoretical framework. The approach of employing active and passive policies provides a higher level of security than would otherwise be possible. The passive traditional Reference Monitor that interprets security policies and permits or prohibits access requests is supplemented by an active monitor to initiate behavior, such as taking positive actions to maintain integrity, taking recovery actions to restore situations after failures, and regularly monitoring the system. This extension to enforcement of various policies supports distributed systems architectures as the appropriate model for thinking about information technology (IT) security.
ER  - 

TY  - JOUR
T1  - Information security in workstation environments
JO  - Computers & Security
VL  - 12
IS  - 2
SP  - 117
EP  - 122
PY  - 1993/3//
T2  - 
AU  - Stahl, Stanley H.
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/0167-4048(93)90090-R
UR  - http://www.sciencedirect.com/science/article/pii/016740489390090R
AB  - This article explores information security issues in a context directed to meeting the needs of distributed workstation environments. It is not intended to be self-contained. Many topics, particularly those that are generic to securing computer and communication systems, are hardly touched on. The intent has been to (i) provide a context for exploring information security issues associated with distributed workstation environments and (ii) explore a few of the most important technology-based counter-measures available in workstation security.
ER  - 

TY  - JOUR
T1  - With MVS/ESA security labels towards B1
JO  - Computers & Security
VL  - 10
IS  - 4
SP  - 309
EP  - 324
PY  - 1991/6//
T2  - 
AU  - Paans, Ronald
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/0167-4048(91)90106-N
UR  - http://www.sciencedirect.com/science/article/pii/016740489190106N
AB  - IBM's MVS operating system at the level MVS/ESA 3.1.3 supplemented by RACF 1.9 has intentionally been designed to function as part of a trusted computer base with B1 security. Among the enhancements there is full support for security labels for both subjects and objects, allowing implementation of a mandatory access control policy and the use of MVS/ESA systems as multi-level security systems. The evolution of the MVS security mechanisms ultimately resulting in the introduction of security labels will be discussed from the view- point of the technical EDP-auditor.
ER  - 

TY  - JOUR
T1  - It security testing, a practical guide — part 5: Security stress/loading testing
JO  - Computer Audit Update
VL  - 1993
IS  - 3
SP  - 7
EP  - 10
PY  - 1993/3//
T2  - 
AU  - Robertson, Bernard
AU  - Pullen, David
SN  - 0960-2593
DO  - http://dx.doi.org/10.1016/0960-2593(93)90041-X
UR  - http://www.sciencedirect.com/science/article/pii/096025939390041X
ER  - 

TY  - JOUR
T1  - Client server architectures and security
JO  - Computer Audit Update
VL  - 1992
IS  - 9
SP  - 8
EP  - 12
PY  - 1992/9//
T2  - 
AU  - Pullen, David
AU  - Robertson, Bernard
SN  - 0960-2593
DO  - http://dx.doi.org/10.1016/0960-2593(92)90011-B
UR  - http://www.sciencedirect.com/science/article/pii/096025939290011B
ER  - 

TY  - JOUR
T1  - Security issues in system development
JO  - Computer Audit Update
VL  - 1992
IS  - 9
SP  - 4
EP  - 8
PY  - 1992/9//
T2  - 
AU  - Price, G.R.
SN  - 0960-2593
DO  - http://dx.doi.org/10.1016/0960-2593(92)90010-K
UR  - http://www.sciencedirect.com/science/article/pii/096025939290010K
ER  - 

TY  - JOUR
T1  - Vulnerabilities and mitigation techniques toning in the cloud: A cost and vulnerabilities coverage optimization approach using Cuckoo search algorithm with Lévy flights
JO  - Computers & Security
VL  - 48
IS  - 
SP  - 1
EP  - 18
PY  - 2015/2//
T2  - 
AU  - Zineddine, Mhamed
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/j.cose.2014.09.002
UR  - http://www.sciencedirect.com/science/article/pii/S0167404814001333
KW  - Cloud computing
KW  - ICT security
KW  - Vulnerabilities mapping
KW  - Cuckoo search algorithm
KW  - Lévy flights algorithm
KW  - Optimization
AB  - Abstract
Information and Communication Technology (ICT) security issues have been a major concern for decades. Today's ICT infrastructure faces sophisticated attacks using combinations of multiple vulnerabilities to penetrate networks with devastating impact. With the recent rise of cloud computing as a new utility computing paradigm, organizations have been considering it as a viable option to outsource major IT services in order to cut costs. Some organizations have opted for a private or hybrid cloud to take advantage of the emerging technologies and services. However, ICT security issues have to be appropriately mitigated. This research proposes a cloud security framework and an approach for vulnerabilities coverage and cost optimization using Cuckoo search algorithm with Lévy flights as random walks. The objective is to mitigate an identified set of vulnerabilities using a selected set of techniques when minimizing cost and maximizing coverage. The results show that Cloud Computing providers and organizations implementing cloud technology within their premises can effectively balance IT security coverage and cost using the proposed approach.
ER  - 

TY  - JOUR
T1  - Auditing and security
JO  - Computer Audit Update
VL  - 1989
IS  - 5
SP  - 2
EP  - 7
PY  - 1989/3//
Y2  - 1989/4//
T2  - 
AU  - Blatchford, Clive
SN  - 0960-2593
DO  - http://dx.doi.org/10.1016/S0960-2593(89)80027-8
UR  - http://www.sciencedirect.com/science/article/pii/S0960259389800278
AB  - Summary
The purpose of this short paper is to create a generic administrative framework in which the operational management of information systems may be supported by effective auditing process.

Emphasis is placed on agreeing a set of overall control principles which may be mapped from the human world to that of the technologically based information systems. Five intial principles have been recognised. This list may be extended as the practical problems of managing and auditing security information systems, especially in the non-Defence sector, are explored.

The objectives of functionally rich, IT ‘open systems’ solutions are used to illustrate the requirements. Where relevant, specific ‘open’ standards activities (especially ISO, CCITT, ect) are referenced as the basis of future development.
ER  - 

TY  - JOUR
T1  - Insider Threat Detection Using Log Analysis and Event Correlation
JO  - Procedia Computer Science
VL  - 45
IS  - 
SP  - 436
EP  - 445
PY  - 2015///
T2  - International Conference on Advanced Computing Technologies and Applications (ICACTA)
AU  - Ambre, Amruta
AU  - Shekokar, Narendra
SN  - 1877-0509
DO  - http://dx.doi.org/10.1016/j.procs.2015.03.175
UR  - http://www.sciencedirect.com/science/article/pii/S1877050915004184
KW  - Security
KW  - log analysis
KW  - event correlation
KW  - Bayesian detection rate
KW  - false alarm rate
AB  - Abstract
Insider threat is one of the most dangerous security threat, and a much more complex issue. These insiders can be a former or a disgruntled employee or any business associate that has or had an authorised access to information for any particular organization. They have control and security measures. Hence continuous monitoring is essential to track each and every activity within the network. Log management is a strong technique which includes both Log analysis with event correlation which provides the root cause of any attack and network can be protected from security violations. Though intrusion detection is complex process, while checking the ability to detect intrusive behaviour within the internal environment, it has to take care of suppressing the false alarm rate. Some strong approach is required on the basis of which decisions can be taken fast. This paper proposes a probabilistic approach which illustrates the frequency of occurrence of event in percentage while still considering the false alarm rate at an acceptable level.
ER  - 

TY  - JOUR
T1  - Audit-Based Access Control for Electronic Health Records
JO  - Electronic Notes in Theoretical Computer Science
VL  - 168
IS  - 
SP  - 221
EP  - 236
PY  - 2007/2/8/
T2  - Proceedings of the Second International Workshop on Views on Designing Complex Architectures (VODCA 2006)
AU  - Dekker, M.A.C.
AU  - Etalle, S.
SN  - 1571-0661
DO  - http://dx.doi.org/10.1016/j.entcs.2006.08.028
UR  - http://www.sciencedirect.com/science/article/pii/S1571066107000382
KW  - distributed access control
KW  - audit
KW  - accountability
KW  - Electronic Health Record (EHR) systems
AB  - Traditional access control mechanisms aim to prevent illegal actions a-priori occurrence, i.e. before granting a request for a document. There are scenarios however where the security decision can not be made on the fly. For these settings we developed a language and a framework for a-posteriori access control. I this paper we show how the framework can be used in a practical scenario. In particular, we work out the example of an Electronic Health Record (EHR) system, we outline the full architecture needed for audit-based access control and we discuss the requirements and limitations of this approach concerning the underlying infrastructure and its users.
ER  - 

TY  - JOUR
T1  - A survey on Advanced Metering Infrastructure
JO  - International Journal of Electrical Power & Energy Systems
VL  - 63
IS  - 
SP  - 473
EP  - 484
PY  - 2014/12//
T2  - 
AU  - Rashed Mohassel, Ramyar
AU  - Fung, Alan
AU  - Mohammadi, Farah
AU  - Raahemifar, Kaamran
SN  - 0142-0615
DO  - http://dx.doi.org/10.1016/j.ijepes.2014.06.025
UR  - http://www.sciencedirect.com/science/article/pii/S0142061514003743
KW  - Advanced Metering Infrastructure
KW  - Smart metering
KW  - Smart Grid
AB  - Abstract
This survey paper is an excerpt of a more comprehensive study on Smart Grid (SG) and the role of Advanced Metering Infrastructure (AMI) in SG. The survey was carried out as part of a feasibility study for creation of a Net-Zero community in a city in Ontario, Canada. SG is not a single technology; rather it is a combination of different areas of engineering, communication and management. This paper introduces AMI technology and its current status, as the foundation of SG, which is responsible for collecting all the data and information from loads and consumers. AMI is also responsible for implementing control signals and commands to perform necessary control actions as well as Demand Side Management (DSM). In this paper we introduce SG and its features, establish the relation between SG and AMI, explain the three main subsystems of AMI and discuss related security issues.
ER  - 

TY  - JOUR
T1  - A brief overview and an experimental evaluation of data confidentiality measures on the cloud
JO  - Journal of Innovation in Digital Ecosystems
VL  - 1
IS  - 1–2
SP  - 1
EP  - 11
PY  - 2014/12//
T2  - 
AU  - Aljafer, Hussain
AU  - Malik, Zaki
AU  - Alodib, Mohammed
AU  - Rezgui, Abdelmounaam
SN  - 2352-6645
DO  - http://dx.doi.org/10.1016/j.jides.2015.02.001
UR  - http://www.sciencedirect.com/science/article/pii/S2352664515000024
KW  - Security and protection
KW  - Cloud
KW  - Encryption
KW  - Measurement
AB  - Abstract
Due to the many advantages offered by the cloud computing paradigm, it is fast becoming an enabling technology for many organizations, and even individual users. Flexibility and availability are two of the most important features that promote the wide spread adoption of this technology. In cloud-based data storage scenarios, where the data is controlled by a third party (i.e. the cloud service provider), the data owner usually does not have full control of its data at all stages. Consequently, this poses a prime security threat, and a major challenge is the development of a secure protocol for data storage, sharing, and retrieval. In recent years, a number of research works have targeted this problem. In this paper, we discuss some of the major approaches for secure data sharing in the cloud computing environment. The goal is to provide a concise survey of existing solutions, discuss their benefits, and point out any shortcomings for future research. Specifically, we focus on the use of encryption schemes, and provide a comparative study of the major schemes, through implementation of some representative frameworks.
ER  - 

TY  - JOUR
T1  - Intelligent security reporting — auditing security logs : Norman Crocker
JO  - Computers & Security
VL  - 15
IS  - 5
SP  - 417
EP  - 418
PY  - 1996///
T2  - 

SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/0167-4048(96)82634-4
UR  - http://www.sciencedirect.com/science/article/pii/0167404896826344
ER  - 

TY  - JOUR
T1  - GSSP (Generally-Accepted system Security Principles): A trip to abilene : Alan Krull
JO  - Computers & Security
VL  - 15
IS  - 5
SP  - 417
EP  - 
PY  - 1996///
T2  - 

SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/0167-4048(96)82630-7
UR  - http://www.sciencedirect.com/science/article/pii/0167404896826307
ER  - 

TY  - JOUR
T1  - Towards a trusted HDFS storage platform: Mitigating threats to Hadoop infrastructures using hardware-accelerated encryption with TPM-rooted key protection
JO  - Journal of Information Security and Applications
VL  - 19
IS  - 3
SP  - 224
EP  - 244
PY  - 2014/7//
T2  - 
AU  - Cohen, Jason C.
AU  - Acharya, Subrata
SN  - 2214-2126
DO  - http://dx.doi.org/10.1016/j.jisa.2014.03.003
UR  - http://www.sciencedirect.com/science/article/pii/S2214212614000155
KW  - Apache Hadoop
KW  - HDFS security
KW  - Trusted computing
KW  - Security
AB  - Abstract
As a follow-on to the authors' previous work, this paper further expands on the concept of creating a trusted Apache Hadoop Distributed File System (HDFS). We discuss our motivation and evaluate a threat model for HDFS, and address a set of common security concerns within HDFS through infrastructure and software involving data-at-rest encryption and integrity validation. To accomplish these goals, we make use of technology from the Trusted Computing Group, such as the pervasively available Trusted Platform Module. In addition, we discuss our design considerations in building an encryption framework for Hadoop in a trustworthy manner, and describe the results of our experiments creating an encryption scheme for Hadoop utilizing hardware key protections and AES-NI for encryption acceleration. As part of this design we evaluate the recently implemented crypto framework for Hadoop and independently test the performance claims of AES-NI regarding mitigating performance overhead.
ER  - 

TY  - JOUR
T1  - Cyber-risk decision models: To insure IT or not?
JO  - Decision Support Systems
VL  - 56
IS  - 
SP  - 11
EP  - 26
PY  - 2013/12//
T2  - 
AU  - Mukhopadhyay, Arunabha
AU  - Chatterjee, Samir
AU  - Saha, Debashis
AU  - Mahanti, Ambuj
AU  - Sadhukhan, Samir K.
SN  - 0167-9236
DO  - http://dx.doi.org/10.1016/j.dss.2013.04.004
UR  - http://www.sciencedirect.com/science/article/pii/S0167923613001115
KW  - Security breach
KW  - Cyber-risk
KW  - Cyber-insurance
KW  - Copula
KW  - Bayesian Belief Network
KW  - Premium
KW  - Utility models
AB  - Abstract
Security breaches adversely impact profit margins, market capitalization and brand image of an organization. Global organizations resort to the use of technological devices to reduce the frequency of a security breach. To minimize the impact of financial losses from security breaches, we advocate the use of cyber-insurance products. This paper proposes models to help firms decide on the utility of cyber-insurance products and to what extent they can use them. In this paper, we propose a Copula-aided Bayesian Belief Network (CBBN) for cyber-vulnerability assessment (C-VA), and expected loss computation. Taking these as an input and using the concepts of collective risk modeling theory, we also compute the premium that a cyber risk insurer can charge to indemnify cyber losses. Further, to assist cyber risk insurers and to effectively design products, we propose a utility based preferential pricing (UBPP) model. UBPP takes into account risk profiles and wealth of the prospective insured firm before proposing the premium.
ER  - 

TY  - JOUR
T1  - Privacy Threat Modeling for Emerging BiobankClouds
JO  - Procedia Computer Science
VL  - 37
IS  - 
SP  - 489
EP  - 496
PY  - 2014///
T2  - The 5th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN-2014)/ The 4th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH 2014)/ Affiliated Workshops
AU  - Gholami, Ali
AU  - Lind, Anna-Sara
AU  - Reichel, Jane
AU  - Litton, Jan-Eric
AU  - Edlund, Ake
AU  - Laure, Erwin
SN  - 1877-0509
DO  - http://dx.doi.org/10.1016/j.procs.2014.08.073
UR  - http://www.sciencedirect.com/science/article/pii/S1877050914010382
KW  - privacy-preservation
KW  - data security
KW  - cloud computing
KW  - threat modeling
KW  - requirement analysis
AB  - Abstract
There is an increased amount of data produced by next generation sequencing (NGS) machines which demand scalable storage and analysis of genomic data. In order to cope with this huge amount of information, many biobanks are interested in cloud computing capabilities such as on-demand elasticity of computing power and storage capacity. There are several security and privacy requirements mandated by personal data protection legislation which hinder biobanks from migrating big data generated by the NGS machines. This paper describes the privacy requirements of platform-as-service BiobankClouds according to the European Data Protection Directive (DPD). It identifies several key privacy threats which leave BiobankClouds vulnerable to an attack. This study benefits health-care application designers in the requirement elicitation cycle when building privacy-preserving BiobankCloud platforms.
ER  - 

TY  - JOUR
T1  - An automated system for rapid and secure device sanitization
JO  - Computers & Security
VL  - 42
IS  - 
SP  - 77
EP  - 91
PY  - 2014/5//
T2  - 
AU  - LaBarge, Ralph
AU  - Mazzuchi, Thomas A.
AU  - Sarkani, Shahram
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/j.cose.2014.01.008
UR  - http://www.sciencedirect.com/science/article/pii/S0167404814000194
KW  - Data privacy
KW  - Data sanitization
KW  - Disk sanitization
KW  - NVRAM sanitization
KW  - Security and privacy protection
AB  - Abstract
Public and private organizations face the challenges of protecting their networks from cyber-attacks, while reducing the amount of time and money spent on Information Technology. Organizations can reduce their expenditures by reusing server, switch and router hardware, but they must use reliable and efficient methods of sanitizing these devices before they can be redeployed. The sanitization process removes proprietary, sensitive or classified data, as well as persistent malware from a device prior to reuse. The Johns Hopkins University Applied Physics Laboratory has developed an automated, rapid, and secure method for sanitizing servers, switches and routers. This sanitization method was implemented and tested on several different types of network devices during the Cyber Measurement &amp; Analysis Center project, which was funded under Phases I and II of the DARPA (2008) National Cyber Range program. The performance of the automated sanitization system was excellent with an order of magnitude reduction in the time required to sanitize servers, routers and switches, and a significant improvement in the effectiveness of the sanitization process through the addition of persistent malware removal.
ER  - 

TY  - JOUR
T1  - A lightweight framework for secure life-logging in smart environments
JO  - Information Security Technical Report
VL  - 17
IS  - 3
SP  - 58
EP  - 70
PY  - 2013/2//
T2  - Security and Privacy for Digital Ecosystems
AU  - Petroulakis, Nikolaos E.
AU  - Tragos, Elias Z.
AU  - Fragkiadakis, Alexandros G.
AU  - Spanoudakis, George
SN  - 1363-4127
DO  - http://dx.doi.org/10.1016/j.istr.2012.10.005
UR  - http://www.sciencedirect.com/science/article/pii/S1363412712000465
KW  - Smart objects
KW  - Internet of Things
KW  - Life-logging
KW  - Security
KW  - Smart environment
KW  - GNUradio
AB  - As the world becomes an interconnected network where objects and humans interact with each other, new challenges and threats appear in the ecosystem. In this interconnected world, smart objects have an important role in giving users the chance for life-logging in smart environments. However, smart devices have several limitations with regards to memory, resources and computation power, hindering the opportunity to apply well-established security algorithms and techniques for secure life-logging on the Internet of Things (IoT) domain. The need for secure and trustworthy life-logging in smart environments is vital, thus, a lightweight approach has to be considered to overcome the constraints of Smart Objects. The purpose of this paper is to present in details the current topics of life-logging in smart environments, while describing interconnection issues, security threats and suggesting a lightweight framework for ensuring security, privacy and trustworthy life-logging. In order to investigate the efficiency of the lightweight framework and the impact of the security attacks on energy consumption, an experimental test-bed was developed including two interconnected users and one smart attacker, who attempts to intercept transmitted messages or interfere with the communication link. Several mitigation factors, such as power control, channel assignment and AES-128 encryption were applied for secure life-logging. Finally, research into the degradation of the consumed energy regarding the described intrusions is presented.
ER  - 

TY  - JOUR
T1  - Active cyber defense with denial and deception: A cyber-wargame experiment
JO  - Computers & Security
VL  - 37
IS  - 
SP  - 72
EP  - 77
PY  - 2013/9//
T2  - 
AU  - Heckman, Kristin E.
AU  - Walsh, Michael J.
AU  - Stech, Frank J.
AU  - O'Boyle, Todd A.
AU  - DiCato, Stephen R.
AU  - Herber, Audra F.
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/j.cose.2013.03.015
UR  - http://www.sciencedirect.com/science/article/pii/S016740481300076X
KW  - Computer security
KW  - Authentication
KW  - Denial
KW  - Deception
KW  - Counterdeception
KW  - Wargame
AB  - Abstract
In January 2012, MITRE performed a real-time, red team/blue team cyber-wargame experiment. This presented the opportunity to blend cyber-warfare with traditional mission planning and execution, including denial and deception tradecraft. The cyber-wargame was designed to test a dynamic network defense cyber-security platform being researched in The MITRE Corporation's Innovation Program called Blackjack, and to investigate the utility of using denial and deception to enhance the defense of information in command and control systems.

The Blackjack tool failed to deny the adversary access to real information on the command and control mission system. The adversary had compromised a number of credentials without the computer network defenders' knowledge, and thereby observed both the real command and control mission system and the fake command and control mission system. However, traditional denial and deception techniques were effective in denying the adversary access to real information on the real command and control mission system, and instead provided the adversary with access to false information on a fake command and control mission system.
ER  - 

TY  - JOUR
T1  - A survey of network flow applications
JO  - Journal of Network and Computer Applications
VL  - 36
IS  - 2
SP  - 567
EP  - 581
PY  - 2013/3//
T2  - 
AU  - Li, Bingdong
AU  - Springer, Jeff
AU  - Bebis, George
AU  - Hadi Gunes, Mehmet
SN  - 1084-8045
DO  - http://dx.doi.org/10.1016/j.jnca.2012.12.020
UR  - http://www.sciencedirect.com/science/article/pii/S1084804512002676
KW  - Machine learning
KW  - NetFlow
KW  - Network traffic analysis
KW  - Network security
KW  - sFlow
AB  - It has been over 16 years since Cisco's NetFlow was patented in 1996. Extensive research has been conducted since then and many applications have been developed. In this survey, we have reviewed an extensive number of studies with emphasis on network flow applications. First, we provide a brief introduction to sFlow, NetFlow and network traffic analysis. Then, we review the state of the art in the field by presenting the main perspectives and methodologies. Our analysis has revealed that network security has been an important research topic since the beginning. Advanced methodologies, such as machine learning, have been very promising. We provide a critique of the studies surveyed about datasets, perspectives, methodologies, challenges, future directions and ideas for potential integration with other Information Technology infrastructure and methods. Finally, we concluded this survey.
ER  - 

TY  - JOUR
T1  - Budget-aware Role Based Access Control
JO  - Computers & Security
VL  - 35
IS  - 
SP  - 37
EP  - 50
PY  - 2013/6//
T2  - Special Issue of the International Conference on Availability, Reliability and Security (ARES)
AU  - Salim, Farzad
AU  - Reid, Jason
AU  - Dulleck, Uwe
AU  - Dawson, Ed
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/j.cose.2012.11.002
UR  - http://www.sciencedirect.com/science/article/pii/S016740481200171X
KW  - Insider problem
KW  - Role Based Access Control
KW  - Misaligned incentives
KW  - Price discrimination
KW  - Economics of Information security
AB  - The suitability of Role Based Access Control (RBAC) is being challenged in dynamic environments like healthcare. In an RBAC system, a user's legitimate access may be denied if their need has not been anticipated by the security administrator at the time of policy specification. Alternatively, even when the policy is correctly specified an authorised user may accidentally or intentionally misuse the granted permission. The heart of the challenge is the intrinsic unpredictability of users' operational needs as well as their incentives to misuse permissions. In this paper we propose a novel Budget-aware Role Based Access Control (B-RBAC) model that extends RBAC with the explicit notion of budget and cost, where users are assigned a limited budget through which they pay for the cost of permissions they need. We propose a model where the value of resources are explicitly defined and an RBAC policy is used as a reference point to discriminate the price of access permissions, as opposed to representing hard and fast rules for making access decisions. This approach has several desirable properties. It enables users to acquire unassigned permissions if they deem them necessary. However, users misuse capability is always bounded by their allocated budget and is further adjustable through the discrimination of permission prices. Finally, it provides a uniform mechanism for the detection and prevention of misuses.
ER  - 

TY  - JOUR
T1  - A survey on gaps, threat remediation challenges and some thoughts for proactive attack detection in cloud computing
JO  - Future Generation Computer Systems
VL  - 28
IS  - 6
SP  - 833
EP  - 851
PY  - 2012/6//
T2  - Including Special sections SS: Volunteer Computing and Desktop Grids and SS: Mobile Ubiquitous Computing
AU  - Khorshed, Md. Tanzim
AU  - Ali, A.B.M. Shawkat
AU  - Wasimi, Saleh A.
SN  - 0167-739X
DO  - http://dx.doi.org/10.1016/j.future.2012.01.006
UR  - http://www.sciencedirect.com/science/article/pii/S0167739X12000180
KW  - Security
KW  - Threats
KW  - Machine learning
KW  - Trust
KW  - Cloud computing
AB  - The long-term potential benefits through reduction of cost of services and improvement of business outcomes make Cloud Computing an attractive proposition these days. To make it more marketable in the wider IT user community one needs to address a variety of information security risks. In this paper, we present an extensive review on cloud computing with the main focus on gaps and security concerns. We identify the top security threats and their existing solutions. We also investigate the challenges/obstacles in implementing threat remediation. To address these issues, we propose a proactive threat detection model by adopting three main goals: (i) detect an attack when it happens, (ii) alert related parties (system admin, data owner) about the attack type and take combating action, and (iii) generate information on the type of attack by analyzing the pattern (even if the cloud provider attempts subreption). To emphasize the importance of monitoring cyber attacks we provide a brief overview of existing literature on cloud computing security. Then we generate some real cyber attacks that can be detected from performance data in a hypervisor and its guest operating systems. We employ modern machine learning techniques as the core of our model and accumulate a large database by considering the top threats. A variety of model performance measurement tools are applied to verify the model attack prediction capability. We observed that the Support Vector Machine technique from statistical machine learning theory is able to identify the top attacks with an accuracy of 97.13%. We have detected the activities using performance data (CPU, disk, network and memory performance) from the hypervisor and its guest operating systems, which can be generated by any cloud customer using built-in or third party software. Thus, one does not have to depend on cloud providers’ security logs and data. We believe our line of thoughts comprising a series of experiments will give researchers, cloud providers and their customers a useful guide to proactively protect themselves from known or even unknown security issues that follow the same patterns.
ER  - 

TY  - JOUR
T1  - Self-sustaining, efficient and forward-secure cryptographic constructions for Unattended Wireless Sensor Networks
JO  - Ad Hoc Networks
VL  - 10
IS  - 7
SP  - 1204
EP  - 1220
PY  - 2012/9//
T2  - 
AU  - Yavuz, Attila Altay
AU  - Ning, Peng
SN  - 1570-8705
DO  - http://dx.doi.org/10.1016/j.adhoc.2012.03.006
UR  - http://www.sciencedirect.com/science/article/pii/S1570870512000479
KW  - Applied cryptography
KW  - Unattended Wireless Sensor Networks (UWSNs)
KW  - Digital signatures
KW  - Forward security
KW  - Aggregate signatures
AB  - Unattended Wireless Sensor Networks (UWSNs) operating in hostile environments face great security and performance challenges due to the lack of continuous real-time communication with the final data receivers (e.g., mobile data collectors). The lack of real-time communication forces sensors to accumulate sensed data possibly for long time periods, along with the corresponding authentication tags. It also makes UWSNs vulnerable to active adversaries, which can compromise sensors and manipulate the collected data. Hence, it is critical to have forward security property such that even if the adversary can compromise the current keying materials, she cannot forge authentication tags generated before the compromise. Forward secure and aggregate signature schemes are developed to address these issues. Unfortunately, existing schemes either impose substantial overhead, or do not allow public verifiability, thereby impractical for resource-constrained UWSNs.

In this paper, we propose a new class of cryptographic schemes, referred to as Hash-BasedSequentialAggregate andForwardSecureSignature (HaSAFSS), which allows a signer to sequentially generate a compact, fixed-size, and publicly verifiable signature efficiently. We develop three HaSAFSS schemes, Symmetric HaSAFSS (Sym-HaSAFSS), Elliptic Curve Cryptography (ECC) based HaSAFSS (ECC-HaSAFSS) and self-SUstaining HaSAFSS (SU-HaSAFSS). These schemes integrate the efficiency of MAC-based aggregate signatures and the public verifiability of Public Key Cryptography (PKC)-based signatures by preserving forward security via Timed-Release Encryption (TRE). We demonstrate that our schemes are secure and also significantly more efficient than previous approaches.
ER  - 

TY  - JOUR
T1  - Online game bot detection based on party-play log analysis
JO  - Computers & Mathematics with Applications
VL  - 65
IS  - 9
SP  - 1384
EP  - 1395
PY  - 2013/5//
T2  - Advanced Information Security
AU  - Kang, Ah Reum
AU  - Woo, Jiyoung
AU  - Park, Juyong
AU  - Kim, Huy Kang
SN  - 0898-1221
DO  - http://dx.doi.org/10.1016/j.camwa.2012.01.034
UR  - http://www.sciencedirect.com/science/article/pii/S0898122112000442
KW  - Online game security
KW  - Game bot
KW  - User behavior analysis
KW  - MMORPG
AB  - As online games become popular and the boundary between virtual and real economies blurs, cheating in games has proliferated in volume and method. In this paper, we propose a framework for user behavior analysis for bot detection in online games. Specifically, we focus on party play which reflects the social activities among gamers: in a Massively Multi-user Online Role Playing Game (MMORPG), party play is a major activity that game bots exploit to keep their characters safe and facilitate the acquisition of cyber assets in a fashion very different from that of normal humans. Through a comprehensive statistical analysis of user behaviors in game activity logs, we establish threshold levels for the activities that allow us to identify game bots. Based on this, we also build a knowledge base of detection rules, which are generic. We apply our rule reasoner to AION, a popular online game serviced by NCsoft, Inc., a leading online game company based in Korea.
ER  - 

TY  - JOUR
T1  - An investigation of hotlinking and its countermeasures
JO  - Computer Communications
VL  - 34
IS  - 4
SP  - 577
EP  - 590
PY  - 2011/4/1/
T2  - Special issue: Building Secure Parallel and Distributed Networks and Systems
AU  - Chu, Zi
AU  - Wang, Haining
SN  - 0140-3664
DO  - http://dx.doi.org/10.1016/j.comcom.2010.05.007
UR  - http://www.sciencedirect.com/science/article/pii/S0140366410002422
KW  - Hotlinking
KW  - Resource hosting
KW  - Web measurement
KW  - Web security
AB  - Hotlinking is a web behavior that links web resources on a hosting site into a webpage belonging to another site. However, unauthorized hotlinking is unethical, because it not only violates the interests of hosting sites by consuming bandwidth and detracting site visiting traffic but also violates the copyrights of protected materials. To fully understand the nature of hotlinking, we conduct a large-scale measurement study and observe that hotlinking widely exists over the Internet and is severe in certain categories of websites. Moreover, we perform a detailed postmortem analysis on a real hotlink–victim site. After analyzing a group of commonly used hotlinking attacks and the weakness of current defense methods, we present an anti-hotlinking framework for protecting materials on hosting servers based on existing network security techniques. The framework can be easily deployed at the server-side with moderate modifications, and is highly customizable with different granularities of protection. We implement a prototype of the framework and evaluate its effectiveness against hotlinking attacks.
ER  - 

TY  - JOUR
T1  - Compliance by design – Bridging the chasm between auditors and IT architects
JO  - Computers & Security
VL  - 30
IS  - 6–7
SP  - 410
EP  - 426
PY  - 2011/9//
Y2  - 2011/10//
T2  - 
AU  - Julisch, Klaus
AU  - Suter, Christophe
AU  - Woitalla, Thomas
AU  - Zimmermann, Olaf
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/j.cose.2011.03.005
UR  - http://www.sciencedirect.com/science/article/pii/S0167404811000514
KW  - Information systems audit
KW  - CAVR
KW  - Compliance
KW  - Security architecture
KW  - Patterns
KW  - Service-oriented architecture
KW  - Business processes
KW  - Enterprise applications
AB  - System and process auditors assure – from an information processing perspective – the correctness and integrity of the data that is aggregated in a company’s financial statements. To do so, they assess whether a company’s business processes and information systems process financial data correctly. The audit process is a complex endeavor that in practice has to rely on simplifying assumptions. These simplifying assumptions mainly result from the need to restrict the audit scope and to focus it on the major risks. This article describes a generalized audit process. According to our experience with this process, there is a risk that material deficiencies remain undiscovered when said simplifying assumptions are not satisfied. To address this risk of deficiencies, the article compiles thirteen control patterns, which – according to our experience – are particularly suited to help information systems satisfy the simplifying assumptions. As such, use of these proven control patterns makes information systems easier to audit and IT architects can use them to build systems that meet audit requirements by design. Additionally, the practices and advice offered in this interdisciplinary article help bridge the gap between the architects and auditors of information systems and show either role how to benefit from an understanding of the other role’s terminology, techniques, and general work approach.
ER  - 

TY  - JOUR
T1  - A framework for incident response management in the petroleum industry
JO  - International Journal of Critical Infrastructure Protection
VL  - 2
IS  - 1–2
SP  - 26
EP  - 37
PY  - 2009/5//
T2  - 
AU  - Jaatun, Martin Gilje
AU  - Albrechtsen, Eirik
AU  - Line, Maria B.
AU  - Tøndel, Inger Anne
AU  - Longva, Odd Helge
SN  - 1874-5482
DO  - http://dx.doi.org/10.1016/j.ijcip.2009.02.004
UR  - http://www.sciencedirect.com/science/article/pii/S1874548209000043
KW  - Incident response
KW  - Process control systems
KW  - Learning
KW  - Security culture
AB  - Incident response is the process of responding to and handling security-related incidents involving information and communications technology (ICT) infrastructure and data. Incident response has traditionally been reactive in nature, focusing mainly on technical issues. This paper presents the Incident Response Management (IRMA) method, which combines traditional incident response with proactive learning and socio-technical perspectives. The IRMA method is targeted at integrated operations within the petroleum industry, but it is also applicable to other industries that rely on process control systems.
ER  - 

TY  - JOUR
T1  - HIPAA compliant auditing system for medical images
JO  - Computerized Medical Imaging and Graphics
VL  - 29
IS  - 2–3
SP  - 235
EP  - 241
PY  - 2005/3//
Y2  - 2005/4//
T2  - Imaging Informatics
AU  - Zhou, Zheng
AU  - Liu, Brent J.
SN  - 0895-6111
DO  - http://dx.doi.org/10.1016/j.compmedimag.2004.09.009
UR  - http://www.sciencedirect.com/science/article/pii/S0895611104001223
KW  - HIPAA
KW  - Security
KW  - HIPAA compliant auditing system
KW  - Auditing
KW  - monitoring
AB  - As an official regulation for healthcare privacy and security, Health Insurance Portability and Accountability Act (HIPAA) mandates health institutions to protect health information against unauthorized use or disclosure. One such method proposed by HIPAA Security Standards is audit trail, which records and examines health information access activities. HIPAA mandates healthcare providers to have the ability to generate audit trails on data access activities for any specific patient. Although current medical imaging systems generate activity logs, there is a lack of formal methodology to interpret these large volumes of log data and generate HIPAA compliant auditing trails.

This paper outlines the design of a HIPAA compliant auditing system (HCAS) for medical images in imaging systems such as PACS and discusses the development of a security monitoring (SM) toolkit based on some of the partial components in HCAS.
ER  - 

TY  - JOUR
T1  - Classification of web robots: An empirical study based on over one billion requests
JO  - Computers & Security
VL  - 28
IS  - 8
SP  - 795
EP  - 802
PY  - 2009/11//
T2  - 
AU  - Lee, Junsup
AU  - Cha, Sungdeok
AU  - Lee, Dongkun
AU  - Lee, Hyungkyu
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/j.cose.2009.05.004
UR  - http://www.sciencedirect.com/science/article/pii/S0167404809000546
KW  - Web robot classification
KW  - Web robot detection
KW  - Web robot characterization
KW  - Web security
KW  - Web usage mining
AB  - Many studies on detection and classification of web robots have focused their attention mostly on text crawlers, and empirical experiments used relatively small data collected at universities. In this paper, we analyzed more than one billion requests to www.microsoft.com in 24 h. Web logs were made anonymous to eliminate potential privacy concerns while preserving essential characteristics (e.g., frequency, queries, etc). We have developed an effective characterization metrics, based on workload characteristics and resource types, in detecting and classifying various web robots including text crawlers, link checkers, and icon crawlers. As expected, web robot behavior was clearly different from that of typical interactive users, and different types of web robots also exhibited different characteristics. However, comparison of the similar type of web robots, text crawlers in particular, revealed different characteristics, thereby enabling characterization with reasonably high confidence level. We divided various feature metrics into five groups, and effectiveness of each group in classification is shown in polar diagram in the decreasing order of effectiveness in the clockwise direction. One can use the findings to classify likely identify of unknown web robots, and organizations can develop appropriate measures to deal with them. Our analysis is based on recent web log data collected at one of the best known site which offers truly global service.
ER  - 

TY  - JOUR
T1  - NetHost-sensor: Monitoring a target host's application via system calls
JO  - Information Security Technical Report
VL  - 11
IS  - 4
SP  - 166
EP  - 175
PY  - 2006///
T2  - 
AU  - Abimbola, A.A.
AU  - Munoz, J.M.
AU  - Buchanan, W.J.
SN  - 1363-4127
DO  - http://dx.doi.org/10.1016/j.istr.2006.10.003
UR  - http://www.sciencedirect.com/science/article/pii/S1363412706000550
KW  - Intrusion detection
KW  - Network
KW  - Host
KW  - Application security
KW  - Dynamic link libraries
KW  - System calls
AB  - Intrusion detection has emerged as an important approach to network, host and application security. Network security includes analysing network packet payload and other inert network packet profiles for intrusive trends; whereas, host security may employ system logs for intrusion detection. In this paper, we contribute to the research community by tackling application security and attempt to detect intrusion via differentiating normal and abnormal application behaviour. A method for anomaly intrusion detection for applications is proposed based on deterministic system call traces derived from a monitored target application's dynamic link libraries (DLLs). We isolate associated DLLs of a monitored target application; log system call traces of the application in real time and use heuristic method to detect intrusion before the application is fully compromised. Our investigative research experiment methodology and set-up are reported, alongside our experimental procedure and results that show our research effort is effective and efficient, and can be used in practice to monitor a target application in real time.
ER  - 

TY  - JOUR
T1  - Intrusion detection aware component-based systems: A specification-based framework
JO  - Journal of Systems and Software
VL  - 80
IS  - 5
SP  - 700
EP  - 710
PY  - 2007/5//
T2  - Component-Based Software Engineering of Trustworthy Embedded Systems
AU  - Hussein, Mohammed
AU  - Zulkernine, Mohammad
SN  - 0164-1212
DO  - http://dx.doi.org/10.1016/j.jss.2006.08.017
UR  - http://www.sciencedirect.com/science/article/pii/S0164121206002263
KW  - Component-based software engineering
KW  - Component security
KW  - UML profile
KW  - Intrusion detection
AB  - Component-Based Software Engineering (CBSE) increases the reusability of software and hence decreases software development time and cost. Unfortunately, developing components for maximum reusability and acquiring third party components invite many security related concerns. The security related issues are more crucial for embedded and real-time systems. Currently, many approaches are proposed to aid the development and evaluation of secure components. However, it is well known among practitioners that, like any other software entities, components cannot be completely secure. This fact leads us to incorporate intrusion detection facilities to equip components with mechanisms to discover intrusions against components. In this paper, we present a framework for developing components with intrusion detection capabilities. This framework uses UMLintr, a UML profile for intrusion specifications. The profile allows developers to specify intrusion scenarios using UML diagrams. Specifying intrusion scenarios using the same language that is used for specifying software behavior eliminates the need for separate languages for describing intrusions. Other software specification languages can be easily adopted into this framework. The outcome of this framework are components equipped with intrusion detectors. Based on UMLintr, a prototype is built and used to generate signatures for some intrusions included in the benchmark DARPA attack datasets. Furthermore, we describe an Intrusion Detection System (IDS) which uses these signatures to detect component intrusions.
ER  - 

TY  - JOUR
T1  - A configurable cryptography subsystem in a middleware framework for embedded systems
JO  - Computer Networks
VL  - 46
IS  - 6
SP  - 771
EP  - 795
PY  - 2004/12/20/
T2  - 
AU  - McKinnon, A. David
AU  - Bakken, David E.
AU  - Shovic, John C.
SN  - 1389-1286
DO  - http://dx.doi.org/10.1016/j.comnet.2004.06.020
UR  - http://www.sciencedirect.com/science/article/pii/S1389128604001732
KW  - Middleware
KW  - Security
KW  - CORBA
KW  - Embedded systems
AB  - Computer and network security is becoming increasingly important as both large systems and, increasingly small, embedded systems are networked. Middleware frameworks aid the system developer who must interconnect individual systems into larger interconnected, distributed systems. However, there exist very few middleware frameworks that have been designed for use with embedded systems, which constitute the vast majority of CPUs produced each year, and none offer the range of security mechanisms required by the wide range of embedded system applications. This paper describes MicroQoSCORBA, a highly configurable middleware framework for embedded systems, and its security subsystem. It first presents an analysis of security requirements for embedded applications and what can and should be done in middleware. It then presents the design of MicroQoSCORBA’s security subsystem and the wide range of mechanisms it supports. Experimental results for these mechanisms are presented for two different embedded systems and one desktop computer that collectively represent a wide range of computational capabilities.
ER  - 

TY  - JOUR
T1  - Wired and wireless intrusion detection system: Classifications, good characteristics and state-of-the-art
JO  - Computer Standards & Interfaces
VL  - 28
IS  - 6
SP  - 670
EP  - 694
PY  - 2006/9//
T2  - 
AU  - Sobh, Tarek S.
SN  - 0920-5489
DO  - http://dx.doi.org/10.1016/j.csi.2005.07.002
UR  - http://www.sciencedirect.com/science/article/pii/S092054890500098X
KW  - Wireless network
KW  - Network-based security
KW  - Host-based security
KW  - Intrusion detection system
KW  - Intrusion prevention systems
KW  - Sniffering
AB  - In computer and network security, standard approaches to intrusion detection and response attempt to detect and prevent individual attacks. Intrusion Detection System (IDS) and intrusion prevention systems (IPS) are real-time software for risk assessment by monitoring for suspicious activity at the network and system layer. Software scanner allows network administrator to audit the network for vulnerabilities and thus securing potential holes before attackers take advantage of them.

In this paper we try to define the intruder, types of intruders, detection behaviors, detection approaches and detection techniques. This paper presents a structural approach to the IDS by introducing a classification of IDS. It presents important features, advantages and disadvantages of each detection approach and the corresponding detection techniques. Furthermore, this paper introduces the wireless intrusion protection systems.

The goal of this paper is to place some characteristics of good IDS and examine the positioning of intrusion prevention as part of an overall layered security strategy and a review of evaluation criteria for identifying and selecting IDS and IPS. With this, we hope to introduce a good characteristic in order to improve the capabilities for early detection of distributed attacks in the preliminary phases against infrastructure and take a full spectrum of manual and automatic response actions against the source of attacks.
ER  - 

TY  - JOUR
T1  - TRINETR: An architecture for collaborative intrusion detection and knowledge-based alert evaluation
JO  - Advanced Engineering Informatics
VL  - 19
IS  - 2
SP  - 93
EP  - 101
PY  - 2005/4//
T2  - Collaorative Environment for Desing and Manufacturing8th International Conference on CSCW
AU  - Yu, Jinqiao
AU  - Ramana Reddy, Y.V.
AU  - Selliah, Sentil
AU  - Reddy, Sumitra
AU  - Bharadwaj, Vijayanand
AU  - Kankanahalli, Srinivas
SN  - 1474-0346
DO  - http://dx.doi.org/10.1016/j.aei.2005.05.004
UR  - http://www.sciencedirect.com/science/article/pii/S1474034605000340
KW  - Network security
KW  - Intrusion detection
KW  - Alert
KW  - Intelligent agents
KW  - CSCW
AB  - Current reactive and standalone network security products are not capable of withstanding the onslaught of diversified network threats. As a result, a new security paradigm, where integrated security devices or systems collaborate closely to achieve enhanced protection and provide multi-layer defenses is emerging. In this paper, we present the design of a collaborative architecture for multiple intrusion detection systems to work together to detect real-time network intrusions. The detection is made more efficient and effective by using collaborative intelligent agents, relevant knowledge base and combination of multiple detection sensors. The architecture is composed of three parts: Collaborative Alert Aggregation, Knowledge-based Alert Evaluation and Alert Correlation. The architecture is aimed at reducing the alert overload by correlating results from multiple sensors to generate condensed views, reducing false positives by integrating network and host system information into the evaluation process and correlating events based on logical relations to generate global and synthesized alert report. The architecture is designed as a layer above intrusion detection for post-detection alert analysis and security actions. The first two parts of the architecture have been implemented and the implementation results are presented in this paper.
ER  - 

TY  - JOUR
T1  - A latent class modeling approach to detect network intrusion
JO  - Computer Communications
VL  - 30
IS  - 1
SP  - 93
EP  - 100
PY  - 2006/12/15/
T2  - 
AU  - Wang, Yun
AU  - Kim, Inyoung
AU  - Mbateng, Gaston
AU  - Ho, Shih-Yieh
SN  - 0140-3664
DO  - http://dx.doi.org/10.1016/j.comcom.2006.07.018
UR  - http://www.sciencedirect.com/science/article/pii/S0140366406002891
KW  - Intrusion detection
KW  - Machine learning
KW  - Classification
KW  - Latent class model
KW  - Computer security
AB  - This study presents a latent class modeling approach to examine network traffic data when labeled abnormal events are absent in training data, or such events are insufficient to fit a conventional regression model. Using six anomaly-associated risk factors identified from previous studies, the latent class model based on an unlabeled sample yielded acceptable classification results compared with a logistic regression model based on a labeled sample (correctly classified: 0.95 vs. 0.98, sensitivity: 0.99 vs. 0.99, and specificity: 0.77 vs. 0.97). The study demonstrates a great potency for using the latent class modeling technique to analyze network traffic data.
ER  - 

TY  - JOUR
T1  - SAD: web session anomaly detection based on parameter estimation
JO  - Computers & Security
VL  - 23
IS  - 4
SP  - 312
EP  - 319
PY  - 2004/6//
T2  - 
AU  - Cho, Sanghyun
AU  - Cha, Sungdeok
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/j.cose.2004.01.006
UR  - http://www.sciencedirect.com/science/article/pii/S0167404804000264
KW  - Computer security
KW  - Intrusion detection
KW  - Anomaly detection
KW  - Web attacks
KW  - Parameter estimation
KW  - Machine learning
AB  - Web attacks are too numerous in numbers and serious in potential consequences for modern society to tolerate. Unfortunately, current generation signature-based intrusion detection systems (IDS) are inadequate, and security techniques such as firewalls or access control mechanisms do not work well when trying to secure web services. In this paper, we empirically demonstrate that the Bayesian parameter estimation method is effective in analyzing web logs and detecting anomalous sessions. When web attacks were simulated with Whisker software, Snort, a well-known IDS based on misuse detection, caught only slightly more than one third of web attacks. Our technique, session anomaly detection (SAD), on the other hand, detected nearly all such attacks without having to rely on attack signatures at all. SAD works by first developing normal usage profile and comparing the web logs, as they are generated, against the expected frequencies. Our research indicates that SAD has the potential of detecting previously unknown web attacks and that the proposed approach would play a key role in developing an integrated environment to provide secure and reliable web services.
ER  - 

TY  - JOUR
T1  - Information Systems Audit Trails in Legal Proceedings as Evidence
JO  - Computers & Security
VL  - 20
IS  - 5
SP  - 409
EP  - 421
PY  - 2001/7/1/
T2  - 
AU  - Allinson, Caroline
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/S0167-4048(01)00513-2
UR  - http://www.sciencedirect.com/science/article/pii/S0167404801005132
KW  - law enforcement
KW  - audit trails
KW  - evidence
KW  - investigation, expert witness
KW  - information security
KW  - policy
KW  - court
KW  - survey
KW  - computer
AB  - Australian State and Commonwealth Governments are interested in the collection, storage and presentation of audit trail information, particularly within a legal framework. Law enforcement agencies have a legal obligation to keep audit records of all activity on information systems used within their operations. Little to no research has been identified in relation to the use of internal audit systems for evidentiary purpose.

A brief history of audit trails is given with requirements for such audit trails beyond the year 2000.

The Queensland Police Service (QPS), Australia, is used as a major case study . Information on principles, techniques and processes used, and the reason for the recording, storing and releasing of audit information for evidentiary purposes have been studied.

To assist in determining current practice in the Australian Commonwealth and State Governments the results of an Australia wide survey of all government departments are given and contrasted to the major study for QPS.

Reference is also made to the legal obligations for authorization of audit analysis, expert witnessing and legal precedence in relation to court acceptance or rejection of audit information used in evidence.

It is shown that most organizations studied generate and retain audit trails but the approach is not consistent nor is it comprehensive. It is suggested that these materials would not withstand a serious legal challenge.
ER  - 

TY  - JOUR
T1  - Evaluation of the performance of ID systems in a switched and distributed environment: the RealSecure case study
JO  - Computer Networks
VL  - 39
IS  - 2
SP  - 93
EP  - 112
PY  - 2002/6/5/
T2  - 
AU  - Iheagwara, Charles
AU  - Blyth, Andrew
SN  - 1389-1286
DO  - http://dx.doi.org/10.1016/S1389-1286(01)00301-2
UR  - http://www.sciencedirect.com/science/article/pii/S1389128601003012
KW  - Intrusion detection
KW  - Traffic analysis
KW  - Network security
KW  - Network surveillance
AB  - With the phenomenal increase of unwarranted Internet traffic into corporate networks the need for the development and effective use of currently available intrusion detection (ID) systems has acquired great importance. Compounding this is the constantly evolving techniques by professional hackers to defeat any and every counter measure designed to stem or at least contain their acts.

In this paper, we present the results of tests conducted to assess the effectiveness of intrusion detection system in a switched and distributed network environment. The results reveal that the performance of ID systems is a function of various factors including network topology, deployment techniques, and network throughput, bandwidth and network traffic conditions.

Within the limits of our studies, the findings can be summarized as: 1.
The detection capability of the ID system diminishes with increase in bandwidth utilization with the obvious implication that better performance could be achieved with the use of multiple sensors.
2.
Deployment at network or domain entry points i.e. outside decoy provides better performance results by up to 11%.
3.
Deployment with packet loss limiting devices produces a better result than deployment with the port mirroring technique by up to 27%.
ER  - 

TY  - JOUR
T1  - NADIR: An automated system for detecting network intrusion and misuse
JO  - Computers & Security
VL  - 12
IS  - 3
SP  - 235
EP  - 248
PY  - 1993/5//
T2  - 
AU  - Hochberg, Judith
AU  - Jackson, Kathleen
AU  - Stallings, Cathy
AU  - McClary, J.F.
AU  - DuBois, David
AU  - Ford, Josephine
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/0167-4048(93)90110-Q
UR  - http://www.sciencedirect.com/science/article/pii/016740489390110Q
KW  - Computer security
KW  - intrusion detection
KW  - misuse detection
KW  - anomaly detection
KW  - expert system
AB  - This paper describes a misuse detection system for Los Alamos National Laboratory's Integrated Computing Network (ICN). This automated expert system, the Network Anomaly Detection and Intrusion Reporter (NADIR), streamlines and supplements the manual audit record review traditionally performed by security auditors. NADIR compares network activity, as summarized in weekly profiles of individual users and the ICN as a whole, against expert rules that define security policy and improper or suspicious behaviour. NADIR reports suspicious behaviour to security auditors and provides tools to aid in follow-up investigations. This paper describes analysis by NADIR of two types of ICN activity: user authentication and access control, and mass file storage. It highlights system design issues of data handling, exploiting existing auditing systems, and performing audit analysis at the network level.
ER  - 

TY  - JOUR
T1  - CareWeb™, a web-based medical record for an integrated health care delivery system
JO  - International Journal of Medical Informatics
VL  - 54
IS  - 1
SP  - 1
EP  - 8
PY  - 1999/4//
T2  - 
AU  - Halamka, John D.
AU  - Osterland, Carsten
AU  - Safran, Charles
SN  - 1386-5056
DO  - http://dx.doi.org/10.1016/S1386-5056(98)00095-1
UR  - http://www.sciencedirect.com/science/article/pii/S1386505698000951
KW  - Patient records
KW  - Internet
KW  - Security
AB  - With the advent of Integrated Health care Delivery Systems, medical records are increasingly distributed across multiple institutions. Timely access to these medical records is a critical need for health care providers. The CareWeb™ project provides an architecture for World Wide Web-based retrieval of electronic medical records from heterogeneous data sources. Using Health Level 7 (HL7), web technologies and readily available software components, we consolidated the electronic records of Boston's Beth Israel and Deaconess Hospitals. We report on the creation of CareWeb™ (freya.bidmc.harvard.edu/careweb.htm) and propose it as a means to electronically link Integrated Health care Delivery Systems and geographically distant information resources.
ER  - 

TY  - JOUR
T1  - Leaving a trace
JO  - Infosecurity
VL  - 5
IS  - 6
SP  - 30
EP  - 35
PY  - 2008/9//
T2  - 
AU  - Gold, Steve
SN  - 1754-4548
DO  - http://dx.doi.org/10.1016/S1754-4548(08)70102-5
UR  - http://www.sciencedirect.com/science/article/pii/S1754454808701025
AB  - IT forensics is seen by many in the industry as something of a black art. But it's actually a highly professional discipline, with professional software to assist, Steve Gold discovers
ER  - 

TY  - JOUR
T1  - Log management for effective incident response
JO  - Network Security
VL  - 2005
IS  - 9
SP  - 4
EP  - 7
PY  - 2005/9//
T2  - 
AU  - Forte, Dario
SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/S1353-4858(05)70279-8
UR  - http://www.sciencedirect.com/science/article/pii/S1353485805702798
AB  - Log file correlation is related to two distinct activities: Intrusion Detection and Network Forensics. It is more important than ever that these two disciplines work together, and in cooperation, to avoid points of failure. This article presents an overview of log analysis and correlation, with special emphasis on the tools and techniques for managing them within a network forensics context.
ER  - 

TY  - JOUR
T1  - Identifying threats in real time
JO  - Network Security
VL  - 2013
IS  - 11
SP  - 5
EP  - 8
PY  - 2013/11//
T2  - 
AU  - Macrae, Alistair
SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/S1353-4858(13)70119-3
UR  - http://www.sciencedirect.com/science/article/pii/S1353485813701193
AB  - The technology landscape is constantly evolving, and as it does cyber-criminals have no problem keeping pace by altering their increasingly sophisticated tactics to exploit vulnerabilities in the networks of organisations, individuals and nation-states. As such, the fast and accurate detection of potential cyber-threats has become a critical capability in order to avoid significant damage to reputations, sensitive information and, in some extreme cases, lives.

As cyber-attacks become an increasing risk for every organisation, the detection and remediation of any issues must occur immediately to avoid irreparable damage. And IT security professionals should gather as much intelligence about a breach as possible.

Only through advanced correlative, statistical, behavioural and pattern recognition techniques can threats be identified in real time. Alistair Macrae of LogRhythm explores the techniques that can be used for such real-time identification and analysis of breaches, as well as how to navigate the complicated minefield of cyber-security forensic requirements and investigative procedures.
ER  - 

TY  - JOUR
T1  - SoTE: Strategy of Triple-E on solving Trojan defense in Cyber-crime cases
JO  - Computer Law & Security Review
VL  - 26
IS  - 1
SP  - 52
EP  - 60
PY  - 2010/1//
T2  - 
AU  - Kao, Da-Yu
AU  - Wang, Shiuh-Jeng
AU  - Fu-Yuan Huang, Frank
SN  - 0267-3649
DO  - http://dx.doi.org/10.1016/j.clsr.2009.09.008
UR  - http://www.sciencedirect.com/science/article/pii/S0267364909001575
KW  - Cyber-crime
KW  - Cyber criminology
KW  - Digital evidence
KW  - Trojan defense
KW  - Triple-E strategy
AB  - Cyber activity has become an essential part of the general public's everyday life. The hacking threats of Cyber-crime are becoming more sophisticated as internet communication services are more popular. To further confirm the final finding of Cyber-crime, this study proposes three analytical tools to clarify the Cyber-crime issues by means of Ideal Log, M-N model and MDFA (Multi-faceted Digital Forensics Analysis) strategy, where Ideal Log is identified as a traceable element of digital evidence including four elements of IP Address, Timestamp, Digital Action, and Response Message. M-N model applies a formal method for collating and analyzing data sets of investigation-relevant logs in view of connected time with ISP logs. MDFA strategy attempts to outline the basic elements of Cyber-crime using new procedural investigative steps, and combining universal types of evidential information in terms of Evidence, Scene, Victim, and Suspect. After researchers figure out what has happened in Cyber-crime events, it will be easier to communicate with offenders, victims or related people. SoTE (Strategy of Triple-E) is discussed to observe Cyber-crime from the viewpoints of Education, Enforcement and Engineering. That approach is further analyzed from the fields of criminology, investigation and forensics. Each field has its different focus in dealing with diverse topics, such as: the policy of 6W1H (What, Which, When, Where, Who, Why, and How) questions, the procedure of MDFA strategy, the process of ideal Logs and M-N model. In addition, the case study and proposed suggestion of this paper are presented to counter Cyber-crime.
ER  - 

TY  - JOUR
T1  - Network investigation methodology for BitTorrent Sync: A Peer-to-Peer based file synchronisation service
JO  - Computers & Security
VL  - 
IS  - 
SP  - 
EP  - 
PY  - 
T2  - 
AU  - Scanlon, Mark
AU  - Farina, Jason
AU  - Kechadi, M-Tahar
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/j.cose.2015.05.003
UR  - http://www.sciencedirect.com/science/article/pii/S016740481500067X
KW  - BitTorrent Sync
KW  - Distributed storage
KW  - Peer-to-Peer
KW  - Network traffic analysis
KW  - Remote evidence acquisition
AB  - Abstract
High availability is no longer just a business continuity concern. Users are increasingly dependant on devices that consume and produce data in ever increasing volumes. A popular solution is to have a central repository which each device accesses after centrally managed authentication. This model of use is facilitated by cloud based file synchronisation services such as Dropbox, OneDrive, Google Drive and Apple iCloud. Cloud architecture allows the provisioning of storage space with “always-on” access. Recent concerns over unauthorised access to third party systems and large scale exposure of private data have made an alternative solution desirable. These events have caused users to assess their own security practices and the level of trust placed in third party storage services. One option is BitTorrent Sync, a cloudless synchronisation utility provides data availability and redundancy. This utility replicates files stored in shares to remote peers with access controlled by keys and permissions. While lacking the economies brought about by scale, complete control over data access has made this a popular solution. The ability to replicate data without oversight introduces risk of abuse by users as well as difficulties for forensic investigators. This paper suggests a methodology for investigation and analysis of the protocol to assist in the control of data flow across security perimeters.
ER  - 

TY  - JOUR
T1  - Businesses still unaware of risks of account data compromise
JO  - Computer Fraud & Security
VL  - 2011
IS  - 1
SP  - 17
EP  - 19
PY  - 2011/1//
T2  - 
AU  - Hosack, Benj
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(11)70007-0
UR  - http://www.sciencedirect.com/science/article/pii/S1361372311700070
AB  - PCI DSS and PA-DSS are helping to secure card payment processing and reduce fraud but data compromise remains a significant problem in the financial services sector.

In nearly all cases, a compromised business will have rogue cardholder data that it was not aware existed in its systems. Should a criminal identify a flaw in security and gain access to this unprotected data, the compromised entity would find itself in a very difficult situation with regard to forensic investigations, remediation costs, card schemes fines and fraud losses. Benj Hosack of Foregenix examines the issues and offers guidance.

In 2004, the Payment Card Industry Data Security Standard (PCI DSS) was announced as a set of security controls based on best practice and designed to protect cardholder data against the rising levels of fraud on credit cards. Formed initially by the card schemes (Visa, MasterCard, AMEX, JCB, Diners and Discover), the PCI DSS is now managed, maintained and developed by the PCI Security Standards Council (PCI SSC).
ER  - 

TY  - JOUR
T1  - IP spoofing and session hijacking
JO  - Network Security
VL  - 1995
IS  - 3
SP  - 6
EP  - 11
PY  - 1995/3//
T2  - 
AU  - Thomsen, Dan
SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/S1353-4858(00)80045-8
UR  - http://www.sciencedirect.com/science/article/pii/S1353485800800458
AB  - The internet, as well as the mainstream media, has been a buzz with discussions of hackers using a new modis operandi in attacking systems. The latest headline grabbing attack was well planned and well executed, like a James Bond covert operation. The attack was directed at Tsutomu Shimomura, a noted Unix security expert at San Diego's Super Computer Center. It is only due to his diligence as a system administrator and skill at computer crime forensics that enabled Tsutomu to piece together how the attack was staged.
ER  - 

TY  - JOUR
T1  - Secure Audit Log Management
JO  - Procedia Computer Science
VL  - 22
IS  - 
SP  - 1249
EP  - 1258
PY  - 2013///
T2  - 17th International Conference in Knowledge Based and Intelligent Information and Engineering Systems - KES2013
AU  - Söderström, Olof
AU  - Moradian, Esmiralda
SN  - 1877-0509
DO  - http://dx.doi.org/10.1016/j.procs.2013.09.212
UR  - http://www.sciencedirect.com/science/article/pii/S1877050913010053
KW  - Secure Log Management
KW  - Log Analysis
KW  - Log Server
KW  - Audit Log Event
AB  - Abstract
Log management and analysis is a vital part of organization's network management and system administration. Logs indicate current status of the system and contain information that refers to different security events, which occur within the system. Logs are used for different purposes, such as recording user activities, track authentication attempts, and other security events. Due to increasing number of threats against networks and systems, the number of security logs increases. However, many organizations that work in a distributed environment face following problems: log generation and storage, log protection, and log analysis. Moreover, ensuring that security, system and network administrators analyze log data in an effective way is another issue. In this research, we propose an approach for receiving, storing and administrating audit log events. Furthermore, we present a solution design that in a secure way allows organizations in distributed environments to send audit log transactions from different local networks to one centralized server.
ER  - 

TY  - JOUR
T1  - Elliptic Curves for Data Provenance
JO  - Procedia Computer Science
VL  - 45
IS  - 
SP  - 470
EP  - 476
PY  - 2015///
T2  - International Conference on Advanced Computing Technologies and Applications (ICACTA)
AU  - Srivastava, Kriti
AU  - Nand, Gaurav
SN  - 1877-0509
DO  - http://dx.doi.org/10.1016/j.procs.2015.03.082
UR  - http://www.sciencedirect.com/science/article/pii/S187705091500318X
KW  - Data Provenance
KW  - Elliptic curve cryptography
KW  - Secure Provenance
AB  - Abstract
Securing provenance data in distributed environment is a challenge and with the rapid increase in its size, volume and variety it becomes more challenging. Provenance data are more sensitive than actual data as they include the workflow or the chain kind of structure. A little change can be disastrous. There are various existing security algorithms and frameworks but distributed nature of infrastructure and large volume of data makes the implementation of existing security models very complex. This paper discusses the security challenges of provenance data and proposes a secure way to store provenance data in highly distributed infrastructure.
ER  - 

TY  - JOUR
T1  - Get ready for PCI DSS 3.0 with real-time monitoring
JO  - Computer Fraud & Security
VL  - 2015
IS  - 2
SP  - 17
EP  - 18
PY  - 2015/2//
T2  - 
AU  - Fernandes, Joel John
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(15)30009-9
UR  - http://www.sciencedirect.com/science/article/pii/S1361372315300099
AB  - PCI DSS 3.0 compliance has gained worldwide acceptance by card service providers – card issuers, banks, and merchants – that plan to protect their customers' cardholder data from being misused. PCI DSS 3.0 has 12 security requirements concerning the protection of cardholder data. All businesses that accept, store, process or transmit customers card data either online or offline have to adhere to those requirements.
ER  - 

TY  - JOUR
T1  - Securing medical networks
JO  - Network Security
VL  - 2007
IS  - 6
SP  - 13
EP  - 16
PY  - 2007/6//
T2  - 
AU  - Dantu, Ram
AU  - Oosterwijk, Herman
AU  - Kolan, Prakash
AU  - Husna, Husain
SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/S1353-4858(07)70055-7
UR  - http://www.sciencedirect.com/science/article/pii/S1353485807700557
AB  - The Health Information Portability and Accountability Act of 1996 (HIPAA) imposes strict regulations on healthcare institutions and commercial vendors to indemnify clinical data against unscrupulous users. Security vulnerabilities concerning hospital information systems not only negatively impact patient healthcare, but may also represent a potential federal violation. For a comprehensive understanding of the security of a radiology communication network, a detailed survey of the Picture Archiving and Communication Systems (PACS) was compiled. In this paper, we present survey results and a set of recommendations for implementing PACS security.
ER  - 

TY  - JOUR
T1  - Modern IP theft and the insider threat
JO  - Computer Fraud & Security
VL  - 2015
IS  - 6
SP  - 5
EP  - 10
PY  - 2015/6//
T2  - 
AU  - Warren, Mark
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(15)30056-7
UR  - http://www.sciencedirect.com/science/article/pii/S1361372315300567
AB  - Security has crept up the corporate agenda for various reasons, including concerns around intellectual property (IP) theft by external hackers, employees or outsiders impersonating employees.

To effectively detect and mitigate advanced threats focused on stealing critical IP such as source code, companies must look to new technologies that support the creation of data-centric internal threat detection. This strategy applies intelligence closer to the sensitive data it is intended to protect and can prevent theft by advanced attacks or insiders, explains Mark Warren of Perforce.

With high-profile data breaches increasingly making the headlines right around the world, cyber-security has become a priority for company boards across virtually all business sectors. The whole topic of security has crept up the corporate agenda for various reasons, including concerns around intellectual property (IP) theft by employees or outsiders impersonating employees.
ER  - 

TY  - JOUR
T1  - Secure log management for privacy assurance in electronic communications
JO  - Computers & Security
VL  - 27
IS  - 7–8
SP  - 298
EP  - 308
PY  - 2008/12//
T2  - 
AU  - Stathopoulos, Vassilios
AU  - Kotzanikolaou, Panayiotis
AU  - Magkos, Emmanouil
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/j.cose.2008.07.010
UR  - http://www.sciencedirect.com/science/article/pii/S0167404808000400
KW  - System logging
KW  - Network providers
KW  - Internal attacks
KW  - Integrity
KW  - Digital signatures
AB  - In this paper we examine logging security in the environment of electronic communication providers. We review existing security threat models for system logging and we extend these to a new security model especially suited for communication network providers, which also considers internal modification attacks. We also propose a framework for secure log management in public communication networks as well as an implementation design, in order to provide traceability under the extended security model. A key role to the proposed framework is given to an independent Regulatory Authority, which is responsible to maintain log integrity proofs in a remote environment and verify the integrity of the provider's log files during security audits.
ER  - 

TY  - JOUR
T1  - Tackling unknown threats
JO  - Network Security
VL  - 2014
IS  - 12
SP  - 16
EP  - 17
PY  - 2014/12//
T2  - 
AU  - Goldberg, Joe
SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/S1353-4858(14)70123-0
UR  - http://www.sciencedirect.com/science/article/pii/S1353485814701230
AB  - As organisations across the world embrace online channels to market, the bad guys of the Internet are being rewarded with more avenues in which to breach businesses. Open networks, online payment portals and even point of sale (POS) devices are leaving organisations open to more advanced and persistent threats. Such threats are coming from hacktivists, cyber-criminals, malicious insiders, and nation states. Aided by speed, persistence and smarts, they adeptly penetrate an organisation and exfiltrate confidential data without alerting traditional security software tools.
ER  - 

TY  - JOUR
T1  - End-to-end policy based encryption techniques for multi-party data management
JO  - Computer Standards & Interfaces
VL  - 36
IS  - 4
SP  - 689
EP  - 703
PY  - 2014/6//
T2  - Security in Information Systems: Advances and new Challenges.
AU  - Beiter, Michael
AU  - Casassa Mont, Marco
AU  - Chen, Liqun
AU  - Pearson, Siani
SN  - 0920-5489
DO  - http://dx.doi.org/10.1016/j.csi.2013.12.004
UR  - http://www.sciencedirect.com/science/article/pii/S0920548913001785
KW  - Cloud
KW  - Sticky policy
KW  - Policy enforcement
KW  - Privacy
KW  - Secret sharing
AB  - Abstract
We describe a data management solution and associated key management approaches to provide accountability within service provision networks, in particular addressing privacy issues in cloud computing applications. Our solution involves machine readable policies that stick to data to define allowed usage and obligations as data travels across multiple parties. Service providers have fine-grained access to specific data based on agreed policies, enforced by interactions with independent third parties that check for policy compliance before releasing decryption keys required for data access. We describe alternative solutions based upon Public Key Infrastructure (PKI), Identity Based Encryption (IBE) and advanced secret sharing schemes.
ER  - 

TY  - JOUR
T1  - Economics and the cyber challenge
JO  - Information Security Technical Report
VL  - 17
IS  - 1–2
SP  - 9
EP  - 18
PY  - 2012/2//
T2  - Human Factors and Bio-metrics
AU  - Walker, Simon
SN  - 1363-4127
DO  - http://dx.doi.org/10.1016/j.istr.2011.12.003
UR  - http://www.sciencedirect.com/science/article/pii/S1363412711000860
AB  - Economics can be used as a tool to explain, describe, and to a certain extent predict many forms of human behaviour. However, there is only a limited body of work on its application to information security, much of which is acknowledged as partial or incomplete. As a consequence, there is a paucity of robust explanatory or predictive models that are tuned for the peculiarities of the “cyber” challenge, either to organisations, or, at a higher level, the nation state.

The effect of this is that the base arguments for information security business cases are often weak or flawed; as a result, there is an argument that both organisations and nation states will therefore tend to underinvest in information security. To improve this position, there would be benefits for information security, as a profession adopting economic models used in other areas of endeavour that historically have suffered similar problems. One potential model is full-cost accounting.

However, there are a number of further implications. These include an underlining of the importance of information security professional “speaking business language”. Also highlighted is the potential value of building a common knowledge base of the true cost of security failures, akin to the actuarial bodies of knowledge used in the insurance industry, rather than the partial and imperfect measures in use today.
ER  - 

TY  - JOUR
T1  - Gearing up for grid computing
JO  - Infosecurity Today
VL  - 2
IS  - 5
SP  - 22
EP  - 25
PY  - 2005/9//
Y2  - 2005/10//
T2  - 
AU  - Myerson, Judith M.
SN  - 1742-6847
DO  - http://dx.doi.org/10.1016/S1742-6847(05)70321-9
UR  - http://www.sciencedirect.com/science/article/pii/S1742684705703219
AB  - As an enterprise's infrastructure breaches the borders of both nations and its own direct control, application security becomes a hot issue, particularly in the new Europe.
ER  - 

TY  - JOUR
T1  - Sarbanes-Oxley: maybe a blessing, maybe a curse
JO  - Computer Fraud & Security
VL  - 2005
IS  - 9
SP  - 4
EP  - 7
PY  - 2005/9//
T2  - 
AU  - Power, Richard
AU  - Forte, Dario
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(05)70250-5
UR  - http://www.sciencedirect.com/science/article/pii/S1361372305702505
AB  - Sarbanes-Oxley can bring benefits and heartache to IT security managers. This article demonstrates the advantages and the headaches that the legislation can cause.
ER  - 

TY  - JOUR
T1  - Crossing Borders: The Right Side of Wrong?
JO  - Infosecurity
VL  - 8
IS  - 5
SP  - 22
EP  - 25
PY  - 2011/9//
Y2  - 2011/10//
T2  - 
AU  - Grossman, Wendy M.
SN  - 1754-4548
DO  - http://dx.doi.org/10.1016/S1754-4548(11)70065-1
UR  - http://www.sciencedirect.com/science/article/pii/S1754454811700651
AB  - Most nations consider travel data to be crucial to protecting national security. How that data is collected, stored, and secured however seems to be a closely guarded secret. Wendy M. Grossman investigates
ER  - 

TY  - JOUR
T1  - The evolution and application of SIEM systems
JO  - Network Security
VL  - 2014
IS  - 5
SP  - 16
EP  - 17
PY  - 2014/5//
T2  - 
AU  - Inns, Jon
SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/S1353-4858(14)70051-0
UR  - http://www.sciencedirect.com/science/article/pii/S1353485814700510
AB  - Anyone in a data-driven job like security, fraud, business intelligence, performance monitoring or any other data-dependant field will have heard about the latest phenomenon of ‘big data’. This exciting technological breakthrough promises to make business more efficient, identify anything that might harm it well ahead of time, and predict the future with pinpoint accuracy.

Big data has the potential to make business more efficient, identify anything that might harm it well ahead of time, and predict the future with pinpoint accuracy.

One of the most obvious commercial applications of this has been the use of SIEM tools. However, as Jon Inns of Accumuli explains, to return real value, SIEM systems need to be used with care and an understanding of what it is you're looking for and trying to achieve – and they need to be tuned to the task in hand.
ER  - 

TY  - JOUR
T1  - The 1999 DARPA off-line intrusion detection evaluation
JO  - Computer Networks
VL  - 34
IS  - 4
SP  - 579
EP  - 595
PY  - 2000/10//
T2  - Recent Advances in Intrusion Detection Systems
AU  - Lippmann, Richard
AU  - Haines, Joshua W
AU  - Fried, David J
AU  - Korba, Jonathan
AU  - Das, Kumar
SN  - 1389-1286
DO  - http://dx.doi.org/10.1016/S1389-1286(00)00139-0
UR  - http://www.sciencedirect.com/science/article/pii/S1389128600001390
KW  - Intrusion detection
KW  - Evaluate
KW  - Attack
KW  - Audit
KW  - Test bed
AB  - Eight sites participated in the second Defense Advanced Research Projects Agency (DARPA) off-line intrusion detection evaluation in 1999. A test bed generated live background traffic similar to that on a government site containing hundreds of users on thousands of hosts. More than 200 instances of 58 attack types were launched against victim UNIX and Windows NT hosts in three weeks of training data and two weeks of test data. False-alarm rates were low (less than 10 per day). The best detection was provided by network-based systems for old probe and old denial-of-service (DoS) attacks and by host-based systems for Solaris user-to-root (U2R) attacks. The best overall performance would have been provided by a combined system that used both host- and network-based intrusion detection. Detection accuracy was poor for previously unseen, new, stealthy and Windows NT attacks. Ten of the 58 attack types were completely missed by all systems. Systems missed attacks because signatures for old attacks did not generalize to new attacks, auditing was not available on all hosts, and protocols and TCP services were not analyzed at all or to the depth required. Promising capabilities were demonstrated by host-based systems, anomaly detection systems and a system that performs forensic analysis on file system data.
ER  - 

TY  - JOUR
T1  - Achieving Accountable MapReduce in cloud computing
JO  - Future Generation Computer Systems
VL  - 30
IS  - 
SP  - 1
EP  - 13
PY  - 2014/1//
T2  - Special Issue on Extreme Scale Parallel Architectures and Systems, Cryptography in Cloud Computing and Recent Advances in Parallel and Distributed Systems, ICPADS 2012 Selected Papers
AU  - Xiao, Zhifeng
AU  - Xiao, Yang
SN  - 0167-739X
DO  - http://dx.doi.org/10.1016/j.future.2013.07.001
UR  - http://www.sciencedirect.com/science/article/pii/S0167739X13001465
KW  - Accountable
KW  - MapReduce
KW  - Cloud computing
AB  - Abstract
MapReduce is a programming model that is capable of processing large data sets in distributed computing environments. The original MapReduce model was designed to be fault-tolerant in case of various network abnormalities. However, fault-tolerance does not guarantee that each working machine will be completely accountable; when nodes are malicious, they may intentionally misrepresent the processing result during mapping or reducing, and they may thus make the final results inaccurate and untrustworthy. In this paper, we propose Accountable MapReduce, which forces each machine to be held responsible for its behaviors. In our approach, we set up a group of auditors to perform an Accountability Test ( A -test) that checks all of the working machines and detects malicious nodes in real time. The A -test can be implemented with different options depending upon how the auditors are assigned. To optimize the utilization resource, we also formalize the Optimal Worker and Auditor Assignment (OWAA) problem, which is aimed at finding the optimal number of workers and auditors in order to minimize the total processing time. Our evaluation results show that the A -test can be practically and effectively applied to existing cloud platforms employing MapReduce.
ER  - 

TY  - JOUR
T1  - The root of the problem – malice, misuse or mistake?
JO  - Computer Fraud & Security
VL  - 2009
IS  - 1
SP  - 6
EP  - 9
PY  - 2009/1//
T2  - 
AU  - Small, Mike
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(09)70008-9
UR  - http://www.sciencedirect.com/science/article/pii/S1361372309700089
AB  - Organisations worldwide rely on business-critical information systems to run their businesses, yet a major loophole for many of these systems is the administrator account. Much has been made of the threat of external hackers who cause security breaches in organisations. Although this is a real problem, the insiders in an organisation may be responsible for as many security breaches as external hackers. In addition, not only malice but also mistakes and misuse by employees are an important reason for loss of vital services. How can organisations protect themselves from these problems?
ER  - 

TY  - JOUR
T1  - Intrusion detection systems as evidence
JO  - Computer Networks
VL  - 31
IS  - 23–24
SP  - 2477
EP  - 2487
PY  - 1999/12/14/
T2  - 
AU  - Sommer, Peter
SN  - 1389-1286
DO  - http://dx.doi.org/10.1016/S1389-1286(99)00113-9
UR  - http://www.sciencedirect.com/science/article/pii/S1389128699001139
KW  - Evidence
KW  - Proof
KW  - Hacker
AB  - Although the main aim of Intrusion Detection Systems (IDSs) is to detect intrusions to prompt evasive measures, a further aim can be to supply evidence in criminal and civil legal proceedings. However the features that make an ID product good at providing early warning may render it less useful as an evidence-acquisition tool. An explanation is provided of admissibility and weight, the two determinants in the legal acceptability of evidence. The problems the courts have in dealing with novel scientific evidence and the differences between `scientific' and `legal' proof are discussed. Criteria for the evaluation of IDSs as sources of legal evidence are proposed, including preservation of evidence, continuity of evidence and transparency of forensic method. It is suggested that the key to successful prosecution of complex intrusions is the finding of multiple independent streams of evidence which corroborate one another. The USAF Rome Labs intrusion of early 1994 is used as a case-study to show how defence experts and lawyers can undermine investigators’ evidence.
ER  - 

TY  - JOUR
T1  - Who's in control: a six-step strategy for secure IT
JO  - Network Security
VL  - 2011
IS  - 11
SP  - 18
EP  - 20
PY  - 2011/11//
T2  - 
AU  - Facey, Stuart
SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/S1353-4858(11)70121-0
UR  - http://www.sciencedirect.com/science/article/pii/S1353485811701210
AB  - As more and more organisations encourage flexible and remote working policies that allow employees to work outside the office, the complexity surrounding remote access and support mechanisms for the IT helpdesk has also increased. There is a growing and unregulated market for solutions that can ‘fix’ IT issues quickly and efficiently no matter where workers are located: however, as with many solutions, remote support and access products have their own inherent security risks that should not be underestimated.
ER  - 

TY  - JOUR
T1  - False alarm minimization techniques in signature-based intrusion detection systems: A survey
JO  - Computer Communications
VL  - 49
IS  - 
SP  - 1
EP  - 17
PY  - 2014/8/1/
T2  - 
AU  - Hubballi, Neminath
AU  - Suryanarayanan, Vinoth
SN  - 0140-3664
DO  - http://dx.doi.org/10.1016/j.comcom.2014.04.012
UR  - http://www.sciencedirect.com/science/article/pii/S0140366414001480
KW  - False alarms
KW  - Correlation
KW  - Intrusion detection
AB  - Abstract
A network based Intrusion Detection System (IDS) gathers and analyzes network packets and report possible low level security violations to a system administrator. In a large network setup, these low level and partial reports become unmanageable to the administrator resulting in some unattended events. Further it is known that state of the art IDS generate many false alarms. There are techniques proposed in IDS literature to minimize false alarms, many of which are widely used in practice in commercial Security Information and Event Management (SIEM) tools. In this paper, we review existing false alarm minimization techniques in signature-based Network Intrusion Detection System (NIDS). We give a taxonomy of false alarm minimization techniques in signature-based IDS and present the pros and cons of each class. We also study few of the prominent commercial SIEM tools which have implemented these techniques along with their performance. Finally, we conclude with some directions to the future research.
ER  - 

TY  - JOUR
T1  - MarketNet: protecting access to information systems through financial market controls
JO  - Decision Support Systems
VL  - 28
IS  - 1–2
SP  - 205
EP  - 216
PY  - 2000/3//
T2  - 
AU  - Yemini, Y.
AU  - Dailianas, A.
AU  - Florissi, D.
AU  - Huberman, G.
SN  - 0167-9236
DO  - http://dx.doi.org/10.1016/S0167-9236(99)00069-X
UR  - http://www.sciencedirect.com/science/article/pii/S016792369900069X
KW  - Information systems protection
KW  - Access control through currency
KW  - Information economy
KW  - Quantifiable exposure to attacks
KW  - Financial-like instruments
AB  - This paper describes novel market-based technologies that uniquely establish quantifiable and adjustable limits on the power of attackers, enable verifiable accountability for malicious attacks, and admit systematic and uniform monitoring and detection of attacks. These technologies, incorporated in the MarketNet system, establish a financial economy to regulate the trade and use of access rights in information systems. Resources are instrumented to use currency for access control and monitoring, establishing accountability in their use. Domains control access to their resources through resource prices and budgets available to clients. Domains control and fine tune their exposure to attacks; adjust this exposure in response to emerging risks; detect intrusion attacks through automated, uniform statistical analysis of currency flows; and tune their exposure to resource unavailability by purchasing protection through financial-like instruments.
ER  - 

TY  - JOUR
T1  - On a taxonomy of delegation
JO  - Computers & Security
VL  - 29
IS  - 5
SP  - 565
EP  - 579
PY  - 2010/7//
T2  - Challenges for Security, Privacy and Trust
AU  - Pham, Quan
AU  - Reid, Jason
AU  - McCullagh, Adrian
AU  - Dawson, Edward
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/j.cose.2009.12.009
UR  - http://www.sciencedirect.com/science/article/pii/S0167404809001473
KW  - Delegation
KW  - Authorisation
KW  - Taxonomy
KW  - Classification
KW  - Access control
AB  - Delegation, from a technical point of view, is widely considered as a potential approach in addressing the problem of providing dynamic access control decisions in activities with a high level of collaboration, either within a single security domain or across multiple security domains. Although delegation continues to attract significant attention from the research community, presently, there is no published work that presents a taxonomy of delegation concepts and models. This article intends to address this gap by presenting a set of taxonomic criteria relevant to the concept of delegation. This article also applies the taxonomy to a selection of significant delegation models published in the literature.
ER  - 

TY  - JOUR
T1  - The first 10 years of the Trojan Horse defence
JO  - Computer Fraud & Security
VL  - 2015
IS  - 1
SP  - 5
EP  - 13
PY  - 2015/1//
T2  - 
AU  - Bowles, Stephen
AU  - Hernandez-Castro, Julio
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(15)70005-9
UR  - http://www.sciencedirect.com/science/article/pii/S1361372315700059
AB  - Apprehended criminals throughout history have always attempted to put the blame on someone else, a strategy popularly known as a SODDI defence (Some Other Dude Did It). When this defence is used, the act of the crime (actus reus) and the guilty mind (mens rea) is blamed on another party. A Trojan Horse Defence (THD) is a type of modern SODDI defence, where the mens rea and actus reus are blamed on a piece of software, known as a trojan.1

It has now become common for people accused of some computer-related crime to claim that the responsibility lies with malware placed on their machine without their knowledge.

This so-called Trojan Horse Defence (THD) was first used a decade ago. In this article, Stephen Bowles and Julio Hernandez-Castro of the University of Kent undertake a timely retrospective with an in-depth and critical literature review plus a detailed look at the peculiarities of many court cases from around the world.
ER  - 

TY  - JOUR
T1  - A framework for preservation of cloud users’ data privacy using dynamic reconstruction of metadata
JO  - Journal of Network and Computer Applications
VL  - 36
IS  - 1
SP  - 235
EP  - 248
PY  - 2013/1//
T2  - 
AU  - Waqar, Adeela
AU  - Raza, Asad
AU  - Abbas, Haider
AU  - Khurram Khan, Muhammad
SN  - 1084-8045
DO  - http://dx.doi.org/10.1016/j.jnca.2012.09.001
UR  - http://www.sciencedirect.com/science/article/pii/S1084804512001890
KW  - Cloud computing
KW  - Private cloud
KW  - Ubuntu Enterprise Cloud Eucalyptus
KW  - Privacy
KW  - Metadata
AB  - In the rising paradigm of cloud computing, attainment of sustainable levels of cloud users’ trust in using cloud services is directly dependent on effective mitigation of its associated impending risks and resultant security threats. Among the various indispensible security services required to ensure effective cloud functionality leading to enhancement of users’ confidence in using cloud offerings, those related to the preservation of cloud users’ data privacy are significantly important and must be matured enough to withstand the imminent security threats, as emphasized in this research paper. This paper highlights the possibility of exploiting the metadata stored in cloud's database in order to compromise the privacy of users’ data items stored using a cloud provider's simple storage service. It, then, proposes a framework based on database schema redesign and dynamic reconstruction of metadata for the preservation of cloud users’ data privacy. Using the sensitivity parameterization parent class membership of cloud database attributes, the database schema is modified using cryptographic as well as relational privacy preservation operations. At the same time, unaltered access to database files is ensured for the cloud provider using dynamic reconstruction of metadata for the restoration of original database schema, when required. The suitability of the proposed technique with respect to private cloud environments is ensured by keeping the formulation of its constituent steps well aligned with the recommendations proposed by various Standards Development Organizations working in this domain.
ER  - 

TY  - JOUR
T1  - Protecting critical control systems
JO  - Network Security
VL  - 2012
IS  - 3
SP  - 7
EP  - 10
PY  - 2012/3//
T2  - 
AU  - Brewer, Ross
SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/S1353-4858(12)70044-2
UR  - http://www.sciencedirect.com/science/article/pii/S1353485812700442
AB  - With cyber-attacks continuing to grow in sophistication and frequency, both public and private organisations have been forced to change their outlook on cyber-security and re-examine their strategies when it comes to protecting their networks. System breaches are no longer considered unlikely, and the mindset has shifted to cyber-attacks being a matter of ‘when’ rather than ‘if’.

With cyber-attacks continuing to grow in sophistication and frequency, both public and private organisations have been forced to change their outlook on cyber-security and re-examine their strategies when it comes to protecting their networks. System breaches are no longer considered unlikely, and the mindset has shifted to cyber-attacks being a matter of ‘when’ rather than ‘if’.

Many critical infrastructure installations are controlled by Supervisory Control And Data Acquisition (SCADA) solutions that were never designed to be secure. However, with the correct strategic focus and resources applied, SCADA systems can be secured. Ross Brewer of LogRhythm suggests that a ‘protective monitoring’ approach can be tailored around networks that support high-value cyber-assets.
ER  - 

TY  - JOUR
T1  - Passwords: Use and Abuse
JO  - Computer Fraud & Security
VL  - 2001
IS  - 9
SP  - 14
EP  - 16
PY  - 2001/9/1/
T2  - 
AU  - Yapp, Peter
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(01)00916-2
UR  - http://www.sciencedirect.com/science/article/pii/S1361372301009162
AB  - The greatest threat to security is still from the inside, which is where nearly 70% of all frauds, misuses and abuses still come from. The perception may be that the hacker is the major threat, but poor password policies and controls are an internal problem.
ER  - 

TY  - JOUR
T1  - Intrusion detection system: A comprehensive review
JO  - Journal of Network and Computer Applications
VL  - 36
IS  - 1
SP  - 16
EP  - 24
PY  - 2013/1//
T2  - 
AU  - Liao, Hung-Jen
AU  - Richard Lin, Chun-Hung
AU  - Lin, Ying-Chih
AU  - Tung, Kuang-Yuan
SN  - 1084-8045
DO  - http://dx.doi.org/10.1016/j.jnca.2012.09.004
UR  - http://www.sciencedirect.com/science/article/pii/S1084804512001944
KW  - Intrusion detection
KW  - Anomaly
KW  - Misuse
AB  - With the increasing amount of network throughput and security threat, the study of intrusion detection systems (IDSs) has received a lot of attention throughout the computer science field. Current IDSs pose challenges on not only capricious intrusion categories, but also huge computational power. Though there is a number of existing literatures to IDS issues, we attempt to give a more elaborate image for a comprehensive review. Through the extensive survey and sophisticated organization, we propose the taxonomy to outline modern IDSs. In addition, tables and figures we summarized in the content contribute to easily grasp the overall picture of IDSs.
ER  - 

TY  - JOUR
T1  - Logging, auditing and filtering for Internet electronic commerce
JO  - Computer Fraud & Security
VL  - 1997
IS  - 8
SP  - 11
EP  - 16
PY  - 1997/8//
T2  - 
AU  - Cresson Wood, Charles
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(97)89846-6
UR  - http://www.sciencedirect.com/science/article/pii/S1361372397898466
AB  - This article provides a survey of the features and capabilities of commercial security products specifically designed for conducting business on the Internet. This article focuses on three not-so-glamorous but nonetheless essential, behind-the-scenes security activities. The author has no marketing, referral, or commission relationship with any of the vendors mentioned below.
ER  - 

TY  - JOUR
T1  - Electronic funds transfer fraud
JO  - Computer Fraud & Security
VL  - 2003
IS  - 12
SP  - 6
EP  - 9
PY  - 2003/12//
T2  - 
AU  - Henderson, Ian
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(03)00006-X
UR  - http://www.sciencedirect.com/science/article/pii/S136137230300006X
AB  - An Electronic Funds Transfer fraud can be a career-limiting event for anyone involved in treasury operations or IT security. This article describes a recent EFT fraud; the techniques used to investigate it and some of the many lessons learnt.
ER  - 

TY  - JOUR
T1  - Access control and audit model for the multidimensional modeling of data warehouses
JO  - Decision Support Systems
VL  - 42
IS  - 3
SP  - 1270
EP  - 1289
PY  - 2006/12//
T2  - 
AU  - Fernández-Medina, Eduardo
AU  - Trujillo, Juan
AU  - Villarroel, Rodolfo
AU  - Piattini, Mario
SN  - 0167-9236
DO  - http://dx.doi.org/10.1016/j.dss.2005.10.008
UR  - http://www.sciencedirect.com/science/article/pii/S0167923605001570
KW  - Data warehouses
KW  - Secure multidimensional modeling
KW  - Access control
KW  - Audit
KW  - UML
AB  - Due to the sensitive data contained in Data Warehouses (DW), it is essential to specify security measures from the early stages of the DW design and enforce them. Traditional access control models for transactional (relational) databases, based on tables, columns and rows, are not appropriate for DWs. Instead, security and audit rules defined for DWs must be specified based on the multidimensional (MD) modeling used to design data warehouses. Current approaches for the conceptual modeling of DWs do not allow us to specify security and confidentiality constraints in the conceptual modeling phase. In this paper, we propose an Access Control and Audit (ACA) model for DWs by specifying security rules in the conceptual MD modeling. Thus, we define authorization rules for users and objects and we assign sensitive information rules and authorization rules to the main elements of a MD model (e.g., facts or dimensions). Moreover, we also specify certain audit rules allowing us to analyze user behaviors. To be able to include and use our ACA model in the conceptual MD modeling, we extend the Unified Modeling Language (UML) with our ACA model, thereby allowing us to design secure MD models. Finally, to show the benefit of our approach, we apply our approach to a health care case study.
ER  - 

TY  - JOUR
T1  - Unsupervised Clustering of Web Sessions to Detect Malicious and Non-malicious Website Users
JO  - Procedia Computer Science
VL  - 5
IS  - 
SP  - 123
EP  - 131
PY  - 2011///
T2  - The 2nd International Conference on Ambient Systems, Networks and Technologies (ANT-2011) / The 8th International Conference on Mobile Web Information Systems (MobiWIS 2011)
AU  - Stevanovic, Dusan
AU  - Vlajic, Natalija
AU  - An, Aijun
SN  - 1877-0509
DO  - http://dx.doi.org/10.1016/j.procs.2011.07.018
UR  - http://www.sciencedirect.com/science/article/pii/S1877050911003437
KW  - Web Crawler Detection
KW  - Neural Networks
KW  - Web Server Access Logs
KW  - Machine Learning
KW  - Clustering
KW  - Denial of Service ;
AB  - Denial of Service (DoS) attacks are recognized as one of the most damaging attacks on the Internet security today. Recently, malicious web crawlers have been used to execute automated DoS attacks on web sites across the WWW. In this study, we examine the use of two unsupervised neural network (NN) learning algorithms for the purpose web-log analysis: the Self- Organizing Map (SOM) and Modified Adaptive Resonance Theory 2 (Modified ART2). In particular, through the use of SOM and Modified ART2, our work aims to obtain a better insight into the types and distribution of visitors to a public web-site based on their link-traversal behaviour, as well as to investigate the relative differences and/or similarities between malicious web crawlers and other non-malicious visitor groups. The results of our study show that, even though there is a pretty clear separation between malicious web-crawlers and other visitor groups, around 8% of malicious crawlers exhibit very ‘human-like’ browsing behaviour and as such pose a particular challenge for future web-site security systems.
ER  - 

TY  - JOUR
T1  - Preventing software piracy
JO  - Network Security
VL  - 1994
IS  - 9
SP  - 17
EP  - 19
PY  - 1994/9//
T2  - 
AU  - Schifreen, Robert
SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/1353-4858(94)90179-1
UR  - http://www.sciencedirect.com/science/article/pii/1353485894901791
AB  - It's up to you, as the person in charge of network security, to ensure that users are not storing pirated software on the server or workstations. Regular audits can help you achieve this.
ER  - 

TY  - JOUR
T1  - An extensible analysable system model
JO  - Information Security Technical Report
VL  - 13
IS  - 4
SP  - 235
EP  - 246
PY  - 2008/11//
T2  - 
AU  - Probst, Christian W.
AU  - Hansen, René Rydhof
SN  - 1363-4127
DO  - http://dx.doi.org/10.1016/j.istr.2008.10.012
UR  - http://www.sciencedirect.com/science/article/pii/S1363412708000502
AB  - Analysing real-world systems for vulnerabilities with respect to security and safety threats is a difficult undertaking, not least due to a lack of availability of formalisations for those systems. While both formalisations and analyses can be found for artificial systems such as software, this does not hold for real physical systems. Approaches such as threat modelling try to target the formalisation of the real-world domain, but still are far from the rigid techniques available in security research. Many currently available approaches to assurance of critical infrastructure security are based on (quite successful) ad-hoc techniques. We believe they can be significantly improved beyond the state-of-the-art by pairing them with static analyses techniques.

In this paper we present an approach to both formalising those real-world systems, as well as providing an underlying semantics, which allows for easy development of analyses for the abstracted systems. We briefly present one application of our approach, namely the analysis of systems for potential insider threats.
ER  - 

TY  - JOUR
T1  - A survey on vehicular cloud computing
JO  - Journal of Network and Computer Applications
VL  - 40
IS  - 
SP  - 325
EP  - 344
PY  - 2014/4//
T2  - 
AU  - Whaiduzzaman, Md
AU  - Sookhak, Mehdi
AU  - Gani, Abdullah
AU  - Buyya, Rajkumar
SN  - 1084-8045
DO  - http://dx.doi.org/10.1016/j.jnca.2013.08.004
UR  - http://www.sciencedirect.com/science/article/pii/S1084804513001793
KW  - Vehicular networks
KW  - Road vehicle control
KW  - Intelligent transportation systems
KW  - Cloud computing
KW  - Vehicular cloud computing
AB  - Abstract
Vehicular networking has become a significant research area due to its specific features and applications such as standardization, efficient traffic management, road safety and infotainment. Vehicles are expected to carry relatively more communication systems, on board computing facilities, storage and increased sensing power. Hence, several technologies have been deployed to maintain and promote Intelligent Transportation Systems (ITS). Recently, a number of solutions were proposed to address the challenges and issues of vehicular networks. Vehicular Cloud Computing (VCC) is one of the solutions. VCC is a new hybrid technology that has a remarkable impact on traffic management and road safety by instantly using vehicular resources, such as computing, storage and internet for decision making. This paper presents the state-of-the-art survey of vehicular cloud computing. Moreover, we present a taxonomy for vehicular cloud in which special attention has been devoted to the extensive applications, cloud formations, key management, inter cloud communication systems, and broad aspects of privacy and security issues. Through an extensive review of the literature, we design an architecture for VCC, itemize the properties required in vehicular cloud that support this model. We compare this mechanism with normal Cloud Computing (CC) and discuss open research issues and future directions. By reviewing and analyzing literature, we found that VCC is a technologically feasible and economically viable technological shifting paradigm for converging intelligent vehicular networks towards autonomous traffic, vehicle control and perception systems.
ER  - 

TY  - JOUR
T1  - On designing usable and secure recognition-based graphical authentication mechanisms
JO  - Interacting with Computers
VL  - 23
IS  - 6
SP  - 582
EP  - 593
PY  - 2011/11//
T2  - 
AU  - Mihajlov, Martin
AU  - Jerman-Blaži?, Borka
SN  - 0953-5438
DO  - http://dx.doi.org/10.1016/j.intcom.2011.09.001
UR  - http://www.sciencedirect.com/science/article/pii/S0953543811000956
KW  - Graphical authentication
KW  - Graphical passwords
KW  - User evaluation
KW  - System design
AB  - In this article we present the development of a new, web-based, graphical authentication mechanism called ImagePass. The authentication mechanism introduces a novel feature based on one-time passwords that increases the security of the system without compromising its usability. Regarding usability, we explore the users’ perception of recognition-based, graphical authentication mechanisms in a web environment. Specifically, we investigate whether the memorability of recognition-based authentication keys is influenced by image content. We also examine how the frequency of use affects the usability of the system and whether user training via mnemonic instructions improves the graphical password recognition rate. The design and development process of the proposed system began with a study that assessed how the users remember abstract, face or single-object images, and showed that single-object images have a higher memorability rate. We then proceeded with the design and development of a recognition-based graphical authentication mechanism, ImagePass, which uses single-objects as the image content and follows usable security guidelines. To conclude the research, in a follow-up study we evaluated the performance of 151 participants under different conditions. We discovered that the frequency of use had a great impact on users’ performance, while the users’ gender had a limited task-specific effect. In contrast, user training through mnemonic instructions showed no differences in the users’ authentication metrics. However, a post-study, focus-group analysis revealed that these instructions greatly influenced the users’ perception for memorability and the usability of the graphical authentication. In general, the results of these studies suggest that single-object graphical authentication can be a complementary replacement for traditional passwords, especially in ubiquitous environments and mobile devices.
ER  - 

TY  - JOUR
T1  - Logs may be found boring, but they are good: NIST
JO  - Computer Fraud & Security
VL  - 2006
IS  - 5
SP  - 2
EP  - 3
PY  - 2006/5//
T2  - 

SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(06)70351-7
UR  - http://www.sciencedirect.com/science/article/pii/S1361372306703517
AB  - A US standards body has said that the benefits of logs are being thrown away because system administrators find it “boring.” It said that the analysis of logs is often “treated as a low-priority task” by administrators because more urgent tasks like fixing vulnerabilities come first.
ER  - 

TY  - JOUR
T1  - Detection of malicious and non-malicious website visitors using unsupervised neural network learning
JO  - Applied Soft Computing
VL  - 13
IS  - 1
SP  - 698
EP  - 708
PY  - 2013/1//
T2  - 
AU  - Stevanovic, Dusan
AU  - Vlajic, Natalija
AU  - An, Aijun
SN  - 1568-4946
DO  - http://dx.doi.org/10.1016/j.asoc.2012.08.028
UR  - http://www.sciencedirect.com/science/article/pii/S1568494612003778
KW  - Web crawler detection
KW  - Neural networks
KW  - Web server access logs
KW  - Machine learning
KW  - Clustering
KW  - Denial of service
AB  - Distributed denials of service (DDoS) attacks are recognized as one of the most damaging attacks on the Internet security today. Recently, malicious web crawlers have been used to execute automated DDoS attacks on web sites across the WWW. In this study, we examine the use of two unsupervised neural network (NN) learning algorithms for the purpose web-log analysis: the Self-Organizing Map (SOM) and Modified Adaptive Resonance Theory 2 (Modified ART2). In particular, through the use of SOM and modified ART2, our work aims to obtain a better insight into the types and distribution of visitors to a public web-site based on their browsing behavior, as well as to investigate the relative differences and/or similarities between malicious web crawlers and other non-malicious visitor groups. The results of our study show that, even though there is a pretty clear separation between malicious web-crawlers and other visitor groups, 52% of malicious crawlers exhibit very ‘human-like’ browsing behavior and as such pose a particular challenge for future web-site security systems. Also, we show that some of the feature values of malicious crawlers that exhibit very ‘human-like’ browsing behavior are not significantly different than the features values of human visitors. Additionally, we show that Google, MSN and Yahoo crawlers exhibit distinct crawling behavior.
ER  - 

TY  - JOUR
T1  - Avoiding the five pitfalls of privileged accounts
JO  - Network Security
VL  - 2013
IS  - 5
SP  - 12
EP  - 14
PY  - 2013/5//
T2  - 
AU  - Grafton, Jane
SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/S1353-4858(13)70060-6
UR  - http://www.sciencedirect.com/science/article/pii/S1353485813700606
AB  - It is a rather human truth that when we hand out privileges they often get abused. Whether you are operating in the high reaches of government or the most basic market the sad fact is – where we find privilege we also find the abuse of privilege. In the world of IT, privileged accounts are identities that have elevated permission to access potentially sensitive data, run programs or change configuration settings. To put it simply, privileged accounts are the keys to the kingdom of IT.

When we hand out privileges they often get abused. In the world of IT, privileged accounts – identities that have elevated permission to access sensitive data, run programs or change settings – are found on every server, workstation and appliance.

In recent years we have witnessed more and more organisations fail to adequately secure such accounts, with catastrophic results. There are common practices that have lead to a number of security breaches and failed IT compliance audits, some of them very high-profile. Jane Grafton of Lieberman Software looks at the five most common errors with privileged accounts – and how to avoid them.
ER  - 

TY  - JOUR
T1  - Flexible composition and execution of large scale applications on distributed e-infrastructures
JO  - Journal of Computational Science
VL  - 5
IS  - 1
SP  - 51
EP  - 62
PY  - 2014/1//
T2  - 
AU  - Zasada, Stefan J.
AU  - Chang, David C.W.
AU  - Haidar, Ali N.
AU  - Coveney, Peter V.
SN  - 1877-7503
DO  - http://dx.doi.org/10.1016/j.jocs.2013.10.009
UR  - http://www.sciencedirect.com/science/article/pii/S1877750313001269
KW  - E-infrastructure
KW  - High performance computing
KW  - Application virtualization
KW  - Usability
AB  - Abstract
Computer simulation is finding a role in an increasing number of scientific disciplines, concomitant with the rise in available computing power. Marshalling this power facilitates new, more effective and different research than has been hitherto possible. Realizing this inevitably requires access to computational power beyond the desktop, making use of clusters, supercomputers, data repositories, networks and distributed aggregations of these resources. The use of diverse e-infrastructure brings with it the ability to perform distributed multiscale simulations. Accessing one such resource entails a number of usability and security problems; when multiple geographically distributed resources are involved, the difficulty is compounded. In this paper we present a solution, the Application Hosting Environment,33
AHE is available to download under the LGPL license from: https://sourceforge.net/projects/ahe3/.
 which provides a Software as a Service layer on top of distributed e-infrastructure resources. We describe the performance and usability enhancements present in AHE version 3, and show how these have led to a high performance, easy to use gateway for computational scientists working in diverse application domains, from computational physics and chemistry, materials science to biology and biomedicine.
ER  - 

TY  - JOUR
T1  - Towards a unified taxonomy and architecture of cloud frameworks
JO  - Future Generation Computer Systems
VL  - 29
IS  - 5
SP  - 1196
EP  - 1210
PY  - 2013/7//
T2  - Special section: Hybrid Cloud Computing
AU  - Dukaric, Robert
AU  - Juric, Matjaz B.
SN  - 0167-739X
DO  - http://dx.doi.org/10.1016/j.future.2012.09.006
UR  - http://www.sciencedirect.com/science/article/pii/S0167739X12001793
KW  - Cloud Computing
KW  - Infrastructure as a service
KW  - Taxonomy
KW  - Architectural framework
AB  - Infrastructure as a Service (IaaS) is one of the most important layers of Cloud Computing. However, there is an evident deficiency of mechanisms for analysis, comparison and evaluation of IaaS cloud implementations, since no unified taxonomy or reference architecture is available. In this article, we propose a unified taxonomy and an IaaS architectural framework. The taxonomy is structured around seven layers: core service layer, support layer, value-added services, control layer, management layer, security layer and resource abstraction. We survey various IaaS systems and map them onto our taxonomy to evaluate the classification. We then introduce an IaaS architectural framework that relies on the unified taxonomy. We provide a detailed description of each layer and define dependencies between the layers and components. Finally, we evaluate the proposed IaaS architectural framework on several real-world projects, while performing a comprehensive analysis of the most important commercial and open-source IaaS products. The evaluation results show notable distinction of feature support and capabilities between commercial and open-source IaaS platforms, significant deficiency of important architectural components in terms of fulfilling true promise of infrastructure clouds, and real-world usability of the proposed taxonomy and architectural framework.
ER  - 

TY  - JOUR
T1  - Threshold-based intrusion detection in ad hoc networks and secure AODV
JO  - Ad Hoc Networks
VL  - 6
IS  - 4
SP  - 578
EP  - 599
PY  - 2008/6//
T2  - 
AU  - Patwardhan, A.
AU  - Parker, J.
AU  - Iorga, M.
AU  - Joshi, A.
AU  - Karygiannis, T.
AU  - Yesha, Y.
SN  - 1570-8705
DO  - http://dx.doi.org/10.1016/j.adhoc.2007.05.001
UR  - http://www.sciencedirect.com/science/article/pii/S1570870507000960
KW  - MANETs
KW  - Secure routing
KW  - Intrusion detection
KW  - SecAODV
AB  - Mobile ad hoc networks (MANETs) play an important role in connecting devices in pervasive environments. MANETs provide inexpensive and versatile communication, yet several challenges remain in addressing their security. So far, numerous schemes have been proposed for secure routing and intrusion detection, with only simulations to validate them; little work exists, in implementing such schemes on small handheld devices. In this paper, we present our approach of securing a MANET using a threshold-based intrusion detection system and a secure routing protocol. We present a proof-of-concept implementation of our IDS deployed on handheld devices and in a MANET testbed connected by a secure version of AODV over IPv6 – SecAODV. While the IDS helps detect attacks on data traffic, SecAODV incorporates security features of non-repudiation and authentication, without relying on the availability of a Certificate Authority (CA) or a Key Distribution Center (KDC). We present the design and implementation details of our system, the practical considerations involved, and how these mechanisms can be used to detect and thwart malicious attacks.
ER  - 

TY  - JOUR
T1  - Autonomous decision on intrusion detection with trained BDI agents
JO  - Computer Communications
VL  - 31
IS  - 9
SP  - 1803
EP  - 1813
PY  - 2008/6/8/
T2  - 
AU  - Orfila, Agustín
AU  - Carbó, Javier
AU  - Ribagorda, Arturo
SN  - 0140-3664
DO  - http://dx.doi.org/10.1016/j.comcom.2007.11.018
UR  - http://www.sciencedirect.com/science/article/pii/S0140366407005038
KW  - Intrusion detection and response
KW  - Multiagent system
KW  - Decision analysis
KW  - Knowledge management and reasoning
AB  - In the context of computer security, the first step to respond to an intrusive incident is the detection of such activity in the monitored system. In recent years, research in intrusion detection has evolved to become a multi-discipline task that involves areas such as data mining, decision analysis, agent-based systems or cost–benefit analysis among others. We propose a multiagent IDS that considers decision analysis techniques in order to configure itself optimally according to the conditions faced. This IDS also provides a quantitative measure of the value of the response decision it can autonomously take. Results regarding the well-known 1999 KDD dataset are shown.
ER  - 

TY  - JOUR
T1  - Beyond RACF: Extending user authentication controls
JO  - Computers & Security
VL  - 10
IS  - 8
SP  - 711
EP  - 722
PY  - 1991/12//
T2  - 
AU  - Lynch, Paul
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/0167-4048(91)90090-Z
UR  - http://www.sciencedirect.com/science/article/pii/016740489190090Z
AB  - A discussion on passwords and user i.d.s as the central point for security control is presented, with recommendations for stricter control, and identifying weaknesses in this approach. Use of “tokens” for authentication is discussed, concentrating on currently available devices, including IBM's TSS. The impact of authentication on the US NCSC standards (the “Orange Book”) and the proposed ITSEC standards are considered.
ER  - 

TY  - JOUR
T1  - Collapsar: A VM-based honeyfarm and reverse honeyfarm architecture for network attack capture and detention
JO  - Journal of Parallel and Distributed Computing
VL  - 66
IS  - 9
SP  - 1165
EP  - 1180
PY  - 2006/9//
T2  - Special Issue: Security in grid and distributed systems
AU  - Jiang, Xuxian
AU  - Xu, Dongyan
AU  - Wang, Yi-Min
SN  - 0743-7315
DO  - http://dx.doi.org/10.1016/j.jpdc.2006.04.012
UR  - http://www.sciencedirect.com/science/article/pii/S0743731506000876
KW  - Honeypot
KW  - Honeyfarm
KW  - Reverse honeyfarm
AB  - The honeypot has emerged as an effective tool to provide insights into new attacks and exploitation trends. However, a single honeypot or multiple independently operated honeypots only provide limited local views of network attacks. Coordinated deployment of honeypots in different network domains not only provides broader views, but also create opportunities of early network anomaly detection, attack correlation, and global network status inference. Unfortunately, coordinated honeypot operation require close collaboration and uniform security expertise across participating network domains. The conflict between distributed presence and uniform management poses a major challenge in honeypot deployment and operation.

To address this challenge, we present Collapsar, a virtual machine-based architecture for network attack capture and detention. A Collapsar center hosts and manages a large number of high-interaction virtual honeypots in a local dedicated network. To attackers, these honeypots appear as real systems in their respective production networks. Decentralized logical presence of honeypots provides a wide diverse view of network attacks, while the centralized operation enables dedicated administration and convenient event correlation, eliminating the need for honeypot expertise in every production network domain. Collapsar realizes the traditional honeyfarm vision as well as our new reverse honeyfarm vision, where honeypots act as vulnerable clients exploited by real-world malicious servers. We present the design, implementation, and evaluation of a Collapsar prototype. Our experiments with a number of real-world attacks demonstrate the effectiveness and practicality of Collapsar.
ER  - 

TY  - JOUR
T1  - Tree-formed verification data for trusted platforms
JO  - Computers & Security
VL  - 32
IS  - 
SP  - 19
EP  - 35
PY  - 2013/2//
T2  - 
AU  - Schmidt, Andreas U.
AU  - Leicher, Andreas
AU  - Brett, Andreas
AU  - Shah, Yogendra
AU  - Cha, Inhyok
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/j.cose.2012.09.004
UR  - http://www.sciencedirect.com/science/article/pii/S016740481200140X
KW  - Trusted platform
KW  - Remote attestation
KW  - Hash tree
KW  - Measurement log
KW  - Verification data
KW  - Validation
AB  - The establishment of trust relationships to a computing platform relies on validation processes. Validation allows an external entity to build trust in the expected behaviour of the platform based on provided evidence of the platform's configuration. In a process like remote attestation, the ‘trusted’ platform submits verification data created during a start up process. These data consist of hardware-protected values of platform configuration registers, containing nested measurement values, e.g., hash values, of loaded or started components. Commonly, the register values are created in linear order by a hardware-secured operation. Fine-grained diagnosis of components, based on the linear order of verification data and associated measurement logs, is not optimal. We propose a method to use tree-formed verification data to validate a platform. Component measurement values represent leaves, and protected registers represent roots of a hash tree. We describe the basic mechanism of validating a platform using tree-formed measurement logs and root registers and show a logarithmic speed-up for the search of faults. Secure creation of a tree is possible using a limited number of hardware-protected registers and a single protected operation. In this way, the security of tree-formed verification data is maintained.
ER  - 

TY  - JOUR
T1  - Risking “trust” in a public key infrastructure: old techniques of managing risk applied to new technology
JO  - Decision Support Systems
VL  - 31
IS  - 3
SP  - 303
EP  - 322
PY  - 2001/8//
T2  - 
AU  - Fernandes, Andrew D
SN  - 0167-9236
DO  - http://dx.doi.org/10.1016/S0167-9236(00)00139-1
UR  - http://www.sciencedirect.com/science/article/pii/S0167923600001391
KW  - Digital signature
KW  - Public key
KW  - PKI
KW  - Risk management
KW  - Digital certificate
KW  - Certification authority
KW  - Trusted third party
KW  - Cryptography
KW  - Certification practice statement
AB  - Installing a public key infrastructure (PKI) can change in the security model of an IT operation in several ways. This article gives a layman's overview of what exactly a PKI is, and how one can be built and operated safely and securely. First, the PKI must be designed using the familiar principles of risk management, rather than “trust management”. Next, although it is not widely appreciated, digital signatures are not equivalent to traditional signatures, and understanding this difference is crucial to understanding how a PKI needs to be audited. Lastly, I will show that for a PKI to provide ongoing security, the principles of compromise–containment and regular auditing must be adhered to.
ER  - 

TY  - JOUR
T1  - Feature evaluation for web crawler detection with data mining techniques
JO  - Expert Systems with Applications
VL  - 39
IS  - 10
SP  - 8707
EP  - 8717
PY  - 2012/8//
T2  - 
AU  - Stevanovic, Dusan
AU  - An, Aijun
AU  - Vlajic, Natalija
SN  - 0957-4174
DO  - http://dx.doi.org/10.1016/j.eswa.2012.01.210
UR  - http://www.sciencedirect.com/science/article/pii/S0957417412002382
KW  - Web crawler detection
KW  - Web server access logs
KW  - Data mining
KW  - Classification
KW  - DDoS
KW  - WEKA
AB  - Distributed Denial of Service (DDoS) is one of the most damaging attacks on the Internet security today. Recently, malicious web crawlers have been used to execute automated DDoS attacks on web sites across the WWW. In this study we examine the effect of applying seven well-established data mining classification algorithms on static web server access logs in order to: (1) classify user sessions as belonging to either automated web crawlers or human visitors and (2) identify which of the automated web crawlers sessions exhibit ‘malicious’ behavior and are potentially participants in a DDoS attack. The classification performance is evaluated in terms of classification accuracy, recall, precision and F1 score. Seven out of nine vector (i.e. web-session) features employed in our work are borrowed from earlier studies on classification of user sessions as belonging to web crawlers. However, we also introduce two novel web-session features: the consecutive sequential request ratio and standard deviation of page request depth. The effectiveness of the new features is evaluated in terms of the information gain and gain ratio metrics. The experimental results demonstrate the potential of the new features to improve the accuracy of data mining classifiers in identifying malicious and well-behaved web crawler sessions.
ER  - 

TY  - JOUR
T1  - How secure is the next generation of IP-based emergency services architecture?
JO  - International Journal of Critical Infrastructure Protection
VL  - 3
IS  - 1
SP  - 41
EP  - 50
PY  - 2010/5//
T2  - 
AU  - Tschofenig, Hannes
AU  - Arumaithurai, Mayutan
AU  - Schulzrinne, Henning
AU  - Aboba, Bernard
SN  - 1874-5482
DO  - http://dx.doi.org/10.1016/j.ijcip.2010.02.001
UR  - http://www.sciencedirect.com/science/article/pii/S1874548210000028
KW  - Emergency services architecture
KW  - Ecrit
AB  - For some location-based applications, such as emergency calling or roadside assistance, it appears that the identity of the requester is less important than accurate and trustworthy location information for accomplishing the main function. Accurate and genuine location is important for these applications to avoid misuse.

In this paper we point to some ongoing efforts regarding transition emergency service architectures that could introduce security vulnerabilities unless countermeasures are developed. Furthermore, we summarize the ongoing work in providing cryptographic assertions for location.

We argue that many of the currently proposed ideas are difficult to deploy and to operate. Additionally, when used without ensuring that the underlying assumptions are met these mechanisms do not provide any additional benefit, but costs.

We conclude this article with a suggestion on what the research community and industry should be investigating to avoid potential problems with IP-based emergency services.
ER  - 

TY  - JOUR
T1  - Botnet Detection with Event-Driven Analysis
JO  - Procedia Computer Science
VL  - 22
IS  - 
SP  - 662
EP  - 671
PY  - 2013///
T2  - 17th International Conference in Knowledge Based and Intelligent Information and Engineering Systems - KES2013
AU  - Ersson, Joakim
AU  - Moradian, Esmiralda
SN  - 1877-0509
DO  - http://dx.doi.org/10.1016/j.procs.2013.09.147
UR  - http://www.sciencedirect.com/science/article/pii/S187705091300940X
KW  - Log analysis
KW  - botnet
KW  - firewall log,network analysis
AB  - Abstract
Due to the huge impact on businesses, botnets are recognized as one of the most serious security threats. Malicious entities use various techniques to conceal and keep themselves undetected durin the proliferation of malware from computer to computer. Detection of a botnet is commonly performed in two ways either by using antivirus software or by analysing logged network data. However antivirus software usually detects malware that is already known and has been analysed, which is a main drawback of such approach due to the constant evolving of malware. The approach of analysis of logged network data do not reveals botnet activities and requires knowledge about botnets and type of data to look for within the collected log. Thus, the significant information can be overlooked and missed. In this paper, we propose event-driven log analysis software that enables detection of botnet activities and indicates whether the end-users machines have become a member of a botnet. Moreover, to optimize software functionality we performed an experiment that demonstrates how botnet communicates between itself and the command and control. Experiment along with the result is presented in this research.
ER  - 

TY  - JOUR
T1  - A remote interactive non-repudiation multimedia-based m-learning system
JO  - Telematics and Informatics
VL  - 27
IS  - 4
SP  - 377
EP  - 393
PY  - 2010/11//
T2  - 
AU  - Adibi, Sasan
SN  - 0736-5853
DO  - http://dx.doi.org/10.1016/j.tele.2010.01.001
UR  - http://www.sciencedirect.com/science/article/pii/S073658531000002X
KW  - m-Leaning
KW  - e-Learning
KW  - Non-repudiation
KW  - Identity management
KW  - Multimedia interactive communication
AB  - One of the current challenges regarding distance learning systems, from a performance point of view, is the efficient and timely delivery of multimedia-enriched learning materials. Providing guaranteed Class of Service (CoS) and Quality of Service (QoS) are also challenging especially for remote sites and rural areas where Internet coverage tends to be limited. On a different note, another challenge is to track the audience accessing the learning materials and more importantly to monitor the true identity of the examination attendees. This paper aims to investigate both of these issues simultaneously, with an introduction of a non-repudiation system that provides a security mechanism, as well as maintaining certain QoS measures. This system not only authenticates the intended party, but also integrates a digital signature scheme accompanied with the transmitted multimedia-based information. The included digital signature prevents a later dispute from the involved parties that the communication ever took place or they ever took part in the communication.

Therefore this paper introduces and discusses a multimedia-enriched interactive non-repudiation system involved in a mobile-based learning (m-learning) environment. The performance of this system is considered and discussed in terms of network-centric parameters, including end-to-end delays, overhead, and bandwidth, using Labview 8.5 mobile-transmitter and mobile-receiver testbeds.
ER  - 

TY  - JOUR
T1  - COMPUTER-BASED EVIDENCE: THE IDENTIFICATION AND RECOVERY OF EVIDENCE IN ELECTRONIC FORMAT
JO  - Computer Law & Security Review
VL  - 16
IS  - 3
SP  - 162
EP  - 165
PY  - 2000/6/1/
T2  - 
AU  - May, Clifford
SN  - 0267-3649
DO  - http://dx.doi.org/10.1016/S0267-3649(00)88909-8
UR  - http://www.sciencedirect.com/science/article/pii/S0267364900889098
AB  - With increasing reliance being placed on the use of computer technology by every sector of society, computer-based evidence is becoming a bigger and bigger issue daily. The task facing the legal profession or the investigator when it comes to uncovering and interpreting potential evidence is markedly different today than, say, just six to 10 years ago. Instead of wading through mountains of paper it is more common to meet evidence held in electronic format on one or more computers. The technology revolution has also generated new avenues for crime, requiring a great deal of technical awareness and computer literacy from everyone involved. Clifford May looks at the special problems involved in the identification and recovery of computer-based evidence in a form that can be successfully presented to court.
ER  - 

TY  - JOUR
T1  - In brief
JO  - Network Security
VL  - 2010
IS  - 6
SP  - 3
EP  - 
PY  - 2010/6//
T2  - 

SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/S1353-4858(10)70079-9
UR  - http://www.sciencedirect.com/science/article/pii/S1353485810700799
ER  - 

TY  - JOUR
T1  - Advanced Persistent threats and how to monitor and deter them
JO  - Network Security
VL  - 2011
IS  - 8
SP  - 16
EP  - 19
PY  - 2011/8//
T2  - 
AU  - Tankard, Colin
SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/S1353-4858(11)70086-1
UR  - http://www.sciencedirect.com/science/article/pii/S1353485811700861
AB  - Advanced Persistent Threat (APT) is a term coined over the past couple of years for a new breed of insidious threats that use multiple attack techniques and vectors and that are conducted by stealth to avoid detection so that hackers can retain control over target systems unnoticed for long periods of time.

Traditional defences aimed at keeping known threats out of the network are no longer sufficient against the exploits being used to conduct such attacks. The focus should be on developing a defence in depth strategy that aims to constantly monitor networks and security controls for their effectiveness, explains Colin Tankard of Digital Pathways.

The UK Government has recently estimated that cybercrime costs the country some £27bn per year and, according to some estimates, the global cost is $1 trillion every year. This crime wave has been greatly facilitated by the rise of electronic communications, primarily those making use of the Internet. The purpose of electronic communications is to make it more efficient and easier to communicate – but they are also relatively easy to attack or intercept. No-one is immune – such attacks are aimed at individuals, small firms, multinationals and governments.
ER  - 

TY  - JOUR
T1  - Enforcement of entailment constraints in distributed service-based business processes
JO  - Information and Software Technology
VL  - 55
IS  - 11
SP  - 1884
EP  - 1903
PY  - 2013/11//
T2  - 
AU  - Hummer, Waldemar
AU  - Gaubatz, Patrick
AU  - Strembeck, Mark
AU  - Zdun, Uwe
AU  - Dustdar, Schahram
SN  - 0950-5849
DO  - http://dx.doi.org/10.1016/j.infsof.2013.05.001
UR  - http://www.sciencedirect.com/science/article/pii/S0950584913001006
KW  - Identity and access management
KW  - Business process management
KW  - Entailment constraints
KW  - Service-Oriented Architecture (SOA)
KW  - WS-BPEL
AB  - AbstractContext
A distributed business process is executed in a distributed computing environment. The service-oriented architecture (SOA) paradigm is a popular option for the integration of software services and execution of distributed business processes. Entailment constraints, such as mutual exclusion and binding constraints, are important means to control process execution. Mutually exclusive tasks result from the division of powerful rights and responsibilities to prevent fraud and abuse. In contrast, binding constraints define that a subject who performed one task must also perform the corresponding bound task(s).
Objective
We aim to provide a model-driven approach for the specification and enforcement of task-based entailment constraints in distributed service-based business processes.
Method
Based on a generic metamodel, we define a domain-specific language (DSL) that maps the different modeling-level artifacts to the implementation-level. The DSL integrates elements from role-based access control (RBAC) with the tasks that are performed in a business process. Process definitions are annotated using the DSL, and our software platform uses automated model transformations to produce executable WS-BPEL specifications which enforce the entailment constraints. We evaluate the impact of constraint enforcement on runtime performance for five selected service-based processes from existing literature.
Results
Our evaluation demonstrates that the approach correctly enforces task-based entailment constraints at runtime. The performance experiments illustrate that the runtime enforcement operates with an overhead that scales well up to the order of several ten thousand logged invocations. Using our DSL annotations, the user-defined process definition remains declarative and clean of security enforcement code.
Conclusion
Our approach decouples the concerns of (non-technical) domain experts from technical details of entailment constraint enforcement. The developed framework integrates seamlessly with WS-BPEL and the Web services technology stack. Our prototype implementation shows the feasibility of the approach, and the evaluation points to future work and further performance optimizations.
ER  - 

TY  - JOUR
T1  - Hacking: Myth or menace, Part I
JO  - Computer Fraud & Security
VL  - 1998
IS  - 2
SP  - 16
EP  - 18
PY  - 1998/2//
T2  - 
AU  - Blatchford, Clive
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(00)87011-6
UR  - http://www.sciencedirect.com/science/article/pii/S1361372300870116
AB  - Is hacking about the disposition of the perpetrator or the situation of the insecure computer network. Improving the security of the perceived target may reduce the opportunity for the hacker, but can it ever eradicate the problem? Criminologists increasingly insist that the solution must lie in understanding the perspective of the hacker and address the problem at the political, social and cultural levels. The following two part study aims to offer insights about the actual threat before embarking on more legislation, policing or prohibitively expensive and probably, inadequate target hardening.
ER  - 

TY  - JOUR
T1  - PalmCIS: A wireless handheld application for satisfying clinician information needs
JO  - Journal of the American Medical Informatics Association
VL  - 11
IS  - 1
SP  - 19
EP  - 28
PY  - 2004/1//
Y2  - 2004/2//
T2  - 
AU  - Chen, Elizabeth S
AU  - Mendonça, Eneida A
AU  - McKnight, Lawrence K
AU  - Stetson, Peter D
AU  - Lei, Jianbo
AU  - Cimino, James J
SN  - 1067-5027
DO  - http://dx.doi.org/10.1197/jamia.M1387
UR  - http://www.sciencedirect.com/science/article/pii/S1067502703002020
AB  - Wireless handheld technology provides new ways to deliver and present information. As with any technology, its unique features must be taken into consideration and its applications designed accordingly. In the clinical setting, availability of needed information can be crucial during the decision-making process. Preliminary studies performed at New York Presbyterian Hospital (NYPH) determined that there are inadequate access to information and ineffective communication among clinicians (potential proximal causes of medical errors). In response to these findings, the authors have been developing extensions to their Web-based clinical information system including PalmCIS, an application that provides access to needed patient information via a wireless personal digital assistant (PDA). The focus was on achieving end-to-end security and developing a highly usable system. This report discusses the motivation behind PalmCIS, design and development of the system, and future directions.
ER  - 

TY  - JOUR
T1  - Access control for smarter healthcare using policy spaces
JO  - Computers & Security
VL  - 29
IS  - 8
SP  - 848
EP  - 858
PY  - 2010/11//
T2  - 
AU  - Ardagna, Claudio A.
AU  - De Capitani di Vimercati, Sabrina
AU  - Foresti, Sara
AU  - Grandison, Tyrone W.
AU  - Jajodia, Sushil
AU  - Samarati, Pierangela
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/j.cose.2010.07.001
UR  - http://www.sciencedirect.com/science/article/pii/S0167404810000623
KW  - Access control
KW  - Break the glass
KW  - Policy spaces
KW  - Exceptions
KW  - Healthcare systems
AB  - A fundamental requirement for the healthcare industry is that the delivery of care comes first and nothing should interfere with it. As a consequence, the access control mechanisms used in healthcare to regulate and restrict the disclosure of data are often bypassed in case of emergencies. This phenomenon, called “break the glass”, is a common pattern in healthcare organizations and, though quite useful and mandatory in emergency situations, from a security perspective, it represents a serious system weakness. Malicious users, in fact, can abuse the system by exploiting the break the glass principle to gain unauthorized privileges and accesses.

In this paper, we propose an access control solution aimed at better regulating break the glass exceptions that occur in healthcare systems. Our solution is based on the definition of different policy spaces, a language, and a composition algebra to regulate access to patient data and to balance the rigorous nature of traditional access control systems with the “delivery of care comes first” principle.
ER  - 

TY  - JOUR
T1  - A portable interceptor mechanism for SOAP frameworks
JO  - Computer Standards & Interfaces
VL  - 36
IS  - 1
SP  - 209
EP  - 218
PY  - 2013/11//
T2  - 
AU  - Lin, Chien-Cheng
AU  - Fang, Chen-Liang
AU  - Liang, Deron
SN  - 0920-5489
DO  - http://dx.doi.org/10.1016/j.csi.2013.04.001
UR  - http://www.sciencedirect.com/science/article/pii/S0920548913000226
KW  - SOAP framework
KW  - Web Service
KW  - Portable interceptor mechanism
KW  - ISO/IEC 9126
AB  - Abstract
An interceptor is a generic architecture pattern, and has been used to resolve specific issues in a number of application domains. Many standard platforms such as CORBA also provide interception interfaces so that an interceptor developed for a specific application can become portable across systems running on the same platform. SOAP frameworks are commonly used platforms to build Web Services. However, there is no standard way to build interceptors portable across current SOAP frameworks, although, some of them provide proprietary interceptor solution within individual framework, such as Axis, XFire, and etc. In this paper, we propose the portable interceptor mechanism (PIM) consisting of a set of application programming interfaces (API) on SOAP engine, a core component of a SOAP framework. An interceptor is able to receive messages passing through the SOAP framework from the SOAP engine via these APIs. Furthermore, the proposed PIM facilitates run-time lifecycle management of interceptors that is a crucial feature to many application domains but is not fully supported by CORBA standard. For concept proving, we implement the proposed PIM on two popular SOAP frameworks, namely, Axis and XFire. We also discuss a number of implementation issues including the performance and reliability of PIM.
ER  - 

TY  - JOUR
T1  - Infosecurity Europe 2007 New Product Launches
JO  - Infosecurity
VL  - 4
IS  - 2
SP  - 16
EP  - 18
PY  - 2007/3//
T2  - 

SN  - 1754-4548
DO  - http://dx.doi.org/10.1016/S1754-4548(07)70034-7
UR  - http://www.sciencedirect.com/science/article/pii/S1754454807700347
AB  - Infosecurity Europe is the event where new products are launched, here is a brief glimpse of what you can see at the show.
ER  - 

TY  - JOUR
T1  - Compliance complacency: How ‘check-box’ compliancy remains a pitfall for many organizations worldwide
JO  - Information Security Technical Report
VL  - 15
IS  - 4
SP  - 154
EP  - 159
PY  - 2010/11//
T2  - Matchmaking between PCI-DSS and Security
AU  - Andrew Valentine, J.
SN  - 1363-4127
DO  - http://dx.doi.org/10.1016/j.istr.2011.02.002
UR  - http://www.sciencedirect.com/science/article/pii/S1363412711000136
ER  - 

TY  - JOUR
T1  - A quantitative study of Public Key Infrastructures
JO  - Computers & Security
VL  - 22
IS  - 1
SP  - 56
EP  - 67
PY  - 2003/1//
T2  - 
AU  - Bruschi, D
AU  - Curti, A
AU  - Rosti, E
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/S0167-4048(03)00113-5
UR  - http://www.sciencedirect.com/science/article/pii/S0167404803001135
AB  - Public Key Infrastructures have not reached the widespread diffusion expected of them, although they are well understood from a security point of view, because, like many say, the killer application has not been found yet. The lack of a clear understanding of the performance of these systems also contributes significantly to their limited diffusion. Studies have appeared of specific aspects of the operations of PKIs, but no complete studies of the overall system are known.

In this paper we present an evaluation study of X.509-compliant Public Key Infrastructures using queuing network models. We focus our analysis on the performance of the subsystem in charge of generating and managing digital certificates, under a variety of load conditions, both in terms of the type of requests and their number. We also investigate the impact on the performance of the system of some implementation choices such as revocation mechanisms and auditing activities. The main result of our analysis is that the system we consider, given the current state of technology, can guarantee acceptable response time in steady state even in the presence of PKI with a consistent number of users. However, in order to guarantee such a performance level, throughput must not exceed 3.5 requests per second, where a request can be a certificate generation or revocation request. Such a limitation hinders the deployment of PKIs with large numbers of users, since recovering after a system compromise may require an unacceptable amount of time.
ER  - 

TY  - JOUR
T1  - Using DNS to protect networks from threats within
JO  - Network Security
VL  - 2014
IS  - 3
SP  - 9
EP  - 11
PY  - 2014/3//
T2  - 
AU  - Barnes, Pat
SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/S1353-4858(14)70030-3
UR  - http://www.sciencedirect.com/science/article/pii/S1353485814700303
AB  - As ISPs become more prone to sophisticated botnet attacks launched from inside the perimeter of the network, DNS has a crucial role to play in maintaining the integrity of network infrastructure.

Legacy ‘defence in depth’ offerings continue to do a good job at protecting the perimeter, but have not focused on activity generated from the inside.

Attacks have a direct impact on network user satisfaction, network integrity and stability, and reputation. And ISPs are becoming more prone to sophisticated botnet attacks launched from inside the perimeter of the network. Pat Barnes of Nominum explains the role of DNS in maintaining the integrity of the network infrastructure.
ER  - 

TY  - JOUR
T1  - Understanding the drivers for secure data storage
JO  - Infosecurity
VL  - 7
IS  - 5
SP  - 32
EP  - 35
PY  - 2010/9//
Y2  - 2010/10//
T2  - 
AU  - Gold, Steve
SN  - 1754-4548
DO  - http://dx.doi.org/10.1016/S1754-4548(10)70089-9
UR  - http://www.sciencedirect.com/science/article/pii/S1754454810700899
AB  - Storing and securing data can be headache for most IT managers but, as Steve Gold explains, the problem can be solved with the correct approach
ER  - 

TY  - JOUR
T1  - Policy enforcement system for secure interoperable control in distributed Smart Grid systems
JO  - Journal of Network and Computer Applications
VL  - 
IS  - 
SP  - 
EP  - 
PY  - 
T2  - 
AU  - Alcaraz, Cristina
AU  - Lopez, Javier
AU  - Wolthusen, Stephen
SN  - 1084-8045
DO  - http://dx.doi.org/10.1016/j.jnca.2015.05.023
UR  - http://www.sciencedirect.com/science/article/pii/S1084804515001629
KW  - Smart Grid
KW  - Distributed control systems
KW  - Controllability
KW  - Interoperability
KW  - Policy enforcement
KW  - Access control
AB  - Abstract
Interoperability of distributed systems in charge of monitoring and maintaining the different critical domains belonging to Smart Grid scenarios comprise the central topic of this paper. Transparency in control transactions under a secure and reliable architecture is the aim of the policy enforcement system proposed here. The approach is based on the degree of observation of a context and on the role-based access control model defined by the IEC-62351-8 standard. Only authenticated and authorised entities are able to take control of those distributed elements (e.g., IEC-61850 objects) located at distant geographical locations and close to the critical infrastructures (e.g., substations). To ensure the effectiveness of the approach, it is built on graphical–theoretical formulations corresponding to graph theory, where it is possible to illustrate power control networks through power-law distributions whose monitoring relies on structural controllability theory. The interconnection of these distributions is subject to a network architecture based on the concept of the supernode where the interoperability depends on a simple rule-based expert system. This expert system focuses not only on accepting or denying access, but also on providing the means to attend to extreme situations, avoiding, as much as possible, the overloading of the communication. Through one practical study we also show the functionalities of the approach and the benefits that the authorisation itself can bring to the interoperability.
ER  - 

TY  - JOUR
T1  - Data mining and machine learning—Towards reducing false positives in intrusion detection
JO  - Information Security Technical Report
VL  - 10
IS  - 3
SP  - 169
EP  - 183
PY  - 2005///
T2  - 
AU  - Pietraszek, Tadeusz
AU  - Tanner, Axel
SN  - 1363-4127
DO  - http://dx.doi.org/10.1016/j.istr.2005.07.001
UR  - http://www.sciencedirect.com/science/article/pii/S1363412705000361
AB  - Intrusion Detection Systems (IDSs) are used to monitor computer systems for signs of security violations. Having detected such signs, IDSs trigger alerts to report them. These alerts are presented to a human analyst, who evaluates them and initiates an adequate response. In practice, IDSs have been observed to trigger thousands of alerts per day, most of which are mistakenly triggered by benign events (i.e., false positives). This makes it extremely difficult for the analyst to correctly identify alerts related to attacks (i.e., true positives).

In this paper, we present two orthogonal and complementary approaches to reduce the number of false positives in intrusion detection using alert postprocessing by data mining and machine learning. Moreover, these two techniques, because of their complementary nature, can be used together in an alert-management system. These concepts have been verified on a variety of data sets, and achieved a significant reduction in the number of false positives in both simulated and real environments.
ER  - 

TY  - JOUR
T1  - German police issue new badges
JO  - Card Technology Today
VL  - 17
IS  - 10
SP  - 16
EP  - 
PY  - 2005/10//
T2  - 

SN  - 0965-2590
DO  - http://dx.doi.org/10.1016/S0965-2590(05)70391-1
UR  - http://www.sciencedirect.com/science/article/pii/S0965259005703911
AB  - Berlin Police Department has taken delivery of an advanced badge solution from French smart card manufacturer Axalto to secure physical and logical access to its computers and facilities. Delivered in collaboration with local provider PPC Card Systems, the smart ID cards will replace the conventional printed, plastic, photo ID pages currently used by the department.
ER  - 

TY  - JOUR
T1  - Design and implementation of a mediation system enabling secure communication among Critical Infrastructures
JO  - International Journal of Critical Infrastructure Protection
VL  - 5
IS  - 2
SP  - 86
EP  - 97
PY  - 2012/7//
T2  - 
AU  - Castrucci, Marco
AU  - Neri, Alessandro
AU  - Caldeira, Filipe
AU  - Aubert, Jocelyn
AU  - Khadraoui, Djamel
AU  - Aubigny, Matthieu
AU  - Harpes, Carlo
AU  - Simões, Paulo
AU  - Suraci, Vincenzo
AU  - Capodieci, Paolo
SN  - 1874-5482
DO  - http://dx.doi.org/10.1016/j.ijcip.2012.04.001
UR  - http://www.sciencedirect.com/science/article/pii/S1874548212000194
KW  - Critical Infrastructure
KW  - Information sharing
KW  - Web services
KW  - MICIE
KW  - Secure mediation gateway
AB  - Nowadays, the increase of interdependencies among different Critical Infrastructures (CI) makes it more and more difficult to protect without using a systemic approach that considers a single infrastructure as part of a complex system of infrastructures. A strong collaboration among CI owners is required to avoid, or at least to limit the propagation of failures from one infrastructure to another and to put CI in safety mode. The key element enabling this required cooperation is the possibility for them to exchange relevant information related to the status of their infrastructures and to the services provided. In this paper, we present a middleware solution that allows CIs sharing real-time information, enabling the design and implementation of fault mitigation strategies and mechanisms to prevent the cascading phenomena generated by the failure propagation from one infrastructure to another.
ER  - 

TY  - JOUR
T1  - Report highlights
JO  - Information Security Technical Report
VL  - 3
IS  - 4
SP  - 3
EP  - 14
PY  - 1998///
T2  - 

SN  - 1363-4127
DO  - http://dx.doi.org/10.1016/S1363-4127(98)80034-4
UR  - http://www.sciencedirect.com/science/article/pii/S1363412798800344
ER  - 

TY  - JOUR
T1  - Anatomy of an advanced persistent threat
JO  - Network Security
VL  - 2015
IS  - 4
SP  - 13
EP  - 16
PY  - 2015/4//
T2  - 
AU  - Auty, Mike
SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/S1353-4858(15)30028-3
UR  - http://www.sciencedirect.com/science/article/pii/S1353485815300283
AB  - Often when a business is the subject of a hacking attack, emotions run high. The knee jerk reaction is one that comes from an emotional place – ‘how could someone do this to me?’ or ‘who is responsible?’. And ultimately, the organisation wants the infiltrator out. However, once the background to nation-state hacking is understood and how these types of attacks operate, what's needed is a change in the mind-set about how we protect IT to a reasoned and rational approach that sees attacks as part and parcel of doing business.

The rise in well-resourced and sustained cyber-attacks, which are often the work of nation-state actors, calls for a new mind-set when it comes to protecting your organisation.

It's important not to have a knee-jerk reaction. Sometimes it's better not to rid the network of the malware and the attackers straight away, but to monitor and analyse what they are doing. Mike Auty of MWR InfoSecurity explains the best approach with the help of two case studies.
ER  - 

TY  - JOUR
T1  - Exhibitors
JO  - Computers & Security
VL  - 17
IS  - 7
SP  - 621
EP  - 631
PY  - 1998///
T2  - 

SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/S0167-4048(98)90308-X
UR  - http://www.sciencedirect.com/science/article/pii/S016740489890308X
ER  - 

TY  - JOUR
T1  - Learning to love SIEM
JO  - Network Security
VL  - 2011
IS  - 4
SP  - 18
EP  - 19
PY  - 2011/4//
T2  - 
AU  - Jenkins, Steve
SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/S1353-4858(11)70041-1
UR  - http://www.sciencedirect.com/science/article/pii/S1353485811700411
AB  - In the 1964 motion picture, Dr Strangelove or: How I Learned to Stop Worrying and Love the Bomb, a paranoid general played by Sterling Hayden is able to hack into a system and initiate a nuclear attack on the Soviet Union without the knowledge of his superiors.
ER  - 

TY  - JOUR
T1  - Issues in Back Tracing Events
JO  - Computer Fraud & Security
VL  - 2003
IS  - 5
SP  - 17
EP  - 20
PY  - 2003/5//
T2  - 
AU  - Stephenson, Peter
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(03)05013-9
UR  - http://www.sciencedirect.com/science/article/pii/S1361372303050139
AB  - We’re back on track after out diversion into post mortems. This column will begin the discussion of back tracing. This is a very complex issue and space prevents us from an exhaustive review of the topic in a single column. However, this month we’ll lay out the issues, look at some tools and some non-traditional approaches and set the stage for further discussions later on.
ER  - 

TY  - JOUR
T1  - Don't underestimate EU data regulation compliance
JO  - Computer Fraud & Security
VL  - 2008
IS  - 2
SP  - 14
EP  - 15
PY  - 2008/2//
T2  - 
AU  - Brewer, Ross
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(08)70027-7
UR  - http://www.sciencedirect.com/science/article/pii/S1361372308700277
AB  - Ross Brewer says time is ticking for telcos to get logs ready for retrieval.
ER  - 

TY  - JOUR
T1  - A DSL for modeling application-specific functionalities of business applications
JO  - Computer Languages, Systems & Structures
VL  - 43
IS  - 
SP  - 69
EP  - 95
PY  - 2015/10//
T2  - 
AU  - Popovic, Aleksandar
AU  - Lukovic, Ivan
AU  - Dimitrieski, Vladimir
AU  - Djukic, Verislav
SN  - 1477-8424
DO  - http://dx.doi.org/10.1016/j.cl.2015.03.003
UR  - http://www.sciencedirect.com/science/article/pii/S1477842415000263
KW  - Domain-specific languages
KW  - IIS?CFuncLang
KW  - Application-specific functionalities
KW  - Model transformations
KW  - IIS?Case
AB  - Abstract
Models have been widely used in the information system development process. Models are not just means for system analysis and documentation. They may be also transformed into system implementation, primarily program code. Generated program code of screen forms and transaction programs mainly implements generic functionalities that can be expressed by simple retrieval, insertion, update, or deletion operations over database records. Besides the program code of generic functionalities, each application usually includes program code for specific business logic that represents application-specific functionalities, which may include complex calculations, as well as a series of database operations. There is a lack of domain-specific and tool-supported techniques for specification of such application-specific functionalities at the level of platform-independent models (PIMs). In this paper, we propose an approach and a domain-specific language (DSL), named IIS?CFuncLang, aimed at enabling a complete specification of application-specific functionalities at the PIM level. We have developed algorithms for transformation of IIS?CFuncLang specifications into executable program code, such as PL/SQL program code. In order to support specification of application-specific functionalities using IIS?CFuncLang, we have also developed appropriate tree-based and textual editors. The language, editors, and the transformations are embedded into a Model-Driven Software Development tool, named Integrated Information Systems CASE (IIS?Case). IIS?Case supports platform-independent design and automated prototyping of information systems, which allows us to verify and test our approach in practice.
ER  - 

TY  - JOUR
T1  - The truth behind single sign-on : Gary Hardy, Zergo
JO  - Computers & Security
VL  - 15
IS  - 5
SP  - 417
EP  - 
PY  - 1996///
T2  - 

SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/0167-4048(96)82631-9
UR  - http://www.sciencedirect.com/science/article/pii/0167404896826319
ER  - 

TY  - JOUR
T1  - Smartcards — is Britain smart enough : Alan Laird, Bull Information Systems
JO  - Computers & Security
VL  - 15
IS  - 5
SP  - 417
EP  - 
PY  - 1996///
T2  - 

SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/0167-4048(96)82632-0
UR  - http://www.sciencedirect.com/science/article/pii/0167404896826320
ER  - 

TY  - JOUR
T1  - Cashless cybercrime : August Bequai
JO  - Computers & Security
VL  - 15
IS  - 5
SP  - 414
EP  - 417
PY  - 1996///
T2  - 

SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/0167-4048(96)82629-0
UR  - http://www.sciencedirect.com/science/article/pii/0167404896826290
ER  - 

TY  - JOUR
T1  - Learning relational policies from electronic health record access logs
JO  - Journal of Biomedical Informatics
VL  - 44
IS  - 2
SP  - 333
EP  - 342
PY  - 2011/4//
T2  - 
AU  - Malin, Bradley
AU  - Nyemba, Steve
AU  - Paulett, John
SN  - 1532-0464
DO  - http://dx.doi.org/10.1016/j.jbi.2011.01.007
UR  - http://www.sciencedirect.com/science/article/pii/S1532046411000098
KW  - Electronic health records
KW  - Organizational behavior
KW  - Knowledge discovery
KW  - Access logs
KW  - Auditing
AB  - Modern healthcare organizations (HCOs) are composed of complex dynamic teams to ensure clinical operations are executed in a quick and competent manner. At the same time, the fluid nature of such environments hinders administrators’ efforts to define access control policies that appropriately balance patient privacy and healthcare functions. Manual efforts to define these policies are labor-intensive and error-prone, often resulting in systems that endow certain care providers with overly broad access to patients’ medical records while restricting other providers from legitimate and timely use. In this work, we propose an alternative method to generate these policies by automatically mining usage patterns from electronic health record (EHR) systems. EHR systems are increasingly being integrated into clinical environments and our approach is designed to be generalizable across HCOs, thus assisting in the design and evaluation of local access control policies. Our technique, which is grounded in data mining and social network analysis theory, extracts a statistical model of the organization from the access logs of its EHRs. In doing so, our approach enables the review of predefined policies, as well as the discovery of unknown behaviors. We evaluate our approach with 5 months of access logs from the Vanderbilt University Medical Center and confirm the existence of stable social structures and intuitive business operations. Additionally, we demonstrate that there is significant turnover in the interactions between users in the HCO and that policies learned at the department-level afford greater stability over time.
ER  - 

TY  - JOUR
T1  - A model-based survey of alert correlation techniques
JO  - Computer Networks
VL  - 57
IS  - 5
SP  - 1289
EP  - 1317
PY  - 2013/4/7/
T2  - 
AU  - Salah, Saeed
AU  - Maciá-Fernández, Gabriel
AU  - Díaz-Verdejo, Jesús E.
SN  - 1389-1286
DO  - http://dx.doi.org/10.1016/j.comnet.2012.10.022
UR  - http://www.sciencedirect.com/science/article/pii/S1389128612004124
KW  - Alert correlation
KW  - Network management systems
KW  - Fault localization
KW  - Intrusion detection systems
KW  - SCADA systems
AB  - As telecommunication networks evolve rapidly in terms of scalability, complexity, and heterogeneity, the efficiency of fault localization procedures and the accuracy in the detection of anomalous behaviors are becoming important factors that largely influence the decision making process in large management companies. For this reason, telecommunication companies are doing a big effort investing in new technologies and projects aimed at finding efficient management solutions. One of the challenging issues for network and system management operators is that of dealing with the huge amount of alerts generated by the managed systems and networks. In order to discover anomalous behaviors and speed up fault localization processes, alert correlation is one of the most popular resources. Although many different alert correlation techniques have been investigated, it is still an active research field. In this paper, a survey of the state of the art in alert correlation techniques is presented. Unlike other authors, we consider that the correlation process is a common problem for different fields in the industry. Thus, we focus on showing the broad influence of this problem. Additionally, we suggest an alert correlation architecture capable of modeling current and prospective proposals. Finally, we also review some of the most important commercial products currently available.
ER  - 

TY  - JOUR
T1  - Subject index to volume 2
JO  - Computers & Security
VL  - 2
IS  - 3
SP  - 305
EP  - 308
PY  - 1983/11//
T2  - 

SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/0167-4048(83)90023-8
UR  - http://www.sciencedirect.com/science/article/pii/0167404883900238
ER  - 

TY  - JOUR
T1  - Bypassing information leakage protection with trusted applications
JO  - Computers & Security
VL  - 31
IS  - 4
SP  - 557
EP  - 568
PY  - 2012/6//
T2  - 
AU  - Blasco, Jorge
AU  - Hernandez-Castro, Julio Cesar
AU  - Tapiador, Juan E.
AU  - Ribagorda, Arturo
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/j.cose.2012.01.008
UR  - http://www.sciencedirect.com/science/article/pii/S0167404812000120
KW  - Malicious insiders
KW  - Information leakage
KW  - Sensitive information
KW  - Evasion
KW  - Trusted applications
KW  - Data leakage
AB  - Insider threats are an increasing concern for most modern organizations. Information leakage is one of the most important insider threats, particularly according to its potential financial impact. Data Leakage Protection (DLP) systems have been developed to tackle this issue and they constitute the main solution to protect information systems against leaks. They work by tracking sensitive information flows and monitoring executed applications to ensure that sensitive information is not leaving the organization. However, current DLP systems do not fully consider that trusted applications represent a threat to sensitive information confidentiality. In this paper, we demonstrate how to use common trusted applications to evade current DLP systems. Thanks to its wide range, trusted applications such as Microsoft Excel can be transformed into standardized block ciphers. Information can thus be encrypted in such a way that current DLP techniques cannot detect that sensitive information is being leaked. This method could be used by non-skilled malicious insiders and leaves almost no traces. We have successfully tested our method against a well-known DLP solution from a commercial provider (TrendMicro LeakProof). Finally, we also analyze the proposed evasion technique from the malicious insider point of view and discuss some possible countermeasures to mitigate its use to steal information.
ER  - 

TY  - JOUR
T1  - Making the internet safe for e-commerce
JO  - Computers & Security
VL  - 15
IS  - 7
SP  - 587
EP  - 
PY  - 1996///
T2  - 
AU  - Meyer, Helen
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/S0167-4048(97)88121-7
UR  - http://www.sciencedirect.com/science/article/pii/S0167404897881217
ER  - 

TY  - JOUR
T1  - Making internet access safe
JO  - Computers & Security
VL  - 15
IS  - 7
SP  - 587
EP  - 
PY  - 1996///
T2  - 
AU  - Meyer, Helen
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/S0167-4048(97)88120-5
UR  - http://www.sciencedirect.com/science/article/pii/S0167404897881205
ER  - 

TY  - JOUR
T1  - Cheyenne, McAfee cure software viruses
JO  - Computers & Security
VL  - 15
IS  - 7
SP  - 587
EP  - 588
PY  - 1996///
T2  - 
AU  - Meyer, Helen
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/S0167-4048(97)88122-9
UR  - http://www.sciencedirect.com/science/article/pii/S0167404897881229
ER  - 

TY  - JOUR
T1  - Network-versus host-based intrusion detection
JO  - Information Security Technical Report
VL  - 3
IS  - 4
SP  - 32
EP  - 42
PY  - 1998///
T2  - 
AU  - Schepers, Filip
SN  - 1363-4127
DO  - http://dx.doi.org/10.1016/S1363-4127(98)80036-8
UR  - http://www.sciencedirect.com/science/article/pii/S1363412798800368
ER  - 

TY  - JOUR
T1  - Data protection: why are organisations still missing the point?
JO  - Computer Fraud & Security
VL  - 2008
IS  - 6
SP  - 5
EP  - 8
PY  - 2008/6//
T2  - 
AU  - Gorge, Mathieu
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(08)70095-2
UR  - http://www.sciencedirect.com/science/article/pii/S1361372308700952
AB  - Mathieu Gorge argues that data protection should be second nature for companies.
ER  - 

TY  - JOUR
T1  - How to protect your data
JO  - Computers & Security
VL  - 15
IS  - 7
SP  - 586
EP  - 587
PY  - 1996///
T2  - 
AU  - Meyer, Helen
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/S0167-4048(97)88119-9
UR  - http://www.sciencedirect.com/science/article/pii/S0167404897881199
ER  - 

TY  - JOUR
T1  - The challenges of large computer evidence cases
JO  - Digital Investigation
VL  - 1
IS  - 1
SP  - 16
EP  - 17
PY  - 2004/2//
T2  - 
AU  - Sommer, Peter
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2004.01.005
UR  - http://www.sciencedirect.com/science/article/pii/S1742287604000106
ER  - 

TY  - JOUR
T1  - The difficult art of managing logs
JO  - Computer Fraud & Security
VL  - 2007
IS  - 10
SP  - 5
EP  - 7
PY  - 2007/10//
T2  - 
AU  - Pasquinucci, Andrea
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(07)70131-8
UR  - http://www.sciencedirect.com/science/article/pii/S1361372307701318
AB  - Andrea Pasquinucci looks at how logs can make life much easier.
ER  - 

TY  - JOUR
T1  - An intrusion detection and prevention system in cloud computing: A systematic review
JO  - Journal of Network and Computer Applications
VL  - 36
IS  - 1
SP  - 25
EP  - 41
PY  - 2013/1//
T2  - 
AU  - Patel, Ahmed
AU  - Taghavi, Mona
AU  - Bakhtiyari, Kaveh
AU  - Celestino Júnior, Joaquim
SN  - 1084-8045
DO  - http://dx.doi.org/10.1016/j.jnca.2012.08.007
UR  - http://www.sciencedirect.com/science/article/pii/S108480451200183X
KW  - Intrusion detection and prevention
KW  - Cloud computing
KW  - Taxonomy
KW  - Alarm correlation
KW  - System requirements
AB  - The distributed and open structure of cloud computing and services becomes an attractive target for potential cyber-attacks by intruders. The traditional Intrusion Detection and Prevention Systems (IDPS) are largely inefficient to be deployed in cloud computing environments due to their openness and specific essence. This paper surveys, explores and informs researchers about the latest developed IDPSs and alarm management techniques by providing a comprehensive taxonomy and investigating possible solutions to detect and prevent intrusions in cloud computing systems. Considering the desired characteristics of IDPS and cloud computing systems, a list of germane requirements is identified and four concepts of autonomic computing self-management, ontology, risk management, and fuzzy theory are leveraged to satisfy these requirements.
ER  - 

TY  - JOUR
T1  - PRINDA: Architecture and design of non-disclosure agreements in privacy policy framework
JO  - Data & Knowledge Engineering
VL  - 63
IS  - 3
SP  - 684
EP  - 698
PY  - 2007/12//
T2  - Some issues in privacy data management (ICDE 2006)
AU  - Gupta, S.K.
AU  - Goyal, Vikram
AU  - Gupta, Anand
AU  - Meshram, Indira
SN  - 0169-023X
DO  - http://dx.doi.org/10.1016/j.datak.2007.03.007
UR  - http://www.sciencedirect.com/science/article/pii/S0169023X07000493
KW  - PRINDA
KW  - Non-disclosure Agreements
KW  - Privacy policies
AB  - Non-disclosure agreements (NDAs) in real life are typically used whenever there is transfer of private or confidential information from one organization to another. The provider organization cannot have any control over privacy mechanisms of the receiving organization. The privacy policy work so far has addressed itself for preventing privacy violations within an organization. We aim to give an architecture of PRINDA (PRIvacy NDA) system which incorporates NDA’s in privacy policy framework. Advantages of PRINDA system will be the following: (i) as there can be traces of malicious activity (invasion of privacy) either at provider-end or at recipient-end, if detected/reported, NDA can help to fix the responsibility to the organization violating the agreement, and will strengthen control in the privacy arena and (ii) owner of data can be provided with detailed description of accesses to the data from every organization to which the data has been transferred (strengthening the principle of compliance).
ER  - 

TY  - JOUR
T1  - Handling Distributed Denial-of-Service Attacks
JO  - Information Security Technical Report
VL  - 6
IS  - 3
SP  - 37
EP  - 44
PY  - 2001/9/1/
T2  - 
AU  - Janczewski, Lech J
SN  - 1363-4127
DO  - http://dx.doi.org/10.1016/S1363-4127(01)00306-5
UR  - http://www.sciencedirect.com/science/article/pii/S1363412701003065
ER  - 

TY  - JOUR
T1  - Baseline controls in some vital but often-overlooked areas of your information protection programme
JO  - Computer Fraud & Security
VL  - 2007
IS  - 12
SP  - 17
EP  - 20
PY  - 2007/12//
T2  - 
AU  - Forte, Dario
AU  - Power, Richard
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(07)70170-7
UR  - http://www.sciencedirect.com/science/article/pii/S1361372307701707
AB  - Dario Forte &amp; Richard Power look at the baseline.
ER  - 

TY  - JOUR
T1  - Secure Access Management: Trends, Drivers and Solutions
JO  - Information Security Technical Report
VL  - 7
IS  - 3
SP  - 81
EP  - 94
PY  - 2002/9//
T2  - 

SN  - 1363-4127
DO  - http://dx.doi.org/10.1016/S1363-4127(02)00309-6
UR  - http://www.sciencedirect.com/science/article/pii/S1363412702003096
ER  - 

TY  - JOUR
T1  - Cyber Wars and other threats
JO  - Computers & Security
VL  - 17
IS  - 2
SP  - 115
EP  - 118
PY  - 1998///
T2  - 
AU  - Hinde, Stephen
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/S0167-4048(97)81979-7
UR  - http://www.sciencedirect.com/science/article/pii/S0167404897819797
ER  - 

TY  - JOUR
T1  - Random bits &amp; bytes
JO  - Computers & Security
VL  - 13
IS  - 8
SP  - 622
EP  - 627
PY  - 1994///
T2  - 
AU  - Highland, HaroldJoseph
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/0167-4048(94)90041-8
UR  - http://www.sciencedirect.com/science/article/pii/0167404894900418
ER  - 

TY  - JOUR
T1  - Subject Index
JO  - Applied Soft Computing
VL  - 11
IS  - 8
SP  - 5829
EP  - 5873
PY  - 2011/12//
T2  - 

SN  - 1568-4946
DO  - http://dx.doi.org/10.1016/S1568-4946(11)00348-6
UR  - http://www.sciencedirect.com/science/article/pii/S1568494611003486
ER  - 

TY  - JOUR
T1  - Preservation-awareness in collaborative engineering
JO  - Computers in Industry
VL  - 65
IS  - 1
SP  - 24
EP  - 36
PY  - 2014/1//
T2  - 
AU  - Heutelbeck, Dominic
AU  - Grabarske, Jens
SN  - 0166-3615
DO  - http://dx.doi.org/10.1016/j.compind.2013.07.002
UR  - http://www.sciencedirect.com/science/article/pii/S016636151300136X
KW  - Digital preservation
KW  - Product life cycle
KW  - Collaboration
KW  - CSCW
KW  - Social networks
KW  - Workflow capturing
AB  - Abstract
In design and engineering, it is important to preserve more than the actual documents making up the product data. For knowledge-intensive industries it is of critical importance to also preserve the soft knowledge of the overall process within the product life cycle. The idea is not only to preserve the designs for the future, but also the knowledge about processes, decision making, and people. In order to preserve this knowledge, it is necessary to captured it at content creation time, a process currently mostly independent from the preservation process. This paper discusses how to make applications in content creation (e.g., in design and engineering) preservation-aware by using the OpenConjurer approach and framework.
ER  - 

TY  - JOUR
T1  - Random bits &amp; bytes
JO  - Computers & Security
VL  - 15
IS  - 1
SP  - 4
EP  - 11
PY  - 1996///
T2  - 
AU  - Highland, Harold Joseph
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/S0167-4048(96)90057-7
UR  - http://www.sciencedirect.com/science/article/pii/S0167404896900577
ER  - 

TY  - JOUR
T1  - Investigating digital fingerprints: advanced log analysis
JO  - Network Security
VL  - 2010
IS  - 10
SP  - 17
EP  - 20
PY  - 2010/10//
T2  - 
AU  - Knight, Eric
SN  - 1353-4858
DO  - http://dx.doi.org/10.1016/S1353-4858(10)70127-6
UR  - http://www.sciencedirect.com/science/article/pii/S1353485810701276
AB  - Perhaps your organisation has recently suffered a data breach, which would not be uncommon as last year set records for information theft. For example, in 2009 the Identity Theft Resource Center reported 498 breaches that exposed over 222 million protected records.1 The unreported numbers traditionally have been three or more times higher. More still go undetected or ignored.
ER  - 

TY  - JOUR
T1  - Continuous auditing technologies and models: A discussion
JO  - Computers & Security
VL  - 25
IS  - 5
SP  - 325
EP  - 331
PY  - 2006/7//
T2  - 
AU  - Flowerday, S.
AU  - Blundell, A.W.
AU  - Von Solms, R.
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/j.cose.2006.06.004
UR  - http://www.sciencedirect.com/science/article/pii/S0167404806000964
KW  - Continuous auditing
KW  - Real-time assurances
KW  - Information integrity
KW  - Internal controls
KW  - Technology-based prevention
AB  - In the age of real-time accounting and real-time communication current audit practices, while effective, often provide audit results long after fraud and/or errors have occurred. Real-time assurances can assist in preventing intentional or unintentional errors. This can best be achieved through continuous auditing which relies heavily on technology. These technologies are embedded within and are crucial to continuous auditing models.
ER  - 

TY  - JOUR
T1  - Privacy and consent in pervasive networks
JO  - Information Security Technical Report
VL  - 14
IS  - 3
SP  - 138
EP  - 142
PY  - 2009/8//
T2  - The Changing Shape of Privacy and Consent
AU  - Malik, Nazir A.
AU  - Tomlinson, Allan
SN  - 1363-4127
DO  - http://dx.doi.org/10.1016/j.istr.2009.10.002
UR  - http://www.sciencedirect.com/science/article/pii/S1363412709000375
AB  - Pervasive networks and location based systems have the potential to provide many new services. However the user of these services often has to provide personal information to allow the service to operate effectively. This article considers the problem of protecting personal information in this environment, and reports on the legislative and technical efforts being made to protect user privacy.
ER  - 

TY  - JOUR
T1  - Guide for authors
JO  - Computers & Security
VL  - 12
IS  - 4
SP  - 419
EP  - 
PY  - 1993/6//
T2  - 

SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/0167-4048(93)90030-9
UR  - http://www.sciencedirect.com/science/article/pii/0167404893900309
ER  - 

TY  - JOUR
T1  - General controls in computer systems
JO  - Computers & Security
VL  - 4
IS  - 1
SP  - 33
EP  - 45
PY  - 1985/3//
T2  - 
AU  - Cerullo, Michael J.
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/0167-4048(85)90007-0
UR  - http://www.sciencedirect.com/science/article/pii/0167404885900070
KW  - Internal control
KW  - plan of organization
KW  - operations controls
KW  - system development and planning controls
KW  - access controls
KW  - documentation
KW  - personnel controls
KW  - operating system controls
KW  - master planning
KW  - contingency planning
AB  - Because almost all business organizations own or will shortly own computer systems, auditors can no longer treat the computer as a “black box” to be ignored when attesting to the fairness with which financial statements present financial position and results of operations. This paper is primarily written for auditors who are becoming involved in auditing computerized accounting systems. It covers in detail general controls in computer systems, the first and most important category of controls evaluated by an auditor. General controls relate to all EDP activities; they span all jobs processed on the computer system. When they are weak or non-existing, the auditor must expand his testing of the entire computer system, often at considerable additional cost to the client.
ER  - 

TY  - JOUR
T1  - Digital logs—proof matters
JO  - Digital Investigation
VL  - 1
IS  - 2
SP  - 94
EP  - 101
PY  - 2004/6//
T2  - 
AU  - Kenneally, Erin E.
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2004.01.006
UR  - http://www.sciencedirect.com/science/article/pii/S1742287604000118
ER  - 

TY  - JOUR
T1  - Internet as a Pornotopia?
JO  - Computers & Security
VL  - 15
IS  - 3
SP  - 203
EP  - 208
PY  - 1996///
T2  - 
AU  - Blatchford, Clive
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/0167-4048(96)89782-3
UR  - http://www.sciencedirect.com/science/article/pii/0167404896897823
ER  - 

TY  - JOUR
T1  - A user-centric federated single sign-on system
JO  - Journal of Network and Computer Applications
VL  - 32
IS  - 2
SP  - 388
EP  - 401
PY  - 2009/3//
T2  - 
AU  - Suriadi, Suriadi
AU  - Foo, Ernest
AU  - Jøsang, Audun
SN  - 1084-8045
DO  - http://dx.doi.org/10.1016/j.jnca.2008.02.016
UR  - http://www.sciencedirect.com/science/article/pii/S1084804508000519
KW  - Identity management
KW  - Privacy
KW  - Private credential
KW  - Single sign-on
KW  - User-centric
AB  - Current identity management systems are not concerned with user privacy. Users must assume that identity providers and service providers will ensure their privacy, which is not always the case. This paper proposes an extension of the existing federated single sign-on (FSSO) systems that adopts the beneficial properties of the user-centric identity management (UCIM) model. This new identity management system allows the users to control and enforce their privacy requirements while still retaining the convenience of single sign-on over a federation of service providers. Colored Petri Nets are used to formally model the new identity management system to provide assurance that the privacy goals are achieved. To our knowledge, Colored Petri Nets have not been used to model privacy in identity management systems before.
ER  - 

TY  - JOUR
T1  - Algorithms for anomaly detection of traces in logs of process aware information systems
JO  - Information Systems
VL  - 38
IS  - 1
SP  - 33
EP  - 44
PY  - 2013/3//
T2  - 
AU  - Bezerra, Fábio
AU  - Wainer, Jacques
SN  - 0306-4379
DO  - http://dx.doi.org/10.1016/j.is.2012.04.004
UR  - http://www.sciencedirect.com/science/article/pii/S0306437912000567
KW  - Anomaly detection
KW  - Process mining
KW  - Process-aware systems
AB  - This paper discusses four algorithms for detecting anomalies in logs of process aware systems. One of the algorithms only marks as potential anomalies traces that are infrequent in the log. The other three algorithms: threshold, iterative and sampling are based on mining a process model from the log, or a subset of it. The algorithms were evaluated on a set of 1500 artificial logs, with different profiles on the number of anomalous traces and the number of times each anomalous traces was present in the log. The sampling algorithm proved to be the most effective solution. We also applied the algorithm to a real log, and compared the resulting detected anomalous traces with the ones detected by a different procedure that relies on manual choices.
ER  - 

TY  - JOUR
T1  - Key concerns in a review of CA-ACF2/MVS
JO  - Computers & Security
VL  - 17
IS  - 1
SP  - 42
EP  - 53
PY  - 1998///
T2  - 
AU  - Crocker, Norman
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/S0167-4048(97)80249-0
UR  - http://www.sciencedirect.com/science/article/pii/S0167404897802490
AB  - IBM mainframe installations started to protect computer data and transactions during the latter part of the 1970s. Since the IBM mainframe operating systems themselves do not incorporate suitable access control facilities several vendors began to market “addon” packages to provide these. The MVS market leader, in terms of total systems protected, has consistently been CA-ACF2. Part of its popularity has been due to its ability to protect complex environments, providing more powerful facilities and options than the other products. The very complexity which makes it such a powerful solution also makes it difficult to understand, particularly for the EDP auditor who must be able to both detect potential exposures in the system and recommend ways of operating in a more controlled manner. Based on about 40 CA-ACF2 audits carried out by the author and 18 years experience with the software, this article will provide pointers to the most common problems found, show how to extract the relevant information from CA-ACF2 and discuss appropriate control measures. The article will concentrate on CA-ACF2 in the MVS environment, but many of the concerns and techniques will also apply in the VM and VSE environments.
ER  - 

TY  - JOUR
T1  - Administrative controls for password-based computer access control systems
JO  - Computer Fraud & Security Bulletin
VL  - 8
IS  - 3
SP  - 5
EP  - 13
PY  - 1986/1//
T2  - 
AU  - Wood, CharlesCresson
SN  - 0142-0496
DO  - http://dx.doi.org/10.1016/0142-0496(86)90043-3
UR  - http://www.sciencedirect.com/science/article/pii/0142049686900433
ER  - 

TY  - JOUR
T1  - Legal admissibility of evidence held in digital form
JO  - Computer Law & Security Review
VL  - 15
IS  - 3
SP  - 185
EP  - 187
PY  - 1999/5//
Y2  - 1999/6//
T2  - 
AU  - Kearsley, Amanda J.
SN  - 0267-3649
DO  - http://dx.doi.org/10.1016/S0267-3649(99)80037-5
UR  - http://www.sciencedirect.com/science/article/pii/S0267364999800375
AB  - Organizations faced with voluminous paper documents are increasingly turning to technology, scanning the paper documents and then destroying the originals, placing increased reliance on electronic document management systems for subsequent retrieval. One key difficulty these organizations face is the uncertain legal status of the electronic record, and whether, if necessary, the digital copy of the original paper document can be used in evidence in legal proceedings. This first part of a two part article considers the effects of the Civil Evidence Act 1995 and also examines current and recommended practices for ensuring the admissibility of digital evidence.
ER  - 

TY  - JOUR
T1  - Privacy, trust and policy-making: Challenges and responses
JO  - Computer Law & Security Review
VL  - 25
IS  - 1
SP  - 69
EP  - 83
PY  - 2009///
T2  - 
AU  - Wright, David
AU  - Gutwirth, Serge
AU  - Friedewald, Michael
AU  - De Hert, Paul
AU  - Langheinrich, Marc
AU  - Moscibroda, Anna
SN  - 0267-3649
DO  - http://dx.doi.org/10.1016/j.clsr.2008.11.004
UR  - http://www.sciencedirect.com/science/article/pii/S0267364908001672
KW  - Ambient intelligence
KW  - Profiling
KW  - RFID
KW  - Data Protection
KW  - Privacy
KW  - Information Society Policies
AB  - The authors contend that the emerging ubiquitous Information Society (aka ambient intelligence, pervasive computing, ubiquitous networking and so on) will raise many privacy and trust issues that are context dependent. These issues will pose many challenges for policy-makers and stakeholders because people's notions of privacy and trust are different and shifting. People's attitudes towards privacy and protecting their personal data can vary significantly according to differing circumstances. In addition, notions of privacy and trust are changing over time. The authors provide numerous examples of the challenges facing policy-makers and identify some possible responses, but they see a need for improvements in the policy-making process in order to deal more effectively with varying contexts. They also identify some useful policy-making tools. They conclude that the broad brush policies of the past are not likely to be adequate to deal with the new challenges and that we are probably entering an era that will require development of “micro-policies”. While the new technologies will pose many challenges, perhaps the biggest challenge of all will be to ensure coherence of these micro-policies.
ER  - 

TY  - JOUR
T1  - CAT Detect (Computer Activity Timeline Detection): A tool for detecting inconsistency in computer activity timelines
JO  - Digital Investigation
VL  - 8, Supplement
IS  - 
SP  - S52
EP  - S61
PY  - 2011/8//
T2  - 11th Annual Digital Forensics Research Conference
AU  - Marrington, Andrew
AU  - Baggili, Ibrahim
AU  - Mohay, George
AU  - Clark, Andrew
SN  - 1742-2876
DO  - http://dx.doi.org/10.1016/j.diin.2011.05.007
UR  - http://www.sciencedirect.com/science/article/pii/S1742287611000314
KW  - Timeline inconsistency
KW  - Event correlation
KW  - Precondition event
KW  - Happened-before
KW  - CAT detect
AB  - The construction of timelines of computer activity is a part of many digital investigations. These timelines of events are composed of traces of historical activity drawn from system logs and potentially from evidence of events found in the computer file system. A potential problem with the use of such information is that some of it may be inconsistent and contradictory thus compromising its value. This work introduces a software tool (CAT Detect) for the detection of inconsistency within timelines of computer activity. We examine the impact of deliberate tampering through experiments conducted with our prototype software tool. Based on the results of these experiments, we discuss techniques which can be employed to deal with such temporal inconsistencies.
ER  - 

TY  - JOUR
T1  - Comprehensive rule-based compliance checking and risk management with process mining
JO  - Decision Support Systems
VL  - 54
IS  - 3
SP  - 1357
EP  - 1369
PY  - 2013/2//
T2  - 
AU  - Caron, Filip
AU  - Vanthienen, Jan
AU  - Baesens, Bart
SN  - 0167-9236
DO  - http://dx.doi.org/10.1016/j.dss.2012.12.012
UR  - http://www.sciencedirect.com/science/article/pii/S0167923612003788
KW  - Business rules
KW  - Compliance checking
KW  - Risk management
KW  - Process mining
KW  - Process-aware information systems
AB  - Abstract
Process mining researchers have primarily focused on developing and improving process discovery techniques, while attention for the applicability of process mining has been below par. As a result, there only exists a partial fit with the traditional requirements for compliance checking and risk management.

This paper proposes a comprehensive rule-based process mining approach for a timely investigation of a complete set of enriched process event data. Additionally, the contribution elaborates a two-dimensional business rule taxonomy that serves as a source of business rules for the comprehensive rule-based compliance checking approach. Finally, the study provides a formal grounding for and an evaluation of the comprehensive rule-based compliance checking approach.
ER  - 

TY  - JOUR
T1  - User provisioning with SPML
JO  - Information Security Technical Report
VL  - 9
IS  - 1
SP  - 86
EP  - 96
PY  - 2004/1//
Y2  - 2004/3//
T2  - 
AU  - Sodhi, Gavenraj
SN  - 1363-4127
DO  - http://dx.doi.org/10.1016/S1363-4127(04)00018-4
UR  - http://www.sciencedirect.com/science/article/pii/S1363412704000184
ER  - 

TY  - JOUR
T1  - A survey of password mechanisms: Weaknesses and potential improvements. Part 1
JO  - Computers & Security
VL  - 8
IS  - 7
SP  - 587
EP  - 604
PY  - 1989/11//
T2  - 
AU  - Jobusch, David L.
AU  - Oldehoeft, Arthur E.
SN  - 0167-4048
DO  - http://dx.doi.org/10.1016/0167-4048(89)90051-5
UR  - http://www.sciencedirect.com/science/article/pii/0167404889900515
KW  - Authentication
KW  - Passwords
KW  - UNIX
KW  - Pass-phrases
AB  - While research continues on more sophisticated methods of authentication, password mechanisms remain the predominant method of identifying computer system users. In this paper, the goals of authentication are reviewed, and the strengths and vulnerabilities of password mechanisms are discussed. The 4.3 Berkeley Software Distribution (4.3BSD) version of UNIX is used as a case study throughout the paper. Several recommendations are presented for the improvement of password mechanisms. In particular, a simple extension of the UNIX password system is described that permits the use of pass-phrases.
ER  - 

TY  - JOUR
T1  - IMENSE: An e-infrastructure environment for patient specific multiscale data integration, modelling and clinical treatment
JO  - Journal of Computational Science
VL  - 3
IS  - 5
SP  - 314
EP  - 327
PY  - 2012/9//
T2  - Advanced Computing Solutions for Health Care and Medicine
AU  - Zasada, Stefan J.
AU  - Wang, Tao
AU  - Haidar, Ali
AU  - Liu, Enjie
AU  - Graf, Norbert
AU  - Clapworthy, Gordon
AU  - Manos, Steven
AU  - Coveney, Peter V.
SN  - 1877-7503
DO  - http://dx.doi.org/10.1016/j.jocs.2011.07.001
UR  - http://www.sciencedirect.com/science/article/pii/S1877750311000639
KW  - Clinical decision support
KW  - Electronic health records
KW  - Virtual Physiological Human
KW  - Medical data management
AB  - Secure access to patient data and analysis tools to run on that data will revolutionize the treatment of a wide range of diseases, by using advanced simulation techniques to underpin the clinical decision making process. To achieve these goals, suitable e-Science infrastructures are required to allow clinicians and researchers to trivially access data and launch simulations. In this paper we describe the open source Individualized MEdiciNe Simulation Environment (IMENSE), which provides a platform to securely manage clinical data, and to perform wide ranging analysis on that data, ultimately with the intention of enhancing clinical decision making with direct impact on patient health care. We motivate the design decisions taken in the development of the IMENSE system by considering the needs of researchers in the ContraCancrum project, which provides a paradigmatic case in which clinicians and researchers require coordinated access to data and simulation tools. We show how the modular nature of the IMENSE system makes it applicable to a wide range of biomedical computing scenarios, from within a single hospital to major international research projects.
ER  - 

TY  - JOUR
T1  - Building Dialysis Workflows into EMRs
JO  - Procedia Technology
VL  - 9
IS  - 
SP  - 985
EP  - 995
PY  - 2013///
T2  - CENTERIS 2013 - Conference on ENTERprise Information Systems / ProjMAN 2013 - International Conference on Project MANagement/ HCIST 2013 - International Conference on Health and Social Care Information Systems and Technologies
AU  - Yu, Bo
AU  - Wijesekera, Duminda
SN  - 2212-0173
DO  - http://dx.doi.org/10.1016/j.protcy.2013.12.110
UR  - http://www.sciencedirect.com/science/article/pii/S2212017313002648
KW  - HemoDialysis EMR
KW  - Workflow
KW  - Quality Care
AB  - Abstract
Hemodialysis is an expensive life extending procedure used world-wide. Hemodialysis centers typically require multiple caregivers—e.g., nephrologists, nurses, technicians and social workers-- who assist each patient multiple times per week. These caregivers follow precise clinical processes that we refer to as a collection of clinical workflows. When combined, these workflows collectively provide comprehensive hemodialysis services. We show how these clinical workflows can be directly enforced using an electronic medical record system. Our proposed system has been used to model the workflows of a functional dialysis center and implemented using open-source software for the EMR components and workflow management system.
ER  - 

TY  - JOUR
T1  - Securing digital signatures for non-repudiation
JO  - Computer Communications
VL  - 22
IS  - 8
SP  - 710
EP  - 716
PY  - 1999/5/25/
T2  - 
AU  - Zhou, J.
AU  - Lam, K.Y.
SN  - 0140-3664
DO  - http://dx.doi.org/10.1016/S0140-3664(99)00031-6
UR  - http://www.sciencedirect.com/science/article/pii/S0140366499000316
KW  - Digital signature
KW  - Non-repudiation
KW  - Secure electronic commerce
AB  - Dispute of transactions is a common problem that could jeopardise business. Hence non-repudiation services are essential in business transactions which provide evidence to enable dispute resolution. To be eligible as non-repudiation evidence, the digital signature on an electronic document should remain valid until its expiry date which is specified by some non-repudiation policy. The conventional approaches are either inefficient or insecure to achieve non-repudiation in electronic commerce. This article presents a practical scheme to secure digital signatures as non-repudiation evidence with an adjustable degree of risk.
ER  - 

TY  - JOUR
T1  - Computer-aided design/drafting management within the united states air force logistics command civil engineering organization
JO  - Computers & Structures
VL  - 41
IS  - 6
SP  - 1169
EP  - 1173
PY  - 1991///
T2  - Special Issue: CIVIL-COMP 89
AU  - Alley, W.D.
SN  - 0045-7949
DO  - http://dx.doi.org/10.1016/0045-7949(91)90254-J
UR  - http://www.sciencedirect.com/science/article/pii/004579499190254J
AB  - The United States Air Force civil engineering organization is responsible for the design, construction, and maintenance of over $115 billion dollars worth of facilities at bases around the world. Many civil engineering functions and offices have procured or are in the process of procuring computer-aided design and drafting (CADD) technology to assist in accomplishing their assigned tasks. Specific areas where automation is anticipated as being most helpful include maintaining facility information, design and engineering, operations and maintenance, project management, master planning, and space planning and management. The actual achievement of these capabilities is less dependent on hardware or software and relies more on a comprehensive and achievable implementation and system management plan. The Air Force civil engineering implementation and management plan consists of four separate but related sections: the benefits/needs analysis; the application/action plan; the CADD standards manual; and the CADD user's guide.
ER  - 

TY  - JOUR
T1  - An ‘intelligent’ approach to audit trail analysis
JO  - Computer Audit Update
VL  - 1991
IS  - 11
SP  - 13
EP  - 17
PY  - 1991/11//
T2  - 
AU  - Hickman, Frank
SN  - 0960-2593
DO  - http://dx.doi.org/10.1016/0960-2593(91)90050-J
UR  - http://www.sciencedirect.com/science/article/pii/096025939190050J
ER  - 

TY  - JOUR
T1  - Discovering the software process by means of stochastic workflow analysis
JO  - Journal of Systems Architecture
VL  - 52
IS  - 11
SP  - 684
EP  - 692
PY  - 2006/11//
T2  - Agile Methodologies for Software Production
AU  - Colombo, Alberto
AU  - Damiani, Ernesto
AU  - Gianini, Gabriele
SN  - 1383-7621
DO  - http://dx.doi.org/10.1016/j.sysarc.2006.06.012
UR  - http://www.sciencedirect.com/science/article/pii/S1383762106000701
KW  - Software process
KW  - Workflow management
KW  - Stochastic dynamics
KW  - Markov chains
KW  - Machine learning
KW  - Bayesian methods
KW  - Time-sequence analysis
KW  - Similarity measures
AB  - A fundamental feature of the software process consists in its own stochastic nature. A convenient approach for extracting the stochastic dynamics of a process from log data is that of modelling the process as a Markov model: in this way the discovery of the short/medium range dynamics of the process is cast in terms of the learning of Markov models of different orders, i.e. in terms of learning the corresponding transition matrices. In this paper we show that the use of a full Bayesian approach in the learning process helps providing robustness against statistical noise and over-fitting, as the size of a transition matrix grows exponentially with the order of the model. We give a specific model–model similarity definition and the corresponding calculation procedure to be used in model-to-sequence or sequence-to-sequence conformance assessment, this similarity definition could also be applied to other inferential tasks, such as unsupervised process learning.
ER  - 

TY  - JOUR
T1  - Investigating student motivation in the context of a learning analytics intervention during a summer bridge program
JO  - Computers in Human Behavior
VL  - 47
IS  - 
SP  - 90
EP  - 97
PY  - 2015/6//
T2  - Learning Analytics, Educational Data Mining and data-driven Educational Decision Making
AU  - Lonn, Steven
AU  - Aguilar, Stephen J.
AU  - Teasley, Stephanie D.
SN  - 0747-5632
DO  - http://dx.doi.org/10.1016/j.chb.2014.07.013
UR  - http://www.sciencedirect.com/science/article/pii/S0747563214003793
KW  - Learning analytics
KW  - Motivation
KW  - Early warning systems
KW  - At-risk students
KW  - Design-based research
AB  - Abstract
Summer bridge programs are designed to improve retention and academic success among at-risk populations in postsecondary education by focusing on successful skills, behaviors, and high impact practices that promote academic performance. Recent research on these programs has focused primarily on how students’ incoming demographics and prior academic performance predict academic performance at the university level. This study investigated changes in students’ academic motivation orientations over the course of one bridge program, and how a learning analytics-based intervention was employed by academic advisors to inform their face-to-face meetings with students. The results of our study show that students’ mastery orientation decreased over the course of the bridge program, and indicate that students’ exposure to displays of their academic performance negatively predicts this change. The findings suggest that student perceptions of their goals and formative performance need to be carefully considered in the design of learning analytics interventions since the resulting tools can affect students’ interpretations of their own data as well as their subsequent academic success.
ER  - 

TY  - JOUR
T1  - Teleservice requirements for management
JO  - Computer Networks and ISDN Systems
VL  - 26, Supplement 4
IS  - 
SP  - S163
EP  - S178
PY  - 1995///
T2  - 
AU  - da Cruz, Alina
AU  - Lewis, Dave
AU  - Crowcroft, Jon
SN  - 0169-7552
DO  - http://dx.doi.org/10.1016/0169-7552(95)90004-7
UR  - http://www.sciencedirect.com/science/article/pii/0169755295900047
KW  - Teleservice
KW  - OSI
KW  - QoS
AB  - Future advances in conferencing will make it feasible to build servers which collaborate to provide services similar to those of an airline booking system over the networks. Such servers should be able to support real-time queries and modifications to the information stored. We present the management requirements for the services that are providd by these servers.
ER  - 

TY  - JOUR
T1  - Closeness Preference – A new interestingness measure for sequential rules mining
JO  - Knowledge-Based Systems
VL  - 44
IS  - 
SP  - 48
EP  - 56
PY  - 2013/5//
T2  - 
AU  - Railean, Ion
AU  - Lenca, Philippe
AU  - Moga, Sorin
AU  - Borda, Monica
SN  - 0950-7051
DO  - http://dx.doi.org/10.1016/j.knosys.2013.01.025
UR  - http://www.sciencedirect.com/science/article/pii/S0950705113000452
KW  - Sequential rules
KW  - Interestingness measures
KW  - User-preference
KW  - Time-interval
KW  - Closeness Preference
AB  - The time-interval between the antecedent and the consequent of a sequential rule can be considered as an important aspect in sequential rules interest. For example, in web logs analysis, the end-user can be interested in predicting the next page that will be visited by an internet surfer based on a history of visited pages. A Closeness Preference measure is proposed to favour the sequential rules with close itemsets based on user time-preference in a post-processing step. We illustrate the interest of the Closeness Preference measure with two real datasets (web logs data and activities of daily living data) for first, a predictive task and second, a descriptive one. Both of them show that Closeness Preference measure is helpful to find small and efficient sets of simple sequential rules.
ER  - 

TY  - JOUR
T1  - A model-integrated authoring environment for privacy policies
JO  - Science of Computer Programming
VL  - 89, Part B
IS  - 
SP  - 105
EP  - 125
PY  - 2014/9/1/
T2  - Special issue on Success Stories in Model Driven Engineering
AU  - Nadas, Andras
AU  - Levendovszky, Tihamer
AU  - Jackson, Ethan K.
AU  - Madari, Istvan
AU  - Sztipanovits, Janos
SN  - 0167-6423
DO  - http://dx.doi.org/10.1016/j.scico.2013.05.004
UR  - http://www.sciencedirect.com/science/article/pii/S016764231300124X
KW  - Privacy policies
KW  - Model-integrated computing
KW  - Constraint logic programming
AB  - Abstract
Privacy policies are rules designed to ensure that individuals’ health data are properly protected. Health Information Systems (HIS) are legally required to adhere to these policies. Since privacy policies are imposed on complex software systems, it is extremely hard to reason about their conformance and consistency. In order to address this problem, we have created a model-driven authoring environment to formally specify privacy policies originally defined in legal terms. In our observation, appropriate formalization of our policy language enabled formal analysis of its policies; these features were key to a successful model-driven engineering process. In this paper we present our modeling language and show its semantic anchoring to analyzable logic programs. We report on several projects where our approach is being applied and validated.
ER  - 

TY  - JOUR
T1  - The establishment of a pilot telemedical information society
JO  - Future Generation Computer Systems
VL  - 15
IS  - 2
SP  - 133
EP  - 156
PY  - 1999/3/11/
T2  - 
AU  - Marsh, Andy
SN  - 0167-739X
DO  - http://dx.doi.org/10.1016/S0167-739X(98)00059-4
UR  - http://www.sciencedirect.com/science/article/pii/S0167739X98000594
KW  - Telemedicine
KW  - Information society
KW  - World Wide Web
AB  - National and international telecommunication infrastructures have been set up through Europe to facilitate the movement of information. One major benefactor of the improved communication infrastructures is the health care community. The accessibility and interoperability of medical information systems is one of the grand challenges for the 21st century. Within Europe current developments in the application of telemedicine are being defined in separate initiatives. There are a number of pilot actions concentrating on various aspects of telemedicine. These actions involving the introduction of new technology or working practices rarely fail for technology related problems. However, in order to fully assess the likely take-up of telemedical technologies it is vital that all the aspects including non-technical are also addressed.

This paper describes how a complete pilot telemedical information society will be set up which facilitates to support secure and standardised remote diagnosis, teleconsultations and advanced medical facilities in a number of sectors covering a crucial spectrum of those required to support a complete telemedical information society. This pilot testbed will then be assessed in the context of a European environment identifying a business plan for its extension to other member states therefore promoting a truly international telemedical information society for the 21st century.
ER  - 

TY  - JOUR
T1  - Catching the malicious insider
JO  - Information Security Technical Report
VL  - 13
IS  - 4
SP  - 220
EP  - 224
PY  - 2008/11//
T2  - 
AU  - Jones, Andy
SN  - 1363-4127
DO  - http://dx.doi.org/10.1016/j.istr.2008.10.008
UR  - http://www.sciencedirect.com/science/article/pii/S1363412708000526
KW  - Insider
KW  - Defence
KW  - Detection holistic
AB  - This paper looks at the issue of the malicious insider and at a range of the environmental and technical issues that have led to the current situation. The paper also examines why the threat from the malicious insider is changing and looks at a range of measures that can be taken in order to minimise the likelihood of an attack and to enhance the probability of detection in the case of an attack.
ER  - 

TY  - JOUR
T1  - An Informatics Blueprint for Healthcare Quality Information Systems
JO  - Journal of the American Medical Informatics Association
VL  - 13
IS  - 4
SP  - 402
EP  - 417
PY  - 2006/7//
Y2  - 2006/8//
T2  - 
AU  - Niland, Joyce C.
AU  - Rouse, Layla
AU  - Stahl, Douglas C.
SN  - 1067-5027
DO  - http://dx.doi.org/10.1197/jamia.M2050
UR  - http://www.sciencedirect.com/science/article/pii/S1067502706000624
AB  - There is a critical gap in our nation’s ability to accurately measure and manage the quality of medical care. A robust healthcare quality information system (HQIS) has the potential to address this deficiency through the capture, codification, and analysis of information about patient treatments and related outcomes. Because non-technical issues often present the greatest challenges, this paper provides an overview of these socio-technical issues in building a successful HQIS, including the human, organizational, and knowledge management (KM) perspectives. Through an extensive literature review and direct experience in building a practical HQIS (the National Comprehensive Cancer Network Outcomes Research Database system), we have formulated an “informatics blueprint” to guide the development of such systems. While the blueprint was developed to facilitate healthcare quality information collection, management, analysis, and reporting, the concepts and advice provided may be extensible to the development of other types of clinical research information systems.
ER  - 

TY  - JOUR
T1  - IT as an Enabler of Computer Fraud
JO  - Information Security Technical Report
VL  - 5
IS  - 2
SP  - 60
EP  - 70
PY  - 2000/6/1/
T2  - 
AU  - Dippel, Tad
SN  - 1363-4127
DO  - http://dx.doi.org/10.1016/S1363-4127(00)02008-2
UR  - http://www.sciencedirect.com/science/article/pii/S1363412700020082
ER  - 

TY  - JOUR
T1  - Local anomaly detection for mobile network monitoring
JO  - Information Sciences
VL  - 178
IS  - 20
SP  - 3840
EP  - 3859
PY  - 2008/10/15/
T2  - Special Issue on Industrial Applications of Neural Networks10th Engineering Applications of Neural Networks 2007
AU  - Kumpulainen, Pekka
AU  - Hätönen, Kimmo
SN  - 0020-0255
DO  - http://dx.doi.org/10.1016/j.ins.2008.05.038
UR  - http://www.sciencedirect.com/science/article/pii/S0020025508001886
KW  - Local anomaly detection
KW  - Outlier
KW  - Mobile networks
KW  - System log
KW  - Self-organizing map
KW  - Adaptive thresholds
AB  - Huge amounts of operation data are constantly collected from various parts of communication networks. These data include measurements from the radio connections and system logs from servers. System operators and developers need robust, easy to use decision support tools based on these data. One of their key applications is to detect anomalous phenomena of the network. In this paper we present an anomaly detection method that describes the normal states of the system with a self-organizing map (SOM) identified from the data. Large deviation in the data samples from the SOM nodes is detected as anomalous behavior. Large deviation has traditionally been detected using global thresholds. If variation of the data occurs in separate parts of the data space, the global thresholds either fail to reveal anomalies or reveal false anomalies. Instead of one global threshold, we can use local thresholds, which depend on the local variation of the data. We also present a method to find an adaptive threshold using the distribution of the deviations. Our anomaly detection method can be used both in exploration of history data or comparison of unforeseen data against a data model derived from history data. It is applicable to wide range of processes that produce multivariate data. In this paper we present examples of this method applied to server log data and radio interface data from mobile networks.
ER  - 

TY  - JOUR
T1  - Impact of electronic funds transfer systems on networks
JO  - Computer Communications
VL  - 1
IS  - 2
SP  - 85
EP  - 90
PY  - 1978/4//
T2  - 
AU  - Pease, DL
SN  - 0140-3664
DO  - http://dx.doi.org/10.1016/0140-3664(78)90167-6
UR  - http://www.sciencedirect.com/science/article/pii/0140366478901676
AB  - Networks for electronic funds transfer systems have very high volumes of transactions to be processed. This means that changes must be made in EFT message-switching computer hardware. High volumes also mean that a transaction-oriented executive must be used. Terminal requirements will vary, depending on whether the transactions are paper-based or paperless transfers of funds. Integrating financial transactions with retail transactions with an electronic point-of-sale subsystem is a real systems challenge. Such a system must provide total control for the financial institution and improved point-of-sale productivity for the supermarket.
ER  - 

TY  - JOUR
T1  - Automated consent through privacy agents: Legal requirements and technical architecture
JO  - Computer Law & Security Review
VL  - 25
IS  - 2
SP  - 136
EP  - 144
PY  - 2009///
T2  - 
AU  - Le Métayer, Daniel
AU  - Monteleone, Shara
SN  - 0267-3649
DO  - http://dx.doi.org/10.1016/j.clsr.2009.02.010
UR  - http://www.sciencedirect.com/science/article/pii/S0267364909000387
KW  - Privacy agents
KW  - Legal analysis of consent
KW  - Technical architecture
KW  - RFID tags
KW  - Pervasive computing
KW  - Ambient intelligence
KW  - Liability
KW  - PET (Privacy Enhancing Technologies)
AB  - The changes imposed by new information technologies, especially pervasive computing and the Internet, require a deep reflection on the fundamental values underlying privacy and the best way to achieve their protection. The explicit consent of the data subject, which is a cornerstone of most data protection regulations, is a typical example of requirement which is very difficult to put into practice in the new world of “pervasive computing” where many data communications necessarily occur without the users' notice. In this paper, we argue that an architecture based on “Privacy Agents” can make privacy rights protection more effective, provided however that this architecture meets a number of legal requirements to ensure the validity of consent delivered through such Privacy Agents. We first present a legal analysis of consent considering successively (1) its nature; (2) its essential features (qualities and defects) and (3) its formal requirements. Then we draw the lessons of this legal analysis for the design of a valid architecture based on Privacy Agents. To conclude, we suggest an implementation of this architecture proposed in a multidisciplinary project involving lawyers and computer scientists.
ER  - 

TY  - JOUR
T1  - Collaborative use of individual search histories
JO  - Interacting with Computers
VL  - 20
IS  - 1
SP  - 184
EP  - 198
PY  - 2008/1//
T2  - 
AU  - Komlodi, Anita
AU  - Lutters, Wayne G.
SN  - 0953-5438
DO  - http://dx.doi.org/10.1016/j.intcom.2007.10.003
UR  - http://www.sciencedirect.com/science/article/pii/S0953543807000811
KW  - Search histories
KW  - Interaction histories
KW  - Awareness
KW  - Coordination
KW  - HCI
KW  - CSCW
AB  - Interaction history tools record interactions between users and systems, allowing users to annotate, edit, and replay their activities. Search history tools, a class of interaction history recorders, preserve search, browse, and other information-seeking steps. These tools include web browser histories and history lists in online full-text databases. Although search history tools were developed to support individuals in their information seeking, individuals often share their histories with one another collaboratively. This paper examines such sharing behaviors in two field studies of knowledge workers who routinely shared their individual search histories with their colleagues. While this practice is widespread, it is not supported by the design of contemporary interaction history tools. The results of the field research highlight core dimensions of this activity and inform considerations for the next generation of collaboration-sensitive interaction history tools.
ER  - 

TY  - JOUR
T1  - A business process gap detecting mechanism between information system process flow and internal control flow
JO  - Decision Support Systems
VL  - 47
IS  - 4
SP  - 436
EP  - 454
PY  - 2009/11//
T2  - Smart Business Networks: Concepts and Empirical Evidence
AU  - Huang, Shi-Ming
AU  - Yen, David C.
AU  - Hung, Yu-Chung
AU  - Zhou, Yen-Ju
AU  - Hua, Jing-Shiuan
SN  - 0167-9236
DO  - http://dx.doi.org/10.1016/j.dss.2009.04.011
UR  - http://www.sciencedirect.com/science/article/pii/S0167923609001043
KW  - Computer-Assisted Audit Techniques and Tools (CAATTs)
KW  - Business process gap detecting mechanism
KW  - Database log
KW  - Resource and value dependence method
KW  - Internal control flow
AB  - The information system (IS) has become an important backbone of the modern enterprise, which has caused electronic data generated by the IS to be more easily manipulated and destroyed than hardcopy data. It is therefore important for auditors to assure that the IS is well-controlled and secure. Auditors generally use Computer-Assisted Audit Techniques and Tools (CAATTs) to assist them in auditing the IS, though given the growing complexity of these systems, it is hard to completely perform a control test of the systems. This research aims to develop a mechanism, namely the Business Process Gap Detecting Mechanism (BPGAP-Detecting Mechanism), to automatically detect the business process gap between IS processes and internal control flows. This study also justifies the feasibility of BPGAP-Detecting Mechanism by providing a real case study. The result indicates that the BPGAP-Detecting Mechanism can assist the case company in resolving data quality problems that have occurred in its ERP (Enterprise Resource Planning) system and can also provide additional information for the company.
ER  - 

TY  - JOUR
T1  - An empirical investigation of Web session workloads: Can self-similarity be explained by deterministic chaos?
JO  - Information Processing & Management
VL  - 50
IS  - 1
SP  - 41
EP  - 53
PY  - 2014/1//
T2  - 
AU  - Dick, Scott
AU  - Yazdanbaksh, Omolbanin
AU  - Tang, Xiuli
AU  - Huynh, Toan
AU  - Miller, James
SN  - 0306-4573
DO  - http://dx.doi.org/10.1016/j.ipm.2013.07.004
UR  - http://www.sciencedirect.com/science/article/pii/S0306457313000769
KW  - Web traffic
KW  - Session workload
KW  - Traffic modeling
KW  - Chaos theory
KW  - Nonlinear time series analysis
AB  - Abstract
Several studies of Web server workloads have hypothesized that these workloads are self-similar. The explanation commonly advanced for this phenomenon is that the distribution of Web server requests may be heavy-tailed. However, there is another possible explanation: self-similarity can also arise from deterministic, chaotic processes. To our knowledge, this possibility has not previously been investigated, and so existing studies on Web workloads lack an adequate comparison against this alternative. We conduct an empirical study of workloads from two different Web sites: one public university, and one private company, using the largest datasets that have been described in the literature. Our study employs methods from nonlinear time series analysis to search for chaotic behavior in the web logs of these two sites. While we do find that the deterministic components (i.e. the well-known “weekend effect”) are significant components in these time series, we do not find evidence of chaotic behavior. Predictive modeling experiments contrasting heavy-tailed with deterministic models showed that both approaches were equally effective in modeling our datasets.
ER  - 

TY  - JOUR
T1  - Temporal information searching behaviour and strategies
JO  - Information Processing & Management
VL  - 
IS  - 
SP  - 
EP  - 
PY  - 
T2  - 
AU  - Joho, Hideo
AU  - Jatowt, Adam
AU  - Blanco, Roi
SN  - 0306-4573
DO  - http://dx.doi.org/10.1016/j.ipm.2015.03.006
UR  - http://www.sciencedirect.com/science/article/pii/S0306457315000448
KW  - Temporal information retrieval
KW  - Information searching behaviour
KW  - Search strategies
KW  - User study
AB  - Abstract
Temporal aspects have been receiving a great deal of interest in Information Retrieval and related fields. Although previous studies have proposed, designed and implemented temporal-aware systems and solutions, understanding of people’s temporal information searching behaviour is still limited. This paper reports the findings of a user study that explored temporal information searching behaviour and strategies in a laboratory setting. Information needs were grouped into three temporal classes (Past, Recency, and Future) to systematically study their characteristics. The main findings of our experiment are as follows. (1) It is intuitive for people to augment topical keywords with temporal expressions such as history, recent, or future as a tactic of temporal search. (2) However, such queries produce mixed results and the success of query reformulations appears to depend on topics to a large extent. (3) Search engine interfaces should detect temporal information needs to trigger the display of temporal search options. (4) Finding a relevant Wikipedia page or similar summary page is a popular starting point of past information needs. (5) Current search engines do a good job for information needs related to recent events, but more work is needed for past and future tasks. (6) Participants found it most difficult to find future information. Searching for domain experts was a key tactic in Future search, and file types of relevant documents are different from other temporal classes. Overall, the comparison of search across temporal classes indicated that Future search was the most difficult and the least successful followed by the search for the Past and then for Recency information. This paper discusses the implications of these findings on the design of future temporal IR systems.
ER  - 

TY  - JOUR
T1  - Traceability in Systems Engineering – Review of industrial practices, state-of-the-art technologies and new research solutions
JO  - Advanced Engineering Informatics
VL  - 26
IS  - 4
SP  - 924
EP  - 940
PY  - 2012/10//
T2  - EG-ICE 2011 + SI: Modern Concurrent Engineering
AU  - Königs, Simon Frederick
AU  - Beier, Grischa
AU  - Figge, Asmus
AU  - Stark, Rainer
SN  - 1474-0346
DO  - http://dx.doi.org/10.1016/j.aei.2012.08.002
UR  - http://www.sciencedirect.com/science/article/pii/S1474034612000766
KW  - Traceability
KW  - Systems Engineering
KW  - Morphological schema
KW  - Systems modelling
KW  - Nomenclature
KW  - Efficient trace link modelling
AB  - This article discusses issues and solutions regarding traceability for Systems Engineering projects. A review of industrial Systems Engineering practice is presented based on observations and studies that have been carried out at different original equipment manufacturers (OEMs). The studies reveal challenges in communication, data-transparency and data-consistency resulting among others from diverse and inhomogeneous toolsets. Existing traceability solutions, which are one possibility to address these challenges, struggle to achieve satisfactory cost/benefit ratios. Thus, further improvements of existing approaches with new concepts and ideas are required.

The article presents a morphological schema for traceability approaches. Its aim is to support attribute recombination to new research solutions and to overcome existing problems with naming conventions. The application of the schema is shown at the example of two novel approaches from Fraunhofer IPK and Daimler AG which are focusing on issues in trace link recording within Systems Engineering projects. The implementation, evaluation and functional comparison for both approaches are presented based on an exemplary automotive system. First results show a significant reduction of required trace recording efforts and good acceptance from engineers in the automotive industry. The developed schema and the two approaches contribute to the establishment of traceability for process improvement in future Systems Engineering projects.
ER  - 

TY  - JOUR
T1  - WELFIT: A remote evaluation tool for identifying Web usage patterns through client-side logging
JO  - International Journal of Human-Computer Studies
VL  - 76
IS  - 
SP  - 40
EP  - 49
PY  - 2015/4//
T2  - 
AU  - Santana, Vagner Figueredo de
AU  - Baranauskas, Maria Cecília Calani
SN  - 1071-5819
DO  - http://dx.doi.org/10.1016/j.ijhcs.2014.12.005
UR  - http://www.sciencedirect.com/science/article/pii/S1071581914001682
KW  - User interface evaluation
KW  - Remote evaluation
KW  - User interface events
KW  - Client-side events
KW  - Web usability
KW  - Web accessibility
AB  - Abstract
Although websites evaluation tools use different data sources (e.g., Web pages, server logs, and mouse tracks), few of them support remote evaluation using detailed observational data. Without considering data that represent the user’s real interaction with the interface, usability problems and/or accessibility barriers may remain unknown. This work contributes to the field by providing a tool to identify usage patterns based on client-side event logs and by presenting event stream composition characteristics. The work results from a long-term project and the tool is now available to the community. The system records usage data during real use, identifies usage patterns, and indicates potential user interface design problems. The proposed tool was experimented, counting on 180 participants, during a 15 month period collecting data from website usage. Results obtained are promising regarding the identification of usage patterns and the characterization of event streams based on the types of events that compose them.
ER  - 

TY  - JOUR
T1  - A stochastic model of e-customer behavior
JO  - Electronic Commerce Research and Applications
VL  - 2
IS  - 1
SP  - 81
EP  - 94
PY  - 2003///Spring
T2  - Containing Special Section: Five Best Papers selected from the International Conference on Electronic Commerce
AU  - Jenamani, Mamata
AU  - Mohapatra, Pratap K.J.
AU  - Ghose, Sujoy
SN  - 1567-4223
DO  - http://dx.doi.org/10.1016/S1567-4223(03)00010-3
UR  - http://www.sciencedirect.com/science/article/pii/S1567422303000103
KW  - E-customer behavior
KW  - Web usage mining
KW  - Discrete-time semi-Markov process
AB  - Web usage mining techniques are increasingly used today to understand e-customers’ within-site behavior. We propose a data mining model that considers e-customers’ activities as a discrete-time semi-Markov process and explains their behavior. An algorithm is proposed to compute transition probability matrix and holding time mass functions from the site navigation data. Finally, the model is used to explain customer behavior in an example site. A software agent, implemented in the site, collects and stores navigation data in the required form and thus helps to avoid data preprocessing. The model results helped to improve the site design and judge its performance.
ER  - 

TY  - JOUR
T1  - Customer responsibility for ensuring usability: Requirements on the user interface development process
JO  - Journal of Systems and Software
VL  - 25
IS  - 3
SP  - 241
EP  - 255
PY  - 1994/6//
T2  - 
AU  - Hix, Deborah
AU  - Hartson, H.Rex
AU  - Siochi, Antonio C.
AU  - Ruppert, David
SN  - 0164-1212
DO  - http://dx.doi.org/10.1016/0164-1212(94)90033-7
UR  - http://www.sciencedirect.com/science/article/pii/0164121294900337
AB  - An organization developing an interactive system will often find usability a compelling, but elusive, goal. The goal of ensuring usability becomes even more difficult to attain when the system is contracted to an outside developer, and control of both process and product becomes remote. Without a clear statement of requirements for the user interface of the system, this control may be lost from the beginning. Further, the cost of interactive system usage is especially significant in the case of contracted-out development, because the customer—the organization that lets a contract for interactive system development—bears the costs of training and poor user productivity.

In spite of the good publicity that usability has received lately, most customers still do not state requirements for the user interface of an interactive system. Although it has been shown that the process by which a user interface is developed has an overwhelmingly large effect on usability of the product, almost never are requirements for the interface development process included in the overall system requirements. How, then, can a customer hope to ensure usability in an interactive system that the customer is contracting out for development? We propose a solution: The customer establishes requirements for the user interface development process, even as early as a Request for Proposal (RFP). To successfully write and enforce such requirements, the customer (or a knowledgeable representative of the customer) must be well informed about human-computer interaction, usability, and existing user interface development techniques. As a case study, we describe how one customer organization, the Bureau of Land Management, produced user interface development process requirements and included them in an RFP. We also discuss how a customer can use product requirements in an RFP to describe the desired general interface style without constraining the developer to a specific design.
ER  - 

TY  - JOUR
T1  - Collaborative multi-agent rock facies classification from wireline well log data
JO  - Engineering Applications of Artificial Intelligence
VL  - 23
IS  - 7
SP  - 1158
EP  - 1172
PY  - 2010/10//
T2  - 
AU  - Gifford, Christopher M.
AU  - Agah, Arvin
SN  - 0952-1976
DO  - http://dx.doi.org/10.1016/j.engappai.2010.02.004
UR  - http://www.sciencedirect.com/science/article/pii/S095219761000062X
KW  - Rock classification
KW  - Well logs
KW  - Collaborative learning
KW  - Multi-agent systems
KW  - Applied artificial intelligence
AB  - Gas and oil reservoirs have been the focus of modeling efforts for decades as an attempt to locate zones with high volumes. Certain subsurface layers and layer sequences, such as those containing shale, are known to be impermeable to gas and/or liquid. Oil and natural gas then become trapped by these layers, making it possible to drill wells to reach the supply, and extract for use. The drilling of these wells, however, is costly. In this paper, we utilize multi-agent machine learning and classifier combination to learn rock facies sequences from wireline well log data. The paper focuses on how to construct a successful set of classifiers, which periodically collaborate, to increase the classification accuracy. Utilizing multiple, heterogeneous collaborative learning agents is shown to be successful for this classification problem. Utilizing the Multi-Agent Collaborative Learning Architecture, 84.5% absolute accuracy was obtained, an improvement of about 6.5% over the best results achieved by the Kansas Geological Survey with the same data set. A number of heuristics are presented for constructing teams of multiple collaborative classifiers for predicting rock facies.
ER  - 

TY  - JOUR
T1  - Mining web navigations for intelligence
JO  - Decision Support Systems
VL  - 41
IS  - 3
SP  - 574
EP  - 591
PY  - 2006/3//
T2  - Intelligence and security informatics
AU  - Wu, Harris
AU  - Gordon, Michael
AU  - DeMaagd, Kurtis
AU  - Fan, Weiguo
SN  - 0167-9236
DO  - http://dx.doi.org/10.1016/j.dss.2004.06.011
UR  - http://www.sciencedirect.com/science/article/pii/S0167923604001320
KW  - Principal clusters analysis
KW  - Intelligence
KW  - Mining
KW  - Trend analysis
KW  - Navigation analysis
KW  - Information overload
KW  - Web community
AB  - The Internet is one of the fastest growing areas of intelligence gathering. We present a statistical approach, called principal clusters analysis, for analyzing millions of user navigations on the Web. This technique identifies prominent navigation clusters on different topics. Furthermore, it can determine information items that are useful starting points to explore a topic, as well as key documents to explore the topic in greater detail. Trends can be detected by observing navigation prominence over time. We apply this technique on a large popular website. The results show promise in web intelligence mining.
ER  - 

TY  - JOUR
T1  - Strategic alternatives in telecare design: Developing a value-configuration-based alignment framework
JO  - The Journal of Strategic Information Systems
VL  - 20
IS  - 2
SP  - 198
EP  - 214
PY  - 2011/6//
T2  - 
AU  - Boonstra, Albert
AU  - Broekhuis, Manda
AU  - Offenbeek, Marjolein van
AU  - Wortmann, Hans
SN  - 0963-8687
DO  - http://dx.doi.org/10.1016/j.jsis.2010.12.001
UR  - http://www.sciencedirect.com/science/article/pii/S0963868710000739
KW  - Strategic alignment
KW  - Telecare
KW  - Value configurations
KW  - Design
AB  - In telecare adoption, the lack of a strategic vision and of consistency in design choices have been identified as critical problems. Existing IS alignment literature only offers limited answers to these problems and does not acknowledge the different value configurations that telecare technology can enable. This paper, therefore, integrates work on strategic value configurations with the strategic IS alignment model in order to widen the latter’s applicability. Based on the value configurations and related service management literature, a framework involving three distinct alignment configurations is developed for telecare. An analysis of two Dutch telecare projects shows how the proposed alignment profiles can explain the contrasting project outcomes more effectively than the traditional strategic alignment model would have done. The discussion reflects on the generalizability and contribution of an extended strategic alignment model.
ER  - 

TY  - JOUR
T1  - Use of XML and Java for collaborative petroleum reservoir modeling on the Internet
JO  - Computers & Geosciences
VL  - 31
IS  - 9
SP  - 1151
EP  - 1164
PY  - 2005/11//
T2  - Application of XML in the Geosciences
AU  - Victorine, John
AU  - Watney, W. Lynn
AU  - Bhattacharya, Saibal
SN  - 0098-3004
DO  - http://dx.doi.org/10.1016/j.cageo.2004.12.007
UR  - http://www.sciencedirect.com/science/article/pii/S0098300405001068
KW  - GEMINI
KW  - Petroleum web-based software
KW  - Java
KW  - Web start
KW  - XML
KW  - Reservoir modeling
AB  - The GEMINI (Geo-Engineering Modeling through INternet Informatics) is a public-domain, web-based freeware that is made up of an integrated suite of 14 Java-based software tools to accomplish on-line, real-time geologic and engineering reservoir modeling. GEMINI facilitates distant collaborations for small company and academic clients, negotiating analyses of both single and multiple wells. The system operates on a single server and an enterprise database. External data sets must be uploaded into this database.

Feedback from GEMINI users provided the impetus to develop Stand Alone Web Start Applications of GEMINI modules that reside in and operate from the user's PC. In this version, the GEMINI modules run as applets, which may reside in local user PCs, on the server, or Java Web Start. In this enhanced version, XML-based data handling procedures are used to access data from remote and local databases and save results for later access and analyses. The XML data handling process also integrates different stand-alone GEMINI modules enabling the user(s) to access multiple databases. It provides flexibility to the user to customize analytical approach, database location, and level of collaboration. An example integrated field-study using GEMINI modules and Stand Alone Web Start Applications is provided to demonstrate the versatile applicability of this freeware for cost-effective reservoir modeling.
ER  - 

TY  - JOUR
T1  - CFTP: a caching FTP server
JO  - Computer Networks and ISDN Systems
VL  - 30
IS  - 22–23
SP  - 2211
EP  - 2222
PY  - 1998/11/25/
T2  - 
AU  - Russell, Mark
AU  - Hopkins, Tim
SN  - 0169-7552
DO  - http://dx.doi.org/10.1016/S0169-7552(98)00240-2
UR  - http://www.sciencedirect.com/science/article/pii/S0169755298002402
KW  - Mirroring
KW  - Caching
KW  - Log summation
KW  - Usage statistics
KW  - Object oriented
AB  - By analyzing the log files generated by the UK National Web Cache and by a number of origin FTP sites we provide evidence that an FTP proxy cache with knowledge of local (national) mirror sites could significantly reduce the amount of data that needs to be transferred across already overused networks. We then describe the design and implementation of CFTP, a caching FTP server, and report on its usage over the first 10 months of its deployment. Finally we discuss a number of ways in which the software could be further enhanced to improve both its efficiency and its usability.
ER  - 

TY  - JOUR
T1  - Factors affecting the selection of search tactics: Tasks, knowledge, process, and systems
JO  - Information Processing & Management
VL  - 48
IS  - 2
SP  - 254
EP  - 270
PY  - 2012/3//
T2  - 
AU  - Xie, Iris
AU  - Joo, Soohyung
SN  - 0306-4573
DO  - http://dx.doi.org/10.1016/j.ipm.2011.08.009
UR  - http://www.sciencedirect.com/science/article/pii/S030645731100094X
KW  - Search tactics
KW  - Factors
KW  - Tasks
KW  - User knowledge
KW  - Search process
KW  - IR systems
AB  - This study investigated whether and how different factors in relation to task, user-perceived knowledge, search process, and system affect users’ search tactic selection. Thirty-one participants, representing the general public with their own tasks, were recruited for this study. Multiple methods were employed to collect data, including pre-questionnaire, verbal protocols, log analysis, diaries, and post-questionnaires. Statistical analysis revealed that seven factors were significantly associated with tactic selection. These factors consist of work task types, search task types, familiarity with topic, search skills, search session length, search phases, and system types. Moreover, the study also discovered, qualitatively, in what ways these factors influence the selection of search tactics. Based on the findings, the authors discuss practical implications for system design to support users’ application of multiple search tactics for each factor.
ER  - 

TY  - JOUR
T1  - Discovering role-based virtual knowledge flows for organizational knowledge support
JO  - Decision Support Systems
VL  - 55
IS  - 1
SP  - 12
EP  - 30
PY  - 2013/4//
T2  - 
AU  - Liu, Duen-Ren
AU  - Lin, Chih-Wei
AU  - Chen, Hui-Fang
SN  - 0167-9236
DO  - http://dx.doi.org/10.1016/j.dss.2012.11.018
UR  - http://www.sciencedirect.com/science/article/pii/S0167923613000316
KW  - Knowledge flow
KW  - Knowledge flow view
KW  - Knowledge support
KW  - Knowledge management
KW  - Role
KW  - Ontology
AB  - Abstract
In knowledge-intensive work environments, workers need task-relevant knowledge and documents to support the execution of tasks. A knowledge flow (KF) represents an individual's or group's knowledge-needs and referencing behavior of codified knowledge during the performance of organizational tasks. Through knowledge flows, organizations can provide workers with task-relevant knowledge to satisfy their knowledge-needs. In teamwork environments, knowledge workers with different roles and task functions usually have diverse knowledge-needs, but conventional KF models cannot satisfy such needs. In a previous work, we proposed a novel concept and theoretical model called Knowledge Flow View (KFV). Based on workers' diverse knowledge-needs, the KFV model abstracts knowledge nodes of partial KFs and generates virtual knowledge nodes through a knowledge concept generalization procedure. However, the KFV model did not consider the diverse knowledge-needs of workers who play different roles in a team. Therefore, in this work, we propose a role-based KFV model that discovers role-based virtual knowledge flows to satisfy the knowledge-needs of different roles. First, we analyze the level of knowledge required by workers to fulfill various roles. Then, we develop role-based knowledge flow abstraction methods that generate appropriate virtual knowledge nodes to provide sufficient knowledge for each role. The proposed role-based KFV model enhances the efficiency of KF usage, as well as the effectiveness of knowledge sharing and knowledge support in organizations.
ER  - 

TY  - JOUR
T1  - Evaluation of e-learning systems based on fuzzy clustering models and statistical tools
JO  - Expert Systems with Applications
VL  - 37
IS  - 10
SP  - 6891
EP  - 6903
PY  - 2010/10//
T2  - 
AU  - Hogo, Mofreh A.
SN  - 0957-4174
DO  - http://dx.doi.org/10.1016/j.eswa.2010.03.032
UR  - http://www.sciencedirect.com/science/article/pii/S0957417410002137
KW  - e-Learning
KW  - Learner profile
KW  - Fuzzy C-means clustering
KW  - Kernelized FCM
KW  - Log file analyzer
AB  - This paper introduces a hybridization approach of AI techniques and statistical tools to evaluate and adapt the e-learning systems including e-learners. Learner’s profile plays a crucial role in the evaluation process and the recommendations to improve the e-learning process. This work classifies the learners into specific categories based on the learner’s profiles; the learners’ classes named as regular, workers, casual, bad, and absent. The work extracted the statistical usage patterns that give a clear map describing the data and helping in constructing the e-learning system. The work tries to find the answers of the question how to return the bad students who are away back to be regular ones and find a method to evaluate the e-learners as well as to adapt the content and structure of the e-learning system. The work introduces the application of different fuzzy clustering techniques (FCM and KFCM) to find the learners profiles. Different phases of the work are presented. Analysis of the results and comparison: There is a match with a 78% with the real world behavior and the fuzzy clustering reflects the learners’ behavior perfectly. Comparison between FCM and KFCM proved that the KFCM is much better than FCM.
ER  - 

TY  - JOUR
T1  - An Integrated Model for Patient Care and Clinical Trials (IMPACT) to support clinical research visit scheduling workflow for future learning health systems
JO  - Journal of Biomedical Informatics
VL  - 46
IS  - 4
SP  - 642
EP  - 652
PY  - 2013/8//
T2  - 
AU  - Weng, Chunhua
AU  - Li, Yu
AU  - Berhe, Solomon
AU  - Boland, Mary Regina
AU  - Gao, Junfeng
AU  - Hruby, Gregory W.
AU  - Steinman, Richard C.
AU  - Lopez-Jimenez, Carlos
AU  - Busacca, Linda
AU  - Hripcsak, George
AU  - Bakken, Suzanne
AU  - Bigger, J. Thomas
SN  - 1532-0464
DO  - http://dx.doi.org/10.1016/j.jbi.2013.05.001
UR  - http://www.sciencedirect.com/science/article/pii/S1532046413000531
KW  - Workflow
KW  - Software
KW  - Personnel staffing and scheduling
KW  - Health resources
KW  - Clinical research informatics
KW  - Learning health systems
AB  - Abstract
We describe a clinical research visit scheduling system that can potentially coordinate clinical research visits with patient care visits and increase efficiency at clinical sites where clinical and research activities occur simultaneously. Participatory Design methods were applied to support requirements engineering and to create this software called Integrated Model for Patient Care and Clinical Trials (IMPACT). Using a multi-user constraint satisfaction and resource optimization algorithm, IMPACT automatically synthesizes temporal availability of various research resources and recommends the optimal dates and times for pending research visits. We conducted scenario-based evaluations with 10 clinical research coordinators (CRCs) from diverse clinical research settings to assess the usefulness, feasibility, and user acceptance of IMPACT. We obtained qualitative feedback using semi-structured interviews with the CRCs. Most CRCs acknowledged the usefulness of IMPACT features. Support for collaboration within research teams and interoperability with electronic health records and clinical trial management systems were highly requested features. Overall, IMPACT received satisfactory user acceptance and proves to be potentially useful for a variety of clinical research settings. Our future work includes comparing the effectiveness of IMPACT with that of existing scheduling solutions on the market and conducting field tests to formally assess user adoption.
ER  - 

TY  - JOUR
T1  - A survey on session detection methods in query logs and a proposal for future evaluation
JO  - Information Sciences
VL  - 179
IS  - 12
SP  - 1822
EP  - 1843
PY  - 2009/5/30/
T2  - Special Section: Web Search
AU  - Gayo-Avello, Daniel
SN  - 0020-0255
DO  - http://dx.doi.org/10.1016/j.ins.2009.01.026
UR  - http://www.sciencedirect.com/science/article/pii/S002002550900053X
KW  - Web searching
KW  - Search engine
KW  - Query log
KW  - Topical session
KW  - Session detection
AB  - Search engine logs provide a highly detailed insight of users’ interactions. Hence, they are both extremely useful and sensitive. The datasets publicly available to scholars are, unfortunately, too few, too dated and too small. There are few because search engine companies are reluctant to release such data; they are dated because they were collected in late 1990s or early 2000s; and they are small because they comprise data for at most one day and just a few hundreds of thousands of users. Even worse, the large query log disclosed by AOL in 2006 caused more harm than good because of a big privacy flaw. In this paper the author provides an overall view of the possible applications of query logs, the privacy concerns researchers must face when working on such datasets, and several ways in which query logs can be easily sanitized. One of such measures consists of segmenting the logs into short topical sessions. Therefore, the author offers a comprehensive survey of session detection methods, as well as a thorough description of a new evaluation framework with performance results for each of the different methods. Additionally, a new, simple, but outperforming session detection method is proposed. It is a heuristic-based technique which works on the basis of a geometric interpretation of both the time gap between queries and the similarity between them in order to flag a topic shift.
ER  - 

TY  - JOUR
T1  - Design and architecture of an interactive eTextbook – The OpenDSA system
JO  - Science of Computer Programming
VL  - 88
IS  - 
SP  - 22
EP  - 40
PY  - 2014/8/1/
T2  - Software Development Concerns in the e-Learning Domain
AU  - Fouh, Eric
AU  - Karavirta, Ville
AU  - Breakiron, Daniel A.
AU  - Hamouda, Sally
AU  - Hall, Simin
AU  - Naps, Thomas L.
AU  - Shaffer, Clifford A.
SN  - 0167-6423
DO  - http://dx.doi.org/10.1016/j.scico.2013.11.040
UR  - http://www.sciencedirect.com/science/article/pii/S016764231300333X
KW  - eLearning
KW  - eTextbook
KW  - Automated assessment
KW  - Algorithm visualization
KW  - Data structures and algorithms
AB  - Abstract
The OpenDSA Project seeks to provide complete instructional materials for data structures and algorithms (DSA) courses. Our vision for a highly interactive eTextbook involves the use of many algorithm visualizations (AVs) and a wide range of interactive exercises with automated assessment. To realize this vision we require a mix of third-party and custom software components that make up a client/server-based web application. The massive amount content development required compels us to adopt an appropriate mix of open-source practices that will encourage broad contribution to the project. In this paper we describe the OpenDSA system architecture and the design goals that led to the present version of the system.
ER  - 

TY  - JOUR
T1  - Usability evaluation methods for the web: A systematic mapping study
JO  - Information and Software Technology
VL  - 53
IS  - 8
SP  - 789
EP  - 817
PY  - 2011/8//
T2  - Advances in functional size measurement and effort estimation - Extended best papers
AU  - Fernandez, Adrian
AU  - Insfran, Emilio
AU  - Abrahão, Silvia
SN  - 0950-5849
DO  - http://dx.doi.org/10.1016/j.infsof.2011.02.007
UR  - http://www.sciencedirect.com/science/article/pii/S0950584911000607
KW  - Usability evaluation methods
KW  - Web development
KW  - Systematic mapping
AB  - Context
In recent years, many usability evaluation methods (UEMs) have been employed to evaluate Web applications. However, many of these applications still do not meet most customers’ usability expectations and many companies have folded as a result of not considering Web usability issues. No studies currently exist with regard to either the use of usability evaluation methods for the Web or the benefits they bring.
Objective
The objective of this paper is to summarize the current knowledge that is available as regards the usability evaluation methods (UEMs) that have been employed to evaluate Web applications over the last 14 years.
Method
A systematic mapping study was performed to assess the UEMs that have been used by researchers to evaluate Web applications and their relation to the Web development process. Systematic mapping studies are useful for categorizing and summarizing the existing information concerning a research question in an unbiased manner.
Results
The results show that around 39% of the papers reviewed reported the use of evaluation methods that had been specifically crafted for the Web. The results also show that the type of method most widely used was that of User Testing. The results identify several research gaps, such as the fact that around 90% of the studies applied evaluations during the implementation phase of the Web application development, which is the most costly phase in which to perform changes. A list of the UEMs that were found is also provided in order to guide novice usability practitioners.
Conclusions
From an initial set of 2703 papers, a total of 206 research papers were selected for the mapping study. The results obtained allowed us to reach conclusions concerning the state-of-the-art of UEMs for evaluating Web applications. This allowed us to identify several research gaps, which subsequently provided us with a framework in which new research activities can be more appropriately positioned, and from which useful information for novice usability practitioners can be extracted.
ER  - 

TY  - JOUR
T1  - A novel mobile device user interface with integrated social networking services
JO  - International Journal of Human-Computer Studies
VL  - 71
IS  - 9
SP  - 919
EP  - 932
PY  - 2013/9//
T2  - Social Networks and Ubiquitous Interactions
AU  - Cui, Yanqing
AU  - Honkala, Mikko
SN  - 1071-5819
DO  - http://dx.doi.org/10.1016/j.ijhcs.2013.03.004
UR  - http://www.sciencedirect.com/science/article/pii/S1071581913000396
KW  - Mobile Web
KW  - Social networking services
KW  - Hypertext navigation
KW  - Automatic filtering
KW  - User experience
AB  - Abstract
Modern mobile devices support accessing Web-based social networking services from the user interface (UI) of Web browsers, applications, and mobile widgets. While effectively accessing these services, people may find it tedious to switch between multiple user interfaces in order to be aware of the latest content. Aiming for an improved user experience, we experimented with integration of these services into mobile devices' main user interface. The integrated content is presented beyond application silos and automatically filtered to highlight the relevant elements. A mobile system called LinkedUI was developed and deployed in one lab test and one field study. Three findings emerge from these studies. Firstly, it is feasible to construct an alternative device UI that supports integration of Web content across applications and services via hyperlinking. Time, publisher (e.g., contacts), content types, and geographical locations are key dimensions for association of content. Secondly, the alternative device UI enables better usability of accessing social networking services than accessing them from individual Web sites on mobile devices. It helps people to be aware of the latest content during microbreaks. Thirdly, automatic filtering, on the basis of one user's data, is one promising approach to identifying relevant content. Given filtered content, most people using the automatic filtering approved the functionality and experienced a better sense of control that is arguably due to the reduced information volume.
ER  - 

TY  - JOUR
T1  - Workflow simulation for operational decision support
JO  - Data & Knowledge Engineering
VL  - 68
IS  - 9
SP  - 834
EP  - 850
PY  - 2009/9//
T2  - Sixth International Conference on Business Process Management (BPM 2008) – Five selected and extended papers
AU  - Rozinat, A.
AU  - Wynn, M.T.
AU  - van der Aalst, W.M.P.
AU  - ter Hofstede, A.H.M.
AU  - Fidge, C.J.
SN  - 0169-023X
DO  - http://dx.doi.org/10.1016/j.datak.2009.02.014
UR  - http://www.sciencedirect.com/science/article/pii/S0169023X09000354
KW  - Workflow management
KW  - Process mining
KW  - Short-term simulation
AB  - Simulation is widely used as a tool for analyzing business processes but is mostly focused on examining abstract steady-state situations. Such analyses are helpful for the initial design of a business process but are less suitable for operational decision making and continuous improvement. Here we describe a simulation system for operational decision support in the context of workflow management. To do this we exploit not only the workflow’s design, but also use logged data describing the system’s observed historic behavior, and incorporate information extracted about the current state of the workflow. Making use of actual data capturing the current state and historic information allows our simulations to accurately predict potential near-future behaviors for different scenarios. The approach is supported by a practical toolset which combines and extends the workflow management system YAWL and the process mining framework ProM.
ER  - 

TY  - JOUR
T1  - An analysis of web proxy logs with query distribution pattern approach for search engines
JO  - Computer Standards & Interfaces
VL  - 34
IS  - 1
SP  - 162
EP  - 170
PY  - 2012/1//
T2  - 
AU  - Taghavi, Mona
AU  - Patel, Ahmed
AU  - Schmidt, Nikita
AU  - Wills, Christopher
AU  - Tew, Yiqi
SN  - 0920-5489
DO  - http://dx.doi.org/10.1016/j.csi.2011.07.001
UR  - http://www.sciencedirect.com/science/article/pii/S0920548911000808
KW  - Web search services
KW  - Search engines
KW  - Query analysis
KW  - Distributed search engines
KW  - Proxy server logs
AB  - This study presents an analysis of users' queries directed at different search engines to investigate trends and suggest better search engine capabilities. The query distribution among search engines that includes spawning of queries, number of terms per query and query lengths is discussed to highlight the principal factors affecting a user's choice of search engines and evaluate the reasons of varying the length of queries. The results could be used to develop long to short term business plans for search engine service providers to determine whether or not to opt for more focused topic specific search offerings to gain better market share.
ER  - 

TY  - JOUR
T1  - Lost Opportunities
JO  - Computer Fraud & Security
VL  - 2003
IS  - 1
SP  - 4
EP  - 6
PY  - 2003/1//
T2  - 
AU  - Wilding, Edward
SN  - 1361-3723
DO  - http://dx.doi.org/10.1016/S1361-3723(03)01009-1
UR  - http://www.sciencedirect.com/science/article/pii/S1361372303010091
AB  - A few months ago, a solicitor approached me on behalf of his client. The client suspected that an employee had stolen high value proprietary software, source code, customer databases, methodologies, and marketing and business development plans. The employee had been suspended and placed on what is euphemistically referred to as ‘gardening leave’. He had been ‘gardening’ for six weeks. The solicitor requested advice on how an investigation should proceed in order to prove, or disprove, these suspicions. His client was facing lengthy legal proceedings on the grounds of unfair dismissal. Significantly, neither the solicitor nor his client had any evidence that any information had actually been stolen other than rumours, office gossip and other hearsay.
ER  - 

TY  - JOUR
T1  - A novel data replication mechanism in P2P VoD system
JO  - Future Generation Computer Systems
VL  - 28
IS  - 6
SP  - 930
EP  - 939
PY  - 2012/6//
T2  - Including Special sections SS: Volunteer Computing and Desktop Grids and SS: Mobile Ubiquitous Computing
AU  - Liao, Xiaofei
AU  - Jin, Hai
AU  - Yu, Linchen
SN  - 0167-739X
DO  - http://dx.doi.org/10.1016/j.future.2011.10.006
UR  - http://www.sciencedirect.com/science/article/pii/S0167739X11001919
KW  - Video-on-demand
KW  - Peer-to-peer
KW  - Peer churn
KW  - Data replication
AB  - With the development of the Internet, high-quality streaming services, including Video-on-Demand, are more popular than ever with the help of P2P technologies. But peer-to-peer (P2P) on-demand streaming systems inevitably suffer from peer churn that is the inherent dynamic characteristic of overlay networks. With frequent peer departure and VCR operations, a large amount of media data cached on peer disks turn off-line and unavailable, which becomes the major reason of heavy server load. And the phenomenon has been proved by the system logs of self-developed P2P based Video-on-Demand platform, called GridCast. To address the above issues, a new proactive data replication mechanism is proposed and implemented into GirdCast. Based on the new mechanism, a peer can proactively replicate data chunks to stable cache servers for future sharing, when it has high possibility to leave the overlay. Two key heuristic algorithms are designed for departure prediction and replicating chunks selection. And the cache servers managements are also described in the submission. Trace driven simulations show that the mechanisms greatly decrease bandwidth load of media source server and improve the availability of chunks highly demanded but poorly provisioned by overlay peers.
ER  - 

TY  - JOUR
T1  - Keyword index
JO  - Computers & Geosciences
VL  - 18
IS  - 10
SP  - 1413
EP  - 1500
PY  - 1992/12//
T2  - 

SN  - 0098-3004
DO  - http://dx.doi.org/10.1016/0098-3004(92)90033-N
UR  - http://www.sciencedirect.com/science/article/pii/009830049290033N
ER  - 


